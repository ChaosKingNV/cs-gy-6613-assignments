[{
  "_id": {
    "$oid": "67563c9e31a67c23afbee07b"
  },
  "url": "https://moveit.picknik.ai/main/",
  "title": "MoveIt 2 Documentation — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt 2 Documentation\nHow-To Use This Website\nTable Of Contents\nAttribution\nCorporate Sponsorship\nWelcome to the unified MoveIt documentation, which includes tutorials, how-to guides, core concepts, and more.\nMoveIt 2 is the robotic manipulation platform for ROS 2 and incorporates the latest advances in motion planning, manipulation, 3D perception, kinematics, control, and navigation. MoveIt 2 was first released in 2019; for ROS 1 documentation, seeMoveIt 1 tutorials.\nFor the commercially supported version seeMoveIt Pro tutorials.\nTutorialswill walk you through creating your first project with MoveIt.\nHow-To Guidesanswers the question “How to do X with MoveIt?”\nConceptsdiscusses the design of MoveIt.\nContributingis a place to learn about making changes to MoveIt and this website.\nExamplescontains other useful pages that have not been adapted to the new layout of this site or are not yet ported from ROS 1.\nAPI Documentationwill redirect you to a reference API page.\nSome major past contributors to the MoveIt tutorials are listed in chronological order: Sachin Chitta, Dave Hershberger, Acorn Pooley, Dave Coleman, Michael Goerner, Francisco Suarez, Mike Lautman, Tyler Weaver, David Lu!!, Vatan Tezer, and Andy Zelenak. These are just some of the46+ Contributors over the yearswho have a big impact on this documentation.\nHelp us improve these docs and we’ll be happy to include you here also!\nThe tutorials had a major update in 2018 during a code sprint sponsored by Franka Emika in collaboration withPickNik Robotics(Check out the blog post!)\nThe tutorials had another major update in 2022 during a doc-a-thon sponsored byPickNik Robotics.\n© Copyright 2024, PickNik Robotics.\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee07c"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/tutorials.html",
  "title": "Tutorials — MoveIt Documentation: Rolling  documentation",
  "content": "Tutorials\nThese tutorials will quickly get you using the MoveIt 2 Motion Planning Framework.\nIn these tutorials, theKinova Gen3robot is used as a quick-start demo.\n© Copyright 2024, PickNik Robotics.",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee07d"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/tutorials.html#tutorials",
  "title": "Tutorials — MoveIt Documentation: Rolling  documentation",
  "content": "Tutorials\nThese tutorials will quickly get you using the MoveIt 2 Motion Planning Framework.\nIn these tutorials, theKinova Gen3robot is used as a quick-start demo.\n© Copyright 2024, PickNik Robotics.",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee07e"
  },
  "url": "https://moveit.picknik.ai/main/doc/examples/examples.html",
  "title": "Examples — MoveIt Documentation: Rolling  documentation",
  "content": "Examples\nMoveGroup - ROS Wrappers in C++\nUsing MoveIt Directly Through the C++ API\nUsing MoveIt Directly Through the Python API\nIntegration with a New Robot\nConfiguration\nMiscellaneous\nThis is a catch-all place for pages that have not yet been updated for the new structure or are still not yet ported from ROS 1.\nTo migrate these pages into the new structure please see the appropriate pages underContributing.\nThe simplest way to use MoveIt through scripting is using themove_group_interface. This interface is ideal for beginners and provides unified access to many of the features of MoveIt.\nBuilding more complex applications with MoveIt often requires developers to dig into MoveIt’s C++ API. As an added plus, using the C++ API directly skips many of the ROS Service/Action layers resulting in significantly faster performance.\nThe MoveIt Python API binds a subset of the C++ API. The Python API is useful for rapid prototyping and experimentation, or if you already are working within a Python development environment.\nBefore attempting to integrate a new robot with MoveIt 2, check whether your robot has already been set up (see thelist of robots running MoveIt). Otherwise, follow the tutorials in this section to integrate your robot with MoveIt.\n© Copyright 2024, PickNik Robotics.\nmove_group_interface",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee07f"
  },
  "url": "https://moveit.picknik.ai/main/doc/examples/examples.html#movegroup-ros-wrappers-in-c",
  "title": "Examples — MoveIt Documentation: Rolling  documentation",
  "content": "Examples\nMoveGroup - ROS Wrappers in C++\nUsing MoveIt Directly Through the C++ API\nUsing MoveIt Directly Through the Python API\nIntegration with a New Robot\nConfiguration\nMiscellaneous\nThis is a catch-all place for pages that have not yet been updated for the new structure or are still not yet ported from ROS 1.\nTo migrate these pages into the new structure please see the appropriate pages underContributing.\nThe simplest way to use MoveIt through scripting is using themove_group_interface. This interface is ideal for beginners and provides unified access to many of the features of MoveIt.\nBuilding more complex applications with MoveIt often requires developers to dig into MoveIt’s C++ API. As an added plus, using the C++ API directly skips many of the ROS Service/Action layers resulting in significantly faster performance.\nThe MoveIt Python API binds a subset of the C++ API. The Python API is useful for rapid prototyping and experimentation, or if you already are working within a Python development environment.\nBefore attempting to integrate a new robot with MoveIt 2, check whether your robot has already been set up (see thelist of robots running MoveIt). Otherwise, follow the tutorials in this section to integrate your robot with MoveIt.\n© Copyright 2024, PickNik Robotics.\nmove_group_interface",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee080"
  },
  "url": "https://moveit.picknik.ai/main/doc/examples/examples.html#using-moveit-directly-through-the-c-api",
  "title": "Examples — MoveIt Documentation: Rolling  documentation",
  "content": "Examples\nMoveGroup - ROS Wrappers in C++\nUsing MoveIt Directly Through the C++ API\nUsing MoveIt Directly Through the Python API\nIntegration with a New Robot\nConfiguration\nMiscellaneous\nThis is a catch-all place for pages that have not yet been updated for the new structure or are still not yet ported from ROS 1.\nTo migrate these pages into the new structure please see the appropriate pages underContributing.\nThe simplest way to use MoveIt through scripting is using themove_group_interface. This interface is ideal for beginners and provides unified access to many of the features of MoveIt.\nBuilding more complex applications with MoveIt often requires developers to dig into MoveIt’s C++ API. As an added plus, using the C++ API directly skips many of the ROS Service/Action layers resulting in significantly faster performance.\nThe MoveIt Python API binds a subset of the C++ API. The Python API is useful for rapid prototyping and experimentation, or if you already are working within a Python development environment.\nBefore attempting to integrate a new robot with MoveIt 2, check whether your robot has already been set up (see thelist of robots running MoveIt). Otherwise, follow the tutorials in this section to integrate your robot with MoveIt.\n© Copyright 2024, PickNik Robotics.\nmove_group_interface",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee081"
  },
  "url": "https://moveit.picknik.ai/main/doc/examples/examples.html#using-moveit-directly-through-the-python-api",
  "title": "Examples — MoveIt Documentation: Rolling  documentation",
  "content": "Examples\nMoveGroup - ROS Wrappers in C++\nUsing MoveIt Directly Through the C++ API\nUsing MoveIt Directly Through the Python API\nIntegration with a New Robot\nConfiguration\nMiscellaneous\nThis is a catch-all place for pages that have not yet been updated for the new structure or are still not yet ported from ROS 1.\nTo migrate these pages into the new structure please see the appropriate pages underContributing.\nThe simplest way to use MoveIt through scripting is using themove_group_interface. This interface is ideal for beginners and provides unified access to many of the features of MoveIt.\nBuilding more complex applications with MoveIt often requires developers to dig into MoveIt’s C++ API. As an added plus, using the C++ API directly skips many of the ROS Service/Action layers resulting in significantly faster performance.\nThe MoveIt Python API binds a subset of the C++ API. The Python API is useful for rapid prototyping and experimentation, or if you already are working within a Python development environment.\nBefore attempting to integrate a new robot with MoveIt 2, check whether your robot has already been set up (see thelist of robots running MoveIt). Otherwise, follow the tutorials in this section to integrate your robot with MoveIt.\n© Copyright 2024, PickNik Robotics.\nmove_group_interface",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee082"
  },
  "url": "https://moveit.picknik.ai/main/doc/examples/examples.html#integration-with-a-new-robot",
  "title": "Examples — MoveIt Documentation: Rolling  documentation",
  "content": "Examples\nMoveGroup - ROS Wrappers in C++\nUsing MoveIt Directly Through the C++ API\nUsing MoveIt Directly Through the Python API\nIntegration with a New Robot\nConfiguration\nMiscellaneous\nThis is a catch-all place for pages that have not yet been updated for the new structure or are still not yet ported from ROS 1.\nTo migrate these pages into the new structure please see the appropriate pages underContributing.\nThe simplest way to use MoveIt through scripting is using themove_group_interface. This interface is ideal for beginners and provides unified access to many of the features of MoveIt.\nBuilding more complex applications with MoveIt often requires developers to dig into MoveIt’s C++ API. As an added plus, using the C++ API directly skips many of the ROS Service/Action layers resulting in significantly faster performance.\nThe MoveIt Python API binds a subset of the C++ API. The Python API is useful for rapid prototyping and experimentation, or if you already are working within a Python development environment.\nBefore attempting to integrate a new robot with MoveIt 2, check whether your robot has already been set up (see thelist of robots running MoveIt). Otherwise, follow the tutorials in this section to integrate your robot with MoveIt.\n© Copyright 2024, PickNik Robotics.\nmove_group_interface",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee083"
  },
  "url": "https://moveit.picknik.ai/main/doc/examples/examples.html#configuration",
  "title": "Examples — MoveIt Documentation: Rolling  documentation",
  "content": "Examples\nMoveGroup - ROS Wrappers in C++\nUsing MoveIt Directly Through the C++ API\nUsing MoveIt Directly Through the Python API\nIntegration with a New Robot\nConfiguration\nMiscellaneous\nThis is a catch-all place for pages that have not yet been updated for the new structure or are still not yet ported from ROS 1.\nTo migrate these pages into the new structure please see the appropriate pages underContributing.\nThe simplest way to use MoveIt through scripting is using themove_group_interface. This interface is ideal for beginners and provides unified access to many of the features of MoveIt.\nBuilding more complex applications with MoveIt often requires developers to dig into MoveIt’s C++ API. As an added plus, using the C++ API directly skips many of the ROS Service/Action layers resulting in significantly faster performance.\nThe MoveIt Python API binds a subset of the C++ API. The Python API is useful for rapid prototyping and experimentation, or if you already are working within a Python development environment.\nBefore attempting to integrate a new robot with MoveIt 2, check whether your robot has already been set up (see thelist of robots running MoveIt). Otherwise, follow the tutorials in this section to integrate your robot with MoveIt.\n© Copyright 2024, PickNik Robotics.\nmove_group_interface",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee084"
  },
  "url": "https://moveit.picknik.ai/main/doc/examples/examples.html#miscellaneous",
  "title": "Examples — MoveIt Documentation: Rolling  documentation",
  "content": "Examples\nMoveGroup - ROS Wrappers in C++\nUsing MoveIt Directly Through the C++ API\nUsing MoveIt Directly Through the Python API\nIntegration with a New Robot\nConfiguration\nMiscellaneous\nThis is a catch-all place for pages that have not yet been updated for the new structure or are still not yet ported from ROS 1.\nTo migrate these pages into the new structure please see the appropriate pages underContributing.\nThe simplest way to use MoveIt through scripting is using themove_group_interface. This interface is ideal for beginners and provides unified access to many of the features of MoveIt.\nBuilding more complex applications with MoveIt often requires developers to dig into MoveIt’s C++ API. As an added plus, using the C++ API directly skips many of the ROS Service/Action layers resulting in significantly faster performance.\nThe MoveIt Python API binds a subset of the C++ API. The Python API is useful for rapid prototyping and experimentation, or if you already are working within a Python development environment.\nBefore attempting to integrate a new robot with MoveIt 2, check whether your robot has already been set up (see thelist of robots running MoveIt). Otherwise, follow the tutorials in this section to integrate your robot with MoveIt.\n© Copyright 2024, PickNik Robotics.\nmove_group_interface",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee085"
  },
  "url": "https://moveit.picknik.ai/main/doc/examples/examples.html#examples",
  "title": "Examples — MoveIt Documentation: Rolling  documentation",
  "content": "Examples\nMoveGroup - ROS Wrappers in C++\nUsing MoveIt Directly Through the C++ API\nUsing MoveIt Directly Through the Python API\nIntegration with a New Robot\nConfiguration\nMiscellaneous\nThis is a catch-all place for pages that have not yet been updated for the new structure or are still not yet ported from ROS 1.\nTo migrate these pages into the new structure please see the appropriate pages underContributing.\nThe simplest way to use MoveIt through scripting is using themove_group_interface. This interface is ideal for beginners and provides unified access to many of the features of MoveIt.\nBuilding more complex applications with MoveIt often requires developers to dig into MoveIt’s C++ API. As an added plus, using the C++ API directly skips many of the ROS Service/Action layers resulting in significantly faster performance.\nThe MoveIt Python API binds a subset of the C++ API. The Python API is useful for rapid prototyping and experimentation, or if you already are working within a Python development environment.\nBefore attempting to integrate a new robot with MoveIt 2, check whether your robot has already been set up (see thelist of robots running MoveIt). Otherwise, follow the tutorials in this section to integrate your robot with MoveIt.\n© Copyright 2024, PickNik Robotics.\nmove_group_interface",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee086"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/concepts.html",
  "title": "Concepts — MoveIt Documentation: Rolling  documentation",
  "content": "Concepts\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee087"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/concepts.html#concepts",
  "title": "Concepts — MoveIt Documentation: Rolling  documentation",
  "content": "Concepts\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee088"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_guides/how_to_guides.html",
  "title": "How-To Guides — MoveIt Documentation: Rolling  documentation",
  "content": "How-To Guides\nConfiguring and Using MoveIt\nDeveloping and Documenting MoveIt\nThese how-to guides will help you quickly solve specific problems using MoveIt.\nFor more information, refer toHow-To Guide.\n© Copyright 2024, PickNik Robotics.",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee089"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_guides/how_to_guides.html#configuring-and-using-moveit",
  "title": "How-To Guides — MoveIt Documentation: Rolling  documentation",
  "content": "How-To Guides\nConfiguring and Using MoveIt\nDeveloping and Documenting MoveIt\nThese how-to guides will help you quickly solve specific problems using MoveIt.\nFor more information, refer toHow-To Guide.\n© Copyright 2024, PickNik Robotics.",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee08a"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_guides/how_to_guides.html#developing-and-documenting-moveit",
  "title": "How-To Guides — MoveIt Documentation: Rolling  documentation",
  "content": "How-To Guides\nConfiguring and Using MoveIt\nDeveloping and Documenting MoveIt\nThese how-to guides will help you quickly solve specific problems using MoveIt.\nFor more information, refer toHow-To Guide.\n© Copyright 2024, PickNik Robotics.",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee08b"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_guides/how_to_guides.html#how-to-guides",
  "title": "How-To Guides — MoveIt Documentation: Rolling  documentation",
  "content": "How-To Guides\nConfiguring and Using MoveIt\nDeveloping and Documenting MoveIt\nThese how-to guides will help you quickly solve specific problems using MoveIt.\nFor more information, refer toHow-To Guide.\n© Copyright 2024, PickNik Robotics.",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee08c"
  },
  "url": "https://moveit.picknik.ai/main/doc/api/api.html",
  "title": "API Documentation — MoveIt Documentation: Rolling  documentation",
  "content": "API Documentation\n© Copyright 2024, PickNik Robotics.",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee08d"
  },
  "url": "https://moveit.picknik.ai/main/doc/api/api.html#api-documentation",
  "title": "API Documentation — MoveIt Documentation: Rolling  documentation",
  "content": "API Documentation\n© Copyright 2024, PickNik Robotics.",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee08e"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_contribute.html",
  "title": "Contributing — MoveIt Documentation: Rolling  documentation",
  "content": "Contributing\nThese how-to guides outline steps for contributing to MoveIt and this website.\nIf you are looking for How-To Guides for using MoveIt, seeHow-To Guides.\n© Copyright 2024, PickNik Robotics.",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee08f"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_contribute.html#contributing",
  "title": "Contributing — MoveIt Documentation: Rolling  documentation",
  "content": "Contributing\nThese how-to guides outline steps for contributing to MoveIt and this website.\nIf you are looking for How-To Guides for using MoveIt, seeHow-To Guides.\n© Copyright 2024, PickNik Robotics.",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee090"
  },
  "url": "https://moveit.picknik.ai/main/#moveit-2-documentation",
  "title": "MoveIt 2 Documentation — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt 2 Documentation\nHow-To Use This Website\nTable Of Contents\nAttribution\nCorporate Sponsorship\nWelcome to the unified MoveIt documentation, which includes tutorials, how-to guides, core concepts, and more.\nMoveIt 2 is the robotic manipulation platform for ROS 2 and incorporates the latest advances in motion planning, manipulation, 3D perception, kinematics, control, and navigation. MoveIt 2 was first released in 2019; for ROS 1 documentation, seeMoveIt 1 tutorials.\nFor the commercially supported version seeMoveIt Pro tutorials.\nTutorialswill walk you through creating your first project with MoveIt.\nHow-To Guidesanswers the question “How to do X with MoveIt?”\nConceptsdiscusses the design of MoveIt.\nContributingis a place to learn about making changes to MoveIt and this website.\nExamplescontains other useful pages that have not been adapted to the new layout of this site or are not yet ported from ROS 1.\nAPI Documentationwill redirect you to a reference API page.\nSome major past contributors to the MoveIt tutorials are listed in chronological order: Sachin Chitta, Dave Hershberger, Acorn Pooley, Dave Coleman, Michael Goerner, Francisco Suarez, Mike Lautman, Tyler Weaver, David Lu!!, Vatan Tezer, and Andy Zelenak. These are just some of the46+ Contributors over the yearswho have a big impact on this documentation.\nHelp us improve these docs and we’ll be happy to include you here also!\nThe tutorials had a major update in 2018 during a code sprint sponsored by Franka Emika in collaboration withPickNik Robotics(Check out the blog post!)\nThe tutorials had another major update in 2022 during a doc-a-thon sponsored byPickNik Robotics.\n© Copyright 2024, PickNik Robotics.\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee091"
  },
  "url": "https://moveit.picknik.ai/main/#how-to-use-this-website",
  "title": "MoveIt 2 Documentation — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt 2 Documentation\nHow-To Use This Website\nTable Of Contents\nAttribution\nCorporate Sponsorship\nWelcome to the unified MoveIt documentation, which includes tutorials, how-to guides, core concepts, and more.\nMoveIt 2 is the robotic manipulation platform for ROS 2 and incorporates the latest advances in motion planning, manipulation, 3D perception, kinematics, control, and navigation. MoveIt 2 was first released in 2019; for ROS 1 documentation, seeMoveIt 1 tutorials.\nFor the commercially supported version seeMoveIt Pro tutorials.\nTutorialswill walk you through creating your first project with MoveIt.\nHow-To Guidesanswers the question “How to do X with MoveIt?”\nConceptsdiscusses the design of MoveIt.\nContributingis a place to learn about making changes to MoveIt and this website.\nExamplescontains other useful pages that have not been adapted to the new layout of this site or are not yet ported from ROS 1.\nAPI Documentationwill redirect you to a reference API page.\nSome major past contributors to the MoveIt tutorials are listed in chronological order: Sachin Chitta, Dave Hershberger, Acorn Pooley, Dave Coleman, Michael Goerner, Francisco Suarez, Mike Lautman, Tyler Weaver, David Lu!!, Vatan Tezer, and Andy Zelenak. These are just some of the46+ Contributors over the yearswho have a big impact on this documentation.\nHelp us improve these docs and we’ll be happy to include you here also!\nThe tutorials had a major update in 2018 during a code sprint sponsored by Franka Emika in collaboration withPickNik Robotics(Check out the blog post!)\nThe tutorials had another major update in 2022 during a doc-a-thon sponsored byPickNik Robotics.\n© Copyright 2024, PickNik Robotics.\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee092"
  },
  "url": "https://moveit.picknik.ai/main/#table-of-contents",
  "title": "MoveIt 2 Documentation — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt 2 Documentation\nHow-To Use This Website\nTable Of Contents\nAttribution\nCorporate Sponsorship\nWelcome to the unified MoveIt documentation, which includes tutorials, how-to guides, core concepts, and more.\nMoveIt 2 is the robotic manipulation platform for ROS 2 and incorporates the latest advances in motion planning, manipulation, 3D perception, kinematics, control, and navigation. MoveIt 2 was first released in 2019; for ROS 1 documentation, seeMoveIt 1 tutorials.\nFor the commercially supported version seeMoveIt Pro tutorials.\nTutorialswill walk you through creating your first project with MoveIt.\nHow-To Guidesanswers the question “How to do X with MoveIt?”\nConceptsdiscusses the design of MoveIt.\nContributingis a place to learn about making changes to MoveIt and this website.\nExamplescontains other useful pages that have not been adapted to the new layout of this site or are not yet ported from ROS 1.\nAPI Documentationwill redirect you to a reference API page.\nSome major past contributors to the MoveIt tutorials are listed in chronological order: Sachin Chitta, Dave Hershberger, Acorn Pooley, Dave Coleman, Michael Goerner, Francisco Suarez, Mike Lautman, Tyler Weaver, David Lu!!, Vatan Tezer, and Andy Zelenak. These are just some of the46+ Contributors over the yearswho have a big impact on this documentation.\nHelp us improve these docs and we’ll be happy to include you here also!\nThe tutorials had a major update in 2018 during a code sprint sponsored by Franka Emika in collaboration withPickNik Robotics(Check out the blog post!)\nThe tutorials had another major update in 2022 during a doc-a-thon sponsored byPickNik Robotics.\n© Copyright 2024, PickNik Robotics.\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee093"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/getting_started/getting_started.html",
  "title": "Getting Started — MoveIt Documentation: Rolling  documentation",
  "content": "Getting Started\nInstall ROS 2 and colcon\nCreate A Colcon Workspace and Download Tutorials\nDownload Source Code of MoveIt and the Tutorials\nBuild your Colcon Workspace\nSetup Your Colcon Workspace\nNext Step\nHere, we will setup your environment for best running the tutorials.\nThis will create a colcon workspace, download all of the latest MoveIt source code, and build everything from source to ensure you have the latest fixes and improvements.\nBuilding all the source code of MoveIt can take 20-30 minutes, depending on the CPU speed and available RAM of your computer.\nIf you are on a less performant system, or generally just want to get started quicker, check out ourDocker Guide.\nMoveIt 2 currently supports multiple versions of ROS.\nInstall whichever version you prefer.\nWe primarily support ROS installed on Ubuntu 22.04 or 24.04 but other methods and platforms may work with small changes to the instructions listed below.\nIf you are just getting started, we recommend you use the latest stable version of ROS (Jazzy) on Ubuntu 24.04 for the most seamless experience.\nRolling Ridley- Rolling Development Release\nJazzy Jalisco- Latest LTS Release - May 2024\nHumble Hawksbill- Supported LTS Release - May 2022\nIt is easy to miss steps when going through the ROS 2 installation tutorial.\nIf you run into errors in the next few steps, a good place to start is to go back and make sure you have installed ROS 2 correctly.\nOne that users commonly forget is to source the ROS 2 install itself.\nNote to source the version of ROS you installed.\nNote\nUnlike ROS 1 setup scripts, in ROS 2 the setup scripts do not attempt to switch what version of ROS you are using.  This means that if you have previously sourced a different version of ROS, including from within your.bashrcfile, you will run into errors during the building step.  To fix this change what is sourced in your.bashrcand start a new terminal.\nInstallrosdepto install system dependencies :\nOnce you have ROS 2 installed, make sure you have the most up to date packages:\nInstallColconthe ROS 2 build system withmixin:\nInstallvcstool:\nFor tutorials you will need to have acolconworkspace setup.\nMove into your Colcon workspace and pull the MoveIt tutorials source, where<branch>can be e.g.humblefor ROS Humble, ormainfor the latest version of the tutorials :\nNext we will download the source code for the rest of MoveIt:\nThe import command may ask for your GitHub credentials.\nYou can just press Enter until it moves on (ignore the “Authentication failed” error).\nFirst remove all previously installed moveit binaries:\nThe following will install from Debian any package dependencies not already in your workspace.\nThis is the step that will install MoveIt and all of its dependencies:\nThe next command will configure your Colcon workspace:\nThis build command will likely take a long time (20+ minutes) depending on your computer speed and amount of RAM available (we recommend 32 GB).\nWarning\nSome of the packages built with this command require up to 16Gb of RAM to build. By default,colcontries to build as many packages as possible at the same time.\nIf you are low on computer memory, or if the build is generally having trouble completing on your computer,\nyou can try appending--executorsequentialto thecolconcommand above to build only one package at a time, or--parallel-workers<X>to limit the number of simultaneous builds. For even more limited machines, you can try runningMAKEFLAGS=\"-j4-l1\"colconbuild--executorsequential.\nIf everything goes well, you should see the message “Summary: X packages finished” where X might be 50. If you have problems, try re-checking yourROS Installation.\nSource the Colcon workspace:\nOptional: add the previous command to your.bashrc:\nNote\nSourcing thesetup.bashautomatically in your~/.bashrcis\nnot required and often skipped by advanced users who use more than one\nColcon workspace at a time, but we recommend it for simplicity.\nNice job!\nNext, we willVisualize a robot with the interactive motion planning plugin for RViz\n© Copyright 2024, PickNik Robotics.\nsource/opt/ros/jazzy/setup.bash\nsudoaptinstallpython3-rosdep\nsudorosdepinitrosdepupdatesudoaptupdatesudoaptdist-upgrade\nsudoaptinstallpython3-colcon-common-extensionssudoaptinstallpython3-colcon-mixincolconmixinadddefaulthttps://raw.githubusercontent.com/colcon/colcon-mixin-repository/master/index.yamlcolconmixinupdatedefault\nsudoaptinstallpython3-vcstool\nmkdir-p~/ws_moveit/src\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit2_tutorials\nvcsimport--recursive<moveit2_tutorials/moveit2_tutorials.repos\nsudo apt remove ros-$ROS_DISTRO-moveit*\nsudo apt update && rosdep install -r --from-paths . --ignore-src --rosdistro $ROS_DISTRO -y\ncd~/ws_moveitcolconbuild--mixinrelease\nsource~/ws_moveit/install/setup.bash\necho'source ~/ws_moveit/install/setup.bash'>>~/.bashrc\n.bashrc\n.bashrc\n<branch>\nhumble\nmain\ncolcon\n--executorsequential\ncolcon\n--parallel-workers<X>\nMAKEFLAGS=\"-j4-l1\"colconbuild--executorsequential\n.bashrc\nsetup.bash\n~/.bashrc",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee094"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/getting_started/getting_started.html#install-ros-2-and-colcon",
  "title": "Getting Started — MoveIt Documentation: Rolling  documentation",
  "content": "Getting Started\nInstall ROS 2 and colcon\nCreate A Colcon Workspace and Download Tutorials\nDownload Source Code of MoveIt and the Tutorials\nBuild your Colcon Workspace\nSetup Your Colcon Workspace\nNext Step\nHere, we will setup your environment for best running the tutorials.\nThis will create a colcon workspace, download all of the latest MoveIt source code, and build everything from source to ensure you have the latest fixes and improvements.\nBuilding all the source code of MoveIt can take 20-30 minutes, depending on the CPU speed and available RAM of your computer.\nIf you are on a less performant system, or generally just want to get started quicker, check out ourDocker Guide.\nMoveIt 2 currently supports multiple versions of ROS.\nInstall whichever version you prefer.\nWe primarily support ROS installed on Ubuntu 22.04 or 24.04 but other methods and platforms may work with small changes to the instructions listed below.\nIf you are just getting started, we recommend you use the latest stable version of ROS (Jazzy) on Ubuntu 24.04 for the most seamless experience.\nRolling Ridley- Rolling Development Release\nJazzy Jalisco- Latest LTS Release - May 2024\nHumble Hawksbill- Supported LTS Release - May 2022\nIt is easy to miss steps when going through the ROS 2 installation tutorial.\nIf you run into errors in the next few steps, a good place to start is to go back and make sure you have installed ROS 2 correctly.\nOne that users commonly forget is to source the ROS 2 install itself.\nNote to source the version of ROS you installed.\nNote\nUnlike ROS 1 setup scripts, in ROS 2 the setup scripts do not attempt to switch what version of ROS you are using.  This means that if you have previously sourced a different version of ROS, including from within your.bashrcfile, you will run into errors during the building step.  To fix this change what is sourced in your.bashrcand start a new terminal.\nInstallrosdepto install system dependencies :\nOnce you have ROS 2 installed, make sure you have the most up to date packages:\nInstallColconthe ROS 2 build system withmixin:\nInstallvcstool:\nFor tutorials you will need to have acolconworkspace setup.\nMove into your Colcon workspace and pull the MoveIt tutorials source, where<branch>can be e.g.humblefor ROS Humble, ormainfor the latest version of the tutorials :\nNext we will download the source code for the rest of MoveIt:\nThe import command may ask for your GitHub credentials.\nYou can just press Enter until it moves on (ignore the “Authentication failed” error).\nFirst remove all previously installed moveit binaries:\nThe following will install from Debian any package dependencies not already in your workspace.\nThis is the step that will install MoveIt and all of its dependencies:\nThe next command will configure your Colcon workspace:\nThis build command will likely take a long time (20+ minutes) depending on your computer speed and amount of RAM available (we recommend 32 GB).\nWarning\nSome of the packages built with this command require up to 16Gb of RAM to build. By default,colcontries to build as many packages as possible at the same time.\nIf you are low on computer memory, or if the build is generally having trouble completing on your computer,\nyou can try appending--executorsequentialto thecolconcommand above to build only one package at a time, or--parallel-workers<X>to limit the number of simultaneous builds. For even more limited machines, you can try runningMAKEFLAGS=\"-j4-l1\"colconbuild--executorsequential.\nIf everything goes well, you should see the message “Summary: X packages finished” where X might be 50. If you have problems, try re-checking yourROS Installation.\nSource the Colcon workspace:\nOptional: add the previous command to your.bashrc:\nNote\nSourcing thesetup.bashautomatically in your~/.bashrcis\nnot required and often skipped by advanced users who use more than one\nColcon workspace at a time, but we recommend it for simplicity.\nNice job!\nNext, we willVisualize a robot with the interactive motion planning plugin for RViz\n© Copyright 2024, PickNik Robotics.\nsource/opt/ros/jazzy/setup.bash\nsudoaptinstallpython3-rosdep\nsudorosdepinitrosdepupdatesudoaptupdatesudoaptdist-upgrade\nsudoaptinstallpython3-colcon-common-extensionssudoaptinstallpython3-colcon-mixincolconmixinadddefaulthttps://raw.githubusercontent.com/colcon/colcon-mixin-repository/master/index.yamlcolconmixinupdatedefault\nsudoaptinstallpython3-vcstool\nmkdir-p~/ws_moveit/src\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit2_tutorials\nvcsimport--recursive<moveit2_tutorials/moveit2_tutorials.repos\nsudo apt remove ros-$ROS_DISTRO-moveit*\nsudo apt update && rosdep install -r --from-paths . --ignore-src --rosdistro $ROS_DISTRO -y\ncd~/ws_moveitcolconbuild--mixinrelease\nsource~/ws_moveit/install/setup.bash\necho'source ~/ws_moveit/install/setup.bash'>>~/.bashrc\n.bashrc\n.bashrc\n<branch>\nhumble\nmain\ncolcon\n--executorsequential\ncolcon\n--parallel-workers<X>\nMAKEFLAGS=\"-j4-l1\"colconbuild--executorsequential\n.bashrc\nsetup.bash\n~/.bashrc",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee095"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/getting_started/getting_started.html#create-a-colcon-workspace-and-download-tutorials",
  "title": "Getting Started — MoveIt Documentation: Rolling  documentation",
  "content": "Getting Started\nInstall ROS 2 and colcon\nCreate A Colcon Workspace and Download Tutorials\nDownload Source Code of MoveIt and the Tutorials\nBuild your Colcon Workspace\nSetup Your Colcon Workspace\nNext Step\nHere, we will setup your environment for best running the tutorials.\nThis will create a colcon workspace, download all of the latest MoveIt source code, and build everything from source to ensure you have the latest fixes and improvements.\nBuilding all the source code of MoveIt can take 20-30 minutes, depending on the CPU speed and available RAM of your computer.\nIf you are on a less performant system, or generally just want to get started quicker, check out ourDocker Guide.\nMoveIt 2 currently supports multiple versions of ROS.\nInstall whichever version you prefer.\nWe primarily support ROS installed on Ubuntu 22.04 or 24.04 but other methods and platforms may work with small changes to the instructions listed below.\nIf you are just getting started, we recommend you use the latest stable version of ROS (Jazzy) on Ubuntu 24.04 for the most seamless experience.\nRolling Ridley- Rolling Development Release\nJazzy Jalisco- Latest LTS Release - May 2024\nHumble Hawksbill- Supported LTS Release - May 2022\nIt is easy to miss steps when going through the ROS 2 installation tutorial.\nIf you run into errors in the next few steps, a good place to start is to go back and make sure you have installed ROS 2 correctly.\nOne that users commonly forget is to source the ROS 2 install itself.\nNote to source the version of ROS you installed.\nNote\nUnlike ROS 1 setup scripts, in ROS 2 the setup scripts do not attempt to switch what version of ROS you are using.  This means that if you have previously sourced a different version of ROS, including from within your.bashrcfile, you will run into errors during the building step.  To fix this change what is sourced in your.bashrcand start a new terminal.\nInstallrosdepto install system dependencies :\nOnce you have ROS 2 installed, make sure you have the most up to date packages:\nInstallColconthe ROS 2 build system withmixin:\nInstallvcstool:\nFor tutorials you will need to have acolconworkspace setup.\nMove into your Colcon workspace and pull the MoveIt tutorials source, where<branch>can be e.g.humblefor ROS Humble, ormainfor the latest version of the tutorials :\nNext we will download the source code for the rest of MoveIt:\nThe import command may ask for your GitHub credentials.\nYou can just press Enter until it moves on (ignore the “Authentication failed” error).\nFirst remove all previously installed moveit binaries:\nThe following will install from Debian any package dependencies not already in your workspace.\nThis is the step that will install MoveIt and all of its dependencies:\nThe next command will configure your Colcon workspace:\nThis build command will likely take a long time (20+ minutes) depending on your computer speed and amount of RAM available (we recommend 32 GB).\nWarning\nSome of the packages built with this command require up to 16Gb of RAM to build. By default,colcontries to build as many packages as possible at the same time.\nIf you are low on computer memory, or if the build is generally having trouble completing on your computer,\nyou can try appending--executorsequentialto thecolconcommand above to build only one package at a time, or--parallel-workers<X>to limit the number of simultaneous builds. For even more limited machines, you can try runningMAKEFLAGS=\"-j4-l1\"colconbuild--executorsequential.\nIf everything goes well, you should see the message “Summary: X packages finished” where X might be 50. If you have problems, try re-checking yourROS Installation.\nSource the Colcon workspace:\nOptional: add the previous command to your.bashrc:\nNote\nSourcing thesetup.bashautomatically in your~/.bashrcis\nnot required and often skipped by advanced users who use more than one\nColcon workspace at a time, but we recommend it for simplicity.\nNice job!\nNext, we willVisualize a robot with the interactive motion planning plugin for RViz\n© Copyright 2024, PickNik Robotics.\nsource/opt/ros/jazzy/setup.bash\nsudoaptinstallpython3-rosdep\nsudorosdepinitrosdepupdatesudoaptupdatesudoaptdist-upgrade\nsudoaptinstallpython3-colcon-common-extensionssudoaptinstallpython3-colcon-mixincolconmixinadddefaulthttps://raw.githubusercontent.com/colcon/colcon-mixin-repository/master/index.yamlcolconmixinupdatedefault\nsudoaptinstallpython3-vcstool\nmkdir-p~/ws_moveit/src\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit2_tutorials\nvcsimport--recursive<moveit2_tutorials/moveit2_tutorials.repos\nsudo apt remove ros-$ROS_DISTRO-moveit*\nsudo apt update && rosdep install -r --from-paths . --ignore-src --rosdistro $ROS_DISTRO -y\ncd~/ws_moveitcolconbuild--mixinrelease\nsource~/ws_moveit/install/setup.bash\necho'source ~/ws_moveit/install/setup.bash'>>~/.bashrc\n.bashrc\n.bashrc\n<branch>\nhumble\nmain\ncolcon\n--executorsequential\ncolcon\n--parallel-workers<X>\nMAKEFLAGS=\"-j4-l1\"colconbuild--executorsequential\n.bashrc\nsetup.bash\n~/.bashrc",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee096"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/getting_started/getting_started.html#download-source-code-of-moveit-and-the-tutorials",
  "title": "Getting Started — MoveIt Documentation: Rolling  documentation",
  "content": "Getting Started\nInstall ROS 2 and colcon\nCreate A Colcon Workspace and Download Tutorials\nDownload Source Code of MoveIt and the Tutorials\nBuild your Colcon Workspace\nSetup Your Colcon Workspace\nNext Step\nHere, we will setup your environment for best running the tutorials.\nThis will create a colcon workspace, download all of the latest MoveIt source code, and build everything from source to ensure you have the latest fixes and improvements.\nBuilding all the source code of MoveIt can take 20-30 minutes, depending on the CPU speed and available RAM of your computer.\nIf you are on a less performant system, or generally just want to get started quicker, check out ourDocker Guide.\nMoveIt 2 currently supports multiple versions of ROS.\nInstall whichever version you prefer.\nWe primarily support ROS installed on Ubuntu 22.04 or 24.04 but other methods and platforms may work with small changes to the instructions listed below.\nIf you are just getting started, we recommend you use the latest stable version of ROS (Jazzy) on Ubuntu 24.04 for the most seamless experience.\nRolling Ridley- Rolling Development Release\nJazzy Jalisco- Latest LTS Release - May 2024\nHumble Hawksbill- Supported LTS Release - May 2022\nIt is easy to miss steps when going through the ROS 2 installation tutorial.\nIf you run into errors in the next few steps, a good place to start is to go back and make sure you have installed ROS 2 correctly.\nOne that users commonly forget is to source the ROS 2 install itself.\nNote to source the version of ROS you installed.\nNote\nUnlike ROS 1 setup scripts, in ROS 2 the setup scripts do not attempt to switch what version of ROS you are using.  This means that if you have previously sourced a different version of ROS, including from within your.bashrcfile, you will run into errors during the building step.  To fix this change what is sourced in your.bashrcand start a new terminal.\nInstallrosdepto install system dependencies :\nOnce you have ROS 2 installed, make sure you have the most up to date packages:\nInstallColconthe ROS 2 build system withmixin:\nInstallvcstool:\nFor tutorials you will need to have acolconworkspace setup.\nMove into your Colcon workspace and pull the MoveIt tutorials source, where<branch>can be e.g.humblefor ROS Humble, ormainfor the latest version of the tutorials :\nNext we will download the source code for the rest of MoveIt:\nThe import command may ask for your GitHub credentials.\nYou can just press Enter until it moves on (ignore the “Authentication failed” error).\nFirst remove all previously installed moveit binaries:\nThe following will install from Debian any package dependencies not already in your workspace.\nThis is the step that will install MoveIt and all of its dependencies:\nThe next command will configure your Colcon workspace:\nThis build command will likely take a long time (20+ minutes) depending on your computer speed and amount of RAM available (we recommend 32 GB).\nWarning\nSome of the packages built with this command require up to 16Gb of RAM to build. By default,colcontries to build as many packages as possible at the same time.\nIf you are low on computer memory, or if the build is generally having trouble completing on your computer,\nyou can try appending--executorsequentialto thecolconcommand above to build only one package at a time, or--parallel-workers<X>to limit the number of simultaneous builds. For even more limited machines, you can try runningMAKEFLAGS=\"-j4-l1\"colconbuild--executorsequential.\nIf everything goes well, you should see the message “Summary: X packages finished” where X might be 50. If you have problems, try re-checking yourROS Installation.\nSource the Colcon workspace:\nOptional: add the previous command to your.bashrc:\nNote\nSourcing thesetup.bashautomatically in your~/.bashrcis\nnot required and often skipped by advanced users who use more than one\nColcon workspace at a time, but we recommend it for simplicity.\nNice job!\nNext, we willVisualize a robot with the interactive motion planning plugin for RViz\n© Copyright 2024, PickNik Robotics.\nsource/opt/ros/jazzy/setup.bash\nsudoaptinstallpython3-rosdep\nsudorosdepinitrosdepupdatesudoaptupdatesudoaptdist-upgrade\nsudoaptinstallpython3-colcon-common-extensionssudoaptinstallpython3-colcon-mixincolconmixinadddefaulthttps://raw.githubusercontent.com/colcon/colcon-mixin-repository/master/index.yamlcolconmixinupdatedefault\nsudoaptinstallpython3-vcstool\nmkdir-p~/ws_moveit/src\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit2_tutorials\nvcsimport--recursive<moveit2_tutorials/moveit2_tutorials.repos\nsudo apt remove ros-$ROS_DISTRO-moveit*\nsudo apt update && rosdep install -r --from-paths . --ignore-src --rosdistro $ROS_DISTRO -y\ncd~/ws_moveitcolconbuild--mixinrelease\nsource~/ws_moveit/install/setup.bash\necho'source ~/ws_moveit/install/setup.bash'>>~/.bashrc\n.bashrc\n.bashrc\n<branch>\nhumble\nmain\ncolcon\n--executorsequential\ncolcon\n--parallel-workers<X>\nMAKEFLAGS=\"-j4-l1\"colconbuild--executorsequential\n.bashrc\nsetup.bash\n~/.bashrc",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee097"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/getting_started/getting_started.html#build-your-colcon-workspace",
  "title": "Getting Started — MoveIt Documentation: Rolling  documentation",
  "content": "Getting Started\nInstall ROS 2 and colcon\nCreate A Colcon Workspace and Download Tutorials\nDownload Source Code of MoveIt and the Tutorials\nBuild your Colcon Workspace\nSetup Your Colcon Workspace\nNext Step\nHere, we will setup your environment for best running the tutorials.\nThis will create a colcon workspace, download all of the latest MoveIt source code, and build everything from source to ensure you have the latest fixes and improvements.\nBuilding all the source code of MoveIt can take 20-30 minutes, depending on the CPU speed and available RAM of your computer.\nIf you are on a less performant system, or generally just want to get started quicker, check out ourDocker Guide.\nMoveIt 2 currently supports multiple versions of ROS.\nInstall whichever version you prefer.\nWe primarily support ROS installed on Ubuntu 22.04 or 24.04 but other methods and platforms may work with small changes to the instructions listed below.\nIf you are just getting started, we recommend you use the latest stable version of ROS (Jazzy) on Ubuntu 24.04 for the most seamless experience.\nRolling Ridley- Rolling Development Release\nJazzy Jalisco- Latest LTS Release - May 2024\nHumble Hawksbill- Supported LTS Release - May 2022\nIt is easy to miss steps when going through the ROS 2 installation tutorial.\nIf you run into errors in the next few steps, a good place to start is to go back and make sure you have installed ROS 2 correctly.\nOne that users commonly forget is to source the ROS 2 install itself.\nNote to source the version of ROS you installed.\nNote\nUnlike ROS 1 setup scripts, in ROS 2 the setup scripts do not attempt to switch what version of ROS you are using.  This means that if you have previously sourced a different version of ROS, including from within your.bashrcfile, you will run into errors during the building step.  To fix this change what is sourced in your.bashrcand start a new terminal.\nInstallrosdepto install system dependencies :\nOnce you have ROS 2 installed, make sure you have the most up to date packages:\nInstallColconthe ROS 2 build system withmixin:\nInstallvcstool:\nFor tutorials you will need to have acolconworkspace setup.\nMove into your Colcon workspace and pull the MoveIt tutorials source, where<branch>can be e.g.humblefor ROS Humble, ormainfor the latest version of the tutorials :\nNext we will download the source code for the rest of MoveIt:\nThe import command may ask for your GitHub credentials.\nYou can just press Enter until it moves on (ignore the “Authentication failed” error).\nFirst remove all previously installed moveit binaries:\nThe following will install from Debian any package dependencies not already in your workspace.\nThis is the step that will install MoveIt and all of its dependencies:\nThe next command will configure your Colcon workspace:\nThis build command will likely take a long time (20+ minutes) depending on your computer speed and amount of RAM available (we recommend 32 GB).\nWarning\nSome of the packages built with this command require up to 16Gb of RAM to build. By default,colcontries to build as many packages as possible at the same time.\nIf you are low on computer memory, or if the build is generally having trouble completing on your computer,\nyou can try appending--executorsequentialto thecolconcommand above to build only one package at a time, or--parallel-workers<X>to limit the number of simultaneous builds. For even more limited machines, you can try runningMAKEFLAGS=\"-j4-l1\"colconbuild--executorsequential.\nIf everything goes well, you should see the message “Summary: X packages finished” where X might be 50. If you have problems, try re-checking yourROS Installation.\nSource the Colcon workspace:\nOptional: add the previous command to your.bashrc:\nNote\nSourcing thesetup.bashautomatically in your~/.bashrcis\nnot required and often skipped by advanced users who use more than one\nColcon workspace at a time, but we recommend it for simplicity.\nNice job!\nNext, we willVisualize a robot with the interactive motion planning plugin for RViz\n© Copyright 2024, PickNik Robotics.\nsource/opt/ros/jazzy/setup.bash\nsudoaptinstallpython3-rosdep\nsudorosdepinitrosdepupdatesudoaptupdatesudoaptdist-upgrade\nsudoaptinstallpython3-colcon-common-extensionssudoaptinstallpython3-colcon-mixincolconmixinadddefaulthttps://raw.githubusercontent.com/colcon/colcon-mixin-repository/master/index.yamlcolconmixinupdatedefault\nsudoaptinstallpython3-vcstool\nmkdir-p~/ws_moveit/src\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit2_tutorials\nvcsimport--recursive<moveit2_tutorials/moveit2_tutorials.repos\nsudo apt remove ros-$ROS_DISTRO-moveit*\nsudo apt update && rosdep install -r --from-paths . --ignore-src --rosdistro $ROS_DISTRO -y\ncd~/ws_moveitcolconbuild--mixinrelease\nsource~/ws_moveit/install/setup.bash\necho'source ~/ws_moveit/install/setup.bash'>>~/.bashrc\n.bashrc\n.bashrc\n<branch>\nhumble\nmain\ncolcon\n--executorsequential\ncolcon\n--parallel-workers<X>\nMAKEFLAGS=\"-j4-l1\"colconbuild--executorsequential\n.bashrc\nsetup.bash\n~/.bashrc",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee098"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/getting_started/getting_started.html#setup-your-colcon-workspace",
  "title": "Getting Started — MoveIt Documentation: Rolling  documentation",
  "content": "Getting Started\nInstall ROS 2 and colcon\nCreate A Colcon Workspace and Download Tutorials\nDownload Source Code of MoveIt and the Tutorials\nBuild your Colcon Workspace\nSetup Your Colcon Workspace\nNext Step\nHere, we will setup your environment for best running the tutorials.\nThis will create a colcon workspace, download all of the latest MoveIt source code, and build everything from source to ensure you have the latest fixes and improvements.\nBuilding all the source code of MoveIt can take 20-30 minutes, depending on the CPU speed and available RAM of your computer.\nIf you are on a less performant system, or generally just want to get started quicker, check out ourDocker Guide.\nMoveIt 2 currently supports multiple versions of ROS.\nInstall whichever version you prefer.\nWe primarily support ROS installed on Ubuntu 22.04 or 24.04 but other methods and platforms may work with small changes to the instructions listed below.\nIf you are just getting started, we recommend you use the latest stable version of ROS (Jazzy) on Ubuntu 24.04 for the most seamless experience.\nRolling Ridley- Rolling Development Release\nJazzy Jalisco- Latest LTS Release - May 2024\nHumble Hawksbill- Supported LTS Release - May 2022\nIt is easy to miss steps when going through the ROS 2 installation tutorial.\nIf you run into errors in the next few steps, a good place to start is to go back and make sure you have installed ROS 2 correctly.\nOne that users commonly forget is to source the ROS 2 install itself.\nNote to source the version of ROS you installed.\nNote\nUnlike ROS 1 setup scripts, in ROS 2 the setup scripts do not attempt to switch what version of ROS you are using.  This means that if you have previously sourced a different version of ROS, including from within your.bashrcfile, you will run into errors during the building step.  To fix this change what is sourced in your.bashrcand start a new terminal.\nInstallrosdepto install system dependencies :\nOnce you have ROS 2 installed, make sure you have the most up to date packages:\nInstallColconthe ROS 2 build system withmixin:\nInstallvcstool:\nFor tutorials you will need to have acolconworkspace setup.\nMove into your Colcon workspace and pull the MoveIt tutorials source, where<branch>can be e.g.humblefor ROS Humble, ormainfor the latest version of the tutorials :\nNext we will download the source code for the rest of MoveIt:\nThe import command may ask for your GitHub credentials.\nYou can just press Enter until it moves on (ignore the “Authentication failed” error).\nFirst remove all previously installed moveit binaries:\nThe following will install from Debian any package dependencies not already in your workspace.\nThis is the step that will install MoveIt and all of its dependencies:\nThe next command will configure your Colcon workspace:\nThis build command will likely take a long time (20+ minutes) depending on your computer speed and amount of RAM available (we recommend 32 GB).\nWarning\nSome of the packages built with this command require up to 16Gb of RAM to build. By default,colcontries to build as many packages as possible at the same time.\nIf you are low on computer memory, or if the build is generally having trouble completing on your computer,\nyou can try appending--executorsequentialto thecolconcommand above to build only one package at a time, or--parallel-workers<X>to limit the number of simultaneous builds. For even more limited machines, you can try runningMAKEFLAGS=\"-j4-l1\"colconbuild--executorsequential.\nIf everything goes well, you should see the message “Summary: X packages finished” where X might be 50. If you have problems, try re-checking yourROS Installation.\nSource the Colcon workspace:\nOptional: add the previous command to your.bashrc:\nNote\nSourcing thesetup.bashautomatically in your~/.bashrcis\nnot required and often skipped by advanced users who use more than one\nColcon workspace at a time, but we recommend it for simplicity.\nNice job!\nNext, we willVisualize a robot with the interactive motion planning plugin for RViz\n© Copyright 2024, PickNik Robotics.\nsource/opt/ros/jazzy/setup.bash\nsudoaptinstallpython3-rosdep\nsudorosdepinitrosdepupdatesudoaptupdatesudoaptdist-upgrade\nsudoaptinstallpython3-colcon-common-extensionssudoaptinstallpython3-colcon-mixincolconmixinadddefaulthttps://raw.githubusercontent.com/colcon/colcon-mixin-repository/master/index.yamlcolconmixinupdatedefault\nsudoaptinstallpython3-vcstool\nmkdir-p~/ws_moveit/src\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit2_tutorials\nvcsimport--recursive<moveit2_tutorials/moveit2_tutorials.repos\nsudo apt remove ros-$ROS_DISTRO-moveit*\nsudo apt update && rosdep install -r --from-paths . --ignore-src --rosdistro $ROS_DISTRO -y\ncd~/ws_moveitcolconbuild--mixinrelease\nsource~/ws_moveit/install/setup.bash\necho'source ~/ws_moveit/install/setup.bash'>>~/.bashrc\n.bashrc\n.bashrc\n<branch>\nhumble\nmain\ncolcon\n--executorsequential\ncolcon\n--parallel-workers<X>\nMAKEFLAGS=\"-j4-l1\"colconbuild--executorsequential\n.bashrc\nsetup.bash\n~/.bashrc",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee099"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/getting_started/getting_started.html#next-step",
  "title": "Getting Started — MoveIt Documentation: Rolling  documentation",
  "content": "Getting Started\nInstall ROS 2 and colcon\nCreate A Colcon Workspace and Download Tutorials\nDownload Source Code of MoveIt and the Tutorials\nBuild your Colcon Workspace\nSetup Your Colcon Workspace\nNext Step\nHere, we will setup your environment for best running the tutorials.\nThis will create a colcon workspace, download all of the latest MoveIt source code, and build everything from source to ensure you have the latest fixes and improvements.\nBuilding all the source code of MoveIt can take 20-30 minutes, depending on the CPU speed and available RAM of your computer.\nIf you are on a less performant system, or generally just want to get started quicker, check out ourDocker Guide.\nMoveIt 2 currently supports multiple versions of ROS.\nInstall whichever version you prefer.\nWe primarily support ROS installed on Ubuntu 22.04 or 24.04 but other methods and platforms may work with small changes to the instructions listed below.\nIf you are just getting started, we recommend you use the latest stable version of ROS (Jazzy) on Ubuntu 24.04 for the most seamless experience.\nRolling Ridley- Rolling Development Release\nJazzy Jalisco- Latest LTS Release - May 2024\nHumble Hawksbill- Supported LTS Release - May 2022\nIt is easy to miss steps when going through the ROS 2 installation tutorial.\nIf you run into errors in the next few steps, a good place to start is to go back and make sure you have installed ROS 2 correctly.\nOne that users commonly forget is to source the ROS 2 install itself.\nNote to source the version of ROS you installed.\nNote\nUnlike ROS 1 setup scripts, in ROS 2 the setup scripts do not attempt to switch what version of ROS you are using.  This means that if you have previously sourced a different version of ROS, including from within your.bashrcfile, you will run into errors during the building step.  To fix this change what is sourced in your.bashrcand start a new terminal.\nInstallrosdepto install system dependencies :\nOnce you have ROS 2 installed, make sure you have the most up to date packages:\nInstallColconthe ROS 2 build system withmixin:\nInstallvcstool:\nFor tutorials you will need to have acolconworkspace setup.\nMove into your Colcon workspace and pull the MoveIt tutorials source, where<branch>can be e.g.humblefor ROS Humble, ormainfor the latest version of the tutorials :\nNext we will download the source code for the rest of MoveIt:\nThe import command may ask for your GitHub credentials.\nYou can just press Enter until it moves on (ignore the “Authentication failed” error).\nFirst remove all previously installed moveit binaries:\nThe following will install from Debian any package dependencies not already in your workspace.\nThis is the step that will install MoveIt and all of its dependencies:\nThe next command will configure your Colcon workspace:\nThis build command will likely take a long time (20+ minutes) depending on your computer speed and amount of RAM available (we recommend 32 GB).\nWarning\nSome of the packages built with this command require up to 16Gb of RAM to build. By default,colcontries to build as many packages as possible at the same time.\nIf you are low on computer memory, or if the build is generally having trouble completing on your computer,\nyou can try appending--executorsequentialto thecolconcommand above to build only one package at a time, or--parallel-workers<X>to limit the number of simultaneous builds. For even more limited machines, you can try runningMAKEFLAGS=\"-j4-l1\"colconbuild--executorsequential.\nIf everything goes well, you should see the message “Summary: X packages finished” where X might be 50. If you have problems, try re-checking yourROS Installation.\nSource the Colcon workspace:\nOptional: add the previous command to your.bashrc:\nNote\nSourcing thesetup.bashautomatically in your~/.bashrcis\nnot required and often skipped by advanced users who use more than one\nColcon workspace at a time, but we recommend it for simplicity.\nNice job!\nNext, we willVisualize a robot with the interactive motion planning plugin for RViz\n© Copyright 2024, PickNik Robotics.\nsource/opt/ros/jazzy/setup.bash\nsudoaptinstallpython3-rosdep\nsudorosdepinitrosdepupdatesudoaptupdatesudoaptdist-upgrade\nsudoaptinstallpython3-colcon-common-extensionssudoaptinstallpython3-colcon-mixincolconmixinadddefaulthttps://raw.githubusercontent.com/colcon/colcon-mixin-repository/master/index.yamlcolconmixinupdatedefault\nsudoaptinstallpython3-vcstool\nmkdir-p~/ws_moveit/src\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit2_tutorials\nvcsimport--recursive<moveit2_tutorials/moveit2_tutorials.repos\nsudo apt remove ros-$ROS_DISTRO-moveit*\nsudo apt update && rosdep install -r --from-paths . --ignore-src --rosdistro $ROS_DISTRO -y\ncd~/ws_moveitcolconbuild--mixinrelease\nsource~/ws_moveit/install/setup.bash\necho'source ~/ws_moveit/install/setup.bash'>>~/.bashrc\n.bashrc\n.bashrc\n<branch>\nhumble\nmain\ncolcon\n--executorsequential\ncolcon\n--parallel-workers<X>\nMAKEFLAGS=\"-j4-l1\"colconbuild--executorsequential\n.bashrc\nsetup.bash\n~/.bashrc",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee09a"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/getting_started/getting_started.html#getting-started",
  "title": "Getting Started — MoveIt Documentation: Rolling  documentation",
  "content": "Getting Started\nInstall ROS 2 and colcon\nCreate A Colcon Workspace and Download Tutorials\nDownload Source Code of MoveIt and the Tutorials\nBuild your Colcon Workspace\nSetup Your Colcon Workspace\nNext Step\nHere, we will setup your environment for best running the tutorials.\nThis will create a colcon workspace, download all of the latest MoveIt source code, and build everything from source to ensure you have the latest fixes and improvements.\nBuilding all the source code of MoveIt can take 20-30 minutes, depending on the CPU speed and available RAM of your computer.\nIf you are on a less performant system, or generally just want to get started quicker, check out ourDocker Guide.\nMoveIt 2 currently supports multiple versions of ROS.\nInstall whichever version you prefer.\nWe primarily support ROS installed on Ubuntu 22.04 or 24.04 but other methods and platforms may work with small changes to the instructions listed below.\nIf you are just getting started, we recommend you use the latest stable version of ROS (Jazzy) on Ubuntu 24.04 for the most seamless experience.\nRolling Ridley- Rolling Development Release\nJazzy Jalisco- Latest LTS Release - May 2024\nHumble Hawksbill- Supported LTS Release - May 2022\nIt is easy to miss steps when going through the ROS 2 installation tutorial.\nIf you run into errors in the next few steps, a good place to start is to go back and make sure you have installed ROS 2 correctly.\nOne that users commonly forget is to source the ROS 2 install itself.\nNote to source the version of ROS you installed.\nNote\nUnlike ROS 1 setup scripts, in ROS 2 the setup scripts do not attempt to switch what version of ROS you are using.  This means that if you have previously sourced a different version of ROS, including from within your.bashrcfile, you will run into errors during the building step.  To fix this change what is sourced in your.bashrcand start a new terminal.\nInstallrosdepto install system dependencies :\nOnce you have ROS 2 installed, make sure you have the most up to date packages:\nInstallColconthe ROS 2 build system withmixin:\nInstallvcstool:\nFor tutorials you will need to have acolconworkspace setup.\nMove into your Colcon workspace and pull the MoveIt tutorials source, where<branch>can be e.g.humblefor ROS Humble, ormainfor the latest version of the tutorials :\nNext we will download the source code for the rest of MoveIt:\nThe import command may ask for your GitHub credentials.\nYou can just press Enter until it moves on (ignore the “Authentication failed” error).\nFirst remove all previously installed moveit binaries:\nThe following will install from Debian any package dependencies not already in your workspace.\nThis is the step that will install MoveIt and all of its dependencies:\nThe next command will configure your Colcon workspace:\nThis build command will likely take a long time (20+ minutes) depending on your computer speed and amount of RAM available (we recommend 32 GB).\nWarning\nSome of the packages built with this command require up to 16Gb of RAM to build. By default,colcontries to build as many packages as possible at the same time.\nIf you are low on computer memory, or if the build is generally having trouble completing on your computer,\nyou can try appending--executorsequentialto thecolconcommand above to build only one package at a time, or--parallel-workers<X>to limit the number of simultaneous builds. For even more limited machines, you can try runningMAKEFLAGS=\"-j4-l1\"colconbuild--executorsequential.\nIf everything goes well, you should see the message “Summary: X packages finished” where X might be 50. If you have problems, try re-checking yourROS Installation.\nSource the Colcon workspace:\nOptional: add the previous command to your.bashrc:\nNote\nSourcing thesetup.bashautomatically in your~/.bashrcis\nnot required and often skipped by advanced users who use more than one\nColcon workspace at a time, but we recommend it for simplicity.\nNice job!\nNext, we willVisualize a robot with the interactive motion planning plugin for RViz\n© Copyright 2024, PickNik Robotics.\nsource/opt/ros/jazzy/setup.bash\nsudoaptinstallpython3-rosdep\nsudorosdepinitrosdepupdatesudoaptupdatesudoaptdist-upgrade\nsudoaptinstallpython3-colcon-common-extensionssudoaptinstallpython3-colcon-mixincolconmixinadddefaulthttps://raw.githubusercontent.com/colcon/colcon-mixin-repository/master/index.yamlcolconmixinupdatedefault\nsudoaptinstallpython3-vcstool\nmkdir-p~/ws_moveit/src\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit2_tutorials\nvcsimport--recursive<moveit2_tutorials/moveit2_tutorials.repos\nsudo apt remove ros-$ROS_DISTRO-moveit*\nsudo apt update && rosdep install -r --from-paths . --ignore-src --rosdistro $ROS_DISTRO -y\ncd~/ws_moveitcolconbuild--mixinrelease\nsource~/ws_moveit/install/setup.bash\necho'source ~/ws_moveit/install/setup.bash'>>~/.bashrc\n.bashrc\n.bashrc\n<branch>\nhumble\nmain\ncolcon\n--executorsequential\ncolcon\n--parallel-workers<X>\nMAKEFLAGS=\"-j4-l1\"colconbuild--executorsequential\n.bashrc\nsetup.bash\n~/.bashrc",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee09b"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/quickstart_in_rviz/quickstart_in_rviz_tutorial.html",
  "title": "MoveIt Quickstart in RViz — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Quickstart in RViz\nGetting Started\nStep 1: Launch the Demo and Configure the Plugin\nStep 2: Play with the Visualized Robots\nStep 3: Interact with the Kinova Gen 3\nStep 4: Use Motion Planning with the Kinova Gen 3\nNext Steps\nThis tutorial will teach you how to create motion plans in MoveIt using RViz and the MoveIt Display plugin. Rviz is the primary visualizer in ROS and a very useful tool for debugging robotics. The MoveIt Display plugin allows you to setup virtual environments (planning scenes), create start and goal states for the robot interactively, test various motion planners, and visualize the output. Let’s get started!\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Startedor ourDocker Guide.\nIf you followed the Docker Guide, also follow theCreate A Colcon Workspace and Download Tutorialsguide onwards to set up the tutorials.\nLaunch the demo:\nIf you are doing this for the first time, you should see an empty world in RViz and will have to add the Motion Planning Plugin:\nYou should see an empty world in RViz:\n\nIn the RViz Displays Tab, pressAdd:\n\nFrom the moveit_ros_visualization folder, choose “MotionPlanning” as the DisplayType. Press “Ok”.\n\nYou should now see the Kinova robot in RViz:\n\nOnce you have the Motion Planning Plugin loaded, we can configure it. In the “Global Options” tab of the “Displays” subwindow, set theFixed Framefield to/base_link\nNow, you can start configuring the Plugin for your robot (the Kinova Gen 3 in this case). Click on “MotionPlanning” within “Displays”.\nMake sure theRobot Descriptionfield is set torobot_description.\nMake sure thePlanning Scene Topicfield is set to/monitored_planning_scene.\nClick on topic name to expose topic-name drop-down.\nMake sure theTrajectory TopicunderPlanned Pathis set to/display_planned_path.\nInPlanning Request, change thePlanning Grouptomanipulator. You can also see this in the MotionPlanning panel in the bottom left.\nThere are four different overlapping visualizations:\nThe robot’s configuration in the/monitored_planning_sceneplanning environment (active by default).\nThe planned path for the robot (active by default).\nGreen: The start state for motion planning (disabled by default).\nOrange: The goal state for motion planning (active by default).\nThe display states for each of these visualizations can be toggled on and off using checkboxes:\nThe planning scene robot using theShow Robot Visualcheckbox in theScene Robottree menu.\nThe planned path using theShow Robot Visualcheckbox in thePlanned Pathtree menu.\nThe start state using theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThe goal state using theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nPlay with all these checkboxes to switch on and off different visualizations.\nFor the next steps we will want only the scene robot, start state and goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theShow Robot Visualcheckbox in theScene Robottree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nCheck theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThere should now be two interactive markers. One marker corresponding to the orange colored arm will be used to set the “Goal State” for motion planning and the other marker corresponding to a green colored arm are used to set the “Start State” for motion planning. If you don’t see the interactive markers pressInteractin the top menu of RViz (Note: some tools may be hidden, press“+”in the top menu to add theInteracttool as shown below).\nYou should now be able to use these markers to drag the arm around and change its orientation. Try it!\nFor this section, hide the planned path and the goal state:\nUn-check theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nNow, only the Start State (the green colored arm) should be visible.  Try to move the arm into a configuration where two of its links are in collision with each other.  (If you find this difficult, make sure the “Use Collision-Aware IK” checkbox under the Planning tab of the MotionPlanning plugin is un-checked.)  Once you do this, the links that are in collision will turn red.\nNow, check the “Use Collision-Aware IK” checkbox, and try again to move two of the links into collision with each other.  When the checkbox is ticked, the IK solver will keep attempting to find a collision-free solution for the desired end-effector pose. When it is not checked, the solver will allow collisions to happen in the solution. The links in collision will always still be visualized in red, regardless of the state of the checkbox.\nNote what happens when you try to move an end-effector out of its reachable workspace.\nBefore moving onto the next section, re-enable the planned path and the goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nYou can use theJointstab to move single joints and the redundant joints of 7-DOF robots. Try moving the “null space exploration” slider as shown in the animation below.\nNow, you can start motion planning with the Kinova Gen 3 in the MoveIt RViz Plugin.\nMove the Start State to a desired location.\nMove the Goal State to another desired location.\nMake sure both states are not in collision with the robot itself.\nUn-check theShow Trailcheckbox in thePlanned Pathtree menu.\nIn theMotionPlanningwindow under thePlanningtab, press thePlanbutton.\nCheck theShow Trailcheckbox in thePlanned Pathtree menu. You should see the arm’s path represented by a series of manipulator poses.\nYou can visually introspect trajectories point by point in RViz.\nFrom “Panels” menu, select “Trajectory - Trajectory Slider”. You’ll see a new Slider panel on RViz.\nSet your goal pose, then runPlan.\nPlay with the “Slider” panel, e.g. move the slider, push “Play” button.\nNote: Once you placed your end-effector to a new goal, be sure to runPlanbefore runningPlay– otherwise you’ll see the waypoints for the previous goal if available.\nIf the “Use Cartesian Path” checkbox is activated, the robot will attempt to move the end effector linearly in cartesian space.\nClicking “Plan & Execute” or “Execute” after a successful plan will send the trajectory to the robot - in this tutorial, since you usedkinova_demo.launch, the robot is only simulated.\nInitially, the default velocity and acceleration are scaled to 10% (0.1) of the robot’s maximum. You can change these scaling factors in the Planning tab shown below, or change these default values in themoveit_configof your robot (injoint_limits.yaml).\nMany of the tutorials usemoveit_visual_toolsto step through a demo. Before continuing on to the next tutorials it is a good idea to enable theRvizVisualToolsGui.\nFrom “Panels” menu, select “Add New Panels”. From the menu, select “RvizVisualToolsGui” and click OK. You’ll see the new panel added to RViz.\nRViz enables you to save your configuration underFile->SaveConfig. You should do this before continuing on to the next tutorials. If you choose to save your configuration under a new name, you can useFile->SaveConfigAsand refer to your configuration file using:\nReplaceyour_rviz_config.rvizwith the name of the file you saved tomoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/and build the workspace so it can be found.\nInYour First MoveIt Project, you will create a C++ program using MoveIt to plan and execute moves.\n© Copyright 2024, PickNik Robotics.\nros2launchmoveit2_tutorialsdemo.launch.py\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=your_rviz_config.rviz\n/base_link\nrobot_description\n/monitored_planning_scene\n/display_planned_path\nmanipulator\n/monitored_planning_scene\nkinova_demo.launch\n0.1\nmoveit_config\njoint_limits.yaml\nmoveit_visual_tools\nFile->SaveConfig\nFile->SaveConfigAs\nyour_rviz_config.rviz\nmoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee09c"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#getting-started",
  "title": "MoveIt Quickstart in RViz — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Quickstart in RViz\nGetting Started\nStep 1: Launch the Demo and Configure the Plugin\nStep 2: Play with the Visualized Robots\nStep 3: Interact with the Kinova Gen 3\nStep 4: Use Motion Planning with the Kinova Gen 3\nNext Steps\nThis tutorial will teach you how to create motion plans in MoveIt using RViz and the MoveIt Display plugin. Rviz is the primary visualizer in ROS and a very useful tool for debugging robotics. The MoveIt Display plugin allows you to setup virtual environments (planning scenes), create start and goal states for the robot interactively, test various motion planners, and visualize the output. Let’s get started!\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Startedor ourDocker Guide.\nIf you followed the Docker Guide, also follow theCreate A Colcon Workspace and Download Tutorialsguide onwards to set up the tutorials.\nLaunch the demo:\nIf you are doing this for the first time, you should see an empty world in RViz and will have to add the Motion Planning Plugin:\nYou should see an empty world in RViz:\n\nIn the RViz Displays Tab, pressAdd:\n\nFrom the moveit_ros_visualization folder, choose “MotionPlanning” as the DisplayType. Press “Ok”.\n\nYou should now see the Kinova robot in RViz:\n\nOnce you have the Motion Planning Plugin loaded, we can configure it. In the “Global Options” tab of the “Displays” subwindow, set theFixed Framefield to/base_link\nNow, you can start configuring the Plugin for your robot (the Kinova Gen 3 in this case). Click on “MotionPlanning” within “Displays”.\nMake sure theRobot Descriptionfield is set torobot_description.\nMake sure thePlanning Scene Topicfield is set to/monitored_planning_scene.\nClick on topic name to expose topic-name drop-down.\nMake sure theTrajectory TopicunderPlanned Pathis set to/display_planned_path.\nInPlanning Request, change thePlanning Grouptomanipulator. You can also see this in the MotionPlanning panel in the bottom left.\nThere are four different overlapping visualizations:\nThe robot’s configuration in the/monitored_planning_sceneplanning environment (active by default).\nThe planned path for the robot (active by default).\nGreen: The start state for motion planning (disabled by default).\nOrange: The goal state for motion planning (active by default).\nThe display states for each of these visualizations can be toggled on and off using checkboxes:\nThe planning scene robot using theShow Robot Visualcheckbox in theScene Robottree menu.\nThe planned path using theShow Robot Visualcheckbox in thePlanned Pathtree menu.\nThe start state using theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThe goal state using theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nPlay with all these checkboxes to switch on and off different visualizations.\nFor the next steps we will want only the scene robot, start state and goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theShow Robot Visualcheckbox in theScene Robottree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nCheck theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThere should now be two interactive markers. One marker corresponding to the orange colored arm will be used to set the “Goal State” for motion planning and the other marker corresponding to a green colored arm are used to set the “Start State” for motion planning. If you don’t see the interactive markers pressInteractin the top menu of RViz (Note: some tools may be hidden, press“+”in the top menu to add theInteracttool as shown below).\nYou should now be able to use these markers to drag the arm around and change its orientation. Try it!\nFor this section, hide the planned path and the goal state:\nUn-check theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nNow, only the Start State (the green colored arm) should be visible.  Try to move the arm into a configuration where two of its links are in collision with each other.  (If you find this difficult, make sure the “Use Collision-Aware IK” checkbox under the Planning tab of the MotionPlanning plugin is un-checked.)  Once you do this, the links that are in collision will turn red.\nNow, check the “Use Collision-Aware IK” checkbox, and try again to move two of the links into collision with each other.  When the checkbox is ticked, the IK solver will keep attempting to find a collision-free solution for the desired end-effector pose. When it is not checked, the solver will allow collisions to happen in the solution. The links in collision will always still be visualized in red, regardless of the state of the checkbox.\nNote what happens when you try to move an end-effector out of its reachable workspace.\nBefore moving onto the next section, re-enable the planned path and the goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nYou can use theJointstab to move single joints and the redundant joints of 7-DOF robots. Try moving the “null space exploration” slider as shown in the animation below.\nNow, you can start motion planning with the Kinova Gen 3 in the MoveIt RViz Plugin.\nMove the Start State to a desired location.\nMove the Goal State to another desired location.\nMake sure both states are not in collision with the robot itself.\nUn-check theShow Trailcheckbox in thePlanned Pathtree menu.\nIn theMotionPlanningwindow under thePlanningtab, press thePlanbutton.\nCheck theShow Trailcheckbox in thePlanned Pathtree menu. You should see the arm’s path represented by a series of manipulator poses.\nYou can visually introspect trajectories point by point in RViz.\nFrom “Panels” menu, select “Trajectory - Trajectory Slider”. You’ll see a new Slider panel on RViz.\nSet your goal pose, then runPlan.\nPlay with the “Slider” panel, e.g. move the slider, push “Play” button.\nNote: Once you placed your end-effector to a new goal, be sure to runPlanbefore runningPlay– otherwise you’ll see the waypoints for the previous goal if available.\nIf the “Use Cartesian Path” checkbox is activated, the robot will attempt to move the end effector linearly in cartesian space.\nClicking “Plan & Execute” or “Execute” after a successful plan will send the trajectory to the robot - in this tutorial, since you usedkinova_demo.launch, the robot is only simulated.\nInitially, the default velocity and acceleration are scaled to 10% (0.1) of the robot’s maximum. You can change these scaling factors in the Planning tab shown below, or change these default values in themoveit_configof your robot (injoint_limits.yaml).\nMany of the tutorials usemoveit_visual_toolsto step through a demo. Before continuing on to the next tutorials it is a good idea to enable theRvizVisualToolsGui.\nFrom “Panels” menu, select “Add New Panels”. From the menu, select “RvizVisualToolsGui” and click OK. You’ll see the new panel added to RViz.\nRViz enables you to save your configuration underFile->SaveConfig. You should do this before continuing on to the next tutorials. If you choose to save your configuration under a new name, you can useFile->SaveConfigAsand refer to your configuration file using:\nReplaceyour_rviz_config.rvizwith the name of the file you saved tomoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/and build the workspace so it can be found.\nInYour First MoveIt Project, you will create a C++ program using MoveIt to plan and execute moves.\n© Copyright 2024, PickNik Robotics.\nros2launchmoveit2_tutorialsdemo.launch.py\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=your_rviz_config.rviz\n/base_link\nrobot_description\n/monitored_planning_scene\n/display_planned_path\nmanipulator\n/monitored_planning_scene\nkinova_demo.launch\n0.1\nmoveit_config\njoint_limits.yaml\nmoveit_visual_tools\nFile->SaveConfig\nFile->SaveConfigAs\nyour_rviz_config.rviz\nmoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee09d"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#step-1-launch-the-demo-and-configure-the-plugin",
  "title": "MoveIt Quickstart in RViz — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Quickstart in RViz\nGetting Started\nStep 1: Launch the Demo and Configure the Plugin\nStep 2: Play with the Visualized Robots\nStep 3: Interact with the Kinova Gen 3\nStep 4: Use Motion Planning with the Kinova Gen 3\nNext Steps\nThis tutorial will teach you how to create motion plans in MoveIt using RViz and the MoveIt Display plugin. Rviz is the primary visualizer in ROS and a very useful tool for debugging robotics. The MoveIt Display plugin allows you to setup virtual environments (planning scenes), create start and goal states for the robot interactively, test various motion planners, and visualize the output. Let’s get started!\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Startedor ourDocker Guide.\nIf you followed the Docker Guide, also follow theCreate A Colcon Workspace and Download Tutorialsguide onwards to set up the tutorials.\nLaunch the demo:\nIf you are doing this for the first time, you should see an empty world in RViz and will have to add the Motion Planning Plugin:\nYou should see an empty world in RViz:\n\nIn the RViz Displays Tab, pressAdd:\n\nFrom the moveit_ros_visualization folder, choose “MotionPlanning” as the DisplayType. Press “Ok”.\n\nYou should now see the Kinova robot in RViz:\n\nOnce you have the Motion Planning Plugin loaded, we can configure it. In the “Global Options” tab of the “Displays” subwindow, set theFixed Framefield to/base_link\nNow, you can start configuring the Plugin for your robot (the Kinova Gen 3 in this case). Click on “MotionPlanning” within “Displays”.\nMake sure theRobot Descriptionfield is set torobot_description.\nMake sure thePlanning Scene Topicfield is set to/monitored_planning_scene.\nClick on topic name to expose topic-name drop-down.\nMake sure theTrajectory TopicunderPlanned Pathis set to/display_planned_path.\nInPlanning Request, change thePlanning Grouptomanipulator. You can also see this in the MotionPlanning panel in the bottom left.\nThere are four different overlapping visualizations:\nThe robot’s configuration in the/monitored_planning_sceneplanning environment (active by default).\nThe planned path for the robot (active by default).\nGreen: The start state for motion planning (disabled by default).\nOrange: The goal state for motion planning (active by default).\nThe display states for each of these visualizations can be toggled on and off using checkboxes:\nThe planning scene robot using theShow Robot Visualcheckbox in theScene Robottree menu.\nThe planned path using theShow Robot Visualcheckbox in thePlanned Pathtree menu.\nThe start state using theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThe goal state using theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nPlay with all these checkboxes to switch on and off different visualizations.\nFor the next steps we will want only the scene robot, start state and goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theShow Robot Visualcheckbox in theScene Robottree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nCheck theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThere should now be two interactive markers. One marker corresponding to the orange colored arm will be used to set the “Goal State” for motion planning and the other marker corresponding to a green colored arm are used to set the “Start State” for motion planning. If you don’t see the interactive markers pressInteractin the top menu of RViz (Note: some tools may be hidden, press“+”in the top menu to add theInteracttool as shown below).\nYou should now be able to use these markers to drag the arm around and change its orientation. Try it!\nFor this section, hide the planned path and the goal state:\nUn-check theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nNow, only the Start State (the green colored arm) should be visible.  Try to move the arm into a configuration where two of its links are in collision with each other.  (If you find this difficult, make sure the “Use Collision-Aware IK” checkbox under the Planning tab of the MotionPlanning plugin is un-checked.)  Once you do this, the links that are in collision will turn red.\nNow, check the “Use Collision-Aware IK” checkbox, and try again to move two of the links into collision with each other.  When the checkbox is ticked, the IK solver will keep attempting to find a collision-free solution for the desired end-effector pose. When it is not checked, the solver will allow collisions to happen in the solution. The links in collision will always still be visualized in red, regardless of the state of the checkbox.\nNote what happens when you try to move an end-effector out of its reachable workspace.\nBefore moving onto the next section, re-enable the planned path and the goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nYou can use theJointstab to move single joints and the redundant joints of 7-DOF robots. Try moving the “null space exploration” slider as shown in the animation below.\nNow, you can start motion planning with the Kinova Gen 3 in the MoveIt RViz Plugin.\nMove the Start State to a desired location.\nMove the Goal State to another desired location.\nMake sure both states are not in collision with the robot itself.\nUn-check theShow Trailcheckbox in thePlanned Pathtree menu.\nIn theMotionPlanningwindow under thePlanningtab, press thePlanbutton.\nCheck theShow Trailcheckbox in thePlanned Pathtree menu. You should see the arm’s path represented by a series of manipulator poses.\nYou can visually introspect trajectories point by point in RViz.\nFrom “Panels” menu, select “Trajectory - Trajectory Slider”. You’ll see a new Slider panel on RViz.\nSet your goal pose, then runPlan.\nPlay with the “Slider” panel, e.g. move the slider, push “Play” button.\nNote: Once you placed your end-effector to a new goal, be sure to runPlanbefore runningPlay– otherwise you’ll see the waypoints for the previous goal if available.\nIf the “Use Cartesian Path” checkbox is activated, the robot will attempt to move the end effector linearly in cartesian space.\nClicking “Plan & Execute” or “Execute” after a successful plan will send the trajectory to the robot - in this tutorial, since you usedkinova_demo.launch, the robot is only simulated.\nInitially, the default velocity and acceleration are scaled to 10% (0.1) of the robot’s maximum. You can change these scaling factors in the Planning tab shown below, or change these default values in themoveit_configof your robot (injoint_limits.yaml).\nMany of the tutorials usemoveit_visual_toolsto step through a demo. Before continuing on to the next tutorials it is a good idea to enable theRvizVisualToolsGui.\nFrom “Panels” menu, select “Add New Panels”. From the menu, select “RvizVisualToolsGui” and click OK. You’ll see the new panel added to RViz.\nRViz enables you to save your configuration underFile->SaveConfig. You should do this before continuing on to the next tutorials. If you choose to save your configuration under a new name, you can useFile->SaveConfigAsand refer to your configuration file using:\nReplaceyour_rviz_config.rvizwith the name of the file you saved tomoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/and build the workspace so it can be found.\nInYour First MoveIt Project, you will create a C++ program using MoveIt to plan and execute moves.\n© Copyright 2024, PickNik Robotics.\nros2launchmoveit2_tutorialsdemo.launch.py\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=your_rviz_config.rviz\n/base_link\nrobot_description\n/monitored_planning_scene\n/display_planned_path\nmanipulator\n/monitored_planning_scene\nkinova_demo.launch\n0.1\nmoveit_config\njoint_limits.yaml\nmoveit_visual_tools\nFile->SaveConfig\nFile->SaveConfigAs\nyour_rviz_config.rviz\nmoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee09e"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#step-2-play-with-the-visualized-robots",
  "title": "MoveIt Quickstart in RViz — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Quickstart in RViz\nGetting Started\nStep 1: Launch the Demo and Configure the Plugin\nStep 2: Play with the Visualized Robots\nStep 3: Interact with the Kinova Gen 3\nStep 4: Use Motion Planning with the Kinova Gen 3\nNext Steps\nThis tutorial will teach you how to create motion plans in MoveIt using RViz and the MoveIt Display plugin. Rviz is the primary visualizer in ROS and a very useful tool for debugging robotics. The MoveIt Display plugin allows you to setup virtual environments (planning scenes), create start and goal states for the robot interactively, test various motion planners, and visualize the output. Let’s get started!\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Startedor ourDocker Guide.\nIf you followed the Docker Guide, also follow theCreate A Colcon Workspace and Download Tutorialsguide onwards to set up the tutorials.\nLaunch the demo:\nIf you are doing this for the first time, you should see an empty world in RViz and will have to add the Motion Planning Plugin:\nYou should see an empty world in RViz:\n\nIn the RViz Displays Tab, pressAdd:\n\nFrom the moveit_ros_visualization folder, choose “MotionPlanning” as the DisplayType. Press “Ok”.\n\nYou should now see the Kinova robot in RViz:\n\nOnce you have the Motion Planning Plugin loaded, we can configure it. In the “Global Options” tab of the “Displays” subwindow, set theFixed Framefield to/base_link\nNow, you can start configuring the Plugin for your robot (the Kinova Gen 3 in this case). Click on “MotionPlanning” within “Displays”.\nMake sure theRobot Descriptionfield is set torobot_description.\nMake sure thePlanning Scene Topicfield is set to/monitored_planning_scene.\nClick on topic name to expose topic-name drop-down.\nMake sure theTrajectory TopicunderPlanned Pathis set to/display_planned_path.\nInPlanning Request, change thePlanning Grouptomanipulator. You can also see this in the MotionPlanning panel in the bottom left.\nThere are four different overlapping visualizations:\nThe robot’s configuration in the/monitored_planning_sceneplanning environment (active by default).\nThe planned path for the robot (active by default).\nGreen: The start state for motion planning (disabled by default).\nOrange: The goal state for motion planning (active by default).\nThe display states for each of these visualizations can be toggled on and off using checkboxes:\nThe planning scene robot using theShow Robot Visualcheckbox in theScene Robottree menu.\nThe planned path using theShow Robot Visualcheckbox in thePlanned Pathtree menu.\nThe start state using theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThe goal state using theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nPlay with all these checkboxes to switch on and off different visualizations.\nFor the next steps we will want only the scene robot, start state and goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theShow Robot Visualcheckbox in theScene Robottree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nCheck theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThere should now be two interactive markers. One marker corresponding to the orange colored arm will be used to set the “Goal State” for motion planning and the other marker corresponding to a green colored arm are used to set the “Start State” for motion planning. If you don’t see the interactive markers pressInteractin the top menu of RViz (Note: some tools may be hidden, press“+”in the top menu to add theInteracttool as shown below).\nYou should now be able to use these markers to drag the arm around and change its orientation. Try it!\nFor this section, hide the planned path and the goal state:\nUn-check theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nNow, only the Start State (the green colored arm) should be visible.  Try to move the arm into a configuration where two of its links are in collision with each other.  (If you find this difficult, make sure the “Use Collision-Aware IK” checkbox under the Planning tab of the MotionPlanning plugin is un-checked.)  Once you do this, the links that are in collision will turn red.\nNow, check the “Use Collision-Aware IK” checkbox, and try again to move two of the links into collision with each other.  When the checkbox is ticked, the IK solver will keep attempting to find a collision-free solution for the desired end-effector pose. When it is not checked, the solver will allow collisions to happen in the solution. The links in collision will always still be visualized in red, regardless of the state of the checkbox.\nNote what happens when you try to move an end-effector out of its reachable workspace.\nBefore moving onto the next section, re-enable the planned path and the goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nYou can use theJointstab to move single joints and the redundant joints of 7-DOF robots. Try moving the “null space exploration” slider as shown in the animation below.\nNow, you can start motion planning with the Kinova Gen 3 in the MoveIt RViz Plugin.\nMove the Start State to a desired location.\nMove the Goal State to another desired location.\nMake sure both states are not in collision with the robot itself.\nUn-check theShow Trailcheckbox in thePlanned Pathtree menu.\nIn theMotionPlanningwindow under thePlanningtab, press thePlanbutton.\nCheck theShow Trailcheckbox in thePlanned Pathtree menu. You should see the arm’s path represented by a series of manipulator poses.\nYou can visually introspect trajectories point by point in RViz.\nFrom “Panels” menu, select “Trajectory - Trajectory Slider”. You’ll see a new Slider panel on RViz.\nSet your goal pose, then runPlan.\nPlay with the “Slider” panel, e.g. move the slider, push “Play” button.\nNote: Once you placed your end-effector to a new goal, be sure to runPlanbefore runningPlay– otherwise you’ll see the waypoints for the previous goal if available.\nIf the “Use Cartesian Path” checkbox is activated, the robot will attempt to move the end effector linearly in cartesian space.\nClicking “Plan & Execute” or “Execute” after a successful plan will send the trajectory to the robot - in this tutorial, since you usedkinova_demo.launch, the robot is only simulated.\nInitially, the default velocity and acceleration are scaled to 10% (0.1) of the robot’s maximum. You can change these scaling factors in the Planning tab shown below, or change these default values in themoveit_configof your robot (injoint_limits.yaml).\nMany of the tutorials usemoveit_visual_toolsto step through a demo. Before continuing on to the next tutorials it is a good idea to enable theRvizVisualToolsGui.\nFrom “Panels” menu, select “Add New Panels”. From the menu, select “RvizVisualToolsGui” and click OK. You’ll see the new panel added to RViz.\nRViz enables you to save your configuration underFile->SaveConfig. You should do this before continuing on to the next tutorials. If you choose to save your configuration under a new name, you can useFile->SaveConfigAsand refer to your configuration file using:\nReplaceyour_rviz_config.rvizwith the name of the file you saved tomoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/and build the workspace so it can be found.\nInYour First MoveIt Project, you will create a C++ program using MoveIt to plan and execute moves.\n© Copyright 2024, PickNik Robotics.\nros2launchmoveit2_tutorialsdemo.launch.py\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=your_rviz_config.rviz\n/base_link\nrobot_description\n/monitored_planning_scene\n/display_planned_path\nmanipulator\n/monitored_planning_scene\nkinova_demo.launch\n0.1\nmoveit_config\njoint_limits.yaml\nmoveit_visual_tools\nFile->SaveConfig\nFile->SaveConfigAs\nyour_rviz_config.rviz\nmoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee09f"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#step-3-interact-with-the-kinova-gen-3",
  "title": "MoveIt Quickstart in RViz — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Quickstart in RViz\nGetting Started\nStep 1: Launch the Demo and Configure the Plugin\nStep 2: Play with the Visualized Robots\nStep 3: Interact with the Kinova Gen 3\nStep 4: Use Motion Planning with the Kinova Gen 3\nNext Steps\nThis tutorial will teach you how to create motion plans in MoveIt using RViz and the MoveIt Display plugin. Rviz is the primary visualizer in ROS and a very useful tool for debugging robotics. The MoveIt Display plugin allows you to setup virtual environments (planning scenes), create start and goal states for the robot interactively, test various motion planners, and visualize the output. Let’s get started!\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Startedor ourDocker Guide.\nIf you followed the Docker Guide, also follow theCreate A Colcon Workspace and Download Tutorialsguide onwards to set up the tutorials.\nLaunch the demo:\nIf you are doing this for the first time, you should see an empty world in RViz and will have to add the Motion Planning Plugin:\nYou should see an empty world in RViz:\n\nIn the RViz Displays Tab, pressAdd:\n\nFrom the moveit_ros_visualization folder, choose “MotionPlanning” as the DisplayType. Press “Ok”.\n\nYou should now see the Kinova robot in RViz:\n\nOnce you have the Motion Planning Plugin loaded, we can configure it. In the “Global Options” tab of the “Displays” subwindow, set theFixed Framefield to/base_link\nNow, you can start configuring the Plugin for your robot (the Kinova Gen 3 in this case). Click on “MotionPlanning” within “Displays”.\nMake sure theRobot Descriptionfield is set torobot_description.\nMake sure thePlanning Scene Topicfield is set to/monitored_planning_scene.\nClick on topic name to expose topic-name drop-down.\nMake sure theTrajectory TopicunderPlanned Pathis set to/display_planned_path.\nInPlanning Request, change thePlanning Grouptomanipulator. You can also see this in the MotionPlanning panel in the bottom left.\nThere are four different overlapping visualizations:\nThe robot’s configuration in the/monitored_planning_sceneplanning environment (active by default).\nThe planned path for the robot (active by default).\nGreen: The start state for motion planning (disabled by default).\nOrange: The goal state for motion planning (active by default).\nThe display states for each of these visualizations can be toggled on and off using checkboxes:\nThe planning scene robot using theShow Robot Visualcheckbox in theScene Robottree menu.\nThe planned path using theShow Robot Visualcheckbox in thePlanned Pathtree menu.\nThe start state using theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThe goal state using theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nPlay with all these checkboxes to switch on and off different visualizations.\nFor the next steps we will want only the scene robot, start state and goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theShow Robot Visualcheckbox in theScene Robottree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nCheck theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThere should now be two interactive markers. One marker corresponding to the orange colored arm will be used to set the “Goal State” for motion planning and the other marker corresponding to a green colored arm are used to set the “Start State” for motion planning. If you don’t see the interactive markers pressInteractin the top menu of RViz (Note: some tools may be hidden, press“+”in the top menu to add theInteracttool as shown below).\nYou should now be able to use these markers to drag the arm around and change its orientation. Try it!\nFor this section, hide the planned path and the goal state:\nUn-check theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nNow, only the Start State (the green colored arm) should be visible.  Try to move the arm into a configuration where two of its links are in collision with each other.  (If you find this difficult, make sure the “Use Collision-Aware IK” checkbox under the Planning tab of the MotionPlanning plugin is un-checked.)  Once you do this, the links that are in collision will turn red.\nNow, check the “Use Collision-Aware IK” checkbox, and try again to move two of the links into collision with each other.  When the checkbox is ticked, the IK solver will keep attempting to find a collision-free solution for the desired end-effector pose. When it is not checked, the solver will allow collisions to happen in the solution. The links in collision will always still be visualized in red, regardless of the state of the checkbox.\nNote what happens when you try to move an end-effector out of its reachable workspace.\nBefore moving onto the next section, re-enable the planned path and the goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nYou can use theJointstab to move single joints and the redundant joints of 7-DOF robots. Try moving the “null space exploration” slider as shown in the animation below.\nNow, you can start motion planning with the Kinova Gen 3 in the MoveIt RViz Plugin.\nMove the Start State to a desired location.\nMove the Goal State to another desired location.\nMake sure both states are not in collision with the robot itself.\nUn-check theShow Trailcheckbox in thePlanned Pathtree menu.\nIn theMotionPlanningwindow under thePlanningtab, press thePlanbutton.\nCheck theShow Trailcheckbox in thePlanned Pathtree menu. You should see the arm’s path represented by a series of manipulator poses.\nYou can visually introspect trajectories point by point in RViz.\nFrom “Panels” menu, select “Trajectory - Trajectory Slider”. You’ll see a new Slider panel on RViz.\nSet your goal pose, then runPlan.\nPlay with the “Slider” panel, e.g. move the slider, push “Play” button.\nNote: Once you placed your end-effector to a new goal, be sure to runPlanbefore runningPlay– otherwise you’ll see the waypoints for the previous goal if available.\nIf the “Use Cartesian Path” checkbox is activated, the robot will attempt to move the end effector linearly in cartesian space.\nClicking “Plan & Execute” or “Execute” after a successful plan will send the trajectory to the robot - in this tutorial, since you usedkinova_demo.launch, the robot is only simulated.\nInitially, the default velocity and acceleration are scaled to 10% (0.1) of the robot’s maximum. You can change these scaling factors in the Planning tab shown below, or change these default values in themoveit_configof your robot (injoint_limits.yaml).\nMany of the tutorials usemoveit_visual_toolsto step through a demo. Before continuing on to the next tutorials it is a good idea to enable theRvizVisualToolsGui.\nFrom “Panels” menu, select “Add New Panels”. From the menu, select “RvizVisualToolsGui” and click OK. You’ll see the new panel added to RViz.\nRViz enables you to save your configuration underFile->SaveConfig. You should do this before continuing on to the next tutorials. If you choose to save your configuration under a new name, you can useFile->SaveConfigAsand refer to your configuration file using:\nReplaceyour_rviz_config.rvizwith the name of the file you saved tomoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/and build the workspace so it can be found.\nInYour First MoveIt Project, you will create a C++ program using MoveIt to plan and execute moves.\n© Copyright 2024, PickNik Robotics.\nros2launchmoveit2_tutorialsdemo.launch.py\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=your_rviz_config.rviz\n/base_link\nrobot_description\n/monitored_planning_scene\n/display_planned_path\nmanipulator\n/monitored_planning_scene\nkinova_demo.launch\n0.1\nmoveit_config\njoint_limits.yaml\nmoveit_visual_tools\nFile->SaveConfig\nFile->SaveConfigAs\nyour_rviz_config.rviz\nmoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0a0"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#moving-into-collision",
  "title": "MoveIt Quickstart in RViz — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Quickstart in RViz\nGetting Started\nStep 1: Launch the Demo and Configure the Plugin\nStep 2: Play with the Visualized Robots\nStep 3: Interact with the Kinova Gen 3\nStep 4: Use Motion Planning with the Kinova Gen 3\nNext Steps\nThis tutorial will teach you how to create motion plans in MoveIt using RViz and the MoveIt Display plugin. Rviz is the primary visualizer in ROS and a very useful tool for debugging robotics. The MoveIt Display plugin allows you to setup virtual environments (planning scenes), create start and goal states for the robot interactively, test various motion planners, and visualize the output. Let’s get started!\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Startedor ourDocker Guide.\nIf you followed the Docker Guide, also follow theCreate A Colcon Workspace and Download Tutorialsguide onwards to set up the tutorials.\nLaunch the demo:\nIf you are doing this for the first time, you should see an empty world in RViz and will have to add the Motion Planning Plugin:\nYou should see an empty world in RViz:\n\nIn the RViz Displays Tab, pressAdd:\n\nFrom the moveit_ros_visualization folder, choose “MotionPlanning” as the DisplayType. Press “Ok”.\n\nYou should now see the Kinova robot in RViz:\n\nOnce you have the Motion Planning Plugin loaded, we can configure it. In the “Global Options” tab of the “Displays” subwindow, set theFixed Framefield to/base_link\nNow, you can start configuring the Plugin for your robot (the Kinova Gen 3 in this case). Click on “MotionPlanning” within “Displays”.\nMake sure theRobot Descriptionfield is set torobot_description.\nMake sure thePlanning Scene Topicfield is set to/monitored_planning_scene.\nClick on topic name to expose topic-name drop-down.\nMake sure theTrajectory TopicunderPlanned Pathis set to/display_planned_path.\nInPlanning Request, change thePlanning Grouptomanipulator. You can also see this in the MotionPlanning panel in the bottom left.\nThere are four different overlapping visualizations:\nThe robot’s configuration in the/monitored_planning_sceneplanning environment (active by default).\nThe planned path for the robot (active by default).\nGreen: The start state for motion planning (disabled by default).\nOrange: The goal state for motion planning (active by default).\nThe display states for each of these visualizations can be toggled on and off using checkboxes:\nThe planning scene robot using theShow Robot Visualcheckbox in theScene Robottree menu.\nThe planned path using theShow Robot Visualcheckbox in thePlanned Pathtree menu.\nThe start state using theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThe goal state using theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nPlay with all these checkboxes to switch on and off different visualizations.\nFor the next steps we will want only the scene robot, start state and goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theShow Robot Visualcheckbox in theScene Robottree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nCheck theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThere should now be two interactive markers. One marker corresponding to the orange colored arm will be used to set the “Goal State” for motion planning and the other marker corresponding to a green colored arm are used to set the “Start State” for motion planning. If you don’t see the interactive markers pressInteractin the top menu of RViz (Note: some tools may be hidden, press“+”in the top menu to add theInteracttool as shown below).\nYou should now be able to use these markers to drag the arm around and change its orientation. Try it!\nFor this section, hide the planned path and the goal state:\nUn-check theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nNow, only the Start State (the green colored arm) should be visible.  Try to move the arm into a configuration where two of its links are in collision with each other.  (If you find this difficult, make sure the “Use Collision-Aware IK” checkbox under the Planning tab of the MotionPlanning plugin is un-checked.)  Once you do this, the links that are in collision will turn red.\nNow, check the “Use Collision-Aware IK” checkbox, and try again to move two of the links into collision with each other.  When the checkbox is ticked, the IK solver will keep attempting to find a collision-free solution for the desired end-effector pose. When it is not checked, the solver will allow collisions to happen in the solution. The links in collision will always still be visualized in red, regardless of the state of the checkbox.\nNote what happens when you try to move an end-effector out of its reachable workspace.\nBefore moving onto the next section, re-enable the planned path and the goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nYou can use theJointstab to move single joints and the redundant joints of 7-DOF robots. Try moving the “null space exploration” slider as shown in the animation below.\nNow, you can start motion planning with the Kinova Gen 3 in the MoveIt RViz Plugin.\nMove the Start State to a desired location.\nMove the Goal State to another desired location.\nMake sure both states are not in collision with the robot itself.\nUn-check theShow Trailcheckbox in thePlanned Pathtree menu.\nIn theMotionPlanningwindow under thePlanningtab, press thePlanbutton.\nCheck theShow Trailcheckbox in thePlanned Pathtree menu. You should see the arm’s path represented by a series of manipulator poses.\nYou can visually introspect trajectories point by point in RViz.\nFrom “Panels” menu, select “Trajectory - Trajectory Slider”. You’ll see a new Slider panel on RViz.\nSet your goal pose, then runPlan.\nPlay with the “Slider” panel, e.g. move the slider, push “Play” button.\nNote: Once you placed your end-effector to a new goal, be sure to runPlanbefore runningPlay– otherwise you’ll see the waypoints for the previous goal if available.\nIf the “Use Cartesian Path” checkbox is activated, the robot will attempt to move the end effector linearly in cartesian space.\nClicking “Plan & Execute” or “Execute” after a successful plan will send the trajectory to the robot - in this tutorial, since you usedkinova_demo.launch, the robot is only simulated.\nInitially, the default velocity and acceleration are scaled to 10% (0.1) of the robot’s maximum. You can change these scaling factors in the Planning tab shown below, or change these default values in themoveit_configof your robot (injoint_limits.yaml).\nMany of the tutorials usemoveit_visual_toolsto step through a demo. Before continuing on to the next tutorials it is a good idea to enable theRvizVisualToolsGui.\nFrom “Panels” menu, select “Add New Panels”. From the menu, select “RvizVisualToolsGui” and click OK. You’ll see the new panel added to RViz.\nRViz enables you to save your configuration underFile->SaveConfig. You should do this before continuing on to the next tutorials. If you choose to save your configuration under a new name, you can useFile->SaveConfigAsand refer to your configuration file using:\nReplaceyour_rviz_config.rvizwith the name of the file you saved tomoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/and build the workspace so it can be found.\nInYour First MoveIt Project, you will create a C++ program using MoveIt to plan and execute moves.\n© Copyright 2024, PickNik Robotics.\nros2launchmoveit2_tutorialsdemo.launch.py\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=your_rviz_config.rviz\n/base_link\nrobot_description\n/monitored_planning_scene\n/display_planned_path\nmanipulator\n/monitored_planning_scene\nkinova_demo.launch\n0.1\nmoveit_config\njoint_limits.yaml\nmoveit_visual_tools\nFile->SaveConfig\nFile->SaveConfigAs\nyour_rviz_config.rviz\nmoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0a1"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#moving-out-of-reachable-workspace",
  "title": "MoveIt Quickstart in RViz — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Quickstart in RViz\nGetting Started\nStep 1: Launch the Demo and Configure the Plugin\nStep 2: Play with the Visualized Robots\nStep 3: Interact with the Kinova Gen 3\nStep 4: Use Motion Planning with the Kinova Gen 3\nNext Steps\nThis tutorial will teach you how to create motion plans in MoveIt using RViz and the MoveIt Display plugin. Rviz is the primary visualizer in ROS and a very useful tool for debugging robotics. The MoveIt Display plugin allows you to setup virtual environments (planning scenes), create start and goal states for the robot interactively, test various motion planners, and visualize the output. Let’s get started!\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Startedor ourDocker Guide.\nIf you followed the Docker Guide, also follow theCreate A Colcon Workspace and Download Tutorialsguide onwards to set up the tutorials.\nLaunch the demo:\nIf you are doing this for the first time, you should see an empty world in RViz and will have to add the Motion Planning Plugin:\nYou should see an empty world in RViz:\n\nIn the RViz Displays Tab, pressAdd:\n\nFrom the moveit_ros_visualization folder, choose “MotionPlanning” as the DisplayType. Press “Ok”.\n\nYou should now see the Kinova robot in RViz:\n\nOnce you have the Motion Planning Plugin loaded, we can configure it. In the “Global Options” tab of the “Displays” subwindow, set theFixed Framefield to/base_link\nNow, you can start configuring the Plugin for your robot (the Kinova Gen 3 in this case). Click on “MotionPlanning” within “Displays”.\nMake sure theRobot Descriptionfield is set torobot_description.\nMake sure thePlanning Scene Topicfield is set to/monitored_planning_scene.\nClick on topic name to expose topic-name drop-down.\nMake sure theTrajectory TopicunderPlanned Pathis set to/display_planned_path.\nInPlanning Request, change thePlanning Grouptomanipulator. You can also see this in the MotionPlanning panel in the bottom left.\nThere are four different overlapping visualizations:\nThe robot’s configuration in the/monitored_planning_sceneplanning environment (active by default).\nThe planned path for the robot (active by default).\nGreen: The start state for motion planning (disabled by default).\nOrange: The goal state for motion planning (active by default).\nThe display states for each of these visualizations can be toggled on and off using checkboxes:\nThe planning scene robot using theShow Robot Visualcheckbox in theScene Robottree menu.\nThe planned path using theShow Robot Visualcheckbox in thePlanned Pathtree menu.\nThe start state using theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThe goal state using theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nPlay with all these checkboxes to switch on and off different visualizations.\nFor the next steps we will want only the scene robot, start state and goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theShow Robot Visualcheckbox in theScene Robottree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nCheck theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThere should now be two interactive markers. One marker corresponding to the orange colored arm will be used to set the “Goal State” for motion planning and the other marker corresponding to a green colored arm are used to set the “Start State” for motion planning. If you don’t see the interactive markers pressInteractin the top menu of RViz (Note: some tools may be hidden, press“+”in the top menu to add theInteracttool as shown below).\nYou should now be able to use these markers to drag the arm around and change its orientation. Try it!\nFor this section, hide the planned path and the goal state:\nUn-check theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nNow, only the Start State (the green colored arm) should be visible.  Try to move the arm into a configuration where two of its links are in collision with each other.  (If you find this difficult, make sure the “Use Collision-Aware IK” checkbox under the Planning tab of the MotionPlanning plugin is un-checked.)  Once you do this, the links that are in collision will turn red.\nNow, check the “Use Collision-Aware IK” checkbox, and try again to move two of the links into collision with each other.  When the checkbox is ticked, the IK solver will keep attempting to find a collision-free solution for the desired end-effector pose. When it is not checked, the solver will allow collisions to happen in the solution. The links in collision will always still be visualized in red, regardless of the state of the checkbox.\nNote what happens when you try to move an end-effector out of its reachable workspace.\nBefore moving onto the next section, re-enable the planned path and the goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nYou can use theJointstab to move single joints and the redundant joints of 7-DOF robots. Try moving the “null space exploration” slider as shown in the animation below.\nNow, you can start motion planning with the Kinova Gen 3 in the MoveIt RViz Plugin.\nMove the Start State to a desired location.\nMove the Goal State to another desired location.\nMake sure both states are not in collision with the robot itself.\nUn-check theShow Trailcheckbox in thePlanned Pathtree menu.\nIn theMotionPlanningwindow under thePlanningtab, press thePlanbutton.\nCheck theShow Trailcheckbox in thePlanned Pathtree menu. You should see the arm’s path represented by a series of manipulator poses.\nYou can visually introspect trajectories point by point in RViz.\nFrom “Panels” menu, select “Trajectory - Trajectory Slider”. You’ll see a new Slider panel on RViz.\nSet your goal pose, then runPlan.\nPlay with the “Slider” panel, e.g. move the slider, push “Play” button.\nNote: Once you placed your end-effector to a new goal, be sure to runPlanbefore runningPlay– otherwise you’ll see the waypoints for the previous goal if available.\nIf the “Use Cartesian Path” checkbox is activated, the robot will attempt to move the end effector linearly in cartesian space.\nClicking “Plan & Execute” or “Execute” after a successful plan will send the trajectory to the robot - in this tutorial, since you usedkinova_demo.launch, the robot is only simulated.\nInitially, the default velocity and acceleration are scaled to 10% (0.1) of the robot’s maximum. You can change these scaling factors in the Planning tab shown below, or change these default values in themoveit_configof your robot (injoint_limits.yaml).\nMany of the tutorials usemoveit_visual_toolsto step through a demo. Before continuing on to the next tutorials it is a good idea to enable theRvizVisualToolsGui.\nFrom “Panels” menu, select “Add New Panels”. From the menu, select “RvizVisualToolsGui” and click OK. You’ll see the new panel added to RViz.\nRViz enables you to save your configuration underFile->SaveConfig. You should do this before continuing on to the next tutorials. If you choose to save your configuration under a new name, you can useFile->SaveConfigAsand refer to your configuration file using:\nReplaceyour_rviz_config.rvizwith the name of the file you saved tomoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/and build the workspace so it can be found.\nInYour First MoveIt Project, you will create a C++ program using MoveIt to plan and execute moves.\n© Copyright 2024, PickNik Robotics.\nros2launchmoveit2_tutorialsdemo.launch.py\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=your_rviz_config.rviz\n/base_link\nrobot_description\n/monitored_planning_scene\n/display_planned_path\nmanipulator\n/monitored_planning_scene\nkinova_demo.launch\n0.1\nmoveit_config\njoint_limits.yaml\nmoveit_visual_tools\nFile->SaveConfig\nFile->SaveConfigAs\nyour_rviz_config.rviz\nmoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0a2"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#moving-joints-or-in-null-space",
  "title": "MoveIt Quickstart in RViz — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Quickstart in RViz\nGetting Started\nStep 1: Launch the Demo and Configure the Plugin\nStep 2: Play with the Visualized Robots\nStep 3: Interact with the Kinova Gen 3\nStep 4: Use Motion Planning with the Kinova Gen 3\nNext Steps\nThis tutorial will teach you how to create motion plans in MoveIt using RViz and the MoveIt Display plugin. Rviz is the primary visualizer in ROS and a very useful tool for debugging robotics. The MoveIt Display plugin allows you to setup virtual environments (planning scenes), create start and goal states for the robot interactively, test various motion planners, and visualize the output. Let’s get started!\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Startedor ourDocker Guide.\nIf you followed the Docker Guide, also follow theCreate A Colcon Workspace and Download Tutorialsguide onwards to set up the tutorials.\nLaunch the demo:\nIf you are doing this for the first time, you should see an empty world in RViz and will have to add the Motion Planning Plugin:\nYou should see an empty world in RViz:\n\nIn the RViz Displays Tab, pressAdd:\n\nFrom the moveit_ros_visualization folder, choose “MotionPlanning” as the DisplayType. Press “Ok”.\n\nYou should now see the Kinova robot in RViz:\n\nOnce you have the Motion Planning Plugin loaded, we can configure it. In the “Global Options” tab of the “Displays” subwindow, set theFixed Framefield to/base_link\nNow, you can start configuring the Plugin for your robot (the Kinova Gen 3 in this case). Click on “MotionPlanning” within “Displays”.\nMake sure theRobot Descriptionfield is set torobot_description.\nMake sure thePlanning Scene Topicfield is set to/monitored_planning_scene.\nClick on topic name to expose topic-name drop-down.\nMake sure theTrajectory TopicunderPlanned Pathis set to/display_planned_path.\nInPlanning Request, change thePlanning Grouptomanipulator. You can also see this in the MotionPlanning panel in the bottom left.\nThere are four different overlapping visualizations:\nThe robot’s configuration in the/monitored_planning_sceneplanning environment (active by default).\nThe planned path for the robot (active by default).\nGreen: The start state for motion planning (disabled by default).\nOrange: The goal state for motion planning (active by default).\nThe display states for each of these visualizations can be toggled on and off using checkboxes:\nThe planning scene robot using theShow Robot Visualcheckbox in theScene Robottree menu.\nThe planned path using theShow Robot Visualcheckbox in thePlanned Pathtree menu.\nThe start state using theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThe goal state using theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nPlay with all these checkboxes to switch on and off different visualizations.\nFor the next steps we will want only the scene robot, start state and goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theShow Robot Visualcheckbox in theScene Robottree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nCheck theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThere should now be two interactive markers. One marker corresponding to the orange colored arm will be used to set the “Goal State” for motion planning and the other marker corresponding to a green colored arm are used to set the “Start State” for motion planning. If you don’t see the interactive markers pressInteractin the top menu of RViz (Note: some tools may be hidden, press“+”in the top menu to add theInteracttool as shown below).\nYou should now be able to use these markers to drag the arm around and change its orientation. Try it!\nFor this section, hide the planned path and the goal state:\nUn-check theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nNow, only the Start State (the green colored arm) should be visible.  Try to move the arm into a configuration where two of its links are in collision with each other.  (If you find this difficult, make sure the “Use Collision-Aware IK” checkbox under the Planning tab of the MotionPlanning plugin is un-checked.)  Once you do this, the links that are in collision will turn red.\nNow, check the “Use Collision-Aware IK” checkbox, and try again to move two of the links into collision with each other.  When the checkbox is ticked, the IK solver will keep attempting to find a collision-free solution for the desired end-effector pose. When it is not checked, the solver will allow collisions to happen in the solution. The links in collision will always still be visualized in red, regardless of the state of the checkbox.\nNote what happens when you try to move an end-effector out of its reachable workspace.\nBefore moving onto the next section, re-enable the planned path and the goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nYou can use theJointstab to move single joints and the redundant joints of 7-DOF robots. Try moving the “null space exploration” slider as shown in the animation below.\nNow, you can start motion planning with the Kinova Gen 3 in the MoveIt RViz Plugin.\nMove the Start State to a desired location.\nMove the Goal State to another desired location.\nMake sure both states are not in collision with the robot itself.\nUn-check theShow Trailcheckbox in thePlanned Pathtree menu.\nIn theMotionPlanningwindow under thePlanningtab, press thePlanbutton.\nCheck theShow Trailcheckbox in thePlanned Pathtree menu. You should see the arm’s path represented by a series of manipulator poses.\nYou can visually introspect trajectories point by point in RViz.\nFrom “Panels” menu, select “Trajectory - Trajectory Slider”. You’ll see a new Slider panel on RViz.\nSet your goal pose, then runPlan.\nPlay with the “Slider” panel, e.g. move the slider, push “Play” button.\nNote: Once you placed your end-effector to a new goal, be sure to runPlanbefore runningPlay– otherwise you’ll see the waypoints for the previous goal if available.\nIf the “Use Cartesian Path” checkbox is activated, the robot will attempt to move the end effector linearly in cartesian space.\nClicking “Plan & Execute” or “Execute” after a successful plan will send the trajectory to the robot - in this tutorial, since you usedkinova_demo.launch, the robot is only simulated.\nInitially, the default velocity and acceleration are scaled to 10% (0.1) of the robot’s maximum. You can change these scaling factors in the Planning tab shown below, or change these default values in themoveit_configof your robot (injoint_limits.yaml).\nMany of the tutorials usemoveit_visual_toolsto step through a demo. Before continuing on to the next tutorials it is a good idea to enable theRvizVisualToolsGui.\nFrom “Panels” menu, select “Add New Panels”. From the menu, select “RvizVisualToolsGui” and click OK. You’ll see the new panel added to RViz.\nRViz enables you to save your configuration underFile->SaveConfig. You should do this before continuing on to the next tutorials. If you choose to save your configuration under a new name, you can useFile->SaveConfigAsand refer to your configuration file using:\nReplaceyour_rviz_config.rvizwith the name of the file you saved tomoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/and build the workspace so it can be found.\nInYour First MoveIt Project, you will create a C++ program using MoveIt to plan and execute moves.\n© Copyright 2024, PickNik Robotics.\nros2launchmoveit2_tutorialsdemo.launch.py\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=your_rviz_config.rviz\n/base_link\nrobot_description\n/monitored_planning_scene\n/display_planned_path\nmanipulator\n/monitored_planning_scene\nkinova_demo.launch\n0.1\nmoveit_config\njoint_limits.yaml\nmoveit_visual_tools\nFile->SaveConfig\nFile->SaveConfigAs\nyour_rviz_config.rviz\nmoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0a3"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#step-4-use-motion-planning-with-the-kinova-gen-3",
  "title": "MoveIt Quickstart in RViz — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Quickstart in RViz\nGetting Started\nStep 1: Launch the Demo and Configure the Plugin\nStep 2: Play with the Visualized Robots\nStep 3: Interact with the Kinova Gen 3\nStep 4: Use Motion Planning with the Kinova Gen 3\nNext Steps\nThis tutorial will teach you how to create motion plans in MoveIt using RViz and the MoveIt Display plugin. Rviz is the primary visualizer in ROS and a very useful tool for debugging robotics. The MoveIt Display plugin allows you to setup virtual environments (planning scenes), create start and goal states for the robot interactively, test various motion planners, and visualize the output. Let’s get started!\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Startedor ourDocker Guide.\nIf you followed the Docker Guide, also follow theCreate A Colcon Workspace and Download Tutorialsguide onwards to set up the tutorials.\nLaunch the demo:\nIf you are doing this for the first time, you should see an empty world in RViz and will have to add the Motion Planning Plugin:\nYou should see an empty world in RViz:\n\nIn the RViz Displays Tab, pressAdd:\n\nFrom the moveit_ros_visualization folder, choose “MotionPlanning” as the DisplayType. Press “Ok”.\n\nYou should now see the Kinova robot in RViz:\n\nOnce you have the Motion Planning Plugin loaded, we can configure it. In the “Global Options” tab of the “Displays” subwindow, set theFixed Framefield to/base_link\nNow, you can start configuring the Plugin for your robot (the Kinova Gen 3 in this case). Click on “MotionPlanning” within “Displays”.\nMake sure theRobot Descriptionfield is set torobot_description.\nMake sure thePlanning Scene Topicfield is set to/monitored_planning_scene.\nClick on topic name to expose topic-name drop-down.\nMake sure theTrajectory TopicunderPlanned Pathis set to/display_planned_path.\nInPlanning Request, change thePlanning Grouptomanipulator. You can also see this in the MotionPlanning panel in the bottom left.\nThere are four different overlapping visualizations:\nThe robot’s configuration in the/monitored_planning_sceneplanning environment (active by default).\nThe planned path for the robot (active by default).\nGreen: The start state for motion planning (disabled by default).\nOrange: The goal state for motion planning (active by default).\nThe display states for each of these visualizations can be toggled on and off using checkboxes:\nThe planning scene robot using theShow Robot Visualcheckbox in theScene Robottree menu.\nThe planned path using theShow Robot Visualcheckbox in thePlanned Pathtree menu.\nThe start state using theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThe goal state using theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nPlay with all these checkboxes to switch on and off different visualizations.\nFor the next steps we will want only the scene robot, start state and goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theShow Robot Visualcheckbox in theScene Robottree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nCheck theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThere should now be two interactive markers. One marker corresponding to the orange colored arm will be used to set the “Goal State” for motion planning and the other marker corresponding to a green colored arm are used to set the “Start State” for motion planning. If you don’t see the interactive markers pressInteractin the top menu of RViz (Note: some tools may be hidden, press“+”in the top menu to add theInteracttool as shown below).\nYou should now be able to use these markers to drag the arm around and change its orientation. Try it!\nFor this section, hide the planned path and the goal state:\nUn-check theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nNow, only the Start State (the green colored arm) should be visible.  Try to move the arm into a configuration where two of its links are in collision with each other.  (If you find this difficult, make sure the “Use Collision-Aware IK” checkbox under the Planning tab of the MotionPlanning plugin is un-checked.)  Once you do this, the links that are in collision will turn red.\nNow, check the “Use Collision-Aware IK” checkbox, and try again to move two of the links into collision with each other.  When the checkbox is ticked, the IK solver will keep attempting to find a collision-free solution for the desired end-effector pose. When it is not checked, the solver will allow collisions to happen in the solution. The links in collision will always still be visualized in red, regardless of the state of the checkbox.\nNote what happens when you try to move an end-effector out of its reachable workspace.\nBefore moving onto the next section, re-enable the planned path and the goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nYou can use theJointstab to move single joints and the redundant joints of 7-DOF robots. Try moving the “null space exploration” slider as shown in the animation below.\nNow, you can start motion planning with the Kinova Gen 3 in the MoveIt RViz Plugin.\nMove the Start State to a desired location.\nMove the Goal State to another desired location.\nMake sure both states are not in collision with the robot itself.\nUn-check theShow Trailcheckbox in thePlanned Pathtree menu.\nIn theMotionPlanningwindow under thePlanningtab, press thePlanbutton.\nCheck theShow Trailcheckbox in thePlanned Pathtree menu. You should see the arm’s path represented by a series of manipulator poses.\nYou can visually introspect trajectories point by point in RViz.\nFrom “Panels” menu, select “Trajectory - Trajectory Slider”. You’ll see a new Slider panel on RViz.\nSet your goal pose, then runPlan.\nPlay with the “Slider” panel, e.g. move the slider, push “Play” button.\nNote: Once you placed your end-effector to a new goal, be sure to runPlanbefore runningPlay– otherwise you’ll see the waypoints for the previous goal if available.\nIf the “Use Cartesian Path” checkbox is activated, the robot will attempt to move the end effector linearly in cartesian space.\nClicking “Plan & Execute” or “Execute” after a successful plan will send the trajectory to the robot - in this tutorial, since you usedkinova_demo.launch, the robot is only simulated.\nInitially, the default velocity and acceleration are scaled to 10% (0.1) of the robot’s maximum. You can change these scaling factors in the Planning tab shown below, or change these default values in themoveit_configof your robot (injoint_limits.yaml).\nMany of the tutorials usemoveit_visual_toolsto step through a demo. Before continuing on to the next tutorials it is a good idea to enable theRvizVisualToolsGui.\nFrom “Panels” menu, select “Add New Panels”. From the menu, select “RvizVisualToolsGui” and click OK. You’ll see the new panel added to RViz.\nRViz enables you to save your configuration underFile->SaveConfig. You should do this before continuing on to the next tutorials. If you choose to save your configuration under a new name, you can useFile->SaveConfigAsand refer to your configuration file using:\nReplaceyour_rviz_config.rvizwith the name of the file you saved tomoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/and build the workspace so it can be found.\nInYour First MoveIt Project, you will create a C++ program using MoveIt to plan and execute moves.\n© Copyright 2024, PickNik Robotics.\nros2launchmoveit2_tutorialsdemo.launch.py\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=your_rviz_config.rviz\n/base_link\nrobot_description\n/monitored_planning_scene\n/display_planned_path\nmanipulator\n/monitored_planning_scene\nkinova_demo.launch\n0.1\nmoveit_config\njoint_limits.yaml\nmoveit_visual_tools\nFile->SaveConfig\nFile->SaveConfigAs\nyour_rviz_config.rviz\nmoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0a4"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#introspecting-trajectory-waypoints",
  "title": "MoveIt Quickstart in RViz — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Quickstart in RViz\nGetting Started\nStep 1: Launch the Demo and Configure the Plugin\nStep 2: Play with the Visualized Robots\nStep 3: Interact with the Kinova Gen 3\nStep 4: Use Motion Planning with the Kinova Gen 3\nNext Steps\nThis tutorial will teach you how to create motion plans in MoveIt using RViz and the MoveIt Display plugin. Rviz is the primary visualizer in ROS and a very useful tool for debugging robotics. The MoveIt Display plugin allows you to setup virtual environments (planning scenes), create start and goal states for the robot interactively, test various motion planners, and visualize the output. Let’s get started!\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Startedor ourDocker Guide.\nIf you followed the Docker Guide, also follow theCreate A Colcon Workspace and Download Tutorialsguide onwards to set up the tutorials.\nLaunch the demo:\nIf you are doing this for the first time, you should see an empty world in RViz and will have to add the Motion Planning Plugin:\nYou should see an empty world in RViz:\n\nIn the RViz Displays Tab, pressAdd:\n\nFrom the moveit_ros_visualization folder, choose “MotionPlanning” as the DisplayType. Press “Ok”.\n\nYou should now see the Kinova robot in RViz:\n\nOnce you have the Motion Planning Plugin loaded, we can configure it. In the “Global Options” tab of the “Displays” subwindow, set theFixed Framefield to/base_link\nNow, you can start configuring the Plugin for your robot (the Kinova Gen 3 in this case). Click on “MotionPlanning” within “Displays”.\nMake sure theRobot Descriptionfield is set torobot_description.\nMake sure thePlanning Scene Topicfield is set to/monitored_planning_scene.\nClick on topic name to expose topic-name drop-down.\nMake sure theTrajectory TopicunderPlanned Pathis set to/display_planned_path.\nInPlanning Request, change thePlanning Grouptomanipulator. You can also see this in the MotionPlanning panel in the bottom left.\nThere are four different overlapping visualizations:\nThe robot’s configuration in the/monitored_planning_sceneplanning environment (active by default).\nThe planned path for the robot (active by default).\nGreen: The start state for motion planning (disabled by default).\nOrange: The goal state for motion planning (active by default).\nThe display states for each of these visualizations can be toggled on and off using checkboxes:\nThe planning scene robot using theShow Robot Visualcheckbox in theScene Robottree menu.\nThe planned path using theShow Robot Visualcheckbox in thePlanned Pathtree menu.\nThe start state using theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThe goal state using theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nPlay with all these checkboxes to switch on and off different visualizations.\nFor the next steps we will want only the scene robot, start state and goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theShow Robot Visualcheckbox in theScene Robottree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nCheck theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThere should now be two interactive markers. One marker corresponding to the orange colored arm will be used to set the “Goal State” for motion planning and the other marker corresponding to a green colored arm are used to set the “Start State” for motion planning. If you don’t see the interactive markers pressInteractin the top menu of RViz (Note: some tools may be hidden, press“+”in the top menu to add theInteracttool as shown below).\nYou should now be able to use these markers to drag the arm around and change its orientation. Try it!\nFor this section, hide the planned path and the goal state:\nUn-check theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nNow, only the Start State (the green colored arm) should be visible.  Try to move the arm into a configuration where two of its links are in collision with each other.  (If you find this difficult, make sure the “Use Collision-Aware IK” checkbox under the Planning tab of the MotionPlanning plugin is un-checked.)  Once you do this, the links that are in collision will turn red.\nNow, check the “Use Collision-Aware IK” checkbox, and try again to move two of the links into collision with each other.  When the checkbox is ticked, the IK solver will keep attempting to find a collision-free solution for the desired end-effector pose. When it is not checked, the solver will allow collisions to happen in the solution. The links in collision will always still be visualized in red, regardless of the state of the checkbox.\nNote what happens when you try to move an end-effector out of its reachable workspace.\nBefore moving onto the next section, re-enable the planned path and the goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nYou can use theJointstab to move single joints and the redundant joints of 7-DOF robots. Try moving the “null space exploration” slider as shown in the animation below.\nNow, you can start motion planning with the Kinova Gen 3 in the MoveIt RViz Plugin.\nMove the Start State to a desired location.\nMove the Goal State to another desired location.\nMake sure both states are not in collision with the robot itself.\nUn-check theShow Trailcheckbox in thePlanned Pathtree menu.\nIn theMotionPlanningwindow under thePlanningtab, press thePlanbutton.\nCheck theShow Trailcheckbox in thePlanned Pathtree menu. You should see the arm’s path represented by a series of manipulator poses.\nYou can visually introspect trajectories point by point in RViz.\nFrom “Panels” menu, select “Trajectory - Trajectory Slider”. You’ll see a new Slider panel on RViz.\nSet your goal pose, then runPlan.\nPlay with the “Slider” panel, e.g. move the slider, push “Play” button.\nNote: Once you placed your end-effector to a new goal, be sure to runPlanbefore runningPlay– otherwise you’ll see the waypoints for the previous goal if available.\nIf the “Use Cartesian Path” checkbox is activated, the robot will attempt to move the end effector linearly in cartesian space.\nClicking “Plan & Execute” or “Execute” after a successful plan will send the trajectory to the robot - in this tutorial, since you usedkinova_demo.launch, the robot is only simulated.\nInitially, the default velocity and acceleration are scaled to 10% (0.1) of the robot’s maximum. You can change these scaling factors in the Planning tab shown below, or change these default values in themoveit_configof your robot (injoint_limits.yaml).\nMany of the tutorials usemoveit_visual_toolsto step through a demo. Before continuing on to the next tutorials it is a good idea to enable theRvizVisualToolsGui.\nFrom “Panels” menu, select “Add New Panels”. From the menu, select “RvizVisualToolsGui” and click OK. You’ll see the new panel added to RViz.\nRViz enables you to save your configuration underFile->SaveConfig. You should do this before continuing on to the next tutorials. If you choose to save your configuration under a new name, you can useFile->SaveConfigAsand refer to your configuration file using:\nReplaceyour_rviz_config.rvizwith the name of the file you saved tomoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/and build the workspace so it can be found.\nInYour First MoveIt Project, you will create a C++ program using MoveIt to plan and execute moves.\n© Copyright 2024, PickNik Robotics.\nros2launchmoveit2_tutorialsdemo.launch.py\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=your_rviz_config.rviz\n/base_link\nrobot_description\n/monitored_planning_scene\n/display_planned_path\nmanipulator\n/monitored_planning_scene\nkinova_demo.launch\n0.1\nmoveit_config\njoint_limits.yaml\nmoveit_visual_tools\nFile->SaveConfig\nFile->SaveConfigAs\nyour_rviz_config.rviz\nmoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0a5"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#plan-cartesian-motions",
  "title": "MoveIt Quickstart in RViz — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Quickstart in RViz\nGetting Started\nStep 1: Launch the Demo and Configure the Plugin\nStep 2: Play with the Visualized Robots\nStep 3: Interact with the Kinova Gen 3\nStep 4: Use Motion Planning with the Kinova Gen 3\nNext Steps\nThis tutorial will teach you how to create motion plans in MoveIt using RViz and the MoveIt Display plugin. Rviz is the primary visualizer in ROS and a very useful tool for debugging robotics. The MoveIt Display plugin allows you to setup virtual environments (planning scenes), create start and goal states for the robot interactively, test various motion planners, and visualize the output. Let’s get started!\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Startedor ourDocker Guide.\nIf you followed the Docker Guide, also follow theCreate A Colcon Workspace and Download Tutorialsguide onwards to set up the tutorials.\nLaunch the demo:\nIf you are doing this for the first time, you should see an empty world in RViz and will have to add the Motion Planning Plugin:\nYou should see an empty world in RViz:\n\nIn the RViz Displays Tab, pressAdd:\n\nFrom the moveit_ros_visualization folder, choose “MotionPlanning” as the DisplayType. Press “Ok”.\n\nYou should now see the Kinova robot in RViz:\n\nOnce you have the Motion Planning Plugin loaded, we can configure it. In the “Global Options” tab of the “Displays” subwindow, set theFixed Framefield to/base_link\nNow, you can start configuring the Plugin for your robot (the Kinova Gen 3 in this case). Click on “MotionPlanning” within “Displays”.\nMake sure theRobot Descriptionfield is set torobot_description.\nMake sure thePlanning Scene Topicfield is set to/monitored_planning_scene.\nClick on topic name to expose topic-name drop-down.\nMake sure theTrajectory TopicunderPlanned Pathis set to/display_planned_path.\nInPlanning Request, change thePlanning Grouptomanipulator. You can also see this in the MotionPlanning panel in the bottom left.\nThere are four different overlapping visualizations:\nThe robot’s configuration in the/monitored_planning_sceneplanning environment (active by default).\nThe planned path for the robot (active by default).\nGreen: The start state for motion planning (disabled by default).\nOrange: The goal state for motion planning (active by default).\nThe display states for each of these visualizations can be toggled on and off using checkboxes:\nThe planning scene robot using theShow Robot Visualcheckbox in theScene Robottree menu.\nThe planned path using theShow Robot Visualcheckbox in thePlanned Pathtree menu.\nThe start state using theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThe goal state using theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nPlay with all these checkboxes to switch on and off different visualizations.\nFor the next steps we will want only the scene robot, start state and goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theShow Robot Visualcheckbox in theScene Robottree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nCheck theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThere should now be two interactive markers. One marker corresponding to the orange colored arm will be used to set the “Goal State” for motion planning and the other marker corresponding to a green colored arm are used to set the “Start State” for motion planning. If you don’t see the interactive markers pressInteractin the top menu of RViz (Note: some tools may be hidden, press“+”in the top menu to add theInteracttool as shown below).\nYou should now be able to use these markers to drag the arm around and change its orientation. Try it!\nFor this section, hide the planned path and the goal state:\nUn-check theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nNow, only the Start State (the green colored arm) should be visible.  Try to move the arm into a configuration where two of its links are in collision with each other.  (If you find this difficult, make sure the “Use Collision-Aware IK” checkbox under the Planning tab of the MotionPlanning plugin is un-checked.)  Once you do this, the links that are in collision will turn red.\nNow, check the “Use Collision-Aware IK” checkbox, and try again to move two of the links into collision with each other.  When the checkbox is ticked, the IK solver will keep attempting to find a collision-free solution for the desired end-effector pose. When it is not checked, the solver will allow collisions to happen in the solution. The links in collision will always still be visualized in red, regardless of the state of the checkbox.\nNote what happens when you try to move an end-effector out of its reachable workspace.\nBefore moving onto the next section, re-enable the planned path and the goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nYou can use theJointstab to move single joints and the redundant joints of 7-DOF robots. Try moving the “null space exploration” slider as shown in the animation below.\nNow, you can start motion planning with the Kinova Gen 3 in the MoveIt RViz Plugin.\nMove the Start State to a desired location.\nMove the Goal State to another desired location.\nMake sure both states are not in collision with the robot itself.\nUn-check theShow Trailcheckbox in thePlanned Pathtree menu.\nIn theMotionPlanningwindow under thePlanningtab, press thePlanbutton.\nCheck theShow Trailcheckbox in thePlanned Pathtree menu. You should see the arm’s path represented by a series of manipulator poses.\nYou can visually introspect trajectories point by point in RViz.\nFrom “Panels” menu, select “Trajectory - Trajectory Slider”. You’ll see a new Slider panel on RViz.\nSet your goal pose, then runPlan.\nPlay with the “Slider” panel, e.g. move the slider, push “Play” button.\nNote: Once you placed your end-effector to a new goal, be sure to runPlanbefore runningPlay– otherwise you’ll see the waypoints for the previous goal if available.\nIf the “Use Cartesian Path” checkbox is activated, the robot will attempt to move the end effector linearly in cartesian space.\nClicking “Plan & Execute” or “Execute” after a successful plan will send the trajectory to the robot - in this tutorial, since you usedkinova_demo.launch, the robot is only simulated.\nInitially, the default velocity and acceleration are scaled to 10% (0.1) of the robot’s maximum. You can change these scaling factors in the Planning tab shown below, or change these default values in themoveit_configof your robot (injoint_limits.yaml).\nMany of the tutorials usemoveit_visual_toolsto step through a demo. Before continuing on to the next tutorials it is a good idea to enable theRvizVisualToolsGui.\nFrom “Panels” menu, select “Add New Panels”. From the menu, select “RvizVisualToolsGui” and click OK. You’ll see the new panel added to RViz.\nRViz enables you to save your configuration underFile->SaveConfig. You should do this before continuing on to the next tutorials. If you choose to save your configuration under a new name, you can useFile->SaveConfigAsand refer to your configuration file using:\nReplaceyour_rviz_config.rvizwith the name of the file you saved tomoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/and build the workspace so it can be found.\nInYour First MoveIt Project, you will create a C++ program using MoveIt to plan and execute moves.\n© Copyright 2024, PickNik Robotics.\nros2launchmoveit2_tutorialsdemo.launch.py\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=your_rviz_config.rviz\n/base_link\nrobot_description\n/monitored_planning_scene\n/display_planned_path\nmanipulator\n/monitored_planning_scene\nkinova_demo.launch\n0.1\nmoveit_config\njoint_limits.yaml\nmoveit_visual_tools\nFile->SaveConfig\nFile->SaveConfigAs\nyour_rviz_config.rviz\nmoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0a6"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#executing-trajectories-adjusting-speed",
  "title": "MoveIt Quickstart in RViz — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Quickstart in RViz\nGetting Started\nStep 1: Launch the Demo and Configure the Plugin\nStep 2: Play with the Visualized Robots\nStep 3: Interact with the Kinova Gen 3\nStep 4: Use Motion Planning with the Kinova Gen 3\nNext Steps\nThis tutorial will teach you how to create motion plans in MoveIt using RViz and the MoveIt Display plugin. Rviz is the primary visualizer in ROS and a very useful tool for debugging robotics. The MoveIt Display plugin allows you to setup virtual environments (planning scenes), create start and goal states for the robot interactively, test various motion planners, and visualize the output. Let’s get started!\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Startedor ourDocker Guide.\nIf you followed the Docker Guide, also follow theCreate A Colcon Workspace and Download Tutorialsguide onwards to set up the tutorials.\nLaunch the demo:\nIf you are doing this for the first time, you should see an empty world in RViz and will have to add the Motion Planning Plugin:\nYou should see an empty world in RViz:\n\nIn the RViz Displays Tab, pressAdd:\n\nFrom the moveit_ros_visualization folder, choose “MotionPlanning” as the DisplayType. Press “Ok”.\n\nYou should now see the Kinova robot in RViz:\n\nOnce you have the Motion Planning Plugin loaded, we can configure it. In the “Global Options” tab of the “Displays” subwindow, set theFixed Framefield to/base_link\nNow, you can start configuring the Plugin for your robot (the Kinova Gen 3 in this case). Click on “MotionPlanning” within “Displays”.\nMake sure theRobot Descriptionfield is set torobot_description.\nMake sure thePlanning Scene Topicfield is set to/monitored_planning_scene.\nClick on topic name to expose topic-name drop-down.\nMake sure theTrajectory TopicunderPlanned Pathis set to/display_planned_path.\nInPlanning Request, change thePlanning Grouptomanipulator. You can also see this in the MotionPlanning panel in the bottom left.\nThere are four different overlapping visualizations:\nThe robot’s configuration in the/monitored_planning_sceneplanning environment (active by default).\nThe planned path for the robot (active by default).\nGreen: The start state for motion planning (disabled by default).\nOrange: The goal state for motion planning (active by default).\nThe display states for each of these visualizations can be toggled on and off using checkboxes:\nThe planning scene robot using theShow Robot Visualcheckbox in theScene Robottree menu.\nThe planned path using theShow Robot Visualcheckbox in thePlanned Pathtree menu.\nThe start state using theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThe goal state using theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nPlay with all these checkboxes to switch on and off different visualizations.\nFor the next steps we will want only the scene robot, start state and goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theShow Robot Visualcheckbox in theScene Robottree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nCheck theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThere should now be two interactive markers. One marker corresponding to the orange colored arm will be used to set the “Goal State” for motion planning and the other marker corresponding to a green colored arm are used to set the “Start State” for motion planning. If you don’t see the interactive markers pressInteractin the top menu of RViz (Note: some tools may be hidden, press“+”in the top menu to add theInteracttool as shown below).\nYou should now be able to use these markers to drag the arm around and change its orientation. Try it!\nFor this section, hide the planned path and the goal state:\nUn-check theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nNow, only the Start State (the green colored arm) should be visible.  Try to move the arm into a configuration where two of its links are in collision with each other.  (If you find this difficult, make sure the “Use Collision-Aware IK” checkbox under the Planning tab of the MotionPlanning plugin is un-checked.)  Once you do this, the links that are in collision will turn red.\nNow, check the “Use Collision-Aware IK” checkbox, and try again to move two of the links into collision with each other.  When the checkbox is ticked, the IK solver will keep attempting to find a collision-free solution for the desired end-effector pose. When it is not checked, the solver will allow collisions to happen in the solution. The links in collision will always still be visualized in red, regardless of the state of the checkbox.\nNote what happens when you try to move an end-effector out of its reachable workspace.\nBefore moving onto the next section, re-enable the planned path and the goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nYou can use theJointstab to move single joints and the redundant joints of 7-DOF robots. Try moving the “null space exploration” slider as shown in the animation below.\nNow, you can start motion planning with the Kinova Gen 3 in the MoveIt RViz Plugin.\nMove the Start State to a desired location.\nMove the Goal State to another desired location.\nMake sure both states are not in collision with the robot itself.\nUn-check theShow Trailcheckbox in thePlanned Pathtree menu.\nIn theMotionPlanningwindow under thePlanningtab, press thePlanbutton.\nCheck theShow Trailcheckbox in thePlanned Pathtree menu. You should see the arm’s path represented by a series of manipulator poses.\nYou can visually introspect trajectories point by point in RViz.\nFrom “Panels” menu, select “Trajectory - Trajectory Slider”. You’ll see a new Slider panel on RViz.\nSet your goal pose, then runPlan.\nPlay with the “Slider” panel, e.g. move the slider, push “Play” button.\nNote: Once you placed your end-effector to a new goal, be sure to runPlanbefore runningPlay– otherwise you’ll see the waypoints for the previous goal if available.\nIf the “Use Cartesian Path” checkbox is activated, the robot will attempt to move the end effector linearly in cartesian space.\nClicking “Plan & Execute” or “Execute” after a successful plan will send the trajectory to the robot - in this tutorial, since you usedkinova_demo.launch, the robot is only simulated.\nInitially, the default velocity and acceleration are scaled to 10% (0.1) of the robot’s maximum. You can change these scaling factors in the Planning tab shown below, or change these default values in themoveit_configof your robot (injoint_limits.yaml).\nMany of the tutorials usemoveit_visual_toolsto step through a demo. Before continuing on to the next tutorials it is a good idea to enable theRvizVisualToolsGui.\nFrom “Panels” menu, select “Add New Panels”. From the menu, select “RvizVisualToolsGui” and click OK. You’ll see the new panel added to RViz.\nRViz enables you to save your configuration underFile->SaveConfig. You should do this before continuing on to the next tutorials. If you choose to save your configuration under a new name, you can useFile->SaveConfigAsand refer to your configuration file using:\nReplaceyour_rviz_config.rvizwith the name of the file you saved tomoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/and build the workspace so it can be found.\nInYour First MoveIt Project, you will create a C++ program using MoveIt to plan and execute moves.\n© Copyright 2024, PickNik Robotics.\nros2launchmoveit2_tutorialsdemo.launch.py\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=your_rviz_config.rviz\n/base_link\nrobot_description\n/monitored_planning_scene\n/display_planned_path\nmanipulator\n/monitored_planning_scene\nkinova_demo.launch\n0.1\nmoveit_config\njoint_limits.yaml\nmoveit_visual_tools\nFile->SaveConfig\nFile->SaveConfigAs\nyour_rviz_config.rviz\nmoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0a7"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#next-steps",
  "title": "MoveIt Quickstart in RViz — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Quickstart in RViz\nGetting Started\nStep 1: Launch the Demo and Configure the Plugin\nStep 2: Play with the Visualized Robots\nStep 3: Interact with the Kinova Gen 3\nStep 4: Use Motion Planning with the Kinova Gen 3\nNext Steps\nThis tutorial will teach you how to create motion plans in MoveIt using RViz and the MoveIt Display plugin. Rviz is the primary visualizer in ROS and a very useful tool for debugging robotics. The MoveIt Display plugin allows you to setup virtual environments (planning scenes), create start and goal states for the robot interactively, test various motion planners, and visualize the output. Let’s get started!\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Startedor ourDocker Guide.\nIf you followed the Docker Guide, also follow theCreate A Colcon Workspace and Download Tutorialsguide onwards to set up the tutorials.\nLaunch the demo:\nIf you are doing this for the first time, you should see an empty world in RViz and will have to add the Motion Planning Plugin:\nYou should see an empty world in RViz:\n\nIn the RViz Displays Tab, pressAdd:\n\nFrom the moveit_ros_visualization folder, choose “MotionPlanning” as the DisplayType. Press “Ok”.\n\nYou should now see the Kinova robot in RViz:\n\nOnce you have the Motion Planning Plugin loaded, we can configure it. In the “Global Options” tab of the “Displays” subwindow, set theFixed Framefield to/base_link\nNow, you can start configuring the Plugin for your robot (the Kinova Gen 3 in this case). Click on “MotionPlanning” within “Displays”.\nMake sure theRobot Descriptionfield is set torobot_description.\nMake sure thePlanning Scene Topicfield is set to/monitored_planning_scene.\nClick on topic name to expose topic-name drop-down.\nMake sure theTrajectory TopicunderPlanned Pathis set to/display_planned_path.\nInPlanning Request, change thePlanning Grouptomanipulator. You can also see this in the MotionPlanning panel in the bottom left.\nThere are four different overlapping visualizations:\nThe robot’s configuration in the/monitored_planning_sceneplanning environment (active by default).\nThe planned path for the robot (active by default).\nGreen: The start state for motion planning (disabled by default).\nOrange: The goal state for motion planning (active by default).\nThe display states for each of these visualizations can be toggled on and off using checkboxes:\nThe planning scene robot using theShow Robot Visualcheckbox in theScene Robottree menu.\nThe planned path using theShow Robot Visualcheckbox in thePlanned Pathtree menu.\nThe start state using theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThe goal state using theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nPlay with all these checkboxes to switch on and off different visualizations.\nFor the next steps we will want only the scene robot, start state and goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theShow Robot Visualcheckbox in theScene Robottree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nCheck theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThere should now be two interactive markers. One marker corresponding to the orange colored arm will be used to set the “Goal State” for motion planning and the other marker corresponding to a green colored arm are used to set the “Start State” for motion planning. If you don’t see the interactive markers pressInteractin the top menu of RViz (Note: some tools may be hidden, press“+”in the top menu to add theInteracttool as shown below).\nYou should now be able to use these markers to drag the arm around and change its orientation. Try it!\nFor this section, hide the planned path and the goal state:\nUn-check theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nNow, only the Start State (the green colored arm) should be visible.  Try to move the arm into a configuration where two of its links are in collision with each other.  (If you find this difficult, make sure the “Use Collision-Aware IK” checkbox under the Planning tab of the MotionPlanning plugin is un-checked.)  Once you do this, the links that are in collision will turn red.\nNow, check the “Use Collision-Aware IK” checkbox, and try again to move two of the links into collision with each other.  When the checkbox is ticked, the IK solver will keep attempting to find a collision-free solution for the desired end-effector pose. When it is not checked, the solver will allow collisions to happen in the solution. The links in collision will always still be visualized in red, regardless of the state of the checkbox.\nNote what happens when you try to move an end-effector out of its reachable workspace.\nBefore moving onto the next section, re-enable the planned path and the goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nYou can use theJointstab to move single joints and the redundant joints of 7-DOF robots. Try moving the “null space exploration” slider as shown in the animation below.\nNow, you can start motion planning with the Kinova Gen 3 in the MoveIt RViz Plugin.\nMove the Start State to a desired location.\nMove the Goal State to another desired location.\nMake sure both states are not in collision with the robot itself.\nUn-check theShow Trailcheckbox in thePlanned Pathtree menu.\nIn theMotionPlanningwindow under thePlanningtab, press thePlanbutton.\nCheck theShow Trailcheckbox in thePlanned Pathtree menu. You should see the arm’s path represented by a series of manipulator poses.\nYou can visually introspect trajectories point by point in RViz.\nFrom “Panels” menu, select “Trajectory - Trajectory Slider”. You’ll see a new Slider panel on RViz.\nSet your goal pose, then runPlan.\nPlay with the “Slider” panel, e.g. move the slider, push “Play” button.\nNote: Once you placed your end-effector to a new goal, be sure to runPlanbefore runningPlay– otherwise you’ll see the waypoints for the previous goal if available.\nIf the “Use Cartesian Path” checkbox is activated, the robot will attempt to move the end effector linearly in cartesian space.\nClicking “Plan & Execute” or “Execute” after a successful plan will send the trajectory to the robot - in this tutorial, since you usedkinova_demo.launch, the robot is only simulated.\nInitially, the default velocity and acceleration are scaled to 10% (0.1) of the robot’s maximum. You can change these scaling factors in the Planning tab shown below, or change these default values in themoveit_configof your robot (injoint_limits.yaml).\nMany of the tutorials usemoveit_visual_toolsto step through a demo. Before continuing on to the next tutorials it is a good idea to enable theRvizVisualToolsGui.\nFrom “Panels” menu, select “Add New Panels”. From the menu, select “RvizVisualToolsGui” and click OK. You’ll see the new panel added to RViz.\nRViz enables you to save your configuration underFile->SaveConfig. You should do this before continuing on to the next tutorials. If you choose to save your configuration under a new name, you can useFile->SaveConfigAsand refer to your configuration file using:\nReplaceyour_rviz_config.rvizwith the name of the file you saved tomoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/and build the workspace so it can be found.\nInYour First MoveIt Project, you will create a C++ program using MoveIt to plan and execute moves.\n© Copyright 2024, PickNik Robotics.\nros2launchmoveit2_tutorialsdemo.launch.py\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=your_rviz_config.rviz\n/base_link\nrobot_description\n/monitored_planning_scene\n/display_planned_path\nmanipulator\n/monitored_planning_scene\nkinova_demo.launch\n0.1\nmoveit_config\njoint_limits.yaml\nmoveit_visual_tools\nFile->SaveConfig\nFile->SaveConfigAs\nyour_rviz_config.rviz\nmoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0a8"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#rviz-visual-tools",
  "title": "MoveIt Quickstart in RViz — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Quickstart in RViz\nGetting Started\nStep 1: Launch the Demo and Configure the Plugin\nStep 2: Play with the Visualized Robots\nStep 3: Interact with the Kinova Gen 3\nStep 4: Use Motion Planning with the Kinova Gen 3\nNext Steps\nThis tutorial will teach you how to create motion plans in MoveIt using RViz and the MoveIt Display plugin. Rviz is the primary visualizer in ROS and a very useful tool for debugging robotics. The MoveIt Display plugin allows you to setup virtual environments (planning scenes), create start and goal states for the robot interactively, test various motion planners, and visualize the output. Let’s get started!\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Startedor ourDocker Guide.\nIf you followed the Docker Guide, also follow theCreate A Colcon Workspace and Download Tutorialsguide onwards to set up the tutorials.\nLaunch the demo:\nIf you are doing this for the first time, you should see an empty world in RViz and will have to add the Motion Planning Plugin:\nYou should see an empty world in RViz:\n\nIn the RViz Displays Tab, pressAdd:\n\nFrom the moveit_ros_visualization folder, choose “MotionPlanning” as the DisplayType. Press “Ok”.\n\nYou should now see the Kinova robot in RViz:\n\nOnce you have the Motion Planning Plugin loaded, we can configure it. In the “Global Options” tab of the “Displays” subwindow, set theFixed Framefield to/base_link\nNow, you can start configuring the Plugin for your robot (the Kinova Gen 3 in this case). Click on “MotionPlanning” within “Displays”.\nMake sure theRobot Descriptionfield is set torobot_description.\nMake sure thePlanning Scene Topicfield is set to/monitored_planning_scene.\nClick on topic name to expose topic-name drop-down.\nMake sure theTrajectory TopicunderPlanned Pathis set to/display_planned_path.\nInPlanning Request, change thePlanning Grouptomanipulator. You can also see this in the MotionPlanning panel in the bottom left.\nThere are four different overlapping visualizations:\nThe robot’s configuration in the/monitored_planning_sceneplanning environment (active by default).\nThe planned path for the robot (active by default).\nGreen: The start state for motion planning (disabled by default).\nOrange: The goal state for motion planning (active by default).\nThe display states for each of these visualizations can be toggled on and off using checkboxes:\nThe planning scene robot using theShow Robot Visualcheckbox in theScene Robottree menu.\nThe planned path using theShow Robot Visualcheckbox in thePlanned Pathtree menu.\nThe start state using theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThe goal state using theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nPlay with all these checkboxes to switch on and off different visualizations.\nFor the next steps we will want only the scene robot, start state and goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theShow Robot Visualcheckbox in theScene Robottree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nCheck theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThere should now be two interactive markers. One marker corresponding to the orange colored arm will be used to set the “Goal State” for motion planning and the other marker corresponding to a green colored arm are used to set the “Start State” for motion planning. If you don’t see the interactive markers pressInteractin the top menu of RViz (Note: some tools may be hidden, press“+”in the top menu to add theInteracttool as shown below).\nYou should now be able to use these markers to drag the arm around and change its orientation. Try it!\nFor this section, hide the planned path and the goal state:\nUn-check theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nNow, only the Start State (the green colored arm) should be visible.  Try to move the arm into a configuration where two of its links are in collision with each other.  (If you find this difficult, make sure the “Use Collision-Aware IK” checkbox under the Planning tab of the MotionPlanning plugin is un-checked.)  Once you do this, the links that are in collision will turn red.\nNow, check the “Use Collision-Aware IK” checkbox, and try again to move two of the links into collision with each other.  When the checkbox is ticked, the IK solver will keep attempting to find a collision-free solution for the desired end-effector pose. When it is not checked, the solver will allow collisions to happen in the solution. The links in collision will always still be visualized in red, regardless of the state of the checkbox.\nNote what happens when you try to move an end-effector out of its reachable workspace.\nBefore moving onto the next section, re-enable the planned path and the goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nYou can use theJointstab to move single joints and the redundant joints of 7-DOF robots. Try moving the “null space exploration” slider as shown in the animation below.\nNow, you can start motion planning with the Kinova Gen 3 in the MoveIt RViz Plugin.\nMove the Start State to a desired location.\nMove the Goal State to another desired location.\nMake sure both states are not in collision with the robot itself.\nUn-check theShow Trailcheckbox in thePlanned Pathtree menu.\nIn theMotionPlanningwindow under thePlanningtab, press thePlanbutton.\nCheck theShow Trailcheckbox in thePlanned Pathtree menu. You should see the arm’s path represented by a series of manipulator poses.\nYou can visually introspect trajectories point by point in RViz.\nFrom “Panels” menu, select “Trajectory - Trajectory Slider”. You’ll see a new Slider panel on RViz.\nSet your goal pose, then runPlan.\nPlay with the “Slider” panel, e.g. move the slider, push “Play” button.\nNote: Once you placed your end-effector to a new goal, be sure to runPlanbefore runningPlay– otherwise you’ll see the waypoints for the previous goal if available.\nIf the “Use Cartesian Path” checkbox is activated, the robot will attempt to move the end effector linearly in cartesian space.\nClicking “Plan & Execute” or “Execute” after a successful plan will send the trajectory to the robot - in this tutorial, since you usedkinova_demo.launch, the robot is only simulated.\nInitially, the default velocity and acceleration are scaled to 10% (0.1) of the robot’s maximum. You can change these scaling factors in the Planning tab shown below, or change these default values in themoveit_configof your robot (injoint_limits.yaml).\nMany of the tutorials usemoveit_visual_toolsto step through a demo. Before continuing on to the next tutorials it is a good idea to enable theRvizVisualToolsGui.\nFrom “Panels” menu, select “Add New Panels”. From the menu, select “RvizVisualToolsGui” and click OK. You’ll see the new panel added to RViz.\nRViz enables you to save your configuration underFile->SaveConfig. You should do this before continuing on to the next tutorials. If you choose to save your configuration under a new name, you can useFile->SaveConfigAsand refer to your configuration file using:\nReplaceyour_rviz_config.rvizwith the name of the file you saved tomoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/and build the workspace so it can be found.\nInYour First MoveIt Project, you will create a C++ program using MoveIt to plan and execute moves.\n© Copyright 2024, PickNik Robotics.\nros2launchmoveit2_tutorialsdemo.launch.py\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=your_rviz_config.rviz\n/base_link\nrobot_description\n/monitored_planning_scene\n/display_planned_path\nmanipulator\n/monitored_planning_scene\nkinova_demo.launch\n0.1\nmoveit_config\njoint_limits.yaml\nmoveit_visual_tools\nFile->SaveConfig\nFile->SaveConfigAs\nyour_rviz_config.rviz\nmoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0a9"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#saving-your-configuration",
  "title": "MoveIt Quickstart in RViz — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Quickstart in RViz\nGetting Started\nStep 1: Launch the Demo and Configure the Plugin\nStep 2: Play with the Visualized Robots\nStep 3: Interact with the Kinova Gen 3\nStep 4: Use Motion Planning with the Kinova Gen 3\nNext Steps\nThis tutorial will teach you how to create motion plans in MoveIt using RViz and the MoveIt Display plugin. Rviz is the primary visualizer in ROS and a very useful tool for debugging robotics. The MoveIt Display plugin allows you to setup virtual environments (planning scenes), create start and goal states for the robot interactively, test various motion planners, and visualize the output. Let’s get started!\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Startedor ourDocker Guide.\nIf you followed the Docker Guide, also follow theCreate A Colcon Workspace and Download Tutorialsguide onwards to set up the tutorials.\nLaunch the demo:\nIf you are doing this for the first time, you should see an empty world in RViz and will have to add the Motion Planning Plugin:\nYou should see an empty world in RViz:\n\nIn the RViz Displays Tab, pressAdd:\n\nFrom the moveit_ros_visualization folder, choose “MotionPlanning” as the DisplayType. Press “Ok”.\n\nYou should now see the Kinova robot in RViz:\n\nOnce you have the Motion Planning Plugin loaded, we can configure it. In the “Global Options” tab of the “Displays” subwindow, set theFixed Framefield to/base_link\nNow, you can start configuring the Plugin for your robot (the Kinova Gen 3 in this case). Click on “MotionPlanning” within “Displays”.\nMake sure theRobot Descriptionfield is set torobot_description.\nMake sure thePlanning Scene Topicfield is set to/monitored_planning_scene.\nClick on topic name to expose topic-name drop-down.\nMake sure theTrajectory TopicunderPlanned Pathis set to/display_planned_path.\nInPlanning Request, change thePlanning Grouptomanipulator. You can also see this in the MotionPlanning panel in the bottom left.\nThere are four different overlapping visualizations:\nThe robot’s configuration in the/monitored_planning_sceneplanning environment (active by default).\nThe planned path for the robot (active by default).\nGreen: The start state for motion planning (disabled by default).\nOrange: The goal state for motion planning (active by default).\nThe display states for each of these visualizations can be toggled on and off using checkboxes:\nThe planning scene robot using theShow Robot Visualcheckbox in theScene Robottree menu.\nThe planned path using theShow Robot Visualcheckbox in thePlanned Pathtree menu.\nThe start state using theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThe goal state using theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nPlay with all these checkboxes to switch on and off different visualizations.\nFor the next steps we will want only the scene robot, start state and goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theShow Robot Visualcheckbox in theScene Robottree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nCheck theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThere should now be two interactive markers. One marker corresponding to the orange colored arm will be used to set the “Goal State” for motion planning and the other marker corresponding to a green colored arm are used to set the “Start State” for motion planning. If you don’t see the interactive markers pressInteractin the top menu of RViz (Note: some tools may be hidden, press“+”in the top menu to add theInteracttool as shown below).\nYou should now be able to use these markers to drag the arm around and change its orientation. Try it!\nFor this section, hide the planned path and the goal state:\nUn-check theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nNow, only the Start State (the green colored arm) should be visible.  Try to move the arm into a configuration where two of its links are in collision with each other.  (If you find this difficult, make sure the “Use Collision-Aware IK” checkbox under the Planning tab of the MotionPlanning plugin is un-checked.)  Once you do this, the links that are in collision will turn red.\nNow, check the “Use Collision-Aware IK” checkbox, and try again to move two of the links into collision with each other.  When the checkbox is ticked, the IK solver will keep attempting to find a collision-free solution for the desired end-effector pose. When it is not checked, the solver will allow collisions to happen in the solution. The links in collision will always still be visualized in red, regardless of the state of the checkbox.\nNote what happens when you try to move an end-effector out of its reachable workspace.\nBefore moving onto the next section, re-enable the planned path and the goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nYou can use theJointstab to move single joints and the redundant joints of 7-DOF robots. Try moving the “null space exploration” slider as shown in the animation below.\nNow, you can start motion planning with the Kinova Gen 3 in the MoveIt RViz Plugin.\nMove the Start State to a desired location.\nMove the Goal State to another desired location.\nMake sure both states are not in collision with the robot itself.\nUn-check theShow Trailcheckbox in thePlanned Pathtree menu.\nIn theMotionPlanningwindow under thePlanningtab, press thePlanbutton.\nCheck theShow Trailcheckbox in thePlanned Pathtree menu. You should see the arm’s path represented by a series of manipulator poses.\nYou can visually introspect trajectories point by point in RViz.\nFrom “Panels” menu, select “Trajectory - Trajectory Slider”. You’ll see a new Slider panel on RViz.\nSet your goal pose, then runPlan.\nPlay with the “Slider” panel, e.g. move the slider, push “Play” button.\nNote: Once you placed your end-effector to a new goal, be sure to runPlanbefore runningPlay– otherwise you’ll see the waypoints for the previous goal if available.\nIf the “Use Cartesian Path” checkbox is activated, the robot will attempt to move the end effector linearly in cartesian space.\nClicking “Plan & Execute” or “Execute” after a successful plan will send the trajectory to the robot - in this tutorial, since you usedkinova_demo.launch, the robot is only simulated.\nInitially, the default velocity and acceleration are scaled to 10% (0.1) of the robot’s maximum. You can change these scaling factors in the Planning tab shown below, or change these default values in themoveit_configof your robot (injoint_limits.yaml).\nMany of the tutorials usemoveit_visual_toolsto step through a demo. Before continuing on to the next tutorials it is a good idea to enable theRvizVisualToolsGui.\nFrom “Panels” menu, select “Add New Panels”. From the menu, select “RvizVisualToolsGui” and click OK. You’ll see the new panel added to RViz.\nRViz enables you to save your configuration underFile->SaveConfig. You should do this before continuing on to the next tutorials. If you choose to save your configuration under a new name, you can useFile->SaveConfigAsand refer to your configuration file using:\nReplaceyour_rviz_config.rvizwith the name of the file you saved tomoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/and build the workspace so it can be found.\nInYour First MoveIt Project, you will create a C++ program using MoveIt to plan and execute moves.\n© Copyright 2024, PickNik Robotics.\nros2launchmoveit2_tutorialsdemo.launch.py\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=your_rviz_config.rviz\n/base_link\nrobot_description\n/monitored_planning_scene\n/display_planned_path\nmanipulator\n/monitored_planning_scene\nkinova_demo.launch\n0.1\nmoveit_config\njoint_limits.yaml\nmoveit_visual_tools\nFile->SaveConfig\nFile->SaveConfigAs\nyour_rviz_config.rviz\nmoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0aa"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#next-tutorial",
  "title": "MoveIt Quickstart in RViz — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Quickstart in RViz\nGetting Started\nStep 1: Launch the Demo and Configure the Plugin\nStep 2: Play with the Visualized Robots\nStep 3: Interact with the Kinova Gen 3\nStep 4: Use Motion Planning with the Kinova Gen 3\nNext Steps\nThis tutorial will teach you how to create motion plans in MoveIt using RViz and the MoveIt Display plugin. Rviz is the primary visualizer in ROS and a very useful tool for debugging robotics. The MoveIt Display plugin allows you to setup virtual environments (planning scenes), create start and goal states for the robot interactively, test various motion planners, and visualize the output. Let’s get started!\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Startedor ourDocker Guide.\nIf you followed the Docker Guide, also follow theCreate A Colcon Workspace and Download Tutorialsguide onwards to set up the tutorials.\nLaunch the demo:\nIf you are doing this for the first time, you should see an empty world in RViz and will have to add the Motion Planning Plugin:\nYou should see an empty world in RViz:\n\nIn the RViz Displays Tab, pressAdd:\n\nFrom the moveit_ros_visualization folder, choose “MotionPlanning” as the DisplayType. Press “Ok”.\n\nYou should now see the Kinova robot in RViz:\n\nOnce you have the Motion Planning Plugin loaded, we can configure it. In the “Global Options” tab of the “Displays” subwindow, set theFixed Framefield to/base_link\nNow, you can start configuring the Plugin for your robot (the Kinova Gen 3 in this case). Click on “MotionPlanning” within “Displays”.\nMake sure theRobot Descriptionfield is set torobot_description.\nMake sure thePlanning Scene Topicfield is set to/monitored_planning_scene.\nClick on topic name to expose topic-name drop-down.\nMake sure theTrajectory TopicunderPlanned Pathis set to/display_planned_path.\nInPlanning Request, change thePlanning Grouptomanipulator. You can also see this in the MotionPlanning panel in the bottom left.\nThere are four different overlapping visualizations:\nThe robot’s configuration in the/monitored_planning_sceneplanning environment (active by default).\nThe planned path for the robot (active by default).\nGreen: The start state for motion planning (disabled by default).\nOrange: The goal state for motion planning (active by default).\nThe display states for each of these visualizations can be toggled on and off using checkboxes:\nThe planning scene robot using theShow Robot Visualcheckbox in theScene Robottree menu.\nThe planned path using theShow Robot Visualcheckbox in thePlanned Pathtree menu.\nThe start state using theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThe goal state using theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nPlay with all these checkboxes to switch on and off different visualizations.\nFor the next steps we will want only the scene robot, start state and goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theShow Robot Visualcheckbox in theScene Robottree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nCheck theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThere should now be two interactive markers. One marker corresponding to the orange colored arm will be used to set the “Goal State” for motion planning and the other marker corresponding to a green colored arm are used to set the “Start State” for motion planning. If you don’t see the interactive markers pressInteractin the top menu of RViz (Note: some tools may be hidden, press“+”in the top menu to add theInteracttool as shown below).\nYou should now be able to use these markers to drag the arm around and change its orientation. Try it!\nFor this section, hide the planned path and the goal state:\nUn-check theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nNow, only the Start State (the green colored arm) should be visible.  Try to move the arm into a configuration where two of its links are in collision with each other.  (If you find this difficult, make sure the “Use Collision-Aware IK” checkbox under the Planning tab of the MotionPlanning plugin is un-checked.)  Once you do this, the links that are in collision will turn red.\nNow, check the “Use Collision-Aware IK” checkbox, and try again to move two of the links into collision with each other.  When the checkbox is ticked, the IK solver will keep attempting to find a collision-free solution for the desired end-effector pose. When it is not checked, the solver will allow collisions to happen in the solution. The links in collision will always still be visualized in red, regardless of the state of the checkbox.\nNote what happens when you try to move an end-effector out of its reachable workspace.\nBefore moving onto the next section, re-enable the planned path and the goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nYou can use theJointstab to move single joints and the redundant joints of 7-DOF robots. Try moving the “null space exploration” slider as shown in the animation below.\nNow, you can start motion planning with the Kinova Gen 3 in the MoveIt RViz Plugin.\nMove the Start State to a desired location.\nMove the Goal State to another desired location.\nMake sure both states are not in collision with the robot itself.\nUn-check theShow Trailcheckbox in thePlanned Pathtree menu.\nIn theMotionPlanningwindow under thePlanningtab, press thePlanbutton.\nCheck theShow Trailcheckbox in thePlanned Pathtree menu. You should see the arm’s path represented by a series of manipulator poses.\nYou can visually introspect trajectories point by point in RViz.\nFrom “Panels” menu, select “Trajectory - Trajectory Slider”. You’ll see a new Slider panel on RViz.\nSet your goal pose, then runPlan.\nPlay with the “Slider” panel, e.g. move the slider, push “Play” button.\nNote: Once you placed your end-effector to a new goal, be sure to runPlanbefore runningPlay– otherwise you’ll see the waypoints for the previous goal if available.\nIf the “Use Cartesian Path” checkbox is activated, the robot will attempt to move the end effector linearly in cartesian space.\nClicking “Plan & Execute” or “Execute” after a successful plan will send the trajectory to the robot - in this tutorial, since you usedkinova_demo.launch, the robot is only simulated.\nInitially, the default velocity and acceleration are scaled to 10% (0.1) of the robot’s maximum. You can change these scaling factors in the Planning tab shown below, or change these default values in themoveit_configof your robot (injoint_limits.yaml).\nMany of the tutorials usemoveit_visual_toolsto step through a demo. Before continuing on to the next tutorials it is a good idea to enable theRvizVisualToolsGui.\nFrom “Panels” menu, select “Add New Panels”. From the menu, select “RvizVisualToolsGui” and click OK. You’ll see the new panel added to RViz.\nRViz enables you to save your configuration underFile->SaveConfig. You should do this before continuing on to the next tutorials. If you choose to save your configuration under a new name, you can useFile->SaveConfigAsand refer to your configuration file using:\nReplaceyour_rviz_config.rvizwith the name of the file you saved tomoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/and build the workspace so it can be found.\nInYour First MoveIt Project, you will create a C++ program using MoveIt to plan and execute moves.\n© Copyright 2024, PickNik Robotics.\nros2launchmoveit2_tutorialsdemo.launch.py\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=your_rviz_config.rviz\n/base_link\nrobot_description\n/monitored_planning_scene\n/display_planned_path\nmanipulator\n/monitored_planning_scene\nkinova_demo.launch\n0.1\nmoveit_config\njoint_limits.yaml\nmoveit_visual_tools\nFile->SaveConfig\nFile->SaveConfigAs\nyour_rviz_config.rviz\nmoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0ab"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#moveit-quickstart-in-rviz",
  "title": "MoveIt Quickstart in RViz — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Quickstart in RViz\nGetting Started\nStep 1: Launch the Demo and Configure the Plugin\nStep 2: Play with the Visualized Robots\nStep 3: Interact with the Kinova Gen 3\nStep 4: Use Motion Planning with the Kinova Gen 3\nNext Steps\nThis tutorial will teach you how to create motion plans in MoveIt using RViz and the MoveIt Display plugin. Rviz is the primary visualizer in ROS and a very useful tool for debugging robotics. The MoveIt Display plugin allows you to setup virtual environments (planning scenes), create start and goal states for the robot interactively, test various motion planners, and visualize the output. Let’s get started!\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Startedor ourDocker Guide.\nIf you followed the Docker Guide, also follow theCreate A Colcon Workspace and Download Tutorialsguide onwards to set up the tutorials.\nLaunch the demo:\nIf you are doing this for the first time, you should see an empty world in RViz and will have to add the Motion Planning Plugin:\nYou should see an empty world in RViz:\n\nIn the RViz Displays Tab, pressAdd:\n\nFrom the moveit_ros_visualization folder, choose “MotionPlanning” as the DisplayType. Press “Ok”.\n\nYou should now see the Kinova robot in RViz:\n\nOnce you have the Motion Planning Plugin loaded, we can configure it. In the “Global Options” tab of the “Displays” subwindow, set theFixed Framefield to/base_link\nNow, you can start configuring the Plugin for your robot (the Kinova Gen 3 in this case). Click on “MotionPlanning” within “Displays”.\nMake sure theRobot Descriptionfield is set torobot_description.\nMake sure thePlanning Scene Topicfield is set to/monitored_planning_scene.\nClick on topic name to expose topic-name drop-down.\nMake sure theTrajectory TopicunderPlanned Pathis set to/display_planned_path.\nInPlanning Request, change thePlanning Grouptomanipulator. You can also see this in the MotionPlanning panel in the bottom left.\nThere are four different overlapping visualizations:\nThe robot’s configuration in the/monitored_planning_sceneplanning environment (active by default).\nThe planned path for the robot (active by default).\nGreen: The start state for motion planning (disabled by default).\nOrange: The goal state for motion planning (active by default).\nThe display states for each of these visualizations can be toggled on and off using checkboxes:\nThe planning scene robot using theShow Robot Visualcheckbox in theScene Robottree menu.\nThe planned path using theShow Robot Visualcheckbox in thePlanned Pathtree menu.\nThe start state using theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThe goal state using theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nPlay with all these checkboxes to switch on and off different visualizations.\nFor the next steps we will want only the scene robot, start state and goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theShow Robot Visualcheckbox in theScene Robottree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nCheck theQuery Start Statecheckbox in thePlanning Requesttree menu.\nThere should now be two interactive markers. One marker corresponding to the orange colored arm will be used to set the “Goal State” for motion planning and the other marker corresponding to a green colored arm are used to set the “Start State” for motion planning. If you don’t see the interactive markers pressInteractin the top menu of RViz (Note: some tools may be hidden, press“+”in the top menu to add theInteracttool as shown below).\nYou should now be able to use these markers to drag the arm around and change its orientation. Try it!\nFor this section, hide the planned path and the goal state:\nUn-check theShow Robot Visualcheckbox in thePlanned Pathtree menu\nUn-check theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nNow, only the Start State (the green colored arm) should be visible.  Try to move the arm into a configuration where two of its links are in collision with each other.  (If you find this difficult, make sure the “Use Collision-Aware IK” checkbox under the Planning tab of the MotionPlanning plugin is un-checked.)  Once you do this, the links that are in collision will turn red.\nNow, check the “Use Collision-Aware IK” checkbox, and try again to move two of the links into collision with each other.  When the checkbox is ticked, the IK solver will keep attempting to find a collision-free solution for the desired end-effector pose. When it is not checked, the solver will allow collisions to happen in the solution. The links in collision will always still be visualized in red, regardless of the state of the checkbox.\nNote what happens when you try to move an end-effector out of its reachable workspace.\nBefore moving onto the next section, re-enable the planned path and the goal state:\nCheck theShow Robot Visualcheckbox in thePlanned Pathtree menu\nCheck theQuery Goal Statecheckbox in thePlanning Requesttree menu.\nYou can use theJointstab to move single joints and the redundant joints of 7-DOF robots. Try moving the “null space exploration” slider as shown in the animation below.\nNow, you can start motion planning with the Kinova Gen 3 in the MoveIt RViz Plugin.\nMove the Start State to a desired location.\nMove the Goal State to another desired location.\nMake sure both states are not in collision with the robot itself.\nUn-check theShow Trailcheckbox in thePlanned Pathtree menu.\nIn theMotionPlanningwindow under thePlanningtab, press thePlanbutton.\nCheck theShow Trailcheckbox in thePlanned Pathtree menu. You should see the arm’s path represented by a series of manipulator poses.\nYou can visually introspect trajectories point by point in RViz.\nFrom “Panels” menu, select “Trajectory - Trajectory Slider”. You’ll see a new Slider panel on RViz.\nSet your goal pose, then runPlan.\nPlay with the “Slider” panel, e.g. move the slider, push “Play” button.\nNote: Once you placed your end-effector to a new goal, be sure to runPlanbefore runningPlay– otherwise you’ll see the waypoints for the previous goal if available.\nIf the “Use Cartesian Path” checkbox is activated, the robot will attempt to move the end effector linearly in cartesian space.\nClicking “Plan & Execute” or “Execute” after a successful plan will send the trajectory to the robot - in this tutorial, since you usedkinova_demo.launch, the robot is only simulated.\nInitially, the default velocity and acceleration are scaled to 10% (0.1) of the robot’s maximum. You can change these scaling factors in the Planning tab shown below, or change these default values in themoveit_configof your robot (injoint_limits.yaml).\nMany of the tutorials usemoveit_visual_toolsto step through a demo. Before continuing on to the next tutorials it is a good idea to enable theRvizVisualToolsGui.\nFrom “Panels” menu, select “Add New Panels”. From the menu, select “RvizVisualToolsGui” and click OK. You’ll see the new panel added to RViz.\nRViz enables you to save your configuration underFile->SaveConfig. You should do this before continuing on to the next tutorials. If you choose to save your configuration under a new name, you can useFile->SaveConfigAsand refer to your configuration file using:\nReplaceyour_rviz_config.rvizwith the name of the file you saved tomoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/and build the workspace so it can be found.\nInYour First MoveIt Project, you will create a C++ program using MoveIt to plan and execute moves.\n© Copyright 2024, PickNik Robotics.\nros2launchmoveit2_tutorialsdemo.launch.py\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=your_rviz_config.rviz\n/base_link\nrobot_description\n/monitored_planning_scene\n/display_planned_path\nmanipulator\n/monitored_planning_scene\nkinova_demo.launch\n0.1\nmoveit_config\njoint_limits.yaml\nmoveit_visual_tools\nFile->SaveConfig\nFile->SaveConfigAs\nyour_rviz_config.rviz\nmoveit2_tutorials/doc/tutorials/quickstart_in_rviz/launch/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0ac"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/your_first_project/your_first_project.html",
  "title": "Your First C++ MoveIt Project — MoveIt Documentation: Rolling  documentation",
  "content": "Your First C++ MoveIt Project\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will step you through writing your first C++ application with MoveIt.\nWarning: Most features in MoveIt will not work properly since additional parameters are required for full Move Group functionality. For a full setup, please continue with theMove Group C++ Interface Tutorial.\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nThis tutorial assumes you understand the basics of ROS 2.\nTo prepare yourself for this please complete theOfficial ROS 2 Tutorialsup until “Writing a simple publisher and Subscriber (C++)”.\nOpen a terminal andsource your ROS 2 installationso thatros2commands will work.\nNavigate to yourws_moveitdirectory you created in theGetting Started Tutorial.\nChange directory into thesrcdirectory, as that is where we put our source code.\nCreate a new package with the ROS 2 command line tools:\nThe output of this will show that it created some files in a new directory.\nNote that we addedmoveit_ros_planning_interfaceandrclcppas dependencies.\nThis will create the necessary changes in thepackage.xmlandCMakeLists.txtfiles so that we can depend on these two packages.\nOpen the new source file created for you atws_moveit/src/hello_moveit/src/hello_moveit.cppin your favorite editor.\nThis first block of code is a bit of boilerplate but you should be used to seeing this from the ROS 2 tutorials.\nWe will build and run the program to see that everything is right before we move on.\nChange the directory back to the workspace directoryws_moveitand run this command:\nAfter this succeeds,open a new terminal, then source the workspace environment script in that new terminal so that we can run our program.\nRun your program and see the output.\nThe program should run and exit without error.\nThe headers included at the top are just some standard C++ headers and the headers for ROS and MoveIt that we will use later.\nAfter that, we have the normal call to initialize rclcpp, and then we create our Node.\nThe first argument is a string that ROS will use to name a unique node.\nThe second is needed for MoveIt because of how we use ROS Parameters.\nNext, wecreate a loggernamed “hello_moveit” to keep our log outputs organized and configurable.\nLastly, we have the code to shutdown ROS.\nIn place of the comment that says “Next step goes here”, add this code:\nJust like before, we need to build the code before we can run it.\nIn the workspace directory,ws_moveit, run this command:\nAfter this succeeds, we need to reuse the demo launch file from the previous tutorial to start RViz and the MoveGroup node.\nIn a separate terminal, source the workspace and then execute this:\nThen in theDisplayswindow underMotionPlanning/PlanningRequest, uncheck the boxQueryGoalState.\nIn a third terminal, source the workspace and run your program.\nThis should cause the robot in RViz to move and end up in this pose:\nNote that if you run the nodehello_moveitwithout launching the demo launch file first, it will wait for 10 seconds and then print this error and exit.\nThis is because thedemo.launch.pylaunch is starting theMoveGroupnode that provides the robot description.\nWhenMoveGroupInterfaceis constructed, it looks for a node publishing a topic with the robot description.\nIf it fails to find that within 10 seconds, it prints this error and terminates the program.\nThe first thing we do is create theMoveGroupInterface.\nThis object will be used to interact withmove_group, which allows us to plan and execute trajectories.\nNote that this is the only mutable object that we create in this program.\nAnother thing to take note of is the second argument to theMoveGroupInterfaceobject we are creating here:\"manipulator\".\nThat is the group of joints as defined in the robot description that we are going to operate on with thisMoveGroupInterface.\nThen, we set our target pose and plan. Note that only the target pose is set (viasetPoseTarget).\nThe starting pose is implicitly the position published by the joint state publisher, which could be changed using theMoveGroupInterface::setStartState*family of functions (but is not in this tutorial).\nOne more thing to note about this next section is the use of lambdas for constructing the message typetarget_poseand planning.\nThis is a pattern you’ll find in modern C++ codebases that enables writing in a more declarative style.\nFor more information about this pattern, there are a couple of links at the end of this tutorial.\nFinally, we execute our plan if planning is successful, otherwise, we log an error:\nYou created a ROS 2 package and wrote your first program using MoveIt.\nYou learned about using the MoveGroupInterface to plan and execute moves.\nHere is a copy of the full hello_moveit.cpp source at the end of this tutorial.\nWe used lambdas to be able to initialize objects as constants.\nThis is known as a technique called IIFE.Read more about this pattern from C++ Stories.\nWe also declared everything we could as const.Read more about the usefulness of const here.\nIn the next tutorialVisualizing in RViz, you will expand on the program you built here to create visual markers that make it easier to understand what MoveIt is doing.\n© Copyright 2024, PickNik Robotics.\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_ros_planning_interfacerclcpp\\--node-namehello_moveithello_moveit\n#include<memory>#include<rclcpp/rclcpp.hpp>#include<moveit/move_group_interface/move_group_interface.h>intmain(intargc,char*argv[]){// Initialize ROS and create the Noderclcpp::init(argc,argv);autoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");// Next step goes here// Shutdown ROSrclcpp::shutdown();return0;}\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\nautoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Shutdown ROSrclcpp::shutdown();return0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\ncolconbuild--mixindebug\nros2launchmoveit2_tutorialsdemo.launch.py\nros2runhello_moveithello_moveit\n[ERROR][1644181704.350825487][hello_moveit]:Couldnotfindparameterrobot_descriptionanddidnotreceiverobot_descriptionviastd_msgs::msg::Stringsubscriptionwithin10.000000seconds.\nusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();\n// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\nros2\nws_moveit\nsrc\nmoveit_ros_planning_interface\nrclcpp\npackage.xml\nCMakeLists.txt\nws_moveit/src/hello_moveit/src/hello_moveit.cpp\nws_moveit\nws_moveit\nDisplays\nMotionPlanning/PlanningRequest\nQueryGoalState\nhello_moveit\ndemo.launch.py\nMoveGroup\nMoveGroupInterface\nMoveGroupInterface\nmove_group\nMoveGroupInterface\n\"manipulator\"\nMoveGroupInterface\nsetPoseTarget\nMoveGroupInterface::setStartState*\ntarget_pose",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0ad"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/your_first_project/your_first_project.html#prerequisites",
  "title": "Your First C++ MoveIt Project — MoveIt Documentation: Rolling  documentation",
  "content": "Your First C++ MoveIt Project\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will step you through writing your first C++ application with MoveIt.\nWarning: Most features in MoveIt will not work properly since additional parameters are required for full Move Group functionality. For a full setup, please continue with theMove Group C++ Interface Tutorial.\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nThis tutorial assumes you understand the basics of ROS 2.\nTo prepare yourself for this please complete theOfficial ROS 2 Tutorialsup until “Writing a simple publisher and Subscriber (C++)”.\nOpen a terminal andsource your ROS 2 installationso thatros2commands will work.\nNavigate to yourws_moveitdirectory you created in theGetting Started Tutorial.\nChange directory into thesrcdirectory, as that is where we put our source code.\nCreate a new package with the ROS 2 command line tools:\nThe output of this will show that it created some files in a new directory.\nNote that we addedmoveit_ros_planning_interfaceandrclcppas dependencies.\nThis will create the necessary changes in thepackage.xmlandCMakeLists.txtfiles so that we can depend on these two packages.\nOpen the new source file created for you atws_moveit/src/hello_moveit/src/hello_moveit.cppin your favorite editor.\nThis first block of code is a bit of boilerplate but you should be used to seeing this from the ROS 2 tutorials.\nWe will build and run the program to see that everything is right before we move on.\nChange the directory back to the workspace directoryws_moveitand run this command:\nAfter this succeeds,open a new terminal, then source the workspace environment script in that new terminal so that we can run our program.\nRun your program and see the output.\nThe program should run and exit without error.\nThe headers included at the top are just some standard C++ headers and the headers for ROS and MoveIt that we will use later.\nAfter that, we have the normal call to initialize rclcpp, and then we create our Node.\nThe first argument is a string that ROS will use to name a unique node.\nThe second is needed for MoveIt because of how we use ROS Parameters.\nNext, wecreate a loggernamed “hello_moveit” to keep our log outputs organized and configurable.\nLastly, we have the code to shutdown ROS.\nIn place of the comment that says “Next step goes here”, add this code:\nJust like before, we need to build the code before we can run it.\nIn the workspace directory,ws_moveit, run this command:\nAfter this succeeds, we need to reuse the demo launch file from the previous tutorial to start RViz and the MoveGroup node.\nIn a separate terminal, source the workspace and then execute this:\nThen in theDisplayswindow underMotionPlanning/PlanningRequest, uncheck the boxQueryGoalState.\nIn a third terminal, source the workspace and run your program.\nThis should cause the robot in RViz to move and end up in this pose:\nNote that if you run the nodehello_moveitwithout launching the demo launch file first, it will wait for 10 seconds and then print this error and exit.\nThis is because thedemo.launch.pylaunch is starting theMoveGroupnode that provides the robot description.\nWhenMoveGroupInterfaceis constructed, it looks for a node publishing a topic with the robot description.\nIf it fails to find that within 10 seconds, it prints this error and terminates the program.\nThe first thing we do is create theMoveGroupInterface.\nThis object will be used to interact withmove_group, which allows us to plan and execute trajectories.\nNote that this is the only mutable object that we create in this program.\nAnother thing to take note of is the second argument to theMoveGroupInterfaceobject we are creating here:\"manipulator\".\nThat is the group of joints as defined in the robot description that we are going to operate on with thisMoveGroupInterface.\nThen, we set our target pose and plan. Note that only the target pose is set (viasetPoseTarget).\nThe starting pose is implicitly the position published by the joint state publisher, which could be changed using theMoveGroupInterface::setStartState*family of functions (but is not in this tutorial).\nOne more thing to note about this next section is the use of lambdas for constructing the message typetarget_poseand planning.\nThis is a pattern you’ll find in modern C++ codebases that enables writing in a more declarative style.\nFor more information about this pattern, there are a couple of links at the end of this tutorial.\nFinally, we execute our plan if planning is successful, otherwise, we log an error:\nYou created a ROS 2 package and wrote your first program using MoveIt.\nYou learned about using the MoveGroupInterface to plan and execute moves.\nHere is a copy of the full hello_moveit.cpp source at the end of this tutorial.\nWe used lambdas to be able to initialize objects as constants.\nThis is known as a technique called IIFE.Read more about this pattern from C++ Stories.\nWe also declared everything we could as const.Read more about the usefulness of const here.\nIn the next tutorialVisualizing in RViz, you will expand on the program you built here to create visual markers that make it easier to understand what MoveIt is doing.\n© Copyright 2024, PickNik Robotics.\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_ros_planning_interfacerclcpp\\--node-namehello_moveithello_moveit\n#include<memory>#include<rclcpp/rclcpp.hpp>#include<moveit/move_group_interface/move_group_interface.h>intmain(intargc,char*argv[]){// Initialize ROS and create the Noderclcpp::init(argc,argv);autoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");// Next step goes here// Shutdown ROSrclcpp::shutdown();return0;}\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\nautoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Shutdown ROSrclcpp::shutdown();return0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\ncolconbuild--mixindebug\nros2launchmoveit2_tutorialsdemo.launch.py\nros2runhello_moveithello_moveit\n[ERROR][1644181704.350825487][hello_moveit]:Couldnotfindparameterrobot_descriptionanddidnotreceiverobot_descriptionviastd_msgs::msg::Stringsubscriptionwithin10.000000seconds.\nusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();\n// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\nros2\nws_moveit\nsrc\nmoveit_ros_planning_interface\nrclcpp\npackage.xml\nCMakeLists.txt\nws_moveit/src/hello_moveit/src/hello_moveit.cpp\nws_moveit\nws_moveit\nDisplays\nMotionPlanning/PlanningRequest\nQueryGoalState\nhello_moveit\ndemo.launch.py\nMoveGroup\nMoveGroupInterface\nMoveGroupInterface\nmove_group\nMoveGroupInterface\n\"manipulator\"\nMoveGroupInterface\nsetPoseTarget\nMoveGroupInterface::setStartState*\ntarget_pose",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0ae"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/your_first_project/your_first_project.html#steps",
  "title": "Your First C++ MoveIt Project — MoveIt Documentation: Rolling  documentation",
  "content": "Your First C++ MoveIt Project\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will step you through writing your first C++ application with MoveIt.\nWarning: Most features in MoveIt will not work properly since additional parameters are required for full Move Group functionality. For a full setup, please continue with theMove Group C++ Interface Tutorial.\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nThis tutorial assumes you understand the basics of ROS 2.\nTo prepare yourself for this please complete theOfficial ROS 2 Tutorialsup until “Writing a simple publisher and Subscriber (C++)”.\nOpen a terminal andsource your ROS 2 installationso thatros2commands will work.\nNavigate to yourws_moveitdirectory you created in theGetting Started Tutorial.\nChange directory into thesrcdirectory, as that is where we put our source code.\nCreate a new package with the ROS 2 command line tools:\nThe output of this will show that it created some files in a new directory.\nNote that we addedmoveit_ros_planning_interfaceandrclcppas dependencies.\nThis will create the necessary changes in thepackage.xmlandCMakeLists.txtfiles so that we can depend on these two packages.\nOpen the new source file created for you atws_moveit/src/hello_moveit/src/hello_moveit.cppin your favorite editor.\nThis first block of code is a bit of boilerplate but you should be used to seeing this from the ROS 2 tutorials.\nWe will build and run the program to see that everything is right before we move on.\nChange the directory back to the workspace directoryws_moveitand run this command:\nAfter this succeeds,open a new terminal, then source the workspace environment script in that new terminal so that we can run our program.\nRun your program and see the output.\nThe program should run and exit without error.\nThe headers included at the top are just some standard C++ headers and the headers for ROS and MoveIt that we will use later.\nAfter that, we have the normal call to initialize rclcpp, and then we create our Node.\nThe first argument is a string that ROS will use to name a unique node.\nThe second is needed for MoveIt because of how we use ROS Parameters.\nNext, wecreate a loggernamed “hello_moveit” to keep our log outputs organized and configurable.\nLastly, we have the code to shutdown ROS.\nIn place of the comment that says “Next step goes here”, add this code:\nJust like before, we need to build the code before we can run it.\nIn the workspace directory,ws_moveit, run this command:\nAfter this succeeds, we need to reuse the demo launch file from the previous tutorial to start RViz and the MoveGroup node.\nIn a separate terminal, source the workspace and then execute this:\nThen in theDisplayswindow underMotionPlanning/PlanningRequest, uncheck the boxQueryGoalState.\nIn a third terminal, source the workspace and run your program.\nThis should cause the robot in RViz to move and end up in this pose:\nNote that if you run the nodehello_moveitwithout launching the demo launch file first, it will wait for 10 seconds and then print this error and exit.\nThis is because thedemo.launch.pylaunch is starting theMoveGroupnode that provides the robot description.\nWhenMoveGroupInterfaceis constructed, it looks for a node publishing a topic with the robot description.\nIf it fails to find that within 10 seconds, it prints this error and terminates the program.\nThe first thing we do is create theMoveGroupInterface.\nThis object will be used to interact withmove_group, which allows us to plan and execute trajectories.\nNote that this is the only mutable object that we create in this program.\nAnother thing to take note of is the second argument to theMoveGroupInterfaceobject we are creating here:\"manipulator\".\nThat is the group of joints as defined in the robot description that we are going to operate on with thisMoveGroupInterface.\nThen, we set our target pose and plan. Note that only the target pose is set (viasetPoseTarget).\nThe starting pose is implicitly the position published by the joint state publisher, which could be changed using theMoveGroupInterface::setStartState*family of functions (but is not in this tutorial).\nOne more thing to note about this next section is the use of lambdas for constructing the message typetarget_poseand planning.\nThis is a pattern you’ll find in modern C++ codebases that enables writing in a more declarative style.\nFor more information about this pattern, there are a couple of links at the end of this tutorial.\nFinally, we execute our plan if planning is successful, otherwise, we log an error:\nYou created a ROS 2 package and wrote your first program using MoveIt.\nYou learned about using the MoveGroupInterface to plan and execute moves.\nHere is a copy of the full hello_moveit.cpp source at the end of this tutorial.\nWe used lambdas to be able to initialize objects as constants.\nThis is known as a technique called IIFE.Read more about this pattern from C++ Stories.\nWe also declared everything we could as const.Read more about the usefulness of const here.\nIn the next tutorialVisualizing in RViz, you will expand on the program you built here to create visual markers that make it easier to understand what MoveIt is doing.\n© Copyright 2024, PickNik Robotics.\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_ros_planning_interfacerclcpp\\--node-namehello_moveithello_moveit\n#include<memory>#include<rclcpp/rclcpp.hpp>#include<moveit/move_group_interface/move_group_interface.h>intmain(intargc,char*argv[]){// Initialize ROS and create the Noderclcpp::init(argc,argv);autoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");// Next step goes here// Shutdown ROSrclcpp::shutdown();return0;}\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\nautoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Shutdown ROSrclcpp::shutdown();return0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\ncolconbuild--mixindebug\nros2launchmoveit2_tutorialsdemo.launch.py\nros2runhello_moveithello_moveit\n[ERROR][1644181704.350825487][hello_moveit]:Couldnotfindparameterrobot_descriptionanddidnotreceiverobot_descriptionviastd_msgs::msg::Stringsubscriptionwithin10.000000seconds.\nusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();\n// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\nros2\nws_moveit\nsrc\nmoveit_ros_planning_interface\nrclcpp\npackage.xml\nCMakeLists.txt\nws_moveit/src/hello_moveit/src/hello_moveit.cpp\nws_moveit\nws_moveit\nDisplays\nMotionPlanning/PlanningRequest\nQueryGoalState\nhello_moveit\ndemo.launch.py\nMoveGroup\nMoveGroupInterface\nMoveGroupInterface\nmove_group\nMoveGroupInterface\n\"manipulator\"\nMoveGroupInterface\nsetPoseTarget\nMoveGroupInterface::setStartState*\ntarget_pose",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0af"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/your_first_project/your_first_project.html#create-a-package",
  "title": "Your First C++ MoveIt Project — MoveIt Documentation: Rolling  documentation",
  "content": "Your First C++ MoveIt Project\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will step you through writing your first C++ application with MoveIt.\nWarning: Most features in MoveIt will not work properly since additional parameters are required for full Move Group functionality. For a full setup, please continue with theMove Group C++ Interface Tutorial.\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nThis tutorial assumes you understand the basics of ROS 2.\nTo prepare yourself for this please complete theOfficial ROS 2 Tutorialsup until “Writing a simple publisher and Subscriber (C++)”.\nOpen a terminal andsource your ROS 2 installationso thatros2commands will work.\nNavigate to yourws_moveitdirectory you created in theGetting Started Tutorial.\nChange directory into thesrcdirectory, as that is where we put our source code.\nCreate a new package with the ROS 2 command line tools:\nThe output of this will show that it created some files in a new directory.\nNote that we addedmoveit_ros_planning_interfaceandrclcppas dependencies.\nThis will create the necessary changes in thepackage.xmlandCMakeLists.txtfiles so that we can depend on these two packages.\nOpen the new source file created for you atws_moveit/src/hello_moveit/src/hello_moveit.cppin your favorite editor.\nThis first block of code is a bit of boilerplate but you should be used to seeing this from the ROS 2 tutorials.\nWe will build and run the program to see that everything is right before we move on.\nChange the directory back to the workspace directoryws_moveitand run this command:\nAfter this succeeds,open a new terminal, then source the workspace environment script in that new terminal so that we can run our program.\nRun your program and see the output.\nThe program should run and exit without error.\nThe headers included at the top are just some standard C++ headers and the headers for ROS and MoveIt that we will use later.\nAfter that, we have the normal call to initialize rclcpp, and then we create our Node.\nThe first argument is a string that ROS will use to name a unique node.\nThe second is needed for MoveIt because of how we use ROS Parameters.\nNext, wecreate a loggernamed “hello_moveit” to keep our log outputs organized and configurable.\nLastly, we have the code to shutdown ROS.\nIn place of the comment that says “Next step goes here”, add this code:\nJust like before, we need to build the code before we can run it.\nIn the workspace directory,ws_moveit, run this command:\nAfter this succeeds, we need to reuse the demo launch file from the previous tutorial to start RViz and the MoveGroup node.\nIn a separate terminal, source the workspace and then execute this:\nThen in theDisplayswindow underMotionPlanning/PlanningRequest, uncheck the boxQueryGoalState.\nIn a third terminal, source the workspace and run your program.\nThis should cause the robot in RViz to move and end up in this pose:\nNote that if you run the nodehello_moveitwithout launching the demo launch file first, it will wait for 10 seconds and then print this error and exit.\nThis is because thedemo.launch.pylaunch is starting theMoveGroupnode that provides the robot description.\nWhenMoveGroupInterfaceis constructed, it looks for a node publishing a topic with the robot description.\nIf it fails to find that within 10 seconds, it prints this error and terminates the program.\nThe first thing we do is create theMoveGroupInterface.\nThis object will be used to interact withmove_group, which allows us to plan and execute trajectories.\nNote that this is the only mutable object that we create in this program.\nAnother thing to take note of is the second argument to theMoveGroupInterfaceobject we are creating here:\"manipulator\".\nThat is the group of joints as defined in the robot description that we are going to operate on with thisMoveGroupInterface.\nThen, we set our target pose and plan. Note that only the target pose is set (viasetPoseTarget).\nThe starting pose is implicitly the position published by the joint state publisher, which could be changed using theMoveGroupInterface::setStartState*family of functions (but is not in this tutorial).\nOne more thing to note about this next section is the use of lambdas for constructing the message typetarget_poseand planning.\nThis is a pattern you’ll find in modern C++ codebases that enables writing in a more declarative style.\nFor more information about this pattern, there are a couple of links at the end of this tutorial.\nFinally, we execute our plan if planning is successful, otherwise, we log an error:\nYou created a ROS 2 package and wrote your first program using MoveIt.\nYou learned about using the MoveGroupInterface to plan and execute moves.\nHere is a copy of the full hello_moveit.cpp source at the end of this tutorial.\nWe used lambdas to be able to initialize objects as constants.\nThis is known as a technique called IIFE.Read more about this pattern from C++ Stories.\nWe also declared everything we could as const.Read more about the usefulness of const here.\nIn the next tutorialVisualizing in RViz, you will expand on the program you built here to create visual markers that make it easier to understand what MoveIt is doing.\n© Copyright 2024, PickNik Robotics.\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_ros_planning_interfacerclcpp\\--node-namehello_moveithello_moveit\n#include<memory>#include<rclcpp/rclcpp.hpp>#include<moveit/move_group_interface/move_group_interface.h>intmain(intargc,char*argv[]){// Initialize ROS and create the Noderclcpp::init(argc,argv);autoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");// Next step goes here// Shutdown ROSrclcpp::shutdown();return0;}\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\nautoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Shutdown ROSrclcpp::shutdown();return0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\ncolconbuild--mixindebug\nros2launchmoveit2_tutorialsdemo.launch.py\nros2runhello_moveithello_moveit\n[ERROR][1644181704.350825487][hello_moveit]:Couldnotfindparameterrobot_descriptionanddidnotreceiverobot_descriptionviastd_msgs::msg::Stringsubscriptionwithin10.000000seconds.\nusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();\n// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\nros2\nws_moveit\nsrc\nmoveit_ros_planning_interface\nrclcpp\npackage.xml\nCMakeLists.txt\nws_moveit/src/hello_moveit/src/hello_moveit.cpp\nws_moveit\nws_moveit\nDisplays\nMotionPlanning/PlanningRequest\nQueryGoalState\nhello_moveit\ndemo.launch.py\nMoveGroup\nMoveGroupInterface\nMoveGroupInterface\nmove_group\nMoveGroupInterface\n\"manipulator\"\nMoveGroupInterface\nsetPoseTarget\nMoveGroupInterface::setStartState*\ntarget_pose",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0b0"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/your_first_project/your_first_project.html#create-a-ros-node-and-executor",
  "title": "Your First C++ MoveIt Project — MoveIt Documentation: Rolling  documentation",
  "content": "Your First C++ MoveIt Project\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will step you through writing your first C++ application with MoveIt.\nWarning: Most features in MoveIt will not work properly since additional parameters are required for full Move Group functionality. For a full setup, please continue with theMove Group C++ Interface Tutorial.\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nThis tutorial assumes you understand the basics of ROS 2.\nTo prepare yourself for this please complete theOfficial ROS 2 Tutorialsup until “Writing a simple publisher and Subscriber (C++)”.\nOpen a terminal andsource your ROS 2 installationso thatros2commands will work.\nNavigate to yourws_moveitdirectory you created in theGetting Started Tutorial.\nChange directory into thesrcdirectory, as that is where we put our source code.\nCreate a new package with the ROS 2 command line tools:\nThe output of this will show that it created some files in a new directory.\nNote that we addedmoveit_ros_planning_interfaceandrclcppas dependencies.\nThis will create the necessary changes in thepackage.xmlandCMakeLists.txtfiles so that we can depend on these two packages.\nOpen the new source file created for you atws_moveit/src/hello_moveit/src/hello_moveit.cppin your favorite editor.\nThis first block of code is a bit of boilerplate but you should be used to seeing this from the ROS 2 tutorials.\nWe will build and run the program to see that everything is right before we move on.\nChange the directory back to the workspace directoryws_moveitand run this command:\nAfter this succeeds,open a new terminal, then source the workspace environment script in that new terminal so that we can run our program.\nRun your program and see the output.\nThe program should run and exit without error.\nThe headers included at the top are just some standard C++ headers and the headers for ROS and MoveIt that we will use later.\nAfter that, we have the normal call to initialize rclcpp, and then we create our Node.\nThe first argument is a string that ROS will use to name a unique node.\nThe second is needed for MoveIt because of how we use ROS Parameters.\nNext, wecreate a loggernamed “hello_moveit” to keep our log outputs organized and configurable.\nLastly, we have the code to shutdown ROS.\nIn place of the comment that says “Next step goes here”, add this code:\nJust like before, we need to build the code before we can run it.\nIn the workspace directory,ws_moveit, run this command:\nAfter this succeeds, we need to reuse the demo launch file from the previous tutorial to start RViz and the MoveGroup node.\nIn a separate terminal, source the workspace and then execute this:\nThen in theDisplayswindow underMotionPlanning/PlanningRequest, uncheck the boxQueryGoalState.\nIn a third terminal, source the workspace and run your program.\nThis should cause the robot in RViz to move and end up in this pose:\nNote that if you run the nodehello_moveitwithout launching the demo launch file first, it will wait for 10 seconds and then print this error and exit.\nThis is because thedemo.launch.pylaunch is starting theMoveGroupnode that provides the robot description.\nWhenMoveGroupInterfaceis constructed, it looks for a node publishing a topic with the robot description.\nIf it fails to find that within 10 seconds, it prints this error and terminates the program.\nThe first thing we do is create theMoveGroupInterface.\nThis object will be used to interact withmove_group, which allows us to plan and execute trajectories.\nNote that this is the only mutable object that we create in this program.\nAnother thing to take note of is the second argument to theMoveGroupInterfaceobject we are creating here:\"manipulator\".\nThat is the group of joints as defined in the robot description that we are going to operate on with thisMoveGroupInterface.\nThen, we set our target pose and plan. Note that only the target pose is set (viasetPoseTarget).\nThe starting pose is implicitly the position published by the joint state publisher, which could be changed using theMoveGroupInterface::setStartState*family of functions (but is not in this tutorial).\nOne more thing to note about this next section is the use of lambdas for constructing the message typetarget_poseand planning.\nThis is a pattern you’ll find in modern C++ codebases that enables writing in a more declarative style.\nFor more information about this pattern, there are a couple of links at the end of this tutorial.\nFinally, we execute our plan if planning is successful, otherwise, we log an error:\nYou created a ROS 2 package and wrote your first program using MoveIt.\nYou learned about using the MoveGroupInterface to plan and execute moves.\nHere is a copy of the full hello_moveit.cpp source at the end of this tutorial.\nWe used lambdas to be able to initialize objects as constants.\nThis is known as a technique called IIFE.Read more about this pattern from C++ Stories.\nWe also declared everything we could as const.Read more about the usefulness of const here.\nIn the next tutorialVisualizing in RViz, you will expand on the program you built here to create visual markers that make it easier to understand what MoveIt is doing.\n© Copyright 2024, PickNik Robotics.\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_ros_planning_interfacerclcpp\\--node-namehello_moveithello_moveit\n#include<memory>#include<rclcpp/rclcpp.hpp>#include<moveit/move_group_interface/move_group_interface.h>intmain(intargc,char*argv[]){// Initialize ROS and create the Noderclcpp::init(argc,argv);autoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");// Next step goes here// Shutdown ROSrclcpp::shutdown();return0;}\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\nautoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Shutdown ROSrclcpp::shutdown();return0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\ncolconbuild--mixindebug\nros2launchmoveit2_tutorialsdemo.launch.py\nros2runhello_moveithello_moveit\n[ERROR][1644181704.350825487][hello_moveit]:Couldnotfindparameterrobot_descriptionanddidnotreceiverobot_descriptionviastd_msgs::msg::Stringsubscriptionwithin10.000000seconds.\nusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();\n// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\nros2\nws_moveit\nsrc\nmoveit_ros_planning_interface\nrclcpp\npackage.xml\nCMakeLists.txt\nws_moveit/src/hello_moveit/src/hello_moveit.cpp\nws_moveit\nws_moveit\nDisplays\nMotionPlanning/PlanningRequest\nQueryGoalState\nhello_moveit\ndemo.launch.py\nMoveGroup\nMoveGroupInterface\nMoveGroupInterface\nmove_group\nMoveGroupInterface\n\"manipulator\"\nMoveGroupInterface\nsetPoseTarget\nMoveGroupInterface::setStartState*\ntarget_pose",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0b1"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/your_first_project/your_first_project.html#build-and-run",
  "title": "Your First C++ MoveIt Project — MoveIt Documentation: Rolling  documentation",
  "content": "Your First C++ MoveIt Project\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will step you through writing your first C++ application with MoveIt.\nWarning: Most features in MoveIt will not work properly since additional parameters are required for full Move Group functionality. For a full setup, please continue with theMove Group C++ Interface Tutorial.\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nThis tutorial assumes you understand the basics of ROS 2.\nTo prepare yourself for this please complete theOfficial ROS 2 Tutorialsup until “Writing a simple publisher and Subscriber (C++)”.\nOpen a terminal andsource your ROS 2 installationso thatros2commands will work.\nNavigate to yourws_moveitdirectory you created in theGetting Started Tutorial.\nChange directory into thesrcdirectory, as that is where we put our source code.\nCreate a new package with the ROS 2 command line tools:\nThe output of this will show that it created some files in a new directory.\nNote that we addedmoveit_ros_planning_interfaceandrclcppas dependencies.\nThis will create the necessary changes in thepackage.xmlandCMakeLists.txtfiles so that we can depend on these two packages.\nOpen the new source file created for you atws_moveit/src/hello_moveit/src/hello_moveit.cppin your favorite editor.\nThis first block of code is a bit of boilerplate but you should be used to seeing this from the ROS 2 tutorials.\nWe will build and run the program to see that everything is right before we move on.\nChange the directory back to the workspace directoryws_moveitand run this command:\nAfter this succeeds,open a new terminal, then source the workspace environment script in that new terminal so that we can run our program.\nRun your program and see the output.\nThe program should run and exit without error.\nThe headers included at the top are just some standard C++ headers and the headers for ROS and MoveIt that we will use later.\nAfter that, we have the normal call to initialize rclcpp, and then we create our Node.\nThe first argument is a string that ROS will use to name a unique node.\nThe second is needed for MoveIt because of how we use ROS Parameters.\nNext, wecreate a loggernamed “hello_moveit” to keep our log outputs organized and configurable.\nLastly, we have the code to shutdown ROS.\nIn place of the comment that says “Next step goes here”, add this code:\nJust like before, we need to build the code before we can run it.\nIn the workspace directory,ws_moveit, run this command:\nAfter this succeeds, we need to reuse the demo launch file from the previous tutorial to start RViz and the MoveGroup node.\nIn a separate terminal, source the workspace and then execute this:\nThen in theDisplayswindow underMotionPlanning/PlanningRequest, uncheck the boxQueryGoalState.\nIn a third terminal, source the workspace and run your program.\nThis should cause the robot in RViz to move and end up in this pose:\nNote that if you run the nodehello_moveitwithout launching the demo launch file first, it will wait for 10 seconds and then print this error and exit.\nThis is because thedemo.launch.pylaunch is starting theMoveGroupnode that provides the robot description.\nWhenMoveGroupInterfaceis constructed, it looks for a node publishing a topic with the robot description.\nIf it fails to find that within 10 seconds, it prints this error and terminates the program.\nThe first thing we do is create theMoveGroupInterface.\nThis object will be used to interact withmove_group, which allows us to plan and execute trajectories.\nNote that this is the only mutable object that we create in this program.\nAnother thing to take note of is the second argument to theMoveGroupInterfaceobject we are creating here:\"manipulator\".\nThat is the group of joints as defined in the robot description that we are going to operate on with thisMoveGroupInterface.\nThen, we set our target pose and plan. Note that only the target pose is set (viasetPoseTarget).\nThe starting pose is implicitly the position published by the joint state publisher, which could be changed using theMoveGroupInterface::setStartState*family of functions (but is not in this tutorial).\nOne more thing to note about this next section is the use of lambdas for constructing the message typetarget_poseand planning.\nThis is a pattern you’ll find in modern C++ codebases that enables writing in a more declarative style.\nFor more information about this pattern, there are a couple of links at the end of this tutorial.\nFinally, we execute our plan if planning is successful, otherwise, we log an error:\nYou created a ROS 2 package and wrote your first program using MoveIt.\nYou learned about using the MoveGroupInterface to plan and execute moves.\nHere is a copy of the full hello_moveit.cpp source at the end of this tutorial.\nWe used lambdas to be able to initialize objects as constants.\nThis is known as a technique called IIFE.Read more about this pattern from C++ Stories.\nWe also declared everything we could as const.Read more about the usefulness of const here.\nIn the next tutorialVisualizing in RViz, you will expand on the program you built here to create visual markers that make it easier to understand what MoveIt is doing.\n© Copyright 2024, PickNik Robotics.\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_ros_planning_interfacerclcpp\\--node-namehello_moveithello_moveit\n#include<memory>#include<rclcpp/rclcpp.hpp>#include<moveit/move_group_interface/move_group_interface.h>intmain(intargc,char*argv[]){// Initialize ROS and create the Noderclcpp::init(argc,argv);autoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");// Next step goes here// Shutdown ROSrclcpp::shutdown();return0;}\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\nautoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Shutdown ROSrclcpp::shutdown();return0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\ncolconbuild--mixindebug\nros2launchmoveit2_tutorialsdemo.launch.py\nros2runhello_moveithello_moveit\n[ERROR][1644181704.350825487][hello_moveit]:Couldnotfindparameterrobot_descriptionanddidnotreceiverobot_descriptionviastd_msgs::msg::Stringsubscriptionwithin10.000000seconds.\nusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();\n// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\nros2\nws_moveit\nsrc\nmoveit_ros_planning_interface\nrclcpp\npackage.xml\nCMakeLists.txt\nws_moveit/src/hello_moveit/src/hello_moveit.cpp\nws_moveit\nws_moveit\nDisplays\nMotionPlanning/PlanningRequest\nQueryGoalState\nhello_moveit\ndemo.launch.py\nMoveGroup\nMoveGroupInterface\nMoveGroupInterface\nmove_group\nMoveGroupInterface\n\"manipulator\"\nMoveGroupInterface\nsetPoseTarget\nMoveGroupInterface::setStartState*\ntarget_pose",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0b2"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/your_first_project/your_first_project.html#examine-the-code",
  "title": "Your First C++ MoveIt Project — MoveIt Documentation: Rolling  documentation",
  "content": "Your First C++ MoveIt Project\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will step you through writing your first C++ application with MoveIt.\nWarning: Most features in MoveIt will not work properly since additional parameters are required for full Move Group functionality. For a full setup, please continue with theMove Group C++ Interface Tutorial.\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nThis tutorial assumes you understand the basics of ROS 2.\nTo prepare yourself for this please complete theOfficial ROS 2 Tutorialsup until “Writing a simple publisher and Subscriber (C++)”.\nOpen a terminal andsource your ROS 2 installationso thatros2commands will work.\nNavigate to yourws_moveitdirectory you created in theGetting Started Tutorial.\nChange directory into thesrcdirectory, as that is where we put our source code.\nCreate a new package with the ROS 2 command line tools:\nThe output of this will show that it created some files in a new directory.\nNote that we addedmoveit_ros_planning_interfaceandrclcppas dependencies.\nThis will create the necessary changes in thepackage.xmlandCMakeLists.txtfiles so that we can depend on these two packages.\nOpen the new source file created for you atws_moveit/src/hello_moveit/src/hello_moveit.cppin your favorite editor.\nThis first block of code is a bit of boilerplate but you should be used to seeing this from the ROS 2 tutorials.\nWe will build and run the program to see that everything is right before we move on.\nChange the directory back to the workspace directoryws_moveitand run this command:\nAfter this succeeds,open a new terminal, then source the workspace environment script in that new terminal so that we can run our program.\nRun your program and see the output.\nThe program should run and exit without error.\nThe headers included at the top are just some standard C++ headers and the headers for ROS and MoveIt that we will use later.\nAfter that, we have the normal call to initialize rclcpp, and then we create our Node.\nThe first argument is a string that ROS will use to name a unique node.\nThe second is needed for MoveIt because of how we use ROS Parameters.\nNext, wecreate a loggernamed “hello_moveit” to keep our log outputs organized and configurable.\nLastly, we have the code to shutdown ROS.\nIn place of the comment that says “Next step goes here”, add this code:\nJust like before, we need to build the code before we can run it.\nIn the workspace directory,ws_moveit, run this command:\nAfter this succeeds, we need to reuse the demo launch file from the previous tutorial to start RViz and the MoveGroup node.\nIn a separate terminal, source the workspace and then execute this:\nThen in theDisplayswindow underMotionPlanning/PlanningRequest, uncheck the boxQueryGoalState.\nIn a third terminal, source the workspace and run your program.\nThis should cause the robot in RViz to move and end up in this pose:\nNote that if you run the nodehello_moveitwithout launching the demo launch file first, it will wait for 10 seconds and then print this error and exit.\nThis is because thedemo.launch.pylaunch is starting theMoveGroupnode that provides the robot description.\nWhenMoveGroupInterfaceis constructed, it looks for a node publishing a topic with the robot description.\nIf it fails to find that within 10 seconds, it prints this error and terminates the program.\nThe first thing we do is create theMoveGroupInterface.\nThis object will be used to interact withmove_group, which allows us to plan and execute trajectories.\nNote that this is the only mutable object that we create in this program.\nAnother thing to take note of is the second argument to theMoveGroupInterfaceobject we are creating here:\"manipulator\".\nThat is the group of joints as defined in the robot description that we are going to operate on with thisMoveGroupInterface.\nThen, we set our target pose and plan. Note that only the target pose is set (viasetPoseTarget).\nThe starting pose is implicitly the position published by the joint state publisher, which could be changed using theMoveGroupInterface::setStartState*family of functions (but is not in this tutorial).\nOne more thing to note about this next section is the use of lambdas for constructing the message typetarget_poseand planning.\nThis is a pattern you’ll find in modern C++ codebases that enables writing in a more declarative style.\nFor more information about this pattern, there are a couple of links at the end of this tutorial.\nFinally, we execute our plan if planning is successful, otherwise, we log an error:\nYou created a ROS 2 package and wrote your first program using MoveIt.\nYou learned about using the MoveGroupInterface to plan and execute moves.\nHere is a copy of the full hello_moveit.cpp source at the end of this tutorial.\nWe used lambdas to be able to initialize objects as constants.\nThis is known as a technique called IIFE.Read more about this pattern from C++ Stories.\nWe also declared everything we could as const.Read more about the usefulness of const here.\nIn the next tutorialVisualizing in RViz, you will expand on the program you built here to create visual markers that make it easier to understand what MoveIt is doing.\n© Copyright 2024, PickNik Robotics.\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_ros_planning_interfacerclcpp\\--node-namehello_moveithello_moveit\n#include<memory>#include<rclcpp/rclcpp.hpp>#include<moveit/move_group_interface/move_group_interface.h>intmain(intargc,char*argv[]){// Initialize ROS and create the Noderclcpp::init(argc,argv);autoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");// Next step goes here// Shutdown ROSrclcpp::shutdown();return0;}\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\nautoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Shutdown ROSrclcpp::shutdown();return0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\ncolconbuild--mixindebug\nros2launchmoveit2_tutorialsdemo.launch.py\nros2runhello_moveithello_moveit\n[ERROR][1644181704.350825487][hello_moveit]:Couldnotfindparameterrobot_descriptionanddidnotreceiverobot_descriptionviastd_msgs::msg::Stringsubscriptionwithin10.000000seconds.\nusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();\n// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\nros2\nws_moveit\nsrc\nmoveit_ros_planning_interface\nrclcpp\npackage.xml\nCMakeLists.txt\nws_moveit/src/hello_moveit/src/hello_moveit.cpp\nws_moveit\nws_moveit\nDisplays\nMotionPlanning/PlanningRequest\nQueryGoalState\nhello_moveit\ndemo.launch.py\nMoveGroup\nMoveGroupInterface\nMoveGroupInterface\nmove_group\nMoveGroupInterface\n\"manipulator\"\nMoveGroupInterface\nsetPoseTarget\nMoveGroupInterface::setStartState*\ntarget_pose",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0b3"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/your_first_project/your_first_project.html#plan-and-execute-using-movegroupinterface",
  "title": "Your First C++ MoveIt Project — MoveIt Documentation: Rolling  documentation",
  "content": "Your First C++ MoveIt Project\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will step you through writing your first C++ application with MoveIt.\nWarning: Most features in MoveIt will not work properly since additional parameters are required for full Move Group functionality. For a full setup, please continue with theMove Group C++ Interface Tutorial.\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nThis tutorial assumes you understand the basics of ROS 2.\nTo prepare yourself for this please complete theOfficial ROS 2 Tutorialsup until “Writing a simple publisher and Subscriber (C++)”.\nOpen a terminal andsource your ROS 2 installationso thatros2commands will work.\nNavigate to yourws_moveitdirectory you created in theGetting Started Tutorial.\nChange directory into thesrcdirectory, as that is where we put our source code.\nCreate a new package with the ROS 2 command line tools:\nThe output of this will show that it created some files in a new directory.\nNote that we addedmoveit_ros_planning_interfaceandrclcppas dependencies.\nThis will create the necessary changes in thepackage.xmlandCMakeLists.txtfiles so that we can depend on these two packages.\nOpen the new source file created for you atws_moveit/src/hello_moveit/src/hello_moveit.cppin your favorite editor.\nThis first block of code is a bit of boilerplate but you should be used to seeing this from the ROS 2 tutorials.\nWe will build and run the program to see that everything is right before we move on.\nChange the directory back to the workspace directoryws_moveitand run this command:\nAfter this succeeds,open a new terminal, then source the workspace environment script in that new terminal so that we can run our program.\nRun your program and see the output.\nThe program should run and exit without error.\nThe headers included at the top are just some standard C++ headers and the headers for ROS and MoveIt that we will use later.\nAfter that, we have the normal call to initialize rclcpp, and then we create our Node.\nThe first argument is a string that ROS will use to name a unique node.\nThe second is needed for MoveIt because of how we use ROS Parameters.\nNext, wecreate a loggernamed “hello_moveit” to keep our log outputs organized and configurable.\nLastly, we have the code to shutdown ROS.\nIn place of the comment that says “Next step goes here”, add this code:\nJust like before, we need to build the code before we can run it.\nIn the workspace directory,ws_moveit, run this command:\nAfter this succeeds, we need to reuse the demo launch file from the previous tutorial to start RViz and the MoveGroup node.\nIn a separate terminal, source the workspace and then execute this:\nThen in theDisplayswindow underMotionPlanning/PlanningRequest, uncheck the boxQueryGoalState.\nIn a third terminal, source the workspace and run your program.\nThis should cause the robot in RViz to move and end up in this pose:\nNote that if you run the nodehello_moveitwithout launching the demo launch file first, it will wait for 10 seconds and then print this error and exit.\nThis is because thedemo.launch.pylaunch is starting theMoveGroupnode that provides the robot description.\nWhenMoveGroupInterfaceis constructed, it looks for a node publishing a topic with the robot description.\nIf it fails to find that within 10 seconds, it prints this error and terminates the program.\nThe first thing we do is create theMoveGroupInterface.\nThis object will be used to interact withmove_group, which allows us to plan and execute trajectories.\nNote that this is the only mutable object that we create in this program.\nAnother thing to take note of is the second argument to theMoveGroupInterfaceobject we are creating here:\"manipulator\".\nThat is the group of joints as defined in the robot description that we are going to operate on with thisMoveGroupInterface.\nThen, we set our target pose and plan. Note that only the target pose is set (viasetPoseTarget).\nThe starting pose is implicitly the position published by the joint state publisher, which could be changed using theMoveGroupInterface::setStartState*family of functions (but is not in this tutorial).\nOne more thing to note about this next section is the use of lambdas for constructing the message typetarget_poseand planning.\nThis is a pattern you’ll find in modern C++ codebases that enables writing in a more declarative style.\nFor more information about this pattern, there are a couple of links at the end of this tutorial.\nFinally, we execute our plan if planning is successful, otherwise, we log an error:\nYou created a ROS 2 package and wrote your first program using MoveIt.\nYou learned about using the MoveGroupInterface to plan and execute moves.\nHere is a copy of the full hello_moveit.cpp source at the end of this tutorial.\nWe used lambdas to be able to initialize objects as constants.\nThis is known as a technique called IIFE.Read more about this pattern from C++ Stories.\nWe also declared everything we could as const.Read more about the usefulness of const here.\nIn the next tutorialVisualizing in RViz, you will expand on the program you built here to create visual markers that make it easier to understand what MoveIt is doing.\n© Copyright 2024, PickNik Robotics.\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_ros_planning_interfacerclcpp\\--node-namehello_moveithello_moveit\n#include<memory>#include<rclcpp/rclcpp.hpp>#include<moveit/move_group_interface/move_group_interface.h>intmain(intargc,char*argv[]){// Initialize ROS and create the Noderclcpp::init(argc,argv);autoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");// Next step goes here// Shutdown ROSrclcpp::shutdown();return0;}\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\nautoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Shutdown ROSrclcpp::shutdown();return0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\ncolconbuild--mixindebug\nros2launchmoveit2_tutorialsdemo.launch.py\nros2runhello_moveithello_moveit\n[ERROR][1644181704.350825487][hello_moveit]:Couldnotfindparameterrobot_descriptionanddidnotreceiverobot_descriptionviastd_msgs::msg::Stringsubscriptionwithin10.000000seconds.\nusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();\n// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\nros2\nws_moveit\nsrc\nmoveit_ros_planning_interface\nrclcpp\npackage.xml\nCMakeLists.txt\nws_moveit/src/hello_moveit/src/hello_moveit.cpp\nws_moveit\nws_moveit\nDisplays\nMotionPlanning/PlanningRequest\nQueryGoalState\nhello_moveit\ndemo.launch.py\nMoveGroup\nMoveGroupInterface\nMoveGroupInterface\nmove_group\nMoveGroupInterface\n\"manipulator\"\nMoveGroupInterface\nsetPoseTarget\nMoveGroupInterface::setStartState*\ntarget_pose",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0b4"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/your_first_project/your_first_project.html#id1",
  "title": "Your First C++ MoveIt Project — MoveIt Documentation: Rolling  documentation",
  "content": "Your First C++ MoveIt Project\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will step you through writing your first C++ application with MoveIt.\nWarning: Most features in MoveIt will not work properly since additional parameters are required for full Move Group functionality. For a full setup, please continue with theMove Group C++ Interface Tutorial.\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nThis tutorial assumes you understand the basics of ROS 2.\nTo prepare yourself for this please complete theOfficial ROS 2 Tutorialsup until “Writing a simple publisher and Subscriber (C++)”.\nOpen a terminal andsource your ROS 2 installationso thatros2commands will work.\nNavigate to yourws_moveitdirectory you created in theGetting Started Tutorial.\nChange directory into thesrcdirectory, as that is where we put our source code.\nCreate a new package with the ROS 2 command line tools:\nThe output of this will show that it created some files in a new directory.\nNote that we addedmoveit_ros_planning_interfaceandrclcppas dependencies.\nThis will create the necessary changes in thepackage.xmlandCMakeLists.txtfiles so that we can depend on these two packages.\nOpen the new source file created for you atws_moveit/src/hello_moveit/src/hello_moveit.cppin your favorite editor.\nThis first block of code is a bit of boilerplate but you should be used to seeing this from the ROS 2 tutorials.\nWe will build and run the program to see that everything is right before we move on.\nChange the directory back to the workspace directoryws_moveitand run this command:\nAfter this succeeds,open a new terminal, then source the workspace environment script in that new terminal so that we can run our program.\nRun your program and see the output.\nThe program should run and exit without error.\nThe headers included at the top are just some standard C++ headers and the headers for ROS and MoveIt that we will use later.\nAfter that, we have the normal call to initialize rclcpp, and then we create our Node.\nThe first argument is a string that ROS will use to name a unique node.\nThe second is needed for MoveIt because of how we use ROS Parameters.\nNext, wecreate a loggernamed “hello_moveit” to keep our log outputs organized and configurable.\nLastly, we have the code to shutdown ROS.\nIn place of the comment that says “Next step goes here”, add this code:\nJust like before, we need to build the code before we can run it.\nIn the workspace directory,ws_moveit, run this command:\nAfter this succeeds, we need to reuse the demo launch file from the previous tutorial to start RViz and the MoveGroup node.\nIn a separate terminal, source the workspace and then execute this:\nThen in theDisplayswindow underMotionPlanning/PlanningRequest, uncheck the boxQueryGoalState.\nIn a third terminal, source the workspace and run your program.\nThis should cause the robot in RViz to move and end up in this pose:\nNote that if you run the nodehello_moveitwithout launching the demo launch file first, it will wait for 10 seconds and then print this error and exit.\nThis is because thedemo.launch.pylaunch is starting theMoveGroupnode that provides the robot description.\nWhenMoveGroupInterfaceis constructed, it looks for a node publishing a topic with the robot description.\nIf it fails to find that within 10 seconds, it prints this error and terminates the program.\nThe first thing we do is create theMoveGroupInterface.\nThis object will be used to interact withmove_group, which allows us to plan and execute trajectories.\nNote that this is the only mutable object that we create in this program.\nAnother thing to take note of is the second argument to theMoveGroupInterfaceobject we are creating here:\"manipulator\".\nThat is the group of joints as defined in the robot description that we are going to operate on with thisMoveGroupInterface.\nThen, we set our target pose and plan. Note that only the target pose is set (viasetPoseTarget).\nThe starting pose is implicitly the position published by the joint state publisher, which could be changed using theMoveGroupInterface::setStartState*family of functions (but is not in this tutorial).\nOne more thing to note about this next section is the use of lambdas for constructing the message typetarget_poseand planning.\nThis is a pattern you’ll find in modern C++ codebases that enables writing in a more declarative style.\nFor more information about this pattern, there are a couple of links at the end of this tutorial.\nFinally, we execute our plan if planning is successful, otherwise, we log an error:\nYou created a ROS 2 package and wrote your first program using MoveIt.\nYou learned about using the MoveGroupInterface to plan and execute moves.\nHere is a copy of the full hello_moveit.cpp source at the end of this tutorial.\nWe used lambdas to be able to initialize objects as constants.\nThis is known as a technique called IIFE.Read more about this pattern from C++ Stories.\nWe also declared everything we could as const.Read more about the usefulness of const here.\nIn the next tutorialVisualizing in RViz, you will expand on the program you built here to create visual markers that make it easier to understand what MoveIt is doing.\n© Copyright 2024, PickNik Robotics.\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_ros_planning_interfacerclcpp\\--node-namehello_moveithello_moveit\n#include<memory>#include<rclcpp/rclcpp.hpp>#include<moveit/move_group_interface/move_group_interface.h>intmain(intargc,char*argv[]){// Initialize ROS and create the Noderclcpp::init(argc,argv);autoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");// Next step goes here// Shutdown ROSrclcpp::shutdown();return0;}\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\nautoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Shutdown ROSrclcpp::shutdown();return0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\ncolconbuild--mixindebug\nros2launchmoveit2_tutorialsdemo.launch.py\nros2runhello_moveithello_moveit\n[ERROR][1644181704.350825487][hello_moveit]:Couldnotfindparameterrobot_descriptionanddidnotreceiverobot_descriptionviastd_msgs::msg::Stringsubscriptionwithin10.000000seconds.\nusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();\n// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\nros2\nws_moveit\nsrc\nmoveit_ros_planning_interface\nrclcpp\npackage.xml\nCMakeLists.txt\nws_moveit/src/hello_moveit/src/hello_moveit.cpp\nws_moveit\nws_moveit\nDisplays\nMotionPlanning/PlanningRequest\nQueryGoalState\nhello_moveit\ndemo.launch.py\nMoveGroup\nMoveGroupInterface\nMoveGroupInterface\nmove_group\nMoveGroupInterface\n\"manipulator\"\nMoveGroupInterface\nsetPoseTarget\nMoveGroupInterface::setStartState*\ntarget_pose",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0b5"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/your_first_project/your_first_project.html#id2",
  "title": "Your First C++ MoveIt Project — MoveIt Documentation: Rolling  documentation",
  "content": "Your First C++ MoveIt Project\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will step you through writing your first C++ application with MoveIt.\nWarning: Most features in MoveIt will not work properly since additional parameters are required for full Move Group functionality. For a full setup, please continue with theMove Group C++ Interface Tutorial.\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nThis tutorial assumes you understand the basics of ROS 2.\nTo prepare yourself for this please complete theOfficial ROS 2 Tutorialsup until “Writing a simple publisher and Subscriber (C++)”.\nOpen a terminal andsource your ROS 2 installationso thatros2commands will work.\nNavigate to yourws_moveitdirectory you created in theGetting Started Tutorial.\nChange directory into thesrcdirectory, as that is where we put our source code.\nCreate a new package with the ROS 2 command line tools:\nThe output of this will show that it created some files in a new directory.\nNote that we addedmoveit_ros_planning_interfaceandrclcppas dependencies.\nThis will create the necessary changes in thepackage.xmlandCMakeLists.txtfiles so that we can depend on these two packages.\nOpen the new source file created for you atws_moveit/src/hello_moveit/src/hello_moveit.cppin your favorite editor.\nThis first block of code is a bit of boilerplate but you should be used to seeing this from the ROS 2 tutorials.\nWe will build and run the program to see that everything is right before we move on.\nChange the directory back to the workspace directoryws_moveitand run this command:\nAfter this succeeds,open a new terminal, then source the workspace environment script in that new terminal so that we can run our program.\nRun your program and see the output.\nThe program should run and exit without error.\nThe headers included at the top are just some standard C++ headers and the headers for ROS and MoveIt that we will use later.\nAfter that, we have the normal call to initialize rclcpp, and then we create our Node.\nThe first argument is a string that ROS will use to name a unique node.\nThe second is needed for MoveIt because of how we use ROS Parameters.\nNext, wecreate a loggernamed “hello_moveit” to keep our log outputs organized and configurable.\nLastly, we have the code to shutdown ROS.\nIn place of the comment that says “Next step goes here”, add this code:\nJust like before, we need to build the code before we can run it.\nIn the workspace directory,ws_moveit, run this command:\nAfter this succeeds, we need to reuse the demo launch file from the previous tutorial to start RViz and the MoveGroup node.\nIn a separate terminal, source the workspace and then execute this:\nThen in theDisplayswindow underMotionPlanning/PlanningRequest, uncheck the boxQueryGoalState.\nIn a third terminal, source the workspace and run your program.\nThis should cause the robot in RViz to move and end up in this pose:\nNote that if you run the nodehello_moveitwithout launching the demo launch file first, it will wait for 10 seconds and then print this error and exit.\nThis is because thedemo.launch.pylaunch is starting theMoveGroupnode that provides the robot description.\nWhenMoveGroupInterfaceis constructed, it looks for a node publishing a topic with the robot description.\nIf it fails to find that within 10 seconds, it prints this error and terminates the program.\nThe first thing we do is create theMoveGroupInterface.\nThis object will be used to interact withmove_group, which allows us to plan and execute trajectories.\nNote that this is the only mutable object that we create in this program.\nAnother thing to take note of is the second argument to theMoveGroupInterfaceobject we are creating here:\"manipulator\".\nThat is the group of joints as defined in the robot description that we are going to operate on with thisMoveGroupInterface.\nThen, we set our target pose and plan. Note that only the target pose is set (viasetPoseTarget).\nThe starting pose is implicitly the position published by the joint state publisher, which could be changed using theMoveGroupInterface::setStartState*family of functions (but is not in this tutorial).\nOne more thing to note about this next section is the use of lambdas for constructing the message typetarget_poseand planning.\nThis is a pattern you’ll find in modern C++ codebases that enables writing in a more declarative style.\nFor more information about this pattern, there are a couple of links at the end of this tutorial.\nFinally, we execute our plan if planning is successful, otherwise, we log an error:\nYou created a ROS 2 package and wrote your first program using MoveIt.\nYou learned about using the MoveGroupInterface to plan and execute moves.\nHere is a copy of the full hello_moveit.cpp source at the end of this tutorial.\nWe used lambdas to be able to initialize objects as constants.\nThis is known as a technique called IIFE.Read more about this pattern from C++ Stories.\nWe also declared everything we could as const.Read more about the usefulness of const here.\nIn the next tutorialVisualizing in RViz, you will expand on the program you built here to create visual markers that make it easier to understand what MoveIt is doing.\n© Copyright 2024, PickNik Robotics.\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_ros_planning_interfacerclcpp\\--node-namehello_moveithello_moveit\n#include<memory>#include<rclcpp/rclcpp.hpp>#include<moveit/move_group_interface/move_group_interface.h>intmain(intargc,char*argv[]){// Initialize ROS and create the Noderclcpp::init(argc,argv);autoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");// Next step goes here// Shutdown ROSrclcpp::shutdown();return0;}\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\nautoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Shutdown ROSrclcpp::shutdown();return0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\ncolconbuild--mixindebug\nros2launchmoveit2_tutorialsdemo.launch.py\nros2runhello_moveithello_moveit\n[ERROR][1644181704.350825487][hello_moveit]:Couldnotfindparameterrobot_descriptionanddidnotreceiverobot_descriptionviastd_msgs::msg::Stringsubscriptionwithin10.000000seconds.\nusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();\n// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\nros2\nws_moveit\nsrc\nmoveit_ros_planning_interface\nrclcpp\npackage.xml\nCMakeLists.txt\nws_moveit/src/hello_moveit/src/hello_moveit.cpp\nws_moveit\nws_moveit\nDisplays\nMotionPlanning/PlanningRequest\nQueryGoalState\nhello_moveit\ndemo.launch.py\nMoveGroup\nMoveGroupInterface\nMoveGroupInterface\nmove_group\nMoveGroupInterface\n\"manipulator\"\nMoveGroupInterface\nsetPoseTarget\nMoveGroupInterface::setStartState*\ntarget_pose",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0b6"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/your_first_project/your_first_project.html#summary",
  "title": "Your First C++ MoveIt Project — MoveIt Documentation: Rolling  documentation",
  "content": "Your First C++ MoveIt Project\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will step you through writing your first C++ application with MoveIt.\nWarning: Most features in MoveIt will not work properly since additional parameters are required for full Move Group functionality. For a full setup, please continue with theMove Group C++ Interface Tutorial.\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nThis tutorial assumes you understand the basics of ROS 2.\nTo prepare yourself for this please complete theOfficial ROS 2 Tutorialsup until “Writing a simple publisher and Subscriber (C++)”.\nOpen a terminal andsource your ROS 2 installationso thatros2commands will work.\nNavigate to yourws_moveitdirectory you created in theGetting Started Tutorial.\nChange directory into thesrcdirectory, as that is where we put our source code.\nCreate a new package with the ROS 2 command line tools:\nThe output of this will show that it created some files in a new directory.\nNote that we addedmoveit_ros_planning_interfaceandrclcppas dependencies.\nThis will create the necessary changes in thepackage.xmlandCMakeLists.txtfiles so that we can depend on these two packages.\nOpen the new source file created for you atws_moveit/src/hello_moveit/src/hello_moveit.cppin your favorite editor.\nThis first block of code is a bit of boilerplate but you should be used to seeing this from the ROS 2 tutorials.\nWe will build and run the program to see that everything is right before we move on.\nChange the directory back to the workspace directoryws_moveitand run this command:\nAfter this succeeds,open a new terminal, then source the workspace environment script in that new terminal so that we can run our program.\nRun your program and see the output.\nThe program should run and exit without error.\nThe headers included at the top are just some standard C++ headers and the headers for ROS and MoveIt that we will use later.\nAfter that, we have the normal call to initialize rclcpp, and then we create our Node.\nThe first argument is a string that ROS will use to name a unique node.\nThe second is needed for MoveIt because of how we use ROS Parameters.\nNext, wecreate a loggernamed “hello_moveit” to keep our log outputs organized and configurable.\nLastly, we have the code to shutdown ROS.\nIn place of the comment that says “Next step goes here”, add this code:\nJust like before, we need to build the code before we can run it.\nIn the workspace directory,ws_moveit, run this command:\nAfter this succeeds, we need to reuse the demo launch file from the previous tutorial to start RViz and the MoveGroup node.\nIn a separate terminal, source the workspace and then execute this:\nThen in theDisplayswindow underMotionPlanning/PlanningRequest, uncheck the boxQueryGoalState.\nIn a third terminal, source the workspace and run your program.\nThis should cause the robot in RViz to move and end up in this pose:\nNote that if you run the nodehello_moveitwithout launching the demo launch file first, it will wait for 10 seconds and then print this error and exit.\nThis is because thedemo.launch.pylaunch is starting theMoveGroupnode that provides the robot description.\nWhenMoveGroupInterfaceis constructed, it looks for a node publishing a topic with the robot description.\nIf it fails to find that within 10 seconds, it prints this error and terminates the program.\nThe first thing we do is create theMoveGroupInterface.\nThis object will be used to interact withmove_group, which allows us to plan and execute trajectories.\nNote that this is the only mutable object that we create in this program.\nAnother thing to take note of is the second argument to theMoveGroupInterfaceobject we are creating here:\"manipulator\".\nThat is the group of joints as defined in the robot description that we are going to operate on with thisMoveGroupInterface.\nThen, we set our target pose and plan. Note that only the target pose is set (viasetPoseTarget).\nThe starting pose is implicitly the position published by the joint state publisher, which could be changed using theMoveGroupInterface::setStartState*family of functions (but is not in this tutorial).\nOne more thing to note about this next section is the use of lambdas for constructing the message typetarget_poseand planning.\nThis is a pattern you’ll find in modern C++ codebases that enables writing in a more declarative style.\nFor more information about this pattern, there are a couple of links at the end of this tutorial.\nFinally, we execute our plan if planning is successful, otherwise, we log an error:\nYou created a ROS 2 package and wrote your first program using MoveIt.\nYou learned about using the MoveGroupInterface to plan and execute moves.\nHere is a copy of the full hello_moveit.cpp source at the end of this tutorial.\nWe used lambdas to be able to initialize objects as constants.\nThis is known as a technique called IIFE.Read more about this pattern from C++ Stories.\nWe also declared everything we could as const.Read more about the usefulness of const here.\nIn the next tutorialVisualizing in RViz, you will expand on the program you built here to create visual markers that make it easier to understand what MoveIt is doing.\n© Copyright 2024, PickNik Robotics.\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_ros_planning_interfacerclcpp\\--node-namehello_moveithello_moveit\n#include<memory>#include<rclcpp/rclcpp.hpp>#include<moveit/move_group_interface/move_group_interface.h>intmain(intargc,char*argv[]){// Initialize ROS and create the Noderclcpp::init(argc,argv);autoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");// Next step goes here// Shutdown ROSrclcpp::shutdown();return0;}\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\nautoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Shutdown ROSrclcpp::shutdown();return0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\ncolconbuild--mixindebug\nros2launchmoveit2_tutorialsdemo.launch.py\nros2runhello_moveithello_moveit\n[ERROR][1644181704.350825487][hello_moveit]:Couldnotfindparameterrobot_descriptionanddidnotreceiverobot_descriptionviastd_msgs::msg::Stringsubscriptionwithin10.000000seconds.\nusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();\n// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\nros2\nws_moveit\nsrc\nmoveit_ros_planning_interface\nrclcpp\npackage.xml\nCMakeLists.txt\nws_moveit/src/hello_moveit/src/hello_moveit.cpp\nws_moveit\nws_moveit\nDisplays\nMotionPlanning/PlanningRequest\nQueryGoalState\nhello_moveit\ndemo.launch.py\nMoveGroup\nMoveGroupInterface\nMoveGroupInterface\nmove_group\nMoveGroupInterface\n\"manipulator\"\nMoveGroupInterface\nsetPoseTarget\nMoveGroupInterface::setStartState*\ntarget_pose",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0b7"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/your_first_project/your_first_project.html#further-reading",
  "title": "Your First C++ MoveIt Project — MoveIt Documentation: Rolling  documentation",
  "content": "Your First C++ MoveIt Project\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will step you through writing your first C++ application with MoveIt.\nWarning: Most features in MoveIt will not work properly since additional parameters are required for full Move Group functionality. For a full setup, please continue with theMove Group C++ Interface Tutorial.\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nThis tutorial assumes you understand the basics of ROS 2.\nTo prepare yourself for this please complete theOfficial ROS 2 Tutorialsup until “Writing a simple publisher and Subscriber (C++)”.\nOpen a terminal andsource your ROS 2 installationso thatros2commands will work.\nNavigate to yourws_moveitdirectory you created in theGetting Started Tutorial.\nChange directory into thesrcdirectory, as that is where we put our source code.\nCreate a new package with the ROS 2 command line tools:\nThe output of this will show that it created some files in a new directory.\nNote that we addedmoveit_ros_planning_interfaceandrclcppas dependencies.\nThis will create the necessary changes in thepackage.xmlandCMakeLists.txtfiles so that we can depend on these two packages.\nOpen the new source file created for you atws_moveit/src/hello_moveit/src/hello_moveit.cppin your favorite editor.\nThis first block of code is a bit of boilerplate but you should be used to seeing this from the ROS 2 tutorials.\nWe will build and run the program to see that everything is right before we move on.\nChange the directory back to the workspace directoryws_moveitand run this command:\nAfter this succeeds,open a new terminal, then source the workspace environment script in that new terminal so that we can run our program.\nRun your program and see the output.\nThe program should run and exit without error.\nThe headers included at the top are just some standard C++ headers and the headers for ROS and MoveIt that we will use later.\nAfter that, we have the normal call to initialize rclcpp, and then we create our Node.\nThe first argument is a string that ROS will use to name a unique node.\nThe second is needed for MoveIt because of how we use ROS Parameters.\nNext, wecreate a loggernamed “hello_moveit” to keep our log outputs organized and configurable.\nLastly, we have the code to shutdown ROS.\nIn place of the comment that says “Next step goes here”, add this code:\nJust like before, we need to build the code before we can run it.\nIn the workspace directory,ws_moveit, run this command:\nAfter this succeeds, we need to reuse the demo launch file from the previous tutorial to start RViz and the MoveGroup node.\nIn a separate terminal, source the workspace and then execute this:\nThen in theDisplayswindow underMotionPlanning/PlanningRequest, uncheck the boxQueryGoalState.\nIn a third terminal, source the workspace and run your program.\nThis should cause the robot in RViz to move and end up in this pose:\nNote that if you run the nodehello_moveitwithout launching the demo launch file first, it will wait for 10 seconds and then print this error and exit.\nThis is because thedemo.launch.pylaunch is starting theMoveGroupnode that provides the robot description.\nWhenMoveGroupInterfaceis constructed, it looks for a node publishing a topic with the robot description.\nIf it fails to find that within 10 seconds, it prints this error and terminates the program.\nThe first thing we do is create theMoveGroupInterface.\nThis object will be used to interact withmove_group, which allows us to plan and execute trajectories.\nNote that this is the only mutable object that we create in this program.\nAnother thing to take note of is the second argument to theMoveGroupInterfaceobject we are creating here:\"manipulator\".\nThat is the group of joints as defined in the robot description that we are going to operate on with thisMoveGroupInterface.\nThen, we set our target pose and plan. Note that only the target pose is set (viasetPoseTarget).\nThe starting pose is implicitly the position published by the joint state publisher, which could be changed using theMoveGroupInterface::setStartState*family of functions (but is not in this tutorial).\nOne more thing to note about this next section is the use of lambdas for constructing the message typetarget_poseand planning.\nThis is a pattern you’ll find in modern C++ codebases that enables writing in a more declarative style.\nFor more information about this pattern, there are a couple of links at the end of this tutorial.\nFinally, we execute our plan if planning is successful, otherwise, we log an error:\nYou created a ROS 2 package and wrote your first program using MoveIt.\nYou learned about using the MoveGroupInterface to plan and execute moves.\nHere is a copy of the full hello_moveit.cpp source at the end of this tutorial.\nWe used lambdas to be able to initialize objects as constants.\nThis is known as a technique called IIFE.Read more about this pattern from C++ Stories.\nWe also declared everything we could as const.Read more about the usefulness of const here.\nIn the next tutorialVisualizing in RViz, you will expand on the program you built here to create visual markers that make it easier to understand what MoveIt is doing.\n© Copyright 2024, PickNik Robotics.\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_ros_planning_interfacerclcpp\\--node-namehello_moveithello_moveit\n#include<memory>#include<rclcpp/rclcpp.hpp>#include<moveit/move_group_interface/move_group_interface.h>intmain(intargc,char*argv[]){// Initialize ROS and create the Noderclcpp::init(argc,argv);autoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");// Next step goes here// Shutdown ROSrclcpp::shutdown();return0;}\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\nautoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Shutdown ROSrclcpp::shutdown();return0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\ncolconbuild--mixindebug\nros2launchmoveit2_tutorialsdemo.launch.py\nros2runhello_moveithello_moveit\n[ERROR][1644181704.350825487][hello_moveit]:Couldnotfindparameterrobot_descriptionanddidnotreceiverobot_descriptionviastd_msgs::msg::Stringsubscriptionwithin10.000000seconds.\nusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();\n// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\nros2\nws_moveit\nsrc\nmoveit_ros_planning_interface\nrclcpp\npackage.xml\nCMakeLists.txt\nws_moveit/src/hello_moveit/src/hello_moveit.cpp\nws_moveit\nws_moveit\nDisplays\nMotionPlanning/PlanningRequest\nQueryGoalState\nhello_moveit\ndemo.launch.py\nMoveGroup\nMoveGroupInterface\nMoveGroupInterface\nmove_group\nMoveGroupInterface\n\"manipulator\"\nMoveGroupInterface\nsetPoseTarget\nMoveGroupInterface::setStartState*\ntarget_pose",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0b8"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/your_first_project/your_first_project.html#next-step",
  "title": "Your First C++ MoveIt Project — MoveIt Documentation: Rolling  documentation",
  "content": "Your First C++ MoveIt Project\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will step you through writing your first C++ application with MoveIt.\nWarning: Most features in MoveIt will not work properly since additional parameters are required for full Move Group functionality. For a full setup, please continue with theMove Group C++ Interface Tutorial.\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nThis tutorial assumes you understand the basics of ROS 2.\nTo prepare yourself for this please complete theOfficial ROS 2 Tutorialsup until “Writing a simple publisher and Subscriber (C++)”.\nOpen a terminal andsource your ROS 2 installationso thatros2commands will work.\nNavigate to yourws_moveitdirectory you created in theGetting Started Tutorial.\nChange directory into thesrcdirectory, as that is where we put our source code.\nCreate a new package with the ROS 2 command line tools:\nThe output of this will show that it created some files in a new directory.\nNote that we addedmoveit_ros_planning_interfaceandrclcppas dependencies.\nThis will create the necessary changes in thepackage.xmlandCMakeLists.txtfiles so that we can depend on these two packages.\nOpen the new source file created for you atws_moveit/src/hello_moveit/src/hello_moveit.cppin your favorite editor.\nThis first block of code is a bit of boilerplate but you should be used to seeing this from the ROS 2 tutorials.\nWe will build and run the program to see that everything is right before we move on.\nChange the directory back to the workspace directoryws_moveitand run this command:\nAfter this succeeds,open a new terminal, then source the workspace environment script in that new terminal so that we can run our program.\nRun your program and see the output.\nThe program should run and exit without error.\nThe headers included at the top are just some standard C++ headers and the headers for ROS and MoveIt that we will use later.\nAfter that, we have the normal call to initialize rclcpp, and then we create our Node.\nThe first argument is a string that ROS will use to name a unique node.\nThe second is needed for MoveIt because of how we use ROS Parameters.\nNext, wecreate a loggernamed “hello_moveit” to keep our log outputs organized and configurable.\nLastly, we have the code to shutdown ROS.\nIn place of the comment that says “Next step goes here”, add this code:\nJust like before, we need to build the code before we can run it.\nIn the workspace directory,ws_moveit, run this command:\nAfter this succeeds, we need to reuse the demo launch file from the previous tutorial to start RViz and the MoveGroup node.\nIn a separate terminal, source the workspace and then execute this:\nThen in theDisplayswindow underMotionPlanning/PlanningRequest, uncheck the boxQueryGoalState.\nIn a third terminal, source the workspace and run your program.\nThis should cause the robot in RViz to move and end up in this pose:\nNote that if you run the nodehello_moveitwithout launching the demo launch file first, it will wait for 10 seconds and then print this error and exit.\nThis is because thedemo.launch.pylaunch is starting theMoveGroupnode that provides the robot description.\nWhenMoveGroupInterfaceis constructed, it looks for a node publishing a topic with the robot description.\nIf it fails to find that within 10 seconds, it prints this error and terminates the program.\nThe first thing we do is create theMoveGroupInterface.\nThis object will be used to interact withmove_group, which allows us to plan and execute trajectories.\nNote that this is the only mutable object that we create in this program.\nAnother thing to take note of is the second argument to theMoveGroupInterfaceobject we are creating here:\"manipulator\".\nThat is the group of joints as defined in the robot description that we are going to operate on with thisMoveGroupInterface.\nThen, we set our target pose and plan. Note that only the target pose is set (viasetPoseTarget).\nThe starting pose is implicitly the position published by the joint state publisher, which could be changed using theMoveGroupInterface::setStartState*family of functions (but is not in this tutorial).\nOne more thing to note about this next section is the use of lambdas for constructing the message typetarget_poseand planning.\nThis is a pattern you’ll find in modern C++ codebases that enables writing in a more declarative style.\nFor more information about this pattern, there are a couple of links at the end of this tutorial.\nFinally, we execute our plan if planning is successful, otherwise, we log an error:\nYou created a ROS 2 package and wrote your first program using MoveIt.\nYou learned about using the MoveGroupInterface to plan and execute moves.\nHere is a copy of the full hello_moveit.cpp source at the end of this tutorial.\nWe used lambdas to be able to initialize objects as constants.\nThis is known as a technique called IIFE.Read more about this pattern from C++ Stories.\nWe also declared everything we could as const.Read more about the usefulness of const here.\nIn the next tutorialVisualizing in RViz, you will expand on the program you built here to create visual markers that make it easier to understand what MoveIt is doing.\n© Copyright 2024, PickNik Robotics.\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_ros_planning_interfacerclcpp\\--node-namehello_moveithello_moveit\n#include<memory>#include<rclcpp/rclcpp.hpp>#include<moveit/move_group_interface/move_group_interface.h>intmain(intargc,char*argv[]){// Initialize ROS and create the Noderclcpp::init(argc,argv);autoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");// Next step goes here// Shutdown ROSrclcpp::shutdown();return0;}\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\nautoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Shutdown ROSrclcpp::shutdown();return0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\ncolconbuild--mixindebug\nros2launchmoveit2_tutorialsdemo.launch.py\nros2runhello_moveithello_moveit\n[ERROR][1644181704.350825487][hello_moveit]:Couldnotfindparameterrobot_descriptionanddidnotreceiverobot_descriptionviastd_msgs::msg::Stringsubscriptionwithin10.000000seconds.\nusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();\n// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\nros2\nws_moveit\nsrc\nmoveit_ros_planning_interface\nrclcpp\npackage.xml\nCMakeLists.txt\nws_moveit/src/hello_moveit/src/hello_moveit.cpp\nws_moveit\nws_moveit\nDisplays\nMotionPlanning/PlanningRequest\nQueryGoalState\nhello_moveit\ndemo.launch.py\nMoveGroup\nMoveGroupInterface\nMoveGroupInterface\nmove_group\nMoveGroupInterface\n\"manipulator\"\nMoveGroupInterface\nsetPoseTarget\nMoveGroupInterface::setStartState*\ntarget_pose",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0b9"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/your_first_project/your_first_project.html#your-first-c-moveit-project",
  "title": "Your First C++ MoveIt Project — MoveIt Documentation: Rolling  documentation",
  "content": "Your First C++ MoveIt Project\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will step you through writing your first C++ application with MoveIt.\nWarning: Most features in MoveIt will not work properly since additional parameters are required for full Move Group functionality. For a full setup, please continue with theMove Group C++ Interface Tutorial.\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nThis tutorial assumes you understand the basics of ROS 2.\nTo prepare yourself for this please complete theOfficial ROS 2 Tutorialsup until “Writing a simple publisher and Subscriber (C++)”.\nOpen a terminal andsource your ROS 2 installationso thatros2commands will work.\nNavigate to yourws_moveitdirectory you created in theGetting Started Tutorial.\nChange directory into thesrcdirectory, as that is where we put our source code.\nCreate a new package with the ROS 2 command line tools:\nThe output of this will show that it created some files in a new directory.\nNote that we addedmoveit_ros_planning_interfaceandrclcppas dependencies.\nThis will create the necessary changes in thepackage.xmlandCMakeLists.txtfiles so that we can depend on these two packages.\nOpen the new source file created for you atws_moveit/src/hello_moveit/src/hello_moveit.cppin your favorite editor.\nThis first block of code is a bit of boilerplate but you should be used to seeing this from the ROS 2 tutorials.\nWe will build and run the program to see that everything is right before we move on.\nChange the directory back to the workspace directoryws_moveitand run this command:\nAfter this succeeds,open a new terminal, then source the workspace environment script in that new terminal so that we can run our program.\nRun your program and see the output.\nThe program should run and exit without error.\nThe headers included at the top are just some standard C++ headers and the headers for ROS and MoveIt that we will use later.\nAfter that, we have the normal call to initialize rclcpp, and then we create our Node.\nThe first argument is a string that ROS will use to name a unique node.\nThe second is needed for MoveIt because of how we use ROS Parameters.\nNext, wecreate a loggernamed “hello_moveit” to keep our log outputs organized and configurable.\nLastly, we have the code to shutdown ROS.\nIn place of the comment that says “Next step goes here”, add this code:\nJust like before, we need to build the code before we can run it.\nIn the workspace directory,ws_moveit, run this command:\nAfter this succeeds, we need to reuse the demo launch file from the previous tutorial to start RViz and the MoveGroup node.\nIn a separate terminal, source the workspace and then execute this:\nThen in theDisplayswindow underMotionPlanning/PlanningRequest, uncheck the boxQueryGoalState.\nIn a third terminal, source the workspace and run your program.\nThis should cause the robot in RViz to move and end up in this pose:\nNote that if you run the nodehello_moveitwithout launching the demo launch file first, it will wait for 10 seconds and then print this error and exit.\nThis is because thedemo.launch.pylaunch is starting theMoveGroupnode that provides the robot description.\nWhenMoveGroupInterfaceis constructed, it looks for a node publishing a topic with the robot description.\nIf it fails to find that within 10 seconds, it prints this error and terminates the program.\nThe first thing we do is create theMoveGroupInterface.\nThis object will be used to interact withmove_group, which allows us to plan and execute trajectories.\nNote that this is the only mutable object that we create in this program.\nAnother thing to take note of is the second argument to theMoveGroupInterfaceobject we are creating here:\"manipulator\".\nThat is the group of joints as defined in the robot description that we are going to operate on with thisMoveGroupInterface.\nThen, we set our target pose and plan. Note that only the target pose is set (viasetPoseTarget).\nThe starting pose is implicitly the position published by the joint state publisher, which could be changed using theMoveGroupInterface::setStartState*family of functions (but is not in this tutorial).\nOne more thing to note about this next section is the use of lambdas for constructing the message typetarget_poseand planning.\nThis is a pattern you’ll find in modern C++ codebases that enables writing in a more declarative style.\nFor more information about this pattern, there are a couple of links at the end of this tutorial.\nFinally, we execute our plan if planning is successful, otherwise, we log an error:\nYou created a ROS 2 package and wrote your first program using MoveIt.\nYou learned about using the MoveGroupInterface to plan and execute moves.\nHere is a copy of the full hello_moveit.cpp source at the end of this tutorial.\nWe used lambdas to be able to initialize objects as constants.\nThis is known as a technique called IIFE.Read more about this pattern from C++ Stories.\nWe also declared everything we could as const.Read more about the usefulness of const here.\nIn the next tutorialVisualizing in RViz, you will expand on the program you built here to create visual markers that make it easier to understand what MoveIt is doing.\n© Copyright 2024, PickNik Robotics.\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_ros_planning_interfacerclcpp\\--node-namehello_moveithello_moveit\n#include<memory>#include<rclcpp/rclcpp.hpp>#include<moveit/move_group_interface/move_group_interface.h>intmain(intargc,char*argv[]){// Initialize ROS and create the Noderclcpp::init(argc,argv);autoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");// Next step goes here// Shutdown ROSrclcpp::shutdown();return0;}\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\nautoconstnode=std::make_shared<rclcpp::Node>(\"hello_moveit\",rclcpp::NodeOptions().automatically_declare_parameters_from_overrides(true));\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Shutdown ROSrclcpp::shutdown();return0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\ncolconbuild--mixindebug\nros2launchmoveit2_tutorialsdemo.launch.py\nros2runhello_moveithello_moveit\n[ERROR][1644181704.350825487][hello_moveit]:Couldnotfindparameterrobot_descriptionanddidnotreceiverobot_descriptionviastd_msgs::msg::Stringsubscriptionwithin10.000000seconds.\nusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseautoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();\n// Execute the planif(success){move_group_interface.execute(plan);}else{RCLCPP_ERROR(logger,\"Planning failed!\");}\nros2\nws_moveit\nsrc\nmoveit_ros_planning_interface\nrclcpp\npackage.xml\nCMakeLists.txt\nws_moveit/src/hello_moveit/src/hello_moveit.cpp\nws_moveit\nws_moveit\nDisplays\nMotionPlanning/PlanningRequest\nQueryGoalState\nhello_moveit\ndemo.launch.py\nMoveGroup\nMoveGroupInterface\nMoveGroupInterface\nmove_group\nMoveGroupInterface\n\"manipulator\"\nMoveGroupInterface\nsetPoseTarget\nMoveGroupInterface::setStartState*\ntarget_pose",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0ba"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/visualizing_in_rviz/visualizing_in_rviz.html",
  "title": "Visualizing In RViz — MoveIt Documentation: Rolling  documentation",
  "content": "Visualizing In RViz\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to a tool that can help you more easily understand what your MoveIt application is doing by rendering visualizations in RViz.\nIf you haven’t already done so, make sure you’ve completed the steps inYour First Project.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off.\nAdd this line to yourpackage.xmlin thehello_moveitproject after the other<depend>statements:\nThen in yourCMakeLists.txtadd this line to the section offind_packagestatements:\nFurther down in the file extend theament_target_dependenciesmacro call to include the new dependency like this:\nTo verify that you added the dependency correctly, add the required include to your source filehello_moveit.cpp:\nTo test that this all worked, open a terminal in the workspace directory (remembering to source your ROS install in opt) and then build with colcon:\nBefore we can initialize MoveItVisualTools, we need to have a executor spinning on our ROS node.\nThis is necessary because of how MoveItVisualTools interacts with ROS services and topics. First, add the threading library to your includes at the top.\nBy creating and naming loggers, we are able to keep our program logs organized.\nNext, add your executor before creating the MoveIt MoveGroup Interface.\nFinally, make sure to join the thread before exiting.\nAfter making these changes, rebuild your workspace to make sure you don’t have any syntax errors.\nNext, we will construct and initialize MoveItVisualTools after the construction of MoveGroupInterface.\nWe pass the following into the constructor: the ROS node, the base link of the robot, the marker topic to use (more on this later), and the robot model (which we get from the move_group_interface).\nNext, we make a call to delete all the markers. This clears any rendered state out of RViz that we have left over from previous runs.\nLastly, we load remote control.\nRemote control is a really simple plugin that lets us have a button in RViz to interact with our program.\nAfter we’ve constructed and initialized, we now create some closures (function objects that have access to variables in our current scope) that we can use later in our program to help render visualizations in RViz.\nEach of the three closures capturemoveit_visual_toolsby reference and the last one captures a pointer to the joint model group object we are planning with.\nEach of these call a function onmoveit_visual_toolsthat changes something in RViz.\nThe first one,draw_titleadds text one meter above the base of the robot. This is a useful way to show the state of your program from a high level.\nThe second one calls a function calledprompt. This function blocks your program until the user presses thenextbutton in RViz. This is helpful for stepping through a program when debugging.\nThe last one draws the tool path of a trajectory that we have planned. This is often helpful for understanding a planned trajectory from the perspective of the tool.\nYou might be asking yourself why we would create lambdas like this, and the reason is simply to make the code that comes later easier to read and understand.\nAs your write software, it is often helpful to break up your functionality into named functions which can be easily reused and tested on their own.\nYou will see in the next section how we use these functions we created.\nNow we’ll augment the code in the middle of your program.\nUpdate your code for planning and executing to include these new features:\nOne thing you’ll quickly notice is that we have to call a method calledtriggeronmoveit_visual_toolsafter each call to change something rendered in RViz.\nThe reason for this is that messages sent to RViz are batched up and sent when you calltriggerto reduce bandwidth of the marker topics.\nLastly, build your project again to make sure all the code additions are correct.\nOpen a new terminal, source the workspace, and then start the demo launch file that opens RViz.\nUncheck “MotionPlanning” in the “Displays” tab to hide it.\nWe aren’t going to be using the “MotionPlanning” plugin for this next part.\nTo add the buttons to interact with the prompts we added to our program open the dialog with the “Panels/Add New Panel” menu:\nThen selectRvizVisualToolsGuiand click OK.\nThis will create a new panel on the bottom left with aNextbutton we’ll use later.\nFinally, we need to add aMarkerArrayto render the visualizations we’ve added.\nClick on the “Add” Button in the “Displays” panel.\nSelectMarkerArrayand clickOK.\nScroll to the bottom of the items in the Displays panel and edit the topic that the new Marker Array is using to/rviz_visual_tools.\nYou are now ready to run your new program with visualizations.\nIn a new terminal, go to the workspace, source the workspace, and runhello_moveit:\nYou’ll notice that your program has stopped with a log that looks like this:\nClick theNextbutton in RViz and see your application advance.\nYou’ll see after you clicked the next button, your application planned, added a title above the robot, and drew a line representing the tool path.\nTo continue, pressNextagain to see your robot execute the plan.\nYou extended the program you wrote with MoveIt to interact with the Gui in RViz, allowing you to step through your program with a button, render some text above the robot, and display the tool path that you planned.\nMoveItVisualTools has many more useful features for visualizing robot motions.You can read more about it here.\nThere are also more examples of usingMoveItVisualToolsinMoveItCpp Tutorial.\nHere is a copy of the full hello_moveit.cpp source.\nIn the next tutorialPlanning Around Objects, you will expand on the program you built here to add to the collision environment and see the robot plan with these changes.\n© Copyright 2024, PickNik Robotics.\n<depend>moveit_visual_tools</depend>\nfind_package(moveit_visual_toolsREQUIRED)\nament_target_dependencies(hello_moveit\"moveit_ros_planning_interface\"\"moveit_visual_tools\"\"rclcpp\")\n#include<moveit_visual_tools/moveit_visual_tools.h>\ncd~/ws_moveit\ncolconbuild--mixindebug\n#include<thread>// <---- add this to the set of includes at the top\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Spin up a SingleThreadedExecutor for MoveItVisualTools to interact with ROSrclcpp::executors::SingleThreadedExecutorexecutor;executor.add_node(node);autospinner=std::thread([&executor](){executor.spin();});// Create the MoveIt MoveGroup Interface...\n// Shutdown ROSrclcpp::shutdown();// <--- This will cause the spin function in the thread to returnspinner.join();// <--- Join the thread before exitingreturn0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Construct and initialize MoveItVisualToolsautomoveit_visual_tools=moveit_visual_tools::MoveItVisualTools{node,\"base_link\",rviz_visual_tools::RVIZ_MARKER_TOPIC,move_group_interface.getRobotModel()};moveit_visual_tools.deleteAllMarkers();moveit_visual_tools.loadRemoteControl();\n// Create closures for visualizationautoconstdraw_title=[&moveit_visual_tools](autotext){autoconsttext_pose=[]{automsg=Eigen::Isometry3d::Identity();msg.translation().z()=1.0;// Place text 1m above the base linkreturnmsg;}();moveit_visual_tools.publishText(text_pose,text,rviz_visual_tools::WHITE,rviz_visual_tools::XLARGE);};autoconstprompt=[&moveit_visual_tools](autotext){moveit_visual_tools.prompt(text);};autoconstdraw_trajectory_tool_path=[&moveit_visual_tools,jmg=move_group_interface.getRobotModel()->getJointModelGroup(\"manipulator\")](autoconsttrajectory){moveit_visual_tools.publishTrajectoryLine(trajectory,jmg);};\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseprompt(\"Press 'Next' in the RvizVisualToolsGui window to plan\");draw_title(\"Planning\");moveit_visual_tools.trigger();autoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){draw_trajectory_tool_path(plan.trajectory);moveit_visual_tools.trigger();prompt(\"Press 'Next' in the RvizVisualToolsGui window to execute\");draw_title(\"Executing\");moveit_visual_tools.trigger();move_group_interface.execute(plan);}else{draw_title(\"Planning Failed!\");moveit_visual_tools.trigger();RCLCPP_ERROR(logger,\"Planning failed!\");}\ncd~/ws_moveitsource/opt/ros/rolling/setup.bash\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2launchmoveit2_tutorialsdemo.launch.py\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\n[INFO][1652822889.492940200][hello_moveit.remote_control]:Waitingtocontinue:Press'Next'intheRvizVisualToolsGuiwindowtoplan\nhello_moveit\npackage.xml\nhello_moveit\n<depend>\nCMakeLists.txt\nfind_package\nament_target_dependencies\nhello_moveit.cpp\nmoveit_visual_tools\nmoveit_visual_tools\ndraw_title\nprompt\nnext\ntrigger\nmoveit_visual_tools\ntrigger\nRvizVisualToolsGui\nNext\nMarkerArray\nMarkerArray\nOK\n/rviz_visual_tools\nhello_moveit\nNext\nNext\nMoveItVisualTools",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0bb"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/visualizing_in_rviz/visualizing_in_rviz.html#prerequisites",
  "title": "Visualizing In RViz — MoveIt Documentation: Rolling  documentation",
  "content": "Visualizing In RViz\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to a tool that can help you more easily understand what your MoveIt application is doing by rendering visualizations in RViz.\nIf you haven’t already done so, make sure you’ve completed the steps inYour First Project.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off.\nAdd this line to yourpackage.xmlin thehello_moveitproject after the other<depend>statements:\nThen in yourCMakeLists.txtadd this line to the section offind_packagestatements:\nFurther down in the file extend theament_target_dependenciesmacro call to include the new dependency like this:\nTo verify that you added the dependency correctly, add the required include to your source filehello_moveit.cpp:\nTo test that this all worked, open a terminal in the workspace directory (remembering to source your ROS install in opt) and then build with colcon:\nBefore we can initialize MoveItVisualTools, we need to have a executor spinning on our ROS node.\nThis is necessary because of how MoveItVisualTools interacts with ROS services and topics. First, add the threading library to your includes at the top.\nBy creating and naming loggers, we are able to keep our program logs organized.\nNext, add your executor before creating the MoveIt MoveGroup Interface.\nFinally, make sure to join the thread before exiting.\nAfter making these changes, rebuild your workspace to make sure you don’t have any syntax errors.\nNext, we will construct and initialize MoveItVisualTools after the construction of MoveGroupInterface.\nWe pass the following into the constructor: the ROS node, the base link of the robot, the marker topic to use (more on this later), and the robot model (which we get from the move_group_interface).\nNext, we make a call to delete all the markers. This clears any rendered state out of RViz that we have left over from previous runs.\nLastly, we load remote control.\nRemote control is a really simple plugin that lets us have a button in RViz to interact with our program.\nAfter we’ve constructed and initialized, we now create some closures (function objects that have access to variables in our current scope) that we can use later in our program to help render visualizations in RViz.\nEach of the three closures capturemoveit_visual_toolsby reference and the last one captures a pointer to the joint model group object we are planning with.\nEach of these call a function onmoveit_visual_toolsthat changes something in RViz.\nThe first one,draw_titleadds text one meter above the base of the robot. This is a useful way to show the state of your program from a high level.\nThe second one calls a function calledprompt. This function blocks your program until the user presses thenextbutton in RViz. This is helpful for stepping through a program when debugging.\nThe last one draws the tool path of a trajectory that we have planned. This is often helpful for understanding a planned trajectory from the perspective of the tool.\nYou might be asking yourself why we would create lambdas like this, and the reason is simply to make the code that comes later easier to read and understand.\nAs your write software, it is often helpful to break up your functionality into named functions which can be easily reused and tested on their own.\nYou will see in the next section how we use these functions we created.\nNow we’ll augment the code in the middle of your program.\nUpdate your code for planning and executing to include these new features:\nOne thing you’ll quickly notice is that we have to call a method calledtriggeronmoveit_visual_toolsafter each call to change something rendered in RViz.\nThe reason for this is that messages sent to RViz are batched up and sent when you calltriggerto reduce bandwidth of the marker topics.\nLastly, build your project again to make sure all the code additions are correct.\nOpen a new terminal, source the workspace, and then start the demo launch file that opens RViz.\nUncheck “MotionPlanning” in the “Displays” tab to hide it.\nWe aren’t going to be using the “MotionPlanning” plugin for this next part.\nTo add the buttons to interact with the prompts we added to our program open the dialog with the “Panels/Add New Panel” menu:\nThen selectRvizVisualToolsGuiand click OK.\nThis will create a new panel on the bottom left with aNextbutton we’ll use later.\nFinally, we need to add aMarkerArrayto render the visualizations we’ve added.\nClick on the “Add” Button in the “Displays” panel.\nSelectMarkerArrayand clickOK.\nScroll to the bottom of the items in the Displays panel and edit the topic that the new Marker Array is using to/rviz_visual_tools.\nYou are now ready to run your new program with visualizations.\nIn a new terminal, go to the workspace, source the workspace, and runhello_moveit:\nYou’ll notice that your program has stopped with a log that looks like this:\nClick theNextbutton in RViz and see your application advance.\nYou’ll see after you clicked the next button, your application planned, added a title above the robot, and drew a line representing the tool path.\nTo continue, pressNextagain to see your robot execute the plan.\nYou extended the program you wrote with MoveIt to interact with the Gui in RViz, allowing you to step through your program with a button, render some text above the robot, and display the tool path that you planned.\nMoveItVisualTools has many more useful features for visualizing robot motions.You can read more about it here.\nThere are also more examples of usingMoveItVisualToolsinMoveItCpp Tutorial.\nHere is a copy of the full hello_moveit.cpp source.\nIn the next tutorialPlanning Around Objects, you will expand on the program you built here to add to the collision environment and see the robot plan with these changes.\n© Copyright 2024, PickNik Robotics.\n<depend>moveit_visual_tools</depend>\nfind_package(moveit_visual_toolsREQUIRED)\nament_target_dependencies(hello_moveit\"moveit_ros_planning_interface\"\"moveit_visual_tools\"\"rclcpp\")\n#include<moveit_visual_tools/moveit_visual_tools.h>\ncd~/ws_moveit\ncolconbuild--mixindebug\n#include<thread>// <---- add this to the set of includes at the top\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Spin up a SingleThreadedExecutor for MoveItVisualTools to interact with ROSrclcpp::executors::SingleThreadedExecutorexecutor;executor.add_node(node);autospinner=std::thread([&executor](){executor.spin();});// Create the MoveIt MoveGroup Interface...\n// Shutdown ROSrclcpp::shutdown();// <--- This will cause the spin function in the thread to returnspinner.join();// <--- Join the thread before exitingreturn0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Construct and initialize MoveItVisualToolsautomoveit_visual_tools=moveit_visual_tools::MoveItVisualTools{node,\"base_link\",rviz_visual_tools::RVIZ_MARKER_TOPIC,move_group_interface.getRobotModel()};moveit_visual_tools.deleteAllMarkers();moveit_visual_tools.loadRemoteControl();\n// Create closures for visualizationautoconstdraw_title=[&moveit_visual_tools](autotext){autoconsttext_pose=[]{automsg=Eigen::Isometry3d::Identity();msg.translation().z()=1.0;// Place text 1m above the base linkreturnmsg;}();moveit_visual_tools.publishText(text_pose,text,rviz_visual_tools::WHITE,rviz_visual_tools::XLARGE);};autoconstprompt=[&moveit_visual_tools](autotext){moveit_visual_tools.prompt(text);};autoconstdraw_trajectory_tool_path=[&moveit_visual_tools,jmg=move_group_interface.getRobotModel()->getJointModelGroup(\"manipulator\")](autoconsttrajectory){moveit_visual_tools.publishTrajectoryLine(trajectory,jmg);};\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseprompt(\"Press 'Next' in the RvizVisualToolsGui window to plan\");draw_title(\"Planning\");moveit_visual_tools.trigger();autoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){draw_trajectory_tool_path(plan.trajectory);moveit_visual_tools.trigger();prompt(\"Press 'Next' in the RvizVisualToolsGui window to execute\");draw_title(\"Executing\");moveit_visual_tools.trigger();move_group_interface.execute(plan);}else{draw_title(\"Planning Failed!\");moveit_visual_tools.trigger();RCLCPP_ERROR(logger,\"Planning failed!\");}\ncd~/ws_moveitsource/opt/ros/rolling/setup.bash\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2launchmoveit2_tutorialsdemo.launch.py\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\n[INFO][1652822889.492940200][hello_moveit.remote_control]:Waitingtocontinue:Press'Next'intheRvizVisualToolsGuiwindowtoplan\nhello_moveit\npackage.xml\nhello_moveit\n<depend>\nCMakeLists.txt\nfind_package\nament_target_dependencies\nhello_moveit.cpp\nmoveit_visual_tools\nmoveit_visual_tools\ndraw_title\nprompt\nnext\ntrigger\nmoveit_visual_tools\ntrigger\nRvizVisualToolsGui\nNext\nMarkerArray\nMarkerArray\nOK\n/rviz_visual_tools\nhello_moveit\nNext\nNext\nMoveItVisualTools",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0bc"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/visualizing_in_rviz/visualizing_in_rviz.html#steps",
  "title": "Visualizing In RViz — MoveIt Documentation: Rolling  documentation",
  "content": "Visualizing In RViz\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to a tool that can help you more easily understand what your MoveIt application is doing by rendering visualizations in RViz.\nIf you haven’t already done so, make sure you’ve completed the steps inYour First Project.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off.\nAdd this line to yourpackage.xmlin thehello_moveitproject after the other<depend>statements:\nThen in yourCMakeLists.txtadd this line to the section offind_packagestatements:\nFurther down in the file extend theament_target_dependenciesmacro call to include the new dependency like this:\nTo verify that you added the dependency correctly, add the required include to your source filehello_moveit.cpp:\nTo test that this all worked, open a terminal in the workspace directory (remembering to source your ROS install in opt) and then build with colcon:\nBefore we can initialize MoveItVisualTools, we need to have a executor spinning on our ROS node.\nThis is necessary because of how MoveItVisualTools interacts with ROS services and topics. First, add the threading library to your includes at the top.\nBy creating and naming loggers, we are able to keep our program logs organized.\nNext, add your executor before creating the MoveIt MoveGroup Interface.\nFinally, make sure to join the thread before exiting.\nAfter making these changes, rebuild your workspace to make sure you don’t have any syntax errors.\nNext, we will construct and initialize MoveItVisualTools after the construction of MoveGroupInterface.\nWe pass the following into the constructor: the ROS node, the base link of the robot, the marker topic to use (more on this later), and the robot model (which we get from the move_group_interface).\nNext, we make a call to delete all the markers. This clears any rendered state out of RViz that we have left over from previous runs.\nLastly, we load remote control.\nRemote control is a really simple plugin that lets us have a button in RViz to interact with our program.\nAfter we’ve constructed and initialized, we now create some closures (function objects that have access to variables in our current scope) that we can use later in our program to help render visualizations in RViz.\nEach of the three closures capturemoveit_visual_toolsby reference and the last one captures a pointer to the joint model group object we are planning with.\nEach of these call a function onmoveit_visual_toolsthat changes something in RViz.\nThe first one,draw_titleadds text one meter above the base of the robot. This is a useful way to show the state of your program from a high level.\nThe second one calls a function calledprompt. This function blocks your program until the user presses thenextbutton in RViz. This is helpful for stepping through a program when debugging.\nThe last one draws the tool path of a trajectory that we have planned. This is often helpful for understanding a planned trajectory from the perspective of the tool.\nYou might be asking yourself why we would create lambdas like this, and the reason is simply to make the code that comes later easier to read and understand.\nAs your write software, it is often helpful to break up your functionality into named functions which can be easily reused and tested on their own.\nYou will see in the next section how we use these functions we created.\nNow we’ll augment the code in the middle of your program.\nUpdate your code for planning and executing to include these new features:\nOne thing you’ll quickly notice is that we have to call a method calledtriggeronmoveit_visual_toolsafter each call to change something rendered in RViz.\nThe reason for this is that messages sent to RViz are batched up and sent when you calltriggerto reduce bandwidth of the marker topics.\nLastly, build your project again to make sure all the code additions are correct.\nOpen a new terminal, source the workspace, and then start the demo launch file that opens RViz.\nUncheck “MotionPlanning” in the “Displays” tab to hide it.\nWe aren’t going to be using the “MotionPlanning” plugin for this next part.\nTo add the buttons to interact with the prompts we added to our program open the dialog with the “Panels/Add New Panel” menu:\nThen selectRvizVisualToolsGuiand click OK.\nThis will create a new panel on the bottom left with aNextbutton we’ll use later.\nFinally, we need to add aMarkerArrayto render the visualizations we’ve added.\nClick on the “Add” Button in the “Displays” panel.\nSelectMarkerArrayand clickOK.\nScroll to the bottom of the items in the Displays panel and edit the topic that the new Marker Array is using to/rviz_visual_tools.\nYou are now ready to run your new program with visualizations.\nIn a new terminal, go to the workspace, source the workspace, and runhello_moveit:\nYou’ll notice that your program has stopped with a log that looks like this:\nClick theNextbutton in RViz and see your application advance.\nYou’ll see after you clicked the next button, your application planned, added a title above the robot, and drew a line representing the tool path.\nTo continue, pressNextagain to see your robot execute the plan.\nYou extended the program you wrote with MoveIt to interact with the Gui in RViz, allowing you to step through your program with a button, render some text above the robot, and display the tool path that you planned.\nMoveItVisualTools has many more useful features for visualizing robot motions.You can read more about it here.\nThere are also more examples of usingMoveItVisualToolsinMoveItCpp Tutorial.\nHere is a copy of the full hello_moveit.cpp source.\nIn the next tutorialPlanning Around Objects, you will expand on the program you built here to add to the collision environment and see the robot plan with these changes.\n© Copyright 2024, PickNik Robotics.\n<depend>moveit_visual_tools</depend>\nfind_package(moveit_visual_toolsREQUIRED)\nament_target_dependencies(hello_moveit\"moveit_ros_planning_interface\"\"moveit_visual_tools\"\"rclcpp\")\n#include<moveit_visual_tools/moveit_visual_tools.h>\ncd~/ws_moveit\ncolconbuild--mixindebug\n#include<thread>// <---- add this to the set of includes at the top\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Spin up a SingleThreadedExecutor for MoveItVisualTools to interact with ROSrclcpp::executors::SingleThreadedExecutorexecutor;executor.add_node(node);autospinner=std::thread([&executor](){executor.spin();});// Create the MoveIt MoveGroup Interface...\n// Shutdown ROSrclcpp::shutdown();// <--- This will cause the spin function in the thread to returnspinner.join();// <--- Join the thread before exitingreturn0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Construct and initialize MoveItVisualToolsautomoveit_visual_tools=moveit_visual_tools::MoveItVisualTools{node,\"base_link\",rviz_visual_tools::RVIZ_MARKER_TOPIC,move_group_interface.getRobotModel()};moveit_visual_tools.deleteAllMarkers();moveit_visual_tools.loadRemoteControl();\n// Create closures for visualizationautoconstdraw_title=[&moveit_visual_tools](autotext){autoconsttext_pose=[]{automsg=Eigen::Isometry3d::Identity();msg.translation().z()=1.0;// Place text 1m above the base linkreturnmsg;}();moveit_visual_tools.publishText(text_pose,text,rviz_visual_tools::WHITE,rviz_visual_tools::XLARGE);};autoconstprompt=[&moveit_visual_tools](autotext){moveit_visual_tools.prompt(text);};autoconstdraw_trajectory_tool_path=[&moveit_visual_tools,jmg=move_group_interface.getRobotModel()->getJointModelGroup(\"manipulator\")](autoconsttrajectory){moveit_visual_tools.publishTrajectoryLine(trajectory,jmg);};\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseprompt(\"Press 'Next' in the RvizVisualToolsGui window to plan\");draw_title(\"Planning\");moveit_visual_tools.trigger();autoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){draw_trajectory_tool_path(plan.trajectory);moveit_visual_tools.trigger();prompt(\"Press 'Next' in the RvizVisualToolsGui window to execute\");draw_title(\"Executing\");moveit_visual_tools.trigger();move_group_interface.execute(plan);}else{draw_title(\"Planning Failed!\");moveit_visual_tools.trigger();RCLCPP_ERROR(logger,\"Planning failed!\");}\ncd~/ws_moveitsource/opt/ros/rolling/setup.bash\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2launchmoveit2_tutorialsdemo.launch.py\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\n[INFO][1652822889.492940200][hello_moveit.remote_control]:Waitingtocontinue:Press'Next'intheRvizVisualToolsGuiwindowtoplan\nhello_moveit\npackage.xml\nhello_moveit\n<depend>\nCMakeLists.txt\nfind_package\nament_target_dependencies\nhello_moveit.cpp\nmoveit_visual_tools\nmoveit_visual_tools\ndraw_title\nprompt\nnext\ntrigger\nmoveit_visual_tools\ntrigger\nRvizVisualToolsGui\nNext\nMarkerArray\nMarkerArray\nOK\n/rviz_visual_tools\nhello_moveit\nNext\nNext\nMoveItVisualTools",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0bd"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/visualizing_in_rviz/visualizing_in_rviz.html#add-the-dependency-moveit-visual-tools",
  "title": "Visualizing In RViz — MoveIt Documentation: Rolling  documentation",
  "content": "Visualizing In RViz\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to a tool that can help you more easily understand what your MoveIt application is doing by rendering visualizations in RViz.\nIf you haven’t already done so, make sure you’ve completed the steps inYour First Project.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off.\nAdd this line to yourpackage.xmlin thehello_moveitproject after the other<depend>statements:\nThen in yourCMakeLists.txtadd this line to the section offind_packagestatements:\nFurther down in the file extend theament_target_dependenciesmacro call to include the new dependency like this:\nTo verify that you added the dependency correctly, add the required include to your source filehello_moveit.cpp:\nTo test that this all worked, open a terminal in the workspace directory (remembering to source your ROS install in opt) and then build with colcon:\nBefore we can initialize MoveItVisualTools, we need to have a executor spinning on our ROS node.\nThis is necessary because of how MoveItVisualTools interacts with ROS services and topics. First, add the threading library to your includes at the top.\nBy creating and naming loggers, we are able to keep our program logs organized.\nNext, add your executor before creating the MoveIt MoveGroup Interface.\nFinally, make sure to join the thread before exiting.\nAfter making these changes, rebuild your workspace to make sure you don’t have any syntax errors.\nNext, we will construct and initialize MoveItVisualTools after the construction of MoveGroupInterface.\nWe pass the following into the constructor: the ROS node, the base link of the robot, the marker topic to use (more on this later), and the robot model (which we get from the move_group_interface).\nNext, we make a call to delete all the markers. This clears any rendered state out of RViz that we have left over from previous runs.\nLastly, we load remote control.\nRemote control is a really simple plugin that lets us have a button in RViz to interact with our program.\nAfter we’ve constructed and initialized, we now create some closures (function objects that have access to variables in our current scope) that we can use later in our program to help render visualizations in RViz.\nEach of the three closures capturemoveit_visual_toolsby reference and the last one captures a pointer to the joint model group object we are planning with.\nEach of these call a function onmoveit_visual_toolsthat changes something in RViz.\nThe first one,draw_titleadds text one meter above the base of the robot. This is a useful way to show the state of your program from a high level.\nThe second one calls a function calledprompt. This function blocks your program until the user presses thenextbutton in RViz. This is helpful for stepping through a program when debugging.\nThe last one draws the tool path of a trajectory that we have planned. This is often helpful for understanding a planned trajectory from the perspective of the tool.\nYou might be asking yourself why we would create lambdas like this, and the reason is simply to make the code that comes later easier to read and understand.\nAs your write software, it is often helpful to break up your functionality into named functions which can be easily reused and tested on their own.\nYou will see in the next section how we use these functions we created.\nNow we’ll augment the code in the middle of your program.\nUpdate your code for planning and executing to include these new features:\nOne thing you’ll quickly notice is that we have to call a method calledtriggeronmoveit_visual_toolsafter each call to change something rendered in RViz.\nThe reason for this is that messages sent to RViz are batched up and sent when you calltriggerto reduce bandwidth of the marker topics.\nLastly, build your project again to make sure all the code additions are correct.\nOpen a new terminal, source the workspace, and then start the demo launch file that opens RViz.\nUncheck “MotionPlanning” in the “Displays” tab to hide it.\nWe aren’t going to be using the “MotionPlanning” plugin for this next part.\nTo add the buttons to interact with the prompts we added to our program open the dialog with the “Panels/Add New Panel” menu:\nThen selectRvizVisualToolsGuiand click OK.\nThis will create a new panel on the bottom left with aNextbutton we’ll use later.\nFinally, we need to add aMarkerArrayto render the visualizations we’ve added.\nClick on the “Add” Button in the “Displays” panel.\nSelectMarkerArrayand clickOK.\nScroll to the bottom of the items in the Displays panel and edit the topic that the new Marker Array is using to/rviz_visual_tools.\nYou are now ready to run your new program with visualizations.\nIn a new terminal, go to the workspace, source the workspace, and runhello_moveit:\nYou’ll notice that your program has stopped with a log that looks like this:\nClick theNextbutton in RViz and see your application advance.\nYou’ll see after you clicked the next button, your application planned, added a title above the robot, and drew a line representing the tool path.\nTo continue, pressNextagain to see your robot execute the plan.\nYou extended the program you wrote with MoveIt to interact with the Gui in RViz, allowing you to step through your program with a button, render some text above the robot, and display the tool path that you planned.\nMoveItVisualTools has many more useful features for visualizing robot motions.You can read more about it here.\nThere are also more examples of usingMoveItVisualToolsinMoveItCpp Tutorial.\nHere is a copy of the full hello_moveit.cpp source.\nIn the next tutorialPlanning Around Objects, you will expand on the program you built here to add to the collision environment and see the robot plan with these changes.\n© Copyright 2024, PickNik Robotics.\n<depend>moveit_visual_tools</depend>\nfind_package(moveit_visual_toolsREQUIRED)\nament_target_dependencies(hello_moveit\"moveit_ros_planning_interface\"\"moveit_visual_tools\"\"rclcpp\")\n#include<moveit_visual_tools/moveit_visual_tools.h>\ncd~/ws_moveit\ncolconbuild--mixindebug\n#include<thread>// <---- add this to the set of includes at the top\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Spin up a SingleThreadedExecutor for MoveItVisualTools to interact with ROSrclcpp::executors::SingleThreadedExecutorexecutor;executor.add_node(node);autospinner=std::thread([&executor](){executor.spin();});// Create the MoveIt MoveGroup Interface...\n// Shutdown ROSrclcpp::shutdown();// <--- This will cause the spin function in the thread to returnspinner.join();// <--- Join the thread before exitingreturn0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Construct and initialize MoveItVisualToolsautomoveit_visual_tools=moveit_visual_tools::MoveItVisualTools{node,\"base_link\",rviz_visual_tools::RVIZ_MARKER_TOPIC,move_group_interface.getRobotModel()};moveit_visual_tools.deleteAllMarkers();moveit_visual_tools.loadRemoteControl();\n// Create closures for visualizationautoconstdraw_title=[&moveit_visual_tools](autotext){autoconsttext_pose=[]{automsg=Eigen::Isometry3d::Identity();msg.translation().z()=1.0;// Place text 1m above the base linkreturnmsg;}();moveit_visual_tools.publishText(text_pose,text,rviz_visual_tools::WHITE,rviz_visual_tools::XLARGE);};autoconstprompt=[&moveit_visual_tools](autotext){moveit_visual_tools.prompt(text);};autoconstdraw_trajectory_tool_path=[&moveit_visual_tools,jmg=move_group_interface.getRobotModel()->getJointModelGroup(\"manipulator\")](autoconsttrajectory){moveit_visual_tools.publishTrajectoryLine(trajectory,jmg);};\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseprompt(\"Press 'Next' in the RvizVisualToolsGui window to plan\");draw_title(\"Planning\");moveit_visual_tools.trigger();autoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){draw_trajectory_tool_path(plan.trajectory);moveit_visual_tools.trigger();prompt(\"Press 'Next' in the RvizVisualToolsGui window to execute\");draw_title(\"Executing\");moveit_visual_tools.trigger();move_group_interface.execute(plan);}else{draw_title(\"Planning Failed!\");moveit_visual_tools.trigger();RCLCPP_ERROR(logger,\"Planning failed!\");}\ncd~/ws_moveitsource/opt/ros/rolling/setup.bash\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2launchmoveit2_tutorialsdemo.launch.py\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\n[INFO][1652822889.492940200][hello_moveit.remote_control]:Waitingtocontinue:Press'Next'intheRvizVisualToolsGuiwindowtoplan\nhello_moveit\npackage.xml\nhello_moveit\n<depend>\nCMakeLists.txt\nfind_package\nament_target_dependencies\nhello_moveit.cpp\nmoveit_visual_tools\nmoveit_visual_tools\ndraw_title\nprompt\nnext\ntrigger\nmoveit_visual_tools\ntrigger\nRvizVisualToolsGui\nNext\nMarkerArray\nMarkerArray\nOK\n/rviz_visual_tools\nhello_moveit\nNext\nNext\nMoveItVisualTools",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0be"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/visualizing_in_rviz/visualizing_in_rviz.html#create-a-ros-executor-and-spin-the-node-on-a-thread",
  "title": "Visualizing In RViz — MoveIt Documentation: Rolling  documentation",
  "content": "Visualizing In RViz\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to a tool that can help you more easily understand what your MoveIt application is doing by rendering visualizations in RViz.\nIf you haven’t already done so, make sure you’ve completed the steps inYour First Project.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off.\nAdd this line to yourpackage.xmlin thehello_moveitproject after the other<depend>statements:\nThen in yourCMakeLists.txtadd this line to the section offind_packagestatements:\nFurther down in the file extend theament_target_dependenciesmacro call to include the new dependency like this:\nTo verify that you added the dependency correctly, add the required include to your source filehello_moveit.cpp:\nTo test that this all worked, open a terminal in the workspace directory (remembering to source your ROS install in opt) and then build with colcon:\nBefore we can initialize MoveItVisualTools, we need to have a executor spinning on our ROS node.\nThis is necessary because of how MoveItVisualTools interacts with ROS services and topics. First, add the threading library to your includes at the top.\nBy creating and naming loggers, we are able to keep our program logs organized.\nNext, add your executor before creating the MoveIt MoveGroup Interface.\nFinally, make sure to join the thread before exiting.\nAfter making these changes, rebuild your workspace to make sure you don’t have any syntax errors.\nNext, we will construct and initialize MoveItVisualTools after the construction of MoveGroupInterface.\nWe pass the following into the constructor: the ROS node, the base link of the robot, the marker topic to use (more on this later), and the robot model (which we get from the move_group_interface).\nNext, we make a call to delete all the markers. This clears any rendered state out of RViz that we have left over from previous runs.\nLastly, we load remote control.\nRemote control is a really simple plugin that lets us have a button in RViz to interact with our program.\nAfter we’ve constructed and initialized, we now create some closures (function objects that have access to variables in our current scope) that we can use later in our program to help render visualizations in RViz.\nEach of the three closures capturemoveit_visual_toolsby reference and the last one captures a pointer to the joint model group object we are planning with.\nEach of these call a function onmoveit_visual_toolsthat changes something in RViz.\nThe first one,draw_titleadds text one meter above the base of the robot. This is a useful way to show the state of your program from a high level.\nThe second one calls a function calledprompt. This function blocks your program until the user presses thenextbutton in RViz. This is helpful for stepping through a program when debugging.\nThe last one draws the tool path of a trajectory that we have planned. This is often helpful for understanding a planned trajectory from the perspective of the tool.\nYou might be asking yourself why we would create lambdas like this, and the reason is simply to make the code that comes later easier to read and understand.\nAs your write software, it is often helpful to break up your functionality into named functions which can be easily reused and tested on their own.\nYou will see in the next section how we use these functions we created.\nNow we’ll augment the code in the middle of your program.\nUpdate your code for planning and executing to include these new features:\nOne thing you’ll quickly notice is that we have to call a method calledtriggeronmoveit_visual_toolsafter each call to change something rendered in RViz.\nThe reason for this is that messages sent to RViz are batched up and sent when you calltriggerto reduce bandwidth of the marker topics.\nLastly, build your project again to make sure all the code additions are correct.\nOpen a new terminal, source the workspace, and then start the demo launch file that opens RViz.\nUncheck “MotionPlanning” in the “Displays” tab to hide it.\nWe aren’t going to be using the “MotionPlanning” plugin for this next part.\nTo add the buttons to interact with the prompts we added to our program open the dialog with the “Panels/Add New Panel” menu:\nThen selectRvizVisualToolsGuiand click OK.\nThis will create a new panel on the bottom left with aNextbutton we’ll use later.\nFinally, we need to add aMarkerArrayto render the visualizations we’ve added.\nClick on the “Add” Button in the “Displays” panel.\nSelectMarkerArrayand clickOK.\nScroll to the bottom of the items in the Displays panel and edit the topic that the new Marker Array is using to/rviz_visual_tools.\nYou are now ready to run your new program with visualizations.\nIn a new terminal, go to the workspace, source the workspace, and runhello_moveit:\nYou’ll notice that your program has stopped with a log that looks like this:\nClick theNextbutton in RViz and see your application advance.\nYou’ll see after you clicked the next button, your application planned, added a title above the robot, and drew a line representing the tool path.\nTo continue, pressNextagain to see your robot execute the plan.\nYou extended the program you wrote with MoveIt to interact with the Gui in RViz, allowing you to step through your program with a button, render some text above the robot, and display the tool path that you planned.\nMoveItVisualTools has many more useful features for visualizing robot motions.You can read more about it here.\nThere are also more examples of usingMoveItVisualToolsinMoveItCpp Tutorial.\nHere is a copy of the full hello_moveit.cpp source.\nIn the next tutorialPlanning Around Objects, you will expand on the program you built here to add to the collision environment and see the robot plan with these changes.\n© Copyright 2024, PickNik Robotics.\n<depend>moveit_visual_tools</depend>\nfind_package(moveit_visual_toolsREQUIRED)\nament_target_dependencies(hello_moveit\"moveit_ros_planning_interface\"\"moveit_visual_tools\"\"rclcpp\")\n#include<moveit_visual_tools/moveit_visual_tools.h>\ncd~/ws_moveit\ncolconbuild--mixindebug\n#include<thread>// <---- add this to the set of includes at the top\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Spin up a SingleThreadedExecutor for MoveItVisualTools to interact with ROSrclcpp::executors::SingleThreadedExecutorexecutor;executor.add_node(node);autospinner=std::thread([&executor](){executor.spin();});// Create the MoveIt MoveGroup Interface...\n// Shutdown ROSrclcpp::shutdown();// <--- This will cause the spin function in the thread to returnspinner.join();// <--- Join the thread before exitingreturn0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Construct and initialize MoveItVisualToolsautomoveit_visual_tools=moveit_visual_tools::MoveItVisualTools{node,\"base_link\",rviz_visual_tools::RVIZ_MARKER_TOPIC,move_group_interface.getRobotModel()};moveit_visual_tools.deleteAllMarkers();moveit_visual_tools.loadRemoteControl();\n// Create closures for visualizationautoconstdraw_title=[&moveit_visual_tools](autotext){autoconsttext_pose=[]{automsg=Eigen::Isometry3d::Identity();msg.translation().z()=1.0;// Place text 1m above the base linkreturnmsg;}();moveit_visual_tools.publishText(text_pose,text,rviz_visual_tools::WHITE,rviz_visual_tools::XLARGE);};autoconstprompt=[&moveit_visual_tools](autotext){moveit_visual_tools.prompt(text);};autoconstdraw_trajectory_tool_path=[&moveit_visual_tools,jmg=move_group_interface.getRobotModel()->getJointModelGroup(\"manipulator\")](autoconsttrajectory){moveit_visual_tools.publishTrajectoryLine(trajectory,jmg);};\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseprompt(\"Press 'Next' in the RvizVisualToolsGui window to plan\");draw_title(\"Planning\");moveit_visual_tools.trigger();autoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){draw_trajectory_tool_path(plan.trajectory);moveit_visual_tools.trigger();prompt(\"Press 'Next' in the RvizVisualToolsGui window to execute\");draw_title(\"Executing\");moveit_visual_tools.trigger();move_group_interface.execute(plan);}else{draw_title(\"Planning Failed!\");moveit_visual_tools.trigger();RCLCPP_ERROR(logger,\"Planning failed!\");}\ncd~/ws_moveitsource/opt/ros/rolling/setup.bash\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2launchmoveit2_tutorialsdemo.launch.py\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\n[INFO][1652822889.492940200][hello_moveit.remote_control]:Waitingtocontinue:Press'Next'intheRvizVisualToolsGuiwindowtoplan\nhello_moveit\npackage.xml\nhello_moveit\n<depend>\nCMakeLists.txt\nfind_package\nament_target_dependencies\nhello_moveit.cpp\nmoveit_visual_tools\nmoveit_visual_tools\ndraw_title\nprompt\nnext\ntrigger\nmoveit_visual_tools\ntrigger\nRvizVisualToolsGui\nNext\nMarkerArray\nMarkerArray\nOK\n/rviz_visual_tools\nhello_moveit\nNext\nNext\nMoveItVisualTools",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0bf"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/visualizing_in_rviz/visualizing_in_rviz.html#create-and-initialize-moveitvisualtools",
  "title": "Visualizing In RViz — MoveIt Documentation: Rolling  documentation",
  "content": "Visualizing In RViz\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to a tool that can help you more easily understand what your MoveIt application is doing by rendering visualizations in RViz.\nIf you haven’t already done so, make sure you’ve completed the steps inYour First Project.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off.\nAdd this line to yourpackage.xmlin thehello_moveitproject after the other<depend>statements:\nThen in yourCMakeLists.txtadd this line to the section offind_packagestatements:\nFurther down in the file extend theament_target_dependenciesmacro call to include the new dependency like this:\nTo verify that you added the dependency correctly, add the required include to your source filehello_moveit.cpp:\nTo test that this all worked, open a terminal in the workspace directory (remembering to source your ROS install in opt) and then build with colcon:\nBefore we can initialize MoveItVisualTools, we need to have a executor spinning on our ROS node.\nThis is necessary because of how MoveItVisualTools interacts with ROS services and topics. First, add the threading library to your includes at the top.\nBy creating and naming loggers, we are able to keep our program logs organized.\nNext, add your executor before creating the MoveIt MoveGroup Interface.\nFinally, make sure to join the thread before exiting.\nAfter making these changes, rebuild your workspace to make sure you don’t have any syntax errors.\nNext, we will construct and initialize MoveItVisualTools after the construction of MoveGroupInterface.\nWe pass the following into the constructor: the ROS node, the base link of the robot, the marker topic to use (more on this later), and the robot model (which we get from the move_group_interface).\nNext, we make a call to delete all the markers. This clears any rendered state out of RViz that we have left over from previous runs.\nLastly, we load remote control.\nRemote control is a really simple plugin that lets us have a button in RViz to interact with our program.\nAfter we’ve constructed and initialized, we now create some closures (function objects that have access to variables in our current scope) that we can use later in our program to help render visualizations in RViz.\nEach of the three closures capturemoveit_visual_toolsby reference and the last one captures a pointer to the joint model group object we are planning with.\nEach of these call a function onmoveit_visual_toolsthat changes something in RViz.\nThe first one,draw_titleadds text one meter above the base of the robot. This is a useful way to show the state of your program from a high level.\nThe second one calls a function calledprompt. This function blocks your program until the user presses thenextbutton in RViz. This is helpful for stepping through a program when debugging.\nThe last one draws the tool path of a trajectory that we have planned. This is often helpful for understanding a planned trajectory from the perspective of the tool.\nYou might be asking yourself why we would create lambdas like this, and the reason is simply to make the code that comes later easier to read and understand.\nAs your write software, it is often helpful to break up your functionality into named functions which can be easily reused and tested on their own.\nYou will see in the next section how we use these functions we created.\nNow we’ll augment the code in the middle of your program.\nUpdate your code for planning and executing to include these new features:\nOne thing you’ll quickly notice is that we have to call a method calledtriggeronmoveit_visual_toolsafter each call to change something rendered in RViz.\nThe reason for this is that messages sent to RViz are batched up and sent when you calltriggerto reduce bandwidth of the marker topics.\nLastly, build your project again to make sure all the code additions are correct.\nOpen a new terminal, source the workspace, and then start the demo launch file that opens RViz.\nUncheck “MotionPlanning” in the “Displays” tab to hide it.\nWe aren’t going to be using the “MotionPlanning” plugin for this next part.\nTo add the buttons to interact with the prompts we added to our program open the dialog with the “Panels/Add New Panel” menu:\nThen selectRvizVisualToolsGuiand click OK.\nThis will create a new panel on the bottom left with aNextbutton we’ll use later.\nFinally, we need to add aMarkerArrayto render the visualizations we’ve added.\nClick on the “Add” Button in the “Displays” panel.\nSelectMarkerArrayand clickOK.\nScroll to the bottom of the items in the Displays panel and edit the topic that the new Marker Array is using to/rviz_visual_tools.\nYou are now ready to run your new program with visualizations.\nIn a new terminal, go to the workspace, source the workspace, and runhello_moveit:\nYou’ll notice that your program has stopped with a log that looks like this:\nClick theNextbutton in RViz and see your application advance.\nYou’ll see after you clicked the next button, your application planned, added a title above the robot, and drew a line representing the tool path.\nTo continue, pressNextagain to see your robot execute the plan.\nYou extended the program you wrote with MoveIt to interact with the Gui in RViz, allowing you to step through your program with a button, render some text above the robot, and display the tool path that you planned.\nMoveItVisualTools has many more useful features for visualizing robot motions.You can read more about it here.\nThere are also more examples of usingMoveItVisualToolsinMoveItCpp Tutorial.\nHere is a copy of the full hello_moveit.cpp source.\nIn the next tutorialPlanning Around Objects, you will expand on the program you built here to add to the collision environment and see the robot plan with these changes.\n© Copyright 2024, PickNik Robotics.\n<depend>moveit_visual_tools</depend>\nfind_package(moveit_visual_toolsREQUIRED)\nament_target_dependencies(hello_moveit\"moveit_ros_planning_interface\"\"moveit_visual_tools\"\"rclcpp\")\n#include<moveit_visual_tools/moveit_visual_tools.h>\ncd~/ws_moveit\ncolconbuild--mixindebug\n#include<thread>// <---- add this to the set of includes at the top\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Spin up a SingleThreadedExecutor for MoveItVisualTools to interact with ROSrclcpp::executors::SingleThreadedExecutorexecutor;executor.add_node(node);autospinner=std::thread([&executor](){executor.spin();});// Create the MoveIt MoveGroup Interface...\n// Shutdown ROSrclcpp::shutdown();// <--- This will cause the spin function in the thread to returnspinner.join();// <--- Join the thread before exitingreturn0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Construct and initialize MoveItVisualToolsautomoveit_visual_tools=moveit_visual_tools::MoveItVisualTools{node,\"base_link\",rviz_visual_tools::RVIZ_MARKER_TOPIC,move_group_interface.getRobotModel()};moveit_visual_tools.deleteAllMarkers();moveit_visual_tools.loadRemoteControl();\n// Create closures for visualizationautoconstdraw_title=[&moveit_visual_tools](autotext){autoconsttext_pose=[]{automsg=Eigen::Isometry3d::Identity();msg.translation().z()=1.0;// Place text 1m above the base linkreturnmsg;}();moveit_visual_tools.publishText(text_pose,text,rviz_visual_tools::WHITE,rviz_visual_tools::XLARGE);};autoconstprompt=[&moveit_visual_tools](autotext){moveit_visual_tools.prompt(text);};autoconstdraw_trajectory_tool_path=[&moveit_visual_tools,jmg=move_group_interface.getRobotModel()->getJointModelGroup(\"manipulator\")](autoconsttrajectory){moveit_visual_tools.publishTrajectoryLine(trajectory,jmg);};\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseprompt(\"Press 'Next' in the RvizVisualToolsGui window to plan\");draw_title(\"Planning\");moveit_visual_tools.trigger();autoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){draw_trajectory_tool_path(plan.trajectory);moveit_visual_tools.trigger();prompt(\"Press 'Next' in the RvizVisualToolsGui window to execute\");draw_title(\"Executing\");moveit_visual_tools.trigger();move_group_interface.execute(plan);}else{draw_title(\"Planning Failed!\");moveit_visual_tools.trigger();RCLCPP_ERROR(logger,\"Planning failed!\");}\ncd~/ws_moveitsource/opt/ros/rolling/setup.bash\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2launchmoveit2_tutorialsdemo.launch.py\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\n[INFO][1652822889.492940200][hello_moveit.remote_control]:Waitingtocontinue:Press'Next'intheRvizVisualToolsGuiwindowtoplan\nhello_moveit\npackage.xml\nhello_moveit\n<depend>\nCMakeLists.txt\nfind_package\nament_target_dependencies\nhello_moveit.cpp\nmoveit_visual_tools\nmoveit_visual_tools\ndraw_title\nprompt\nnext\ntrigger\nmoveit_visual_tools\ntrigger\nRvizVisualToolsGui\nNext\nMarkerArray\nMarkerArray\nOK\n/rviz_visual_tools\nhello_moveit\nNext\nNext\nMoveItVisualTools",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0c0"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/visualizing_in_rviz/visualizing_in_rviz.html#write-closures-for-visualizations",
  "title": "Visualizing In RViz — MoveIt Documentation: Rolling  documentation",
  "content": "Visualizing In RViz\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to a tool that can help you more easily understand what your MoveIt application is doing by rendering visualizations in RViz.\nIf you haven’t already done so, make sure you’ve completed the steps inYour First Project.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off.\nAdd this line to yourpackage.xmlin thehello_moveitproject after the other<depend>statements:\nThen in yourCMakeLists.txtadd this line to the section offind_packagestatements:\nFurther down in the file extend theament_target_dependenciesmacro call to include the new dependency like this:\nTo verify that you added the dependency correctly, add the required include to your source filehello_moveit.cpp:\nTo test that this all worked, open a terminal in the workspace directory (remembering to source your ROS install in opt) and then build with colcon:\nBefore we can initialize MoveItVisualTools, we need to have a executor spinning on our ROS node.\nThis is necessary because of how MoveItVisualTools interacts with ROS services and topics. First, add the threading library to your includes at the top.\nBy creating and naming loggers, we are able to keep our program logs organized.\nNext, add your executor before creating the MoveIt MoveGroup Interface.\nFinally, make sure to join the thread before exiting.\nAfter making these changes, rebuild your workspace to make sure you don’t have any syntax errors.\nNext, we will construct and initialize MoveItVisualTools after the construction of MoveGroupInterface.\nWe pass the following into the constructor: the ROS node, the base link of the robot, the marker topic to use (more on this later), and the robot model (which we get from the move_group_interface).\nNext, we make a call to delete all the markers. This clears any rendered state out of RViz that we have left over from previous runs.\nLastly, we load remote control.\nRemote control is a really simple plugin that lets us have a button in RViz to interact with our program.\nAfter we’ve constructed and initialized, we now create some closures (function objects that have access to variables in our current scope) that we can use later in our program to help render visualizations in RViz.\nEach of the three closures capturemoveit_visual_toolsby reference and the last one captures a pointer to the joint model group object we are planning with.\nEach of these call a function onmoveit_visual_toolsthat changes something in RViz.\nThe first one,draw_titleadds text one meter above the base of the robot. This is a useful way to show the state of your program from a high level.\nThe second one calls a function calledprompt. This function blocks your program until the user presses thenextbutton in RViz. This is helpful for stepping through a program when debugging.\nThe last one draws the tool path of a trajectory that we have planned. This is often helpful for understanding a planned trajectory from the perspective of the tool.\nYou might be asking yourself why we would create lambdas like this, and the reason is simply to make the code that comes later easier to read and understand.\nAs your write software, it is often helpful to break up your functionality into named functions which can be easily reused and tested on their own.\nYou will see in the next section how we use these functions we created.\nNow we’ll augment the code in the middle of your program.\nUpdate your code for planning and executing to include these new features:\nOne thing you’ll quickly notice is that we have to call a method calledtriggeronmoveit_visual_toolsafter each call to change something rendered in RViz.\nThe reason for this is that messages sent to RViz are batched up and sent when you calltriggerto reduce bandwidth of the marker topics.\nLastly, build your project again to make sure all the code additions are correct.\nOpen a new terminal, source the workspace, and then start the demo launch file that opens RViz.\nUncheck “MotionPlanning” in the “Displays” tab to hide it.\nWe aren’t going to be using the “MotionPlanning” plugin for this next part.\nTo add the buttons to interact with the prompts we added to our program open the dialog with the “Panels/Add New Panel” menu:\nThen selectRvizVisualToolsGuiand click OK.\nThis will create a new panel on the bottom left with aNextbutton we’ll use later.\nFinally, we need to add aMarkerArrayto render the visualizations we’ve added.\nClick on the “Add” Button in the “Displays” panel.\nSelectMarkerArrayand clickOK.\nScroll to the bottom of the items in the Displays panel and edit the topic that the new Marker Array is using to/rviz_visual_tools.\nYou are now ready to run your new program with visualizations.\nIn a new terminal, go to the workspace, source the workspace, and runhello_moveit:\nYou’ll notice that your program has stopped with a log that looks like this:\nClick theNextbutton in RViz and see your application advance.\nYou’ll see after you clicked the next button, your application planned, added a title above the robot, and drew a line representing the tool path.\nTo continue, pressNextagain to see your robot execute the plan.\nYou extended the program you wrote with MoveIt to interact with the Gui in RViz, allowing you to step through your program with a button, render some text above the robot, and display the tool path that you planned.\nMoveItVisualTools has many more useful features for visualizing robot motions.You can read more about it here.\nThere are also more examples of usingMoveItVisualToolsinMoveItCpp Tutorial.\nHere is a copy of the full hello_moveit.cpp source.\nIn the next tutorialPlanning Around Objects, you will expand on the program you built here to add to the collision environment and see the robot plan with these changes.\n© Copyright 2024, PickNik Robotics.\n<depend>moveit_visual_tools</depend>\nfind_package(moveit_visual_toolsREQUIRED)\nament_target_dependencies(hello_moveit\"moveit_ros_planning_interface\"\"moveit_visual_tools\"\"rclcpp\")\n#include<moveit_visual_tools/moveit_visual_tools.h>\ncd~/ws_moveit\ncolconbuild--mixindebug\n#include<thread>// <---- add this to the set of includes at the top\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Spin up a SingleThreadedExecutor for MoveItVisualTools to interact with ROSrclcpp::executors::SingleThreadedExecutorexecutor;executor.add_node(node);autospinner=std::thread([&executor](){executor.spin();});// Create the MoveIt MoveGroup Interface...\n// Shutdown ROSrclcpp::shutdown();// <--- This will cause the spin function in the thread to returnspinner.join();// <--- Join the thread before exitingreturn0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Construct and initialize MoveItVisualToolsautomoveit_visual_tools=moveit_visual_tools::MoveItVisualTools{node,\"base_link\",rviz_visual_tools::RVIZ_MARKER_TOPIC,move_group_interface.getRobotModel()};moveit_visual_tools.deleteAllMarkers();moveit_visual_tools.loadRemoteControl();\n// Create closures for visualizationautoconstdraw_title=[&moveit_visual_tools](autotext){autoconsttext_pose=[]{automsg=Eigen::Isometry3d::Identity();msg.translation().z()=1.0;// Place text 1m above the base linkreturnmsg;}();moveit_visual_tools.publishText(text_pose,text,rviz_visual_tools::WHITE,rviz_visual_tools::XLARGE);};autoconstprompt=[&moveit_visual_tools](autotext){moveit_visual_tools.prompt(text);};autoconstdraw_trajectory_tool_path=[&moveit_visual_tools,jmg=move_group_interface.getRobotModel()->getJointModelGroup(\"manipulator\")](autoconsttrajectory){moveit_visual_tools.publishTrajectoryLine(trajectory,jmg);};\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseprompt(\"Press 'Next' in the RvizVisualToolsGui window to plan\");draw_title(\"Planning\");moveit_visual_tools.trigger();autoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){draw_trajectory_tool_path(plan.trajectory);moveit_visual_tools.trigger();prompt(\"Press 'Next' in the RvizVisualToolsGui window to execute\");draw_title(\"Executing\");moveit_visual_tools.trigger();move_group_interface.execute(plan);}else{draw_title(\"Planning Failed!\");moveit_visual_tools.trigger();RCLCPP_ERROR(logger,\"Planning failed!\");}\ncd~/ws_moveitsource/opt/ros/rolling/setup.bash\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2launchmoveit2_tutorialsdemo.launch.py\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\n[INFO][1652822889.492940200][hello_moveit.remote_control]:Waitingtocontinue:Press'Next'intheRvizVisualToolsGuiwindowtoplan\nhello_moveit\npackage.xml\nhello_moveit\n<depend>\nCMakeLists.txt\nfind_package\nament_target_dependencies\nhello_moveit.cpp\nmoveit_visual_tools\nmoveit_visual_tools\ndraw_title\nprompt\nnext\ntrigger\nmoveit_visual_tools\ntrigger\nRvizVisualToolsGui\nNext\nMarkerArray\nMarkerArray\nOK\n/rviz_visual_tools\nhello_moveit\nNext\nNext\nMoveItVisualTools",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0c1"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/visualizing_in_rviz/visualizing_in_rviz.html#visualize-the-steps-of-your-program",
  "title": "Visualizing In RViz — MoveIt Documentation: Rolling  documentation",
  "content": "Visualizing In RViz\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to a tool that can help you more easily understand what your MoveIt application is doing by rendering visualizations in RViz.\nIf you haven’t already done so, make sure you’ve completed the steps inYour First Project.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off.\nAdd this line to yourpackage.xmlin thehello_moveitproject after the other<depend>statements:\nThen in yourCMakeLists.txtadd this line to the section offind_packagestatements:\nFurther down in the file extend theament_target_dependenciesmacro call to include the new dependency like this:\nTo verify that you added the dependency correctly, add the required include to your source filehello_moveit.cpp:\nTo test that this all worked, open a terminal in the workspace directory (remembering to source your ROS install in opt) and then build with colcon:\nBefore we can initialize MoveItVisualTools, we need to have a executor spinning on our ROS node.\nThis is necessary because of how MoveItVisualTools interacts with ROS services and topics. First, add the threading library to your includes at the top.\nBy creating and naming loggers, we are able to keep our program logs organized.\nNext, add your executor before creating the MoveIt MoveGroup Interface.\nFinally, make sure to join the thread before exiting.\nAfter making these changes, rebuild your workspace to make sure you don’t have any syntax errors.\nNext, we will construct and initialize MoveItVisualTools after the construction of MoveGroupInterface.\nWe pass the following into the constructor: the ROS node, the base link of the robot, the marker topic to use (more on this later), and the robot model (which we get from the move_group_interface).\nNext, we make a call to delete all the markers. This clears any rendered state out of RViz that we have left over from previous runs.\nLastly, we load remote control.\nRemote control is a really simple plugin that lets us have a button in RViz to interact with our program.\nAfter we’ve constructed and initialized, we now create some closures (function objects that have access to variables in our current scope) that we can use later in our program to help render visualizations in RViz.\nEach of the three closures capturemoveit_visual_toolsby reference and the last one captures a pointer to the joint model group object we are planning with.\nEach of these call a function onmoveit_visual_toolsthat changes something in RViz.\nThe first one,draw_titleadds text one meter above the base of the robot. This is a useful way to show the state of your program from a high level.\nThe second one calls a function calledprompt. This function blocks your program until the user presses thenextbutton in RViz. This is helpful for stepping through a program when debugging.\nThe last one draws the tool path of a trajectory that we have planned. This is often helpful for understanding a planned trajectory from the perspective of the tool.\nYou might be asking yourself why we would create lambdas like this, and the reason is simply to make the code that comes later easier to read and understand.\nAs your write software, it is often helpful to break up your functionality into named functions which can be easily reused and tested on their own.\nYou will see in the next section how we use these functions we created.\nNow we’ll augment the code in the middle of your program.\nUpdate your code for planning and executing to include these new features:\nOne thing you’ll quickly notice is that we have to call a method calledtriggeronmoveit_visual_toolsafter each call to change something rendered in RViz.\nThe reason for this is that messages sent to RViz are batched up and sent when you calltriggerto reduce bandwidth of the marker topics.\nLastly, build your project again to make sure all the code additions are correct.\nOpen a new terminal, source the workspace, and then start the demo launch file that opens RViz.\nUncheck “MotionPlanning” in the “Displays” tab to hide it.\nWe aren’t going to be using the “MotionPlanning” plugin for this next part.\nTo add the buttons to interact with the prompts we added to our program open the dialog with the “Panels/Add New Panel” menu:\nThen selectRvizVisualToolsGuiand click OK.\nThis will create a new panel on the bottom left with aNextbutton we’ll use later.\nFinally, we need to add aMarkerArrayto render the visualizations we’ve added.\nClick on the “Add” Button in the “Displays” panel.\nSelectMarkerArrayand clickOK.\nScroll to the bottom of the items in the Displays panel and edit the topic that the new Marker Array is using to/rviz_visual_tools.\nYou are now ready to run your new program with visualizations.\nIn a new terminal, go to the workspace, source the workspace, and runhello_moveit:\nYou’ll notice that your program has stopped with a log that looks like this:\nClick theNextbutton in RViz and see your application advance.\nYou’ll see after you clicked the next button, your application planned, added a title above the robot, and drew a line representing the tool path.\nTo continue, pressNextagain to see your robot execute the plan.\nYou extended the program you wrote with MoveIt to interact with the Gui in RViz, allowing you to step through your program with a button, render some text above the robot, and display the tool path that you planned.\nMoveItVisualTools has many more useful features for visualizing robot motions.You can read more about it here.\nThere are also more examples of usingMoveItVisualToolsinMoveItCpp Tutorial.\nHere is a copy of the full hello_moveit.cpp source.\nIn the next tutorialPlanning Around Objects, you will expand on the program you built here to add to the collision environment and see the robot plan with these changes.\n© Copyright 2024, PickNik Robotics.\n<depend>moveit_visual_tools</depend>\nfind_package(moveit_visual_toolsREQUIRED)\nament_target_dependencies(hello_moveit\"moveit_ros_planning_interface\"\"moveit_visual_tools\"\"rclcpp\")\n#include<moveit_visual_tools/moveit_visual_tools.h>\ncd~/ws_moveit\ncolconbuild--mixindebug\n#include<thread>// <---- add this to the set of includes at the top\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Spin up a SingleThreadedExecutor for MoveItVisualTools to interact with ROSrclcpp::executors::SingleThreadedExecutorexecutor;executor.add_node(node);autospinner=std::thread([&executor](){executor.spin();});// Create the MoveIt MoveGroup Interface...\n// Shutdown ROSrclcpp::shutdown();// <--- This will cause the spin function in the thread to returnspinner.join();// <--- Join the thread before exitingreturn0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Construct and initialize MoveItVisualToolsautomoveit_visual_tools=moveit_visual_tools::MoveItVisualTools{node,\"base_link\",rviz_visual_tools::RVIZ_MARKER_TOPIC,move_group_interface.getRobotModel()};moveit_visual_tools.deleteAllMarkers();moveit_visual_tools.loadRemoteControl();\n// Create closures for visualizationautoconstdraw_title=[&moveit_visual_tools](autotext){autoconsttext_pose=[]{automsg=Eigen::Isometry3d::Identity();msg.translation().z()=1.0;// Place text 1m above the base linkreturnmsg;}();moveit_visual_tools.publishText(text_pose,text,rviz_visual_tools::WHITE,rviz_visual_tools::XLARGE);};autoconstprompt=[&moveit_visual_tools](autotext){moveit_visual_tools.prompt(text);};autoconstdraw_trajectory_tool_path=[&moveit_visual_tools,jmg=move_group_interface.getRobotModel()->getJointModelGroup(\"manipulator\")](autoconsttrajectory){moveit_visual_tools.publishTrajectoryLine(trajectory,jmg);};\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseprompt(\"Press 'Next' in the RvizVisualToolsGui window to plan\");draw_title(\"Planning\");moveit_visual_tools.trigger();autoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){draw_trajectory_tool_path(plan.trajectory);moveit_visual_tools.trigger();prompt(\"Press 'Next' in the RvizVisualToolsGui window to execute\");draw_title(\"Executing\");moveit_visual_tools.trigger();move_group_interface.execute(plan);}else{draw_title(\"Planning Failed!\");moveit_visual_tools.trigger();RCLCPP_ERROR(logger,\"Planning failed!\");}\ncd~/ws_moveitsource/opt/ros/rolling/setup.bash\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2launchmoveit2_tutorialsdemo.launch.py\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\n[INFO][1652822889.492940200][hello_moveit.remote_control]:Waitingtocontinue:Press'Next'intheRvizVisualToolsGuiwindowtoplan\nhello_moveit\npackage.xml\nhello_moveit\n<depend>\nCMakeLists.txt\nfind_package\nament_target_dependencies\nhello_moveit.cpp\nmoveit_visual_tools\nmoveit_visual_tools\ndraw_title\nprompt\nnext\ntrigger\nmoveit_visual_tools\ntrigger\nRvizVisualToolsGui\nNext\nMarkerArray\nMarkerArray\nOK\n/rviz_visual_tools\nhello_moveit\nNext\nNext\nMoveItVisualTools",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0c2"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/visualizing_in_rviz/visualizing_in_rviz.html#enable-visualizations-in-rviz",
  "title": "Visualizing In RViz — MoveIt Documentation: Rolling  documentation",
  "content": "Visualizing In RViz\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to a tool that can help you more easily understand what your MoveIt application is doing by rendering visualizations in RViz.\nIf you haven’t already done so, make sure you’ve completed the steps inYour First Project.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off.\nAdd this line to yourpackage.xmlin thehello_moveitproject after the other<depend>statements:\nThen in yourCMakeLists.txtadd this line to the section offind_packagestatements:\nFurther down in the file extend theament_target_dependenciesmacro call to include the new dependency like this:\nTo verify that you added the dependency correctly, add the required include to your source filehello_moveit.cpp:\nTo test that this all worked, open a terminal in the workspace directory (remembering to source your ROS install in opt) and then build with colcon:\nBefore we can initialize MoveItVisualTools, we need to have a executor spinning on our ROS node.\nThis is necessary because of how MoveItVisualTools interacts with ROS services and topics. First, add the threading library to your includes at the top.\nBy creating and naming loggers, we are able to keep our program logs organized.\nNext, add your executor before creating the MoveIt MoveGroup Interface.\nFinally, make sure to join the thread before exiting.\nAfter making these changes, rebuild your workspace to make sure you don’t have any syntax errors.\nNext, we will construct and initialize MoveItVisualTools after the construction of MoveGroupInterface.\nWe pass the following into the constructor: the ROS node, the base link of the robot, the marker topic to use (more on this later), and the robot model (which we get from the move_group_interface).\nNext, we make a call to delete all the markers. This clears any rendered state out of RViz that we have left over from previous runs.\nLastly, we load remote control.\nRemote control is a really simple plugin that lets us have a button in RViz to interact with our program.\nAfter we’ve constructed and initialized, we now create some closures (function objects that have access to variables in our current scope) that we can use later in our program to help render visualizations in RViz.\nEach of the three closures capturemoveit_visual_toolsby reference and the last one captures a pointer to the joint model group object we are planning with.\nEach of these call a function onmoveit_visual_toolsthat changes something in RViz.\nThe first one,draw_titleadds text one meter above the base of the robot. This is a useful way to show the state of your program from a high level.\nThe second one calls a function calledprompt. This function blocks your program until the user presses thenextbutton in RViz. This is helpful for stepping through a program when debugging.\nThe last one draws the tool path of a trajectory that we have planned. This is often helpful for understanding a planned trajectory from the perspective of the tool.\nYou might be asking yourself why we would create lambdas like this, and the reason is simply to make the code that comes later easier to read and understand.\nAs your write software, it is often helpful to break up your functionality into named functions which can be easily reused and tested on their own.\nYou will see in the next section how we use these functions we created.\nNow we’ll augment the code in the middle of your program.\nUpdate your code for planning and executing to include these new features:\nOne thing you’ll quickly notice is that we have to call a method calledtriggeronmoveit_visual_toolsafter each call to change something rendered in RViz.\nThe reason for this is that messages sent to RViz are batched up and sent when you calltriggerto reduce bandwidth of the marker topics.\nLastly, build your project again to make sure all the code additions are correct.\nOpen a new terminal, source the workspace, and then start the demo launch file that opens RViz.\nUncheck “MotionPlanning” in the “Displays” tab to hide it.\nWe aren’t going to be using the “MotionPlanning” plugin for this next part.\nTo add the buttons to interact with the prompts we added to our program open the dialog with the “Panels/Add New Panel” menu:\nThen selectRvizVisualToolsGuiand click OK.\nThis will create a new panel on the bottom left with aNextbutton we’ll use later.\nFinally, we need to add aMarkerArrayto render the visualizations we’ve added.\nClick on the “Add” Button in the “Displays” panel.\nSelectMarkerArrayand clickOK.\nScroll to the bottom of the items in the Displays panel and edit the topic that the new Marker Array is using to/rviz_visual_tools.\nYou are now ready to run your new program with visualizations.\nIn a new terminal, go to the workspace, source the workspace, and runhello_moveit:\nYou’ll notice that your program has stopped with a log that looks like this:\nClick theNextbutton in RViz and see your application advance.\nYou’ll see after you clicked the next button, your application planned, added a title above the robot, and drew a line representing the tool path.\nTo continue, pressNextagain to see your robot execute the plan.\nYou extended the program you wrote with MoveIt to interact with the Gui in RViz, allowing you to step through your program with a button, render some text above the robot, and display the tool path that you planned.\nMoveItVisualTools has many more useful features for visualizing robot motions.You can read more about it here.\nThere are also more examples of usingMoveItVisualToolsinMoveItCpp Tutorial.\nHere is a copy of the full hello_moveit.cpp source.\nIn the next tutorialPlanning Around Objects, you will expand on the program you built here to add to the collision environment and see the robot plan with these changes.\n© Copyright 2024, PickNik Robotics.\n<depend>moveit_visual_tools</depend>\nfind_package(moveit_visual_toolsREQUIRED)\nament_target_dependencies(hello_moveit\"moveit_ros_planning_interface\"\"moveit_visual_tools\"\"rclcpp\")\n#include<moveit_visual_tools/moveit_visual_tools.h>\ncd~/ws_moveit\ncolconbuild--mixindebug\n#include<thread>// <---- add this to the set of includes at the top\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Spin up a SingleThreadedExecutor for MoveItVisualTools to interact with ROSrclcpp::executors::SingleThreadedExecutorexecutor;executor.add_node(node);autospinner=std::thread([&executor](){executor.spin();});// Create the MoveIt MoveGroup Interface...\n// Shutdown ROSrclcpp::shutdown();// <--- This will cause the spin function in the thread to returnspinner.join();// <--- Join the thread before exitingreturn0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Construct and initialize MoveItVisualToolsautomoveit_visual_tools=moveit_visual_tools::MoveItVisualTools{node,\"base_link\",rviz_visual_tools::RVIZ_MARKER_TOPIC,move_group_interface.getRobotModel()};moveit_visual_tools.deleteAllMarkers();moveit_visual_tools.loadRemoteControl();\n// Create closures for visualizationautoconstdraw_title=[&moveit_visual_tools](autotext){autoconsttext_pose=[]{automsg=Eigen::Isometry3d::Identity();msg.translation().z()=1.0;// Place text 1m above the base linkreturnmsg;}();moveit_visual_tools.publishText(text_pose,text,rviz_visual_tools::WHITE,rviz_visual_tools::XLARGE);};autoconstprompt=[&moveit_visual_tools](autotext){moveit_visual_tools.prompt(text);};autoconstdraw_trajectory_tool_path=[&moveit_visual_tools,jmg=move_group_interface.getRobotModel()->getJointModelGroup(\"manipulator\")](autoconsttrajectory){moveit_visual_tools.publishTrajectoryLine(trajectory,jmg);};\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseprompt(\"Press 'Next' in the RvizVisualToolsGui window to plan\");draw_title(\"Planning\");moveit_visual_tools.trigger();autoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){draw_trajectory_tool_path(plan.trajectory);moveit_visual_tools.trigger();prompt(\"Press 'Next' in the RvizVisualToolsGui window to execute\");draw_title(\"Executing\");moveit_visual_tools.trigger();move_group_interface.execute(plan);}else{draw_title(\"Planning Failed!\");moveit_visual_tools.trigger();RCLCPP_ERROR(logger,\"Planning failed!\");}\ncd~/ws_moveitsource/opt/ros/rolling/setup.bash\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2launchmoveit2_tutorialsdemo.launch.py\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\n[INFO][1652822889.492940200][hello_moveit.remote_control]:Waitingtocontinue:Press'Next'intheRvizVisualToolsGuiwindowtoplan\nhello_moveit\npackage.xml\nhello_moveit\n<depend>\nCMakeLists.txt\nfind_package\nament_target_dependencies\nhello_moveit.cpp\nmoveit_visual_tools\nmoveit_visual_tools\ndraw_title\nprompt\nnext\ntrigger\nmoveit_visual_tools\ntrigger\nRvizVisualToolsGui\nNext\nMarkerArray\nMarkerArray\nOK\n/rviz_visual_tools\nhello_moveit\nNext\nNext\nMoveItVisualTools",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0c3"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/visualizing_in_rviz/visualizing_in_rviz.html#run-the-program",
  "title": "Visualizing In RViz — MoveIt Documentation: Rolling  documentation",
  "content": "Visualizing In RViz\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to a tool that can help you more easily understand what your MoveIt application is doing by rendering visualizations in RViz.\nIf you haven’t already done so, make sure you’ve completed the steps inYour First Project.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off.\nAdd this line to yourpackage.xmlin thehello_moveitproject after the other<depend>statements:\nThen in yourCMakeLists.txtadd this line to the section offind_packagestatements:\nFurther down in the file extend theament_target_dependenciesmacro call to include the new dependency like this:\nTo verify that you added the dependency correctly, add the required include to your source filehello_moveit.cpp:\nTo test that this all worked, open a terminal in the workspace directory (remembering to source your ROS install in opt) and then build with colcon:\nBefore we can initialize MoveItVisualTools, we need to have a executor spinning on our ROS node.\nThis is necessary because of how MoveItVisualTools interacts with ROS services and topics. First, add the threading library to your includes at the top.\nBy creating and naming loggers, we are able to keep our program logs organized.\nNext, add your executor before creating the MoveIt MoveGroup Interface.\nFinally, make sure to join the thread before exiting.\nAfter making these changes, rebuild your workspace to make sure you don’t have any syntax errors.\nNext, we will construct and initialize MoveItVisualTools after the construction of MoveGroupInterface.\nWe pass the following into the constructor: the ROS node, the base link of the robot, the marker topic to use (more on this later), and the robot model (which we get from the move_group_interface).\nNext, we make a call to delete all the markers. This clears any rendered state out of RViz that we have left over from previous runs.\nLastly, we load remote control.\nRemote control is a really simple plugin that lets us have a button in RViz to interact with our program.\nAfter we’ve constructed and initialized, we now create some closures (function objects that have access to variables in our current scope) that we can use later in our program to help render visualizations in RViz.\nEach of the three closures capturemoveit_visual_toolsby reference and the last one captures a pointer to the joint model group object we are planning with.\nEach of these call a function onmoveit_visual_toolsthat changes something in RViz.\nThe first one,draw_titleadds text one meter above the base of the robot. This is a useful way to show the state of your program from a high level.\nThe second one calls a function calledprompt. This function blocks your program until the user presses thenextbutton in RViz. This is helpful for stepping through a program when debugging.\nThe last one draws the tool path of a trajectory that we have planned. This is often helpful for understanding a planned trajectory from the perspective of the tool.\nYou might be asking yourself why we would create lambdas like this, and the reason is simply to make the code that comes later easier to read and understand.\nAs your write software, it is often helpful to break up your functionality into named functions which can be easily reused and tested on their own.\nYou will see in the next section how we use these functions we created.\nNow we’ll augment the code in the middle of your program.\nUpdate your code for planning and executing to include these new features:\nOne thing you’ll quickly notice is that we have to call a method calledtriggeronmoveit_visual_toolsafter each call to change something rendered in RViz.\nThe reason for this is that messages sent to RViz are batched up and sent when you calltriggerto reduce bandwidth of the marker topics.\nLastly, build your project again to make sure all the code additions are correct.\nOpen a new terminal, source the workspace, and then start the demo launch file that opens RViz.\nUncheck “MotionPlanning” in the “Displays” tab to hide it.\nWe aren’t going to be using the “MotionPlanning” plugin for this next part.\nTo add the buttons to interact with the prompts we added to our program open the dialog with the “Panels/Add New Panel” menu:\nThen selectRvizVisualToolsGuiand click OK.\nThis will create a new panel on the bottom left with aNextbutton we’ll use later.\nFinally, we need to add aMarkerArrayto render the visualizations we’ve added.\nClick on the “Add” Button in the “Displays” panel.\nSelectMarkerArrayand clickOK.\nScroll to the bottom of the items in the Displays panel and edit the topic that the new Marker Array is using to/rviz_visual_tools.\nYou are now ready to run your new program with visualizations.\nIn a new terminal, go to the workspace, source the workspace, and runhello_moveit:\nYou’ll notice that your program has stopped with a log that looks like this:\nClick theNextbutton in RViz and see your application advance.\nYou’ll see after you clicked the next button, your application planned, added a title above the robot, and drew a line representing the tool path.\nTo continue, pressNextagain to see your robot execute the plan.\nYou extended the program you wrote with MoveIt to interact with the Gui in RViz, allowing you to step through your program with a button, render some text above the robot, and display the tool path that you planned.\nMoveItVisualTools has many more useful features for visualizing robot motions.You can read more about it here.\nThere are also more examples of usingMoveItVisualToolsinMoveItCpp Tutorial.\nHere is a copy of the full hello_moveit.cpp source.\nIn the next tutorialPlanning Around Objects, you will expand on the program you built here to add to the collision environment and see the robot plan with these changes.\n© Copyright 2024, PickNik Robotics.\n<depend>moveit_visual_tools</depend>\nfind_package(moveit_visual_toolsREQUIRED)\nament_target_dependencies(hello_moveit\"moveit_ros_planning_interface\"\"moveit_visual_tools\"\"rclcpp\")\n#include<moveit_visual_tools/moveit_visual_tools.h>\ncd~/ws_moveit\ncolconbuild--mixindebug\n#include<thread>// <---- add this to the set of includes at the top\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Spin up a SingleThreadedExecutor for MoveItVisualTools to interact with ROSrclcpp::executors::SingleThreadedExecutorexecutor;executor.add_node(node);autospinner=std::thread([&executor](){executor.spin();});// Create the MoveIt MoveGroup Interface...\n// Shutdown ROSrclcpp::shutdown();// <--- This will cause the spin function in the thread to returnspinner.join();// <--- Join the thread before exitingreturn0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Construct and initialize MoveItVisualToolsautomoveit_visual_tools=moveit_visual_tools::MoveItVisualTools{node,\"base_link\",rviz_visual_tools::RVIZ_MARKER_TOPIC,move_group_interface.getRobotModel()};moveit_visual_tools.deleteAllMarkers();moveit_visual_tools.loadRemoteControl();\n// Create closures for visualizationautoconstdraw_title=[&moveit_visual_tools](autotext){autoconsttext_pose=[]{automsg=Eigen::Isometry3d::Identity();msg.translation().z()=1.0;// Place text 1m above the base linkreturnmsg;}();moveit_visual_tools.publishText(text_pose,text,rviz_visual_tools::WHITE,rviz_visual_tools::XLARGE);};autoconstprompt=[&moveit_visual_tools](autotext){moveit_visual_tools.prompt(text);};autoconstdraw_trajectory_tool_path=[&moveit_visual_tools,jmg=move_group_interface.getRobotModel()->getJointModelGroup(\"manipulator\")](autoconsttrajectory){moveit_visual_tools.publishTrajectoryLine(trajectory,jmg);};\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseprompt(\"Press 'Next' in the RvizVisualToolsGui window to plan\");draw_title(\"Planning\");moveit_visual_tools.trigger();autoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){draw_trajectory_tool_path(plan.trajectory);moveit_visual_tools.trigger();prompt(\"Press 'Next' in the RvizVisualToolsGui window to execute\");draw_title(\"Executing\");moveit_visual_tools.trigger();move_group_interface.execute(plan);}else{draw_title(\"Planning Failed!\");moveit_visual_tools.trigger();RCLCPP_ERROR(logger,\"Planning failed!\");}\ncd~/ws_moveitsource/opt/ros/rolling/setup.bash\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2launchmoveit2_tutorialsdemo.launch.py\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\n[INFO][1652822889.492940200][hello_moveit.remote_control]:Waitingtocontinue:Press'Next'intheRvizVisualToolsGuiwindowtoplan\nhello_moveit\npackage.xml\nhello_moveit\n<depend>\nCMakeLists.txt\nfind_package\nament_target_dependencies\nhello_moveit.cpp\nmoveit_visual_tools\nmoveit_visual_tools\ndraw_title\nprompt\nnext\ntrigger\nmoveit_visual_tools\ntrigger\nRvizVisualToolsGui\nNext\nMarkerArray\nMarkerArray\nOK\n/rviz_visual_tools\nhello_moveit\nNext\nNext\nMoveItVisualTools",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0c4"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/visualizing_in_rviz/visualizing_in_rviz.html#summary",
  "title": "Visualizing In RViz — MoveIt Documentation: Rolling  documentation",
  "content": "Visualizing In RViz\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to a tool that can help you more easily understand what your MoveIt application is doing by rendering visualizations in RViz.\nIf you haven’t already done so, make sure you’ve completed the steps inYour First Project.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off.\nAdd this line to yourpackage.xmlin thehello_moveitproject after the other<depend>statements:\nThen in yourCMakeLists.txtadd this line to the section offind_packagestatements:\nFurther down in the file extend theament_target_dependenciesmacro call to include the new dependency like this:\nTo verify that you added the dependency correctly, add the required include to your source filehello_moveit.cpp:\nTo test that this all worked, open a terminal in the workspace directory (remembering to source your ROS install in opt) and then build with colcon:\nBefore we can initialize MoveItVisualTools, we need to have a executor spinning on our ROS node.\nThis is necessary because of how MoveItVisualTools interacts with ROS services and topics. First, add the threading library to your includes at the top.\nBy creating and naming loggers, we are able to keep our program logs organized.\nNext, add your executor before creating the MoveIt MoveGroup Interface.\nFinally, make sure to join the thread before exiting.\nAfter making these changes, rebuild your workspace to make sure you don’t have any syntax errors.\nNext, we will construct and initialize MoveItVisualTools after the construction of MoveGroupInterface.\nWe pass the following into the constructor: the ROS node, the base link of the robot, the marker topic to use (more on this later), and the robot model (which we get from the move_group_interface).\nNext, we make a call to delete all the markers. This clears any rendered state out of RViz that we have left over from previous runs.\nLastly, we load remote control.\nRemote control is a really simple plugin that lets us have a button in RViz to interact with our program.\nAfter we’ve constructed and initialized, we now create some closures (function objects that have access to variables in our current scope) that we can use later in our program to help render visualizations in RViz.\nEach of the three closures capturemoveit_visual_toolsby reference and the last one captures a pointer to the joint model group object we are planning with.\nEach of these call a function onmoveit_visual_toolsthat changes something in RViz.\nThe first one,draw_titleadds text one meter above the base of the robot. This is a useful way to show the state of your program from a high level.\nThe second one calls a function calledprompt. This function blocks your program until the user presses thenextbutton in RViz. This is helpful for stepping through a program when debugging.\nThe last one draws the tool path of a trajectory that we have planned. This is often helpful for understanding a planned trajectory from the perspective of the tool.\nYou might be asking yourself why we would create lambdas like this, and the reason is simply to make the code that comes later easier to read and understand.\nAs your write software, it is often helpful to break up your functionality into named functions which can be easily reused and tested on their own.\nYou will see in the next section how we use these functions we created.\nNow we’ll augment the code in the middle of your program.\nUpdate your code for planning and executing to include these new features:\nOne thing you’ll quickly notice is that we have to call a method calledtriggeronmoveit_visual_toolsafter each call to change something rendered in RViz.\nThe reason for this is that messages sent to RViz are batched up and sent when you calltriggerto reduce bandwidth of the marker topics.\nLastly, build your project again to make sure all the code additions are correct.\nOpen a new terminal, source the workspace, and then start the demo launch file that opens RViz.\nUncheck “MotionPlanning” in the “Displays” tab to hide it.\nWe aren’t going to be using the “MotionPlanning” plugin for this next part.\nTo add the buttons to interact with the prompts we added to our program open the dialog with the “Panels/Add New Panel” menu:\nThen selectRvizVisualToolsGuiand click OK.\nThis will create a new panel on the bottom left with aNextbutton we’ll use later.\nFinally, we need to add aMarkerArrayto render the visualizations we’ve added.\nClick on the “Add” Button in the “Displays” panel.\nSelectMarkerArrayand clickOK.\nScroll to the bottom of the items in the Displays panel and edit the topic that the new Marker Array is using to/rviz_visual_tools.\nYou are now ready to run your new program with visualizations.\nIn a new terminal, go to the workspace, source the workspace, and runhello_moveit:\nYou’ll notice that your program has stopped with a log that looks like this:\nClick theNextbutton in RViz and see your application advance.\nYou’ll see after you clicked the next button, your application planned, added a title above the robot, and drew a line representing the tool path.\nTo continue, pressNextagain to see your robot execute the plan.\nYou extended the program you wrote with MoveIt to interact with the Gui in RViz, allowing you to step through your program with a button, render some text above the robot, and display the tool path that you planned.\nMoveItVisualTools has many more useful features for visualizing robot motions.You can read more about it here.\nThere are also more examples of usingMoveItVisualToolsinMoveItCpp Tutorial.\nHere is a copy of the full hello_moveit.cpp source.\nIn the next tutorialPlanning Around Objects, you will expand on the program you built here to add to the collision environment and see the robot plan with these changes.\n© Copyright 2024, PickNik Robotics.\n<depend>moveit_visual_tools</depend>\nfind_package(moveit_visual_toolsREQUIRED)\nament_target_dependencies(hello_moveit\"moveit_ros_planning_interface\"\"moveit_visual_tools\"\"rclcpp\")\n#include<moveit_visual_tools/moveit_visual_tools.h>\ncd~/ws_moveit\ncolconbuild--mixindebug\n#include<thread>// <---- add this to the set of includes at the top\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Spin up a SingleThreadedExecutor for MoveItVisualTools to interact with ROSrclcpp::executors::SingleThreadedExecutorexecutor;executor.add_node(node);autospinner=std::thread([&executor](){executor.spin();});// Create the MoveIt MoveGroup Interface...\n// Shutdown ROSrclcpp::shutdown();// <--- This will cause the spin function in the thread to returnspinner.join();// <--- Join the thread before exitingreturn0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Construct and initialize MoveItVisualToolsautomoveit_visual_tools=moveit_visual_tools::MoveItVisualTools{node,\"base_link\",rviz_visual_tools::RVIZ_MARKER_TOPIC,move_group_interface.getRobotModel()};moveit_visual_tools.deleteAllMarkers();moveit_visual_tools.loadRemoteControl();\n// Create closures for visualizationautoconstdraw_title=[&moveit_visual_tools](autotext){autoconsttext_pose=[]{automsg=Eigen::Isometry3d::Identity();msg.translation().z()=1.0;// Place text 1m above the base linkreturnmsg;}();moveit_visual_tools.publishText(text_pose,text,rviz_visual_tools::WHITE,rviz_visual_tools::XLARGE);};autoconstprompt=[&moveit_visual_tools](autotext){moveit_visual_tools.prompt(text);};autoconstdraw_trajectory_tool_path=[&moveit_visual_tools,jmg=move_group_interface.getRobotModel()->getJointModelGroup(\"manipulator\")](autoconsttrajectory){moveit_visual_tools.publishTrajectoryLine(trajectory,jmg);};\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseprompt(\"Press 'Next' in the RvizVisualToolsGui window to plan\");draw_title(\"Planning\");moveit_visual_tools.trigger();autoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){draw_trajectory_tool_path(plan.trajectory);moveit_visual_tools.trigger();prompt(\"Press 'Next' in the RvizVisualToolsGui window to execute\");draw_title(\"Executing\");moveit_visual_tools.trigger();move_group_interface.execute(plan);}else{draw_title(\"Planning Failed!\");moveit_visual_tools.trigger();RCLCPP_ERROR(logger,\"Planning failed!\");}\ncd~/ws_moveitsource/opt/ros/rolling/setup.bash\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2launchmoveit2_tutorialsdemo.launch.py\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\n[INFO][1652822889.492940200][hello_moveit.remote_control]:Waitingtocontinue:Press'Next'intheRvizVisualToolsGuiwindowtoplan\nhello_moveit\npackage.xml\nhello_moveit\n<depend>\nCMakeLists.txt\nfind_package\nament_target_dependencies\nhello_moveit.cpp\nmoveit_visual_tools\nmoveit_visual_tools\ndraw_title\nprompt\nnext\ntrigger\nmoveit_visual_tools\ntrigger\nRvizVisualToolsGui\nNext\nMarkerArray\nMarkerArray\nOK\n/rviz_visual_tools\nhello_moveit\nNext\nNext\nMoveItVisualTools",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0c5"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/visualizing_in_rviz/visualizing_in_rviz.html#further-reading",
  "title": "Visualizing In RViz — MoveIt Documentation: Rolling  documentation",
  "content": "Visualizing In RViz\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to a tool that can help you more easily understand what your MoveIt application is doing by rendering visualizations in RViz.\nIf you haven’t already done so, make sure you’ve completed the steps inYour First Project.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off.\nAdd this line to yourpackage.xmlin thehello_moveitproject after the other<depend>statements:\nThen in yourCMakeLists.txtadd this line to the section offind_packagestatements:\nFurther down in the file extend theament_target_dependenciesmacro call to include the new dependency like this:\nTo verify that you added the dependency correctly, add the required include to your source filehello_moveit.cpp:\nTo test that this all worked, open a terminal in the workspace directory (remembering to source your ROS install in opt) and then build with colcon:\nBefore we can initialize MoveItVisualTools, we need to have a executor spinning on our ROS node.\nThis is necessary because of how MoveItVisualTools interacts with ROS services and topics. First, add the threading library to your includes at the top.\nBy creating and naming loggers, we are able to keep our program logs organized.\nNext, add your executor before creating the MoveIt MoveGroup Interface.\nFinally, make sure to join the thread before exiting.\nAfter making these changes, rebuild your workspace to make sure you don’t have any syntax errors.\nNext, we will construct and initialize MoveItVisualTools after the construction of MoveGroupInterface.\nWe pass the following into the constructor: the ROS node, the base link of the robot, the marker topic to use (more on this later), and the robot model (which we get from the move_group_interface).\nNext, we make a call to delete all the markers. This clears any rendered state out of RViz that we have left over from previous runs.\nLastly, we load remote control.\nRemote control is a really simple plugin that lets us have a button in RViz to interact with our program.\nAfter we’ve constructed and initialized, we now create some closures (function objects that have access to variables in our current scope) that we can use later in our program to help render visualizations in RViz.\nEach of the three closures capturemoveit_visual_toolsby reference and the last one captures a pointer to the joint model group object we are planning with.\nEach of these call a function onmoveit_visual_toolsthat changes something in RViz.\nThe first one,draw_titleadds text one meter above the base of the robot. This is a useful way to show the state of your program from a high level.\nThe second one calls a function calledprompt. This function blocks your program until the user presses thenextbutton in RViz. This is helpful for stepping through a program when debugging.\nThe last one draws the tool path of a trajectory that we have planned. This is often helpful for understanding a planned trajectory from the perspective of the tool.\nYou might be asking yourself why we would create lambdas like this, and the reason is simply to make the code that comes later easier to read and understand.\nAs your write software, it is often helpful to break up your functionality into named functions which can be easily reused and tested on their own.\nYou will see in the next section how we use these functions we created.\nNow we’ll augment the code in the middle of your program.\nUpdate your code for planning and executing to include these new features:\nOne thing you’ll quickly notice is that we have to call a method calledtriggeronmoveit_visual_toolsafter each call to change something rendered in RViz.\nThe reason for this is that messages sent to RViz are batched up and sent when you calltriggerto reduce bandwidth of the marker topics.\nLastly, build your project again to make sure all the code additions are correct.\nOpen a new terminal, source the workspace, and then start the demo launch file that opens RViz.\nUncheck “MotionPlanning” in the “Displays” tab to hide it.\nWe aren’t going to be using the “MotionPlanning” plugin for this next part.\nTo add the buttons to interact with the prompts we added to our program open the dialog with the “Panels/Add New Panel” menu:\nThen selectRvizVisualToolsGuiand click OK.\nThis will create a new panel on the bottom left with aNextbutton we’ll use later.\nFinally, we need to add aMarkerArrayto render the visualizations we’ve added.\nClick on the “Add” Button in the “Displays” panel.\nSelectMarkerArrayand clickOK.\nScroll to the bottom of the items in the Displays panel and edit the topic that the new Marker Array is using to/rviz_visual_tools.\nYou are now ready to run your new program with visualizations.\nIn a new terminal, go to the workspace, source the workspace, and runhello_moveit:\nYou’ll notice that your program has stopped with a log that looks like this:\nClick theNextbutton in RViz and see your application advance.\nYou’ll see after you clicked the next button, your application planned, added a title above the robot, and drew a line representing the tool path.\nTo continue, pressNextagain to see your robot execute the plan.\nYou extended the program you wrote with MoveIt to interact with the Gui in RViz, allowing you to step through your program with a button, render some text above the robot, and display the tool path that you planned.\nMoveItVisualTools has many more useful features for visualizing robot motions.You can read more about it here.\nThere are also more examples of usingMoveItVisualToolsinMoveItCpp Tutorial.\nHere is a copy of the full hello_moveit.cpp source.\nIn the next tutorialPlanning Around Objects, you will expand on the program you built here to add to the collision environment and see the robot plan with these changes.\n© Copyright 2024, PickNik Robotics.\n<depend>moveit_visual_tools</depend>\nfind_package(moveit_visual_toolsREQUIRED)\nament_target_dependencies(hello_moveit\"moveit_ros_planning_interface\"\"moveit_visual_tools\"\"rclcpp\")\n#include<moveit_visual_tools/moveit_visual_tools.h>\ncd~/ws_moveit\ncolconbuild--mixindebug\n#include<thread>// <---- add this to the set of includes at the top\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Spin up a SingleThreadedExecutor for MoveItVisualTools to interact with ROSrclcpp::executors::SingleThreadedExecutorexecutor;executor.add_node(node);autospinner=std::thread([&executor](){executor.spin();});// Create the MoveIt MoveGroup Interface...\n// Shutdown ROSrclcpp::shutdown();// <--- This will cause the spin function in the thread to returnspinner.join();// <--- Join the thread before exitingreturn0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Construct and initialize MoveItVisualToolsautomoveit_visual_tools=moveit_visual_tools::MoveItVisualTools{node,\"base_link\",rviz_visual_tools::RVIZ_MARKER_TOPIC,move_group_interface.getRobotModel()};moveit_visual_tools.deleteAllMarkers();moveit_visual_tools.loadRemoteControl();\n// Create closures for visualizationautoconstdraw_title=[&moveit_visual_tools](autotext){autoconsttext_pose=[]{automsg=Eigen::Isometry3d::Identity();msg.translation().z()=1.0;// Place text 1m above the base linkreturnmsg;}();moveit_visual_tools.publishText(text_pose,text,rviz_visual_tools::WHITE,rviz_visual_tools::XLARGE);};autoconstprompt=[&moveit_visual_tools](autotext){moveit_visual_tools.prompt(text);};autoconstdraw_trajectory_tool_path=[&moveit_visual_tools,jmg=move_group_interface.getRobotModel()->getJointModelGroup(\"manipulator\")](autoconsttrajectory){moveit_visual_tools.publishTrajectoryLine(trajectory,jmg);};\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseprompt(\"Press 'Next' in the RvizVisualToolsGui window to plan\");draw_title(\"Planning\");moveit_visual_tools.trigger();autoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){draw_trajectory_tool_path(plan.trajectory);moveit_visual_tools.trigger();prompt(\"Press 'Next' in the RvizVisualToolsGui window to execute\");draw_title(\"Executing\");moveit_visual_tools.trigger();move_group_interface.execute(plan);}else{draw_title(\"Planning Failed!\");moveit_visual_tools.trigger();RCLCPP_ERROR(logger,\"Planning failed!\");}\ncd~/ws_moveitsource/opt/ros/rolling/setup.bash\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2launchmoveit2_tutorialsdemo.launch.py\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\n[INFO][1652822889.492940200][hello_moveit.remote_control]:Waitingtocontinue:Press'Next'intheRvizVisualToolsGuiwindowtoplan\nhello_moveit\npackage.xml\nhello_moveit\n<depend>\nCMakeLists.txt\nfind_package\nament_target_dependencies\nhello_moveit.cpp\nmoveit_visual_tools\nmoveit_visual_tools\ndraw_title\nprompt\nnext\ntrigger\nmoveit_visual_tools\ntrigger\nRvizVisualToolsGui\nNext\nMarkerArray\nMarkerArray\nOK\n/rviz_visual_tools\nhello_moveit\nNext\nNext\nMoveItVisualTools",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0c6"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/visualizing_in_rviz/visualizing_in_rviz.html#next-step",
  "title": "Visualizing In RViz — MoveIt Documentation: Rolling  documentation",
  "content": "Visualizing In RViz\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to a tool that can help you more easily understand what your MoveIt application is doing by rendering visualizations in RViz.\nIf you haven’t already done so, make sure you’ve completed the steps inYour First Project.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off.\nAdd this line to yourpackage.xmlin thehello_moveitproject after the other<depend>statements:\nThen in yourCMakeLists.txtadd this line to the section offind_packagestatements:\nFurther down in the file extend theament_target_dependenciesmacro call to include the new dependency like this:\nTo verify that you added the dependency correctly, add the required include to your source filehello_moveit.cpp:\nTo test that this all worked, open a terminal in the workspace directory (remembering to source your ROS install in opt) and then build with colcon:\nBefore we can initialize MoveItVisualTools, we need to have a executor spinning on our ROS node.\nThis is necessary because of how MoveItVisualTools interacts with ROS services and topics. First, add the threading library to your includes at the top.\nBy creating and naming loggers, we are able to keep our program logs organized.\nNext, add your executor before creating the MoveIt MoveGroup Interface.\nFinally, make sure to join the thread before exiting.\nAfter making these changes, rebuild your workspace to make sure you don’t have any syntax errors.\nNext, we will construct and initialize MoveItVisualTools after the construction of MoveGroupInterface.\nWe pass the following into the constructor: the ROS node, the base link of the robot, the marker topic to use (more on this later), and the robot model (which we get from the move_group_interface).\nNext, we make a call to delete all the markers. This clears any rendered state out of RViz that we have left over from previous runs.\nLastly, we load remote control.\nRemote control is a really simple plugin that lets us have a button in RViz to interact with our program.\nAfter we’ve constructed and initialized, we now create some closures (function objects that have access to variables in our current scope) that we can use later in our program to help render visualizations in RViz.\nEach of the three closures capturemoveit_visual_toolsby reference and the last one captures a pointer to the joint model group object we are planning with.\nEach of these call a function onmoveit_visual_toolsthat changes something in RViz.\nThe first one,draw_titleadds text one meter above the base of the robot. This is a useful way to show the state of your program from a high level.\nThe second one calls a function calledprompt. This function blocks your program until the user presses thenextbutton in RViz. This is helpful for stepping through a program when debugging.\nThe last one draws the tool path of a trajectory that we have planned. This is often helpful for understanding a planned trajectory from the perspective of the tool.\nYou might be asking yourself why we would create lambdas like this, and the reason is simply to make the code that comes later easier to read and understand.\nAs your write software, it is often helpful to break up your functionality into named functions which can be easily reused and tested on their own.\nYou will see in the next section how we use these functions we created.\nNow we’ll augment the code in the middle of your program.\nUpdate your code for planning and executing to include these new features:\nOne thing you’ll quickly notice is that we have to call a method calledtriggeronmoveit_visual_toolsafter each call to change something rendered in RViz.\nThe reason for this is that messages sent to RViz are batched up and sent when you calltriggerto reduce bandwidth of the marker topics.\nLastly, build your project again to make sure all the code additions are correct.\nOpen a new terminal, source the workspace, and then start the demo launch file that opens RViz.\nUncheck “MotionPlanning” in the “Displays” tab to hide it.\nWe aren’t going to be using the “MotionPlanning” plugin for this next part.\nTo add the buttons to interact with the prompts we added to our program open the dialog with the “Panels/Add New Panel” menu:\nThen selectRvizVisualToolsGuiand click OK.\nThis will create a new panel on the bottom left with aNextbutton we’ll use later.\nFinally, we need to add aMarkerArrayto render the visualizations we’ve added.\nClick on the “Add” Button in the “Displays” panel.\nSelectMarkerArrayand clickOK.\nScroll to the bottom of the items in the Displays panel and edit the topic that the new Marker Array is using to/rviz_visual_tools.\nYou are now ready to run your new program with visualizations.\nIn a new terminal, go to the workspace, source the workspace, and runhello_moveit:\nYou’ll notice that your program has stopped with a log that looks like this:\nClick theNextbutton in RViz and see your application advance.\nYou’ll see after you clicked the next button, your application planned, added a title above the robot, and drew a line representing the tool path.\nTo continue, pressNextagain to see your robot execute the plan.\nYou extended the program you wrote with MoveIt to interact with the Gui in RViz, allowing you to step through your program with a button, render some text above the robot, and display the tool path that you planned.\nMoveItVisualTools has many more useful features for visualizing robot motions.You can read more about it here.\nThere are also more examples of usingMoveItVisualToolsinMoveItCpp Tutorial.\nHere is a copy of the full hello_moveit.cpp source.\nIn the next tutorialPlanning Around Objects, you will expand on the program you built here to add to the collision environment and see the robot plan with these changes.\n© Copyright 2024, PickNik Robotics.\n<depend>moveit_visual_tools</depend>\nfind_package(moveit_visual_toolsREQUIRED)\nament_target_dependencies(hello_moveit\"moveit_ros_planning_interface\"\"moveit_visual_tools\"\"rclcpp\")\n#include<moveit_visual_tools/moveit_visual_tools.h>\ncd~/ws_moveit\ncolconbuild--mixindebug\n#include<thread>// <---- add this to the set of includes at the top\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Spin up a SingleThreadedExecutor for MoveItVisualTools to interact with ROSrclcpp::executors::SingleThreadedExecutorexecutor;executor.add_node(node);autospinner=std::thread([&executor](){executor.spin();});// Create the MoveIt MoveGroup Interface...\n// Shutdown ROSrclcpp::shutdown();// <--- This will cause the spin function in the thread to returnspinner.join();// <--- Join the thread before exitingreturn0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Construct and initialize MoveItVisualToolsautomoveit_visual_tools=moveit_visual_tools::MoveItVisualTools{node,\"base_link\",rviz_visual_tools::RVIZ_MARKER_TOPIC,move_group_interface.getRobotModel()};moveit_visual_tools.deleteAllMarkers();moveit_visual_tools.loadRemoteControl();\n// Create closures for visualizationautoconstdraw_title=[&moveit_visual_tools](autotext){autoconsttext_pose=[]{automsg=Eigen::Isometry3d::Identity();msg.translation().z()=1.0;// Place text 1m above the base linkreturnmsg;}();moveit_visual_tools.publishText(text_pose,text,rviz_visual_tools::WHITE,rviz_visual_tools::XLARGE);};autoconstprompt=[&moveit_visual_tools](autotext){moveit_visual_tools.prompt(text);};autoconstdraw_trajectory_tool_path=[&moveit_visual_tools,jmg=move_group_interface.getRobotModel()->getJointModelGroup(\"manipulator\")](autoconsttrajectory){moveit_visual_tools.publishTrajectoryLine(trajectory,jmg);};\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseprompt(\"Press 'Next' in the RvizVisualToolsGui window to plan\");draw_title(\"Planning\");moveit_visual_tools.trigger();autoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){draw_trajectory_tool_path(plan.trajectory);moveit_visual_tools.trigger();prompt(\"Press 'Next' in the RvizVisualToolsGui window to execute\");draw_title(\"Executing\");moveit_visual_tools.trigger();move_group_interface.execute(plan);}else{draw_title(\"Planning Failed!\");moveit_visual_tools.trigger();RCLCPP_ERROR(logger,\"Planning failed!\");}\ncd~/ws_moveitsource/opt/ros/rolling/setup.bash\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2launchmoveit2_tutorialsdemo.launch.py\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\n[INFO][1652822889.492940200][hello_moveit.remote_control]:Waitingtocontinue:Press'Next'intheRvizVisualToolsGuiwindowtoplan\nhello_moveit\npackage.xml\nhello_moveit\n<depend>\nCMakeLists.txt\nfind_package\nament_target_dependencies\nhello_moveit.cpp\nmoveit_visual_tools\nmoveit_visual_tools\ndraw_title\nprompt\nnext\ntrigger\nmoveit_visual_tools\ntrigger\nRvizVisualToolsGui\nNext\nMarkerArray\nMarkerArray\nOK\n/rviz_visual_tools\nhello_moveit\nNext\nNext\nMoveItVisualTools",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0c7"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/visualizing_in_rviz/visualizing_in_rviz.html#visualizing-in-rviz",
  "title": "Visualizing In RViz — MoveIt Documentation: Rolling  documentation",
  "content": "Visualizing In RViz\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to a tool that can help you more easily understand what your MoveIt application is doing by rendering visualizations in RViz.\nIf you haven’t already done so, make sure you’ve completed the steps inYour First Project.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off.\nAdd this line to yourpackage.xmlin thehello_moveitproject after the other<depend>statements:\nThen in yourCMakeLists.txtadd this line to the section offind_packagestatements:\nFurther down in the file extend theament_target_dependenciesmacro call to include the new dependency like this:\nTo verify that you added the dependency correctly, add the required include to your source filehello_moveit.cpp:\nTo test that this all worked, open a terminal in the workspace directory (remembering to source your ROS install in opt) and then build with colcon:\nBefore we can initialize MoveItVisualTools, we need to have a executor spinning on our ROS node.\nThis is necessary because of how MoveItVisualTools interacts with ROS services and topics. First, add the threading library to your includes at the top.\nBy creating and naming loggers, we are able to keep our program logs organized.\nNext, add your executor before creating the MoveIt MoveGroup Interface.\nFinally, make sure to join the thread before exiting.\nAfter making these changes, rebuild your workspace to make sure you don’t have any syntax errors.\nNext, we will construct and initialize MoveItVisualTools after the construction of MoveGroupInterface.\nWe pass the following into the constructor: the ROS node, the base link of the robot, the marker topic to use (more on this later), and the robot model (which we get from the move_group_interface).\nNext, we make a call to delete all the markers. This clears any rendered state out of RViz that we have left over from previous runs.\nLastly, we load remote control.\nRemote control is a really simple plugin that lets us have a button in RViz to interact with our program.\nAfter we’ve constructed and initialized, we now create some closures (function objects that have access to variables in our current scope) that we can use later in our program to help render visualizations in RViz.\nEach of the three closures capturemoveit_visual_toolsby reference and the last one captures a pointer to the joint model group object we are planning with.\nEach of these call a function onmoveit_visual_toolsthat changes something in RViz.\nThe first one,draw_titleadds text one meter above the base of the robot. This is a useful way to show the state of your program from a high level.\nThe second one calls a function calledprompt. This function blocks your program until the user presses thenextbutton in RViz. This is helpful for stepping through a program when debugging.\nThe last one draws the tool path of a trajectory that we have planned. This is often helpful for understanding a planned trajectory from the perspective of the tool.\nYou might be asking yourself why we would create lambdas like this, and the reason is simply to make the code that comes later easier to read and understand.\nAs your write software, it is often helpful to break up your functionality into named functions which can be easily reused and tested on their own.\nYou will see in the next section how we use these functions we created.\nNow we’ll augment the code in the middle of your program.\nUpdate your code for planning and executing to include these new features:\nOne thing you’ll quickly notice is that we have to call a method calledtriggeronmoveit_visual_toolsafter each call to change something rendered in RViz.\nThe reason for this is that messages sent to RViz are batched up and sent when you calltriggerto reduce bandwidth of the marker topics.\nLastly, build your project again to make sure all the code additions are correct.\nOpen a new terminal, source the workspace, and then start the demo launch file that opens RViz.\nUncheck “MotionPlanning” in the “Displays” tab to hide it.\nWe aren’t going to be using the “MotionPlanning” plugin for this next part.\nTo add the buttons to interact with the prompts we added to our program open the dialog with the “Panels/Add New Panel” menu:\nThen selectRvizVisualToolsGuiand click OK.\nThis will create a new panel on the bottom left with aNextbutton we’ll use later.\nFinally, we need to add aMarkerArrayto render the visualizations we’ve added.\nClick on the “Add” Button in the “Displays” panel.\nSelectMarkerArrayand clickOK.\nScroll to the bottom of the items in the Displays panel and edit the topic that the new Marker Array is using to/rviz_visual_tools.\nYou are now ready to run your new program with visualizations.\nIn a new terminal, go to the workspace, source the workspace, and runhello_moveit:\nYou’ll notice that your program has stopped with a log that looks like this:\nClick theNextbutton in RViz and see your application advance.\nYou’ll see after you clicked the next button, your application planned, added a title above the robot, and drew a line representing the tool path.\nTo continue, pressNextagain to see your robot execute the plan.\nYou extended the program you wrote with MoveIt to interact with the Gui in RViz, allowing you to step through your program with a button, render some text above the robot, and display the tool path that you planned.\nMoveItVisualTools has many more useful features for visualizing robot motions.You can read more about it here.\nThere are also more examples of usingMoveItVisualToolsinMoveItCpp Tutorial.\nHere is a copy of the full hello_moveit.cpp source.\nIn the next tutorialPlanning Around Objects, you will expand on the program you built here to add to the collision environment and see the robot plan with these changes.\n© Copyright 2024, PickNik Robotics.\n<depend>moveit_visual_tools</depend>\nfind_package(moveit_visual_toolsREQUIRED)\nament_target_dependencies(hello_moveit\"moveit_ros_planning_interface\"\"moveit_visual_tools\"\"rclcpp\")\n#include<moveit_visual_tools/moveit_visual_tools.h>\ncd~/ws_moveit\ncolconbuild--mixindebug\n#include<thread>// <---- add this to the set of includes at the top\n// Create a ROS loggerautoconstlogger=rclcpp::get_logger(\"hello_moveit\");\n// Spin up a SingleThreadedExecutor for MoveItVisualTools to interact with ROSrclcpp::executors::SingleThreadedExecutorexecutor;executor.add_node(node);autospinner=std::thread([&executor](){executor.spin();});// Create the MoveIt MoveGroup Interface...\n// Shutdown ROSrclcpp::shutdown();// <--- This will cause the spin function in the thread to returnspinner.join();// <--- Join the thread before exitingreturn0;\n// Create the MoveIt MoveGroup Interfaceusingmoveit::planning_interface::MoveGroupInterface;automove_group_interface=MoveGroupInterface(node,\"manipulator\");// Construct and initialize MoveItVisualToolsautomoveit_visual_tools=moveit_visual_tools::MoveItVisualTools{node,\"base_link\",rviz_visual_tools::RVIZ_MARKER_TOPIC,move_group_interface.getRobotModel()};moveit_visual_tools.deleteAllMarkers();moveit_visual_tools.loadRemoteControl();\n// Create closures for visualizationautoconstdraw_title=[&moveit_visual_tools](autotext){autoconsttext_pose=[]{automsg=Eigen::Isometry3d::Identity();msg.translation().z()=1.0;// Place text 1m above the base linkreturnmsg;}();moveit_visual_tools.publishText(text_pose,text,rviz_visual_tools::WHITE,rviz_visual_tools::XLARGE);};autoconstprompt=[&moveit_visual_tools](autotext){moveit_visual_tools.prompt(text);};autoconstdraw_trajectory_tool_path=[&moveit_visual_tools,jmg=move_group_interface.getRobotModel()->getJointModelGroup(\"manipulator\")](autoconsttrajectory){moveit_visual_tools.publishTrajectoryLine(trajectory,jmg);};\n// Set a target Poseautoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.w=1.0;msg.position.x=0.28;msg.position.y=-0.2;msg.position.z=0.5;returnmsg;}();move_group_interface.setPoseTarget(target_pose);// Create a plan to that target poseprompt(\"Press 'Next' in the RvizVisualToolsGui window to plan\");draw_title(\"Planning\");moveit_visual_tools.trigger();autoconst[success,plan]=[&move_group_interface]{moveit::planning_interface::MoveGroupInterface::Planmsg;autoconstok=static_cast<bool>(move_group_interface.plan(msg));returnstd::make_pair(ok,msg);}();// Execute the planif(success){draw_trajectory_tool_path(plan.trajectory);moveit_visual_tools.trigger();prompt(\"Press 'Next' in the RvizVisualToolsGui window to execute\");draw_title(\"Executing\");moveit_visual_tools.trigger();move_group_interface.execute(plan);}else{draw_title(\"Planning Failed!\");moveit_visual_tools.trigger();RCLCPP_ERROR(logger,\"Planning failed!\");}\ncd~/ws_moveitsource/opt/ros/rolling/setup.bash\ncolconbuild--mixindebug\ncd~/ws_moveitsourceinstall/setup.bash\nros2launchmoveit2_tutorialsdemo.launch.py\ncd~/ws_moveitsourceinstall/setup.bash\nros2runhello_moveithello_moveit\n[INFO][1652822889.492940200][hello_moveit.remote_control]:Waitingtocontinue:Press'Next'intheRvizVisualToolsGuiwindowtoplan\nhello_moveit\npackage.xml\nhello_moveit\n<depend>\nCMakeLists.txt\nfind_package\nament_target_dependencies\nhello_moveit.cpp\nmoveit_visual_tools\nmoveit_visual_tools\ndraw_title\nprompt\nnext\ntrigger\nmoveit_visual_tools\ntrigger\nRvizVisualToolsGui\nNext\nMarkerArray\nMarkerArray\nOK\n/rviz_visual_tools\nhello_moveit\nNext\nNext\nMoveItVisualTools",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0c8"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/planning_around_objects/planning_around_objects.html",
  "title": "Planning Around Objects — MoveIt Documentation: Rolling  documentation",
  "content": "Planning Around Objects\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to inserting objects into the planning scene and planning around them.\nIf you haven’t already done so, make sure you’ve completed the steps inVisualizing in RViz.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off. If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nAt the top of your source file, add this to the list of includes:\nFirst, update the target pose with the following change to make the robot plan to a different location:\nIn the next block of code, we create a collision object.\nThe first thing to notice is that it is being placed in the coordinate frame of the robot.\nIf we had a perception system that reported the location of obstacles in our scene, then this is the sort of message it would build.\nBecause this is just an example, we are creating it manually.\nOne thing to notice at the end of this block of code is that we set the operation on this message toADD.\nThis results in the object getting added to the collision scene.\nPlace this code block between setting the target pose from the previous step and creating a plan.\nFinally, we need to add this object to the collision scene.\nTo do this, we use an object called thePlanningSceneInterfacethat uses ROS interfaces to communicate changes to the planning scene toMoveGroup.\nThis code block should directly follow the code block that creates the collision object.\nJust as we did in the last tutorial, start RViz using thedemo.launch.pyscript and run our program. If you’re using one of the Docker tutorial containers, you can specify a different RViz configuration that already has the RvizVisualToolsGui panel added using:\nYou extended the program you wrote with MoveIt to plan around an object in the scene.\nHere is a copy of the full hello_moveit.cpp source.\nExamples of using the Planning Scene for collision and constraint checking.\nExamples of using the Planning Scene ROS API.\nExample of visualizing collision objects.\nExample of subframes used for planning with objects.\nIn the next tutorialPick and Place with MoveIt Task Constructor, you will be introduced to a higher layer tool designed to solve harder motion plans.\nIn this next tutorial, you will create a program to pick and place an object.\n© Copyright 2024, PickNik Robotics.\n#include<moveit/planning_scene_interface/planning_scene_interface.h>\n// Set a target Pose with updated values !!!autoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.y=0.8;msg.orientation.w=0.6;msg.position.x=0.1;msg.position.y=0.4;msg.position.z=0.4;returnmsg;}();move_group_interface.setPoseTarget(target_pose);\n// Create collision object for the robot to avoidautoconstcollision_object=[frame_id=move_group_interface.getPlanningFrame()]{moveit_msgs::msg::CollisionObjectcollision_object;collision_object.header.frame_id=frame_id;collision_object.id=\"box1\";shape_msgs::msg::SolidPrimitiveprimitive;// Define the size of the box in metersprimitive.type=primitive.BOX;primitive.dimensions.resize(3);primitive.dimensions[primitive.BOX_X]=0.5;primitive.dimensions[primitive.BOX_Y]=0.1;primitive.dimensions[primitive.BOX_Z]=0.5;// Define the pose of the box (relative to the frame_id)geometry_msgs::msg::Posebox_pose;box_pose.orientation.w=1.0;// We can leave out the x, y, and z components of the quaternion since they are initialized to 0box_pose.position.x=0.2;box_pose.position.y=0.2;box_pose.position.z=0.25;collision_object.primitives.push_back(primitive);collision_object.primitive_poses.push_back(box_pose);collision_object.operation=collision_object.ADD;returncollision_object;}();\n// Add the collision object to the scenemoveit::planning_interface::PlanningSceneInterfaceplanning_scene_interface;planning_scene_interface.applyCollisionObject(collision_object);\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=kinova_hello_moveit.rviz\nhello_moveit\nADD\nPlanningSceneInterface\nMoveGroup\ndemo.launch.py",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0c9"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/planning_around_objects/planning_around_objects.html#prerequisites",
  "title": "Planning Around Objects — MoveIt Documentation: Rolling  documentation",
  "content": "Planning Around Objects\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to inserting objects into the planning scene and planning around them.\nIf you haven’t already done so, make sure you’ve completed the steps inVisualizing in RViz.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off. If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nAt the top of your source file, add this to the list of includes:\nFirst, update the target pose with the following change to make the robot plan to a different location:\nIn the next block of code, we create a collision object.\nThe first thing to notice is that it is being placed in the coordinate frame of the robot.\nIf we had a perception system that reported the location of obstacles in our scene, then this is the sort of message it would build.\nBecause this is just an example, we are creating it manually.\nOne thing to notice at the end of this block of code is that we set the operation on this message toADD.\nThis results in the object getting added to the collision scene.\nPlace this code block between setting the target pose from the previous step and creating a plan.\nFinally, we need to add this object to the collision scene.\nTo do this, we use an object called thePlanningSceneInterfacethat uses ROS interfaces to communicate changes to the planning scene toMoveGroup.\nThis code block should directly follow the code block that creates the collision object.\nJust as we did in the last tutorial, start RViz using thedemo.launch.pyscript and run our program. If you’re using one of the Docker tutorial containers, you can specify a different RViz configuration that already has the RvizVisualToolsGui panel added using:\nYou extended the program you wrote with MoveIt to plan around an object in the scene.\nHere is a copy of the full hello_moveit.cpp source.\nExamples of using the Planning Scene for collision and constraint checking.\nExamples of using the Planning Scene ROS API.\nExample of visualizing collision objects.\nExample of subframes used for planning with objects.\nIn the next tutorialPick and Place with MoveIt Task Constructor, you will be introduced to a higher layer tool designed to solve harder motion plans.\nIn this next tutorial, you will create a program to pick and place an object.\n© Copyright 2024, PickNik Robotics.\n#include<moveit/planning_scene_interface/planning_scene_interface.h>\n// Set a target Pose with updated values !!!autoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.y=0.8;msg.orientation.w=0.6;msg.position.x=0.1;msg.position.y=0.4;msg.position.z=0.4;returnmsg;}();move_group_interface.setPoseTarget(target_pose);\n// Create collision object for the robot to avoidautoconstcollision_object=[frame_id=move_group_interface.getPlanningFrame()]{moveit_msgs::msg::CollisionObjectcollision_object;collision_object.header.frame_id=frame_id;collision_object.id=\"box1\";shape_msgs::msg::SolidPrimitiveprimitive;// Define the size of the box in metersprimitive.type=primitive.BOX;primitive.dimensions.resize(3);primitive.dimensions[primitive.BOX_X]=0.5;primitive.dimensions[primitive.BOX_Y]=0.1;primitive.dimensions[primitive.BOX_Z]=0.5;// Define the pose of the box (relative to the frame_id)geometry_msgs::msg::Posebox_pose;box_pose.orientation.w=1.0;// We can leave out the x, y, and z components of the quaternion since they are initialized to 0box_pose.position.x=0.2;box_pose.position.y=0.2;box_pose.position.z=0.25;collision_object.primitives.push_back(primitive);collision_object.primitive_poses.push_back(box_pose);collision_object.operation=collision_object.ADD;returncollision_object;}();\n// Add the collision object to the scenemoveit::planning_interface::PlanningSceneInterfaceplanning_scene_interface;planning_scene_interface.applyCollisionObject(collision_object);\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=kinova_hello_moveit.rviz\nhello_moveit\nADD\nPlanningSceneInterface\nMoveGroup\ndemo.launch.py",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0ca"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/planning_around_objects/planning_around_objects.html#steps",
  "title": "Planning Around Objects — MoveIt Documentation: Rolling  documentation",
  "content": "Planning Around Objects\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to inserting objects into the planning scene and planning around them.\nIf you haven’t already done so, make sure you’ve completed the steps inVisualizing in RViz.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off. If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nAt the top of your source file, add this to the list of includes:\nFirst, update the target pose with the following change to make the robot plan to a different location:\nIn the next block of code, we create a collision object.\nThe first thing to notice is that it is being placed in the coordinate frame of the robot.\nIf we had a perception system that reported the location of obstacles in our scene, then this is the sort of message it would build.\nBecause this is just an example, we are creating it manually.\nOne thing to notice at the end of this block of code is that we set the operation on this message toADD.\nThis results in the object getting added to the collision scene.\nPlace this code block between setting the target pose from the previous step and creating a plan.\nFinally, we need to add this object to the collision scene.\nTo do this, we use an object called thePlanningSceneInterfacethat uses ROS interfaces to communicate changes to the planning scene toMoveGroup.\nThis code block should directly follow the code block that creates the collision object.\nJust as we did in the last tutorial, start RViz using thedemo.launch.pyscript and run our program. If you’re using one of the Docker tutorial containers, you can specify a different RViz configuration that already has the RvizVisualToolsGui panel added using:\nYou extended the program you wrote with MoveIt to plan around an object in the scene.\nHere is a copy of the full hello_moveit.cpp source.\nExamples of using the Planning Scene for collision and constraint checking.\nExamples of using the Planning Scene ROS API.\nExample of visualizing collision objects.\nExample of subframes used for planning with objects.\nIn the next tutorialPick and Place with MoveIt Task Constructor, you will be introduced to a higher layer tool designed to solve harder motion plans.\nIn this next tutorial, you will create a program to pick and place an object.\n© Copyright 2024, PickNik Robotics.\n#include<moveit/planning_scene_interface/planning_scene_interface.h>\n// Set a target Pose with updated values !!!autoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.y=0.8;msg.orientation.w=0.6;msg.position.x=0.1;msg.position.y=0.4;msg.position.z=0.4;returnmsg;}();move_group_interface.setPoseTarget(target_pose);\n// Create collision object for the robot to avoidautoconstcollision_object=[frame_id=move_group_interface.getPlanningFrame()]{moveit_msgs::msg::CollisionObjectcollision_object;collision_object.header.frame_id=frame_id;collision_object.id=\"box1\";shape_msgs::msg::SolidPrimitiveprimitive;// Define the size of the box in metersprimitive.type=primitive.BOX;primitive.dimensions.resize(3);primitive.dimensions[primitive.BOX_X]=0.5;primitive.dimensions[primitive.BOX_Y]=0.1;primitive.dimensions[primitive.BOX_Z]=0.5;// Define the pose of the box (relative to the frame_id)geometry_msgs::msg::Posebox_pose;box_pose.orientation.w=1.0;// We can leave out the x, y, and z components of the quaternion since they are initialized to 0box_pose.position.x=0.2;box_pose.position.y=0.2;box_pose.position.z=0.25;collision_object.primitives.push_back(primitive);collision_object.primitive_poses.push_back(box_pose);collision_object.operation=collision_object.ADD;returncollision_object;}();\n// Add the collision object to the scenemoveit::planning_interface::PlanningSceneInterfaceplanning_scene_interface;planning_scene_interface.applyCollisionObject(collision_object);\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=kinova_hello_moveit.rviz\nhello_moveit\nADD\nPlanningSceneInterface\nMoveGroup\ndemo.launch.py",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0cb"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/planning_around_objects/planning_around_objects.html#add-include-for-planning-scene-interface",
  "title": "Planning Around Objects — MoveIt Documentation: Rolling  documentation",
  "content": "Planning Around Objects\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to inserting objects into the planning scene and planning around them.\nIf you haven’t already done so, make sure you’ve completed the steps inVisualizing in RViz.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off. If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nAt the top of your source file, add this to the list of includes:\nFirst, update the target pose with the following change to make the robot plan to a different location:\nIn the next block of code, we create a collision object.\nThe first thing to notice is that it is being placed in the coordinate frame of the robot.\nIf we had a perception system that reported the location of obstacles in our scene, then this is the sort of message it would build.\nBecause this is just an example, we are creating it manually.\nOne thing to notice at the end of this block of code is that we set the operation on this message toADD.\nThis results in the object getting added to the collision scene.\nPlace this code block between setting the target pose from the previous step and creating a plan.\nFinally, we need to add this object to the collision scene.\nTo do this, we use an object called thePlanningSceneInterfacethat uses ROS interfaces to communicate changes to the planning scene toMoveGroup.\nThis code block should directly follow the code block that creates the collision object.\nJust as we did in the last tutorial, start RViz using thedemo.launch.pyscript and run our program. If you’re using one of the Docker tutorial containers, you can specify a different RViz configuration that already has the RvizVisualToolsGui panel added using:\nYou extended the program you wrote with MoveIt to plan around an object in the scene.\nHere is a copy of the full hello_moveit.cpp source.\nExamples of using the Planning Scene for collision and constraint checking.\nExamples of using the Planning Scene ROS API.\nExample of visualizing collision objects.\nExample of subframes used for planning with objects.\nIn the next tutorialPick and Place with MoveIt Task Constructor, you will be introduced to a higher layer tool designed to solve harder motion plans.\nIn this next tutorial, you will create a program to pick and place an object.\n© Copyright 2024, PickNik Robotics.\n#include<moveit/planning_scene_interface/planning_scene_interface.h>\n// Set a target Pose with updated values !!!autoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.y=0.8;msg.orientation.w=0.6;msg.position.x=0.1;msg.position.y=0.4;msg.position.z=0.4;returnmsg;}();move_group_interface.setPoseTarget(target_pose);\n// Create collision object for the robot to avoidautoconstcollision_object=[frame_id=move_group_interface.getPlanningFrame()]{moveit_msgs::msg::CollisionObjectcollision_object;collision_object.header.frame_id=frame_id;collision_object.id=\"box1\";shape_msgs::msg::SolidPrimitiveprimitive;// Define the size of the box in metersprimitive.type=primitive.BOX;primitive.dimensions.resize(3);primitive.dimensions[primitive.BOX_X]=0.5;primitive.dimensions[primitive.BOX_Y]=0.1;primitive.dimensions[primitive.BOX_Z]=0.5;// Define the pose of the box (relative to the frame_id)geometry_msgs::msg::Posebox_pose;box_pose.orientation.w=1.0;// We can leave out the x, y, and z components of the quaternion since they are initialized to 0box_pose.position.x=0.2;box_pose.position.y=0.2;box_pose.position.z=0.25;collision_object.primitives.push_back(primitive);collision_object.primitive_poses.push_back(box_pose);collision_object.operation=collision_object.ADD;returncollision_object;}();\n// Add the collision object to the scenemoveit::planning_interface::PlanningSceneInterfaceplanning_scene_interface;planning_scene_interface.applyCollisionObject(collision_object);\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=kinova_hello_moveit.rviz\nhello_moveit\nADD\nPlanningSceneInterface\nMoveGroup\ndemo.launch.py",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0cc"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/planning_around_objects/planning_around_objects.html#change-the-target-pose",
  "title": "Planning Around Objects — MoveIt Documentation: Rolling  documentation",
  "content": "Planning Around Objects\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to inserting objects into the planning scene and planning around them.\nIf you haven’t already done so, make sure you’ve completed the steps inVisualizing in RViz.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off. If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nAt the top of your source file, add this to the list of includes:\nFirst, update the target pose with the following change to make the robot plan to a different location:\nIn the next block of code, we create a collision object.\nThe first thing to notice is that it is being placed in the coordinate frame of the robot.\nIf we had a perception system that reported the location of obstacles in our scene, then this is the sort of message it would build.\nBecause this is just an example, we are creating it manually.\nOne thing to notice at the end of this block of code is that we set the operation on this message toADD.\nThis results in the object getting added to the collision scene.\nPlace this code block between setting the target pose from the previous step and creating a plan.\nFinally, we need to add this object to the collision scene.\nTo do this, we use an object called thePlanningSceneInterfacethat uses ROS interfaces to communicate changes to the planning scene toMoveGroup.\nThis code block should directly follow the code block that creates the collision object.\nJust as we did in the last tutorial, start RViz using thedemo.launch.pyscript and run our program. If you’re using one of the Docker tutorial containers, you can specify a different RViz configuration that already has the RvizVisualToolsGui panel added using:\nYou extended the program you wrote with MoveIt to plan around an object in the scene.\nHere is a copy of the full hello_moveit.cpp source.\nExamples of using the Planning Scene for collision and constraint checking.\nExamples of using the Planning Scene ROS API.\nExample of visualizing collision objects.\nExample of subframes used for planning with objects.\nIn the next tutorialPick and Place with MoveIt Task Constructor, you will be introduced to a higher layer tool designed to solve harder motion plans.\nIn this next tutorial, you will create a program to pick and place an object.\n© Copyright 2024, PickNik Robotics.\n#include<moveit/planning_scene_interface/planning_scene_interface.h>\n// Set a target Pose with updated values !!!autoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.y=0.8;msg.orientation.w=0.6;msg.position.x=0.1;msg.position.y=0.4;msg.position.z=0.4;returnmsg;}();move_group_interface.setPoseTarget(target_pose);\n// Create collision object for the robot to avoidautoconstcollision_object=[frame_id=move_group_interface.getPlanningFrame()]{moveit_msgs::msg::CollisionObjectcollision_object;collision_object.header.frame_id=frame_id;collision_object.id=\"box1\";shape_msgs::msg::SolidPrimitiveprimitive;// Define the size of the box in metersprimitive.type=primitive.BOX;primitive.dimensions.resize(3);primitive.dimensions[primitive.BOX_X]=0.5;primitive.dimensions[primitive.BOX_Y]=0.1;primitive.dimensions[primitive.BOX_Z]=0.5;// Define the pose of the box (relative to the frame_id)geometry_msgs::msg::Posebox_pose;box_pose.orientation.w=1.0;// We can leave out the x, y, and z components of the quaternion since they are initialized to 0box_pose.position.x=0.2;box_pose.position.y=0.2;box_pose.position.z=0.25;collision_object.primitives.push_back(primitive);collision_object.primitive_poses.push_back(box_pose);collision_object.operation=collision_object.ADD;returncollision_object;}();\n// Add the collision object to the scenemoveit::planning_interface::PlanningSceneInterfaceplanning_scene_interface;planning_scene_interface.applyCollisionObject(collision_object);\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=kinova_hello_moveit.rviz\nhello_moveit\nADD\nPlanningSceneInterface\nMoveGroup\ndemo.launch.py",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0cd"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/planning_around_objects/planning_around_objects.html#create-a-collision-object",
  "title": "Planning Around Objects — MoveIt Documentation: Rolling  documentation",
  "content": "Planning Around Objects\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to inserting objects into the planning scene and planning around them.\nIf you haven’t already done so, make sure you’ve completed the steps inVisualizing in RViz.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off. If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nAt the top of your source file, add this to the list of includes:\nFirst, update the target pose with the following change to make the robot plan to a different location:\nIn the next block of code, we create a collision object.\nThe first thing to notice is that it is being placed in the coordinate frame of the robot.\nIf we had a perception system that reported the location of obstacles in our scene, then this is the sort of message it would build.\nBecause this is just an example, we are creating it manually.\nOne thing to notice at the end of this block of code is that we set the operation on this message toADD.\nThis results in the object getting added to the collision scene.\nPlace this code block between setting the target pose from the previous step and creating a plan.\nFinally, we need to add this object to the collision scene.\nTo do this, we use an object called thePlanningSceneInterfacethat uses ROS interfaces to communicate changes to the planning scene toMoveGroup.\nThis code block should directly follow the code block that creates the collision object.\nJust as we did in the last tutorial, start RViz using thedemo.launch.pyscript and run our program. If you’re using one of the Docker tutorial containers, you can specify a different RViz configuration that already has the RvizVisualToolsGui panel added using:\nYou extended the program you wrote with MoveIt to plan around an object in the scene.\nHere is a copy of the full hello_moveit.cpp source.\nExamples of using the Planning Scene for collision and constraint checking.\nExamples of using the Planning Scene ROS API.\nExample of visualizing collision objects.\nExample of subframes used for planning with objects.\nIn the next tutorialPick and Place with MoveIt Task Constructor, you will be introduced to a higher layer tool designed to solve harder motion plans.\nIn this next tutorial, you will create a program to pick and place an object.\n© Copyright 2024, PickNik Robotics.\n#include<moveit/planning_scene_interface/planning_scene_interface.h>\n// Set a target Pose with updated values !!!autoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.y=0.8;msg.orientation.w=0.6;msg.position.x=0.1;msg.position.y=0.4;msg.position.z=0.4;returnmsg;}();move_group_interface.setPoseTarget(target_pose);\n// Create collision object for the robot to avoidautoconstcollision_object=[frame_id=move_group_interface.getPlanningFrame()]{moveit_msgs::msg::CollisionObjectcollision_object;collision_object.header.frame_id=frame_id;collision_object.id=\"box1\";shape_msgs::msg::SolidPrimitiveprimitive;// Define the size of the box in metersprimitive.type=primitive.BOX;primitive.dimensions.resize(3);primitive.dimensions[primitive.BOX_X]=0.5;primitive.dimensions[primitive.BOX_Y]=0.1;primitive.dimensions[primitive.BOX_Z]=0.5;// Define the pose of the box (relative to the frame_id)geometry_msgs::msg::Posebox_pose;box_pose.orientation.w=1.0;// We can leave out the x, y, and z components of the quaternion since they are initialized to 0box_pose.position.x=0.2;box_pose.position.y=0.2;box_pose.position.z=0.25;collision_object.primitives.push_back(primitive);collision_object.primitive_poses.push_back(box_pose);collision_object.operation=collision_object.ADD;returncollision_object;}();\n// Add the collision object to the scenemoveit::planning_interface::PlanningSceneInterfaceplanning_scene_interface;planning_scene_interface.applyCollisionObject(collision_object);\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=kinova_hello_moveit.rviz\nhello_moveit\nADD\nPlanningSceneInterface\nMoveGroup\ndemo.launch.py",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0ce"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/planning_around_objects/planning_around_objects.html#add-the-object-to-the-planning-scene",
  "title": "Planning Around Objects — MoveIt Documentation: Rolling  documentation",
  "content": "Planning Around Objects\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to inserting objects into the planning scene and planning around them.\nIf you haven’t already done so, make sure you’ve completed the steps inVisualizing in RViz.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off. If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nAt the top of your source file, add this to the list of includes:\nFirst, update the target pose with the following change to make the robot plan to a different location:\nIn the next block of code, we create a collision object.\nThe first thing to notice is that it is being placed in the coordinate frame of the robot.\nIf we had a perception system that reported the location of obstacles in our scene, then this is the sort of message it would build.\nBecause this is just an example, we are creating it manually.\nOne thing to notice at the end of this block of code is that we set the operation on this message toADD.\nThis results in the object getting added to the collision scene.\nPlace this code block between setting the target pose from the previous step and creating a plan.\nFinally, we need to add this object to the collision scene.\nTo do this, we use an object called thePlanningSceneInterfacethat uses ROS interfaces to communicate changes to the planning scene toMoveGroup.\nThis code block should directly follow the code block that creates the collision object.\nJust as we did in the last tutorial, start RViz using thedemo.launch.pyscript and run our program. If you’re using one of the Docker tutorial containers, you can specify a different RViz configuration that already has the RvizVisualToolsGui panel added using:\nYou extended the program you wrote with MoveIt to plan around an object in the scene.\nHere is a copy of the full hello_moveit.cpp source.\nExamples of using the Planning Scene for collision and constraint checking.\nExamples of using the Planning Scene ROS API.\nExample of visualizing collision objects.\nExample of subframes used for planning with objects.\nIn the next tutorialPick and Place with MoveIt Task Constructor, you will be introduced to a higher layer tool designed to solve harder motion plans.\nIn this next tutorial, you will create a program to pick and place an object.\n© Copyright 2024, PickNik Robotics.\n#include<moveit/planning_scene_interface/planning_scene_interface.h>\n// Set a target Pose with updated values !!!autoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.y=0.8;msg.orientation.w=0.6;msg.position.x=0.1;msg.position.y=0.4;msg.position.z=0.4;returnmsg;}();move_group_interface.setPoseTarget(target_pose);\n// Create collision object for the robot to avoidautoconstcollision_object=[frame_id=move_group_interface.getPlanningFrame()]{moveit_msgs::msg::CollisionObjectcollision_object;collision_object.header.frame_id=frame_id;collision_object.id=\"box1\";shape_msgs::msg::SolidPrimitiveprimitive;// Define the size of the box in metersprimitive.type=primitive.BOX;primitive.dimensions.resize(3);primitive.dimensions[primitive.BOX_X]=0.5;primitive.dimensions[primitive.BOX_Y]=0.1;primitive.dimensions[primitive.BOX_Z]=0.5;// Define the pose of the box (relative to the frame_id)geometry_msgs::msg::Posebox_pose;box_pose.orientation.w=1.0;// We can leave out the x, y, and z components of the quaternion since they are initialized to 0box_pose.position.x=0.2;box_pose.position.y=0.2;box_pose.position.z=0.25;collision_object.primitives.push_back(primitive);collision_object.primitive_poses.push_back(box_pose);collision_object.operation=collision_object.ADD;returncollision_object;}();\n// Add the collision object to the scenemoveit::planning_interface::PlanningSceneInterfaceplanning_scene_interface;planning_scene_interface.applyCollisionObject(collision_object);\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=kinova_hello_moveit.rviz\nhello_moveit\nADD\nPlanningSceneInterface\nMoveGroup\ndemo.launch.py",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0cf"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/planning_around_objects/planning_around_objects.html#run-the-program-and-observe-the-change",
  "title": "Planning Around Objects — MoveIt Documentation: Rolling  documentation",
  "content": "Planning Around Objects\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to inserting objects into the planning scene and planning around them.\nIf you haven’t already done so, make sure you’ve completed the steps inVisualizing in RViz.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off. If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nAt the top of your source file, add this to the list of includes:\nFirst, update the target pose with the following change to make the robot plan to a different location:\nIn the next block of code, we create a collision object.\nThe first thing to notice is that it is being placed in the coordinate frame of the robot.\nIf we had a perception system that reported the location of obstacles in our scene, then this is the sort of message it would build.\nBecause this is just an example, we are creating it manually.\nOne thing to notice at the end of this block of code is that we set the operation on this message toADD.\nThis results in the object getting added to the collision scene.\nPlace this code block between setting the target pose from the previous step and creating a plan.\nFinally, we need to add this object to the collision scene.\nTo do this, we use an object called thePlanningSceneInterfacethat uses ROS interfaces to communicate changes to the planning scene toMoveGroup.\nThis code block should directly follow the code block that creates the collision object.\nJust as we did in the last tutorial, start RViz using thedemo.launch.pyscript and run our program. If you’re using one of the Docker tutorial containers, you can specify a different RViz configuration that already has the RvizVisualToolsGui panel added using:\nYou extended the program you wrote with MoveIt to plan around an object in the scene.\nHere is a copy of the full hello_moveit.cpp source.\nExamples of using the Planning Scene for collision and constraint checking.\nExamples of using the Planning Scene ROS API.\nExample of visualizing collision objects.\nExample of subframes used for planning with objects.\nIn the next tutorialPick and Place with MoveIt Task Constructor, you will be introduced to a higher layer tool designed to solve harder motion plans.\nIn this next tutorial, you will create a program to pick and place an object.\n© Copyright 2024, PickNik Robotics.\n#include<moveit/planning_scene_interface/planning_scene_interface.h>\n// Set a target Pose with updated values !!!autoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.y=0.8;msg.orientation.w=0.6;msg.position.x=0.1;msg.position.y=0.4;msg.position.z=0.4;returnmsg;}();move_group_interface.setPoseTarget(target_pose);\n// Create collision object for the robot to avoidautoconstcollision_object=[frame_id=move_group_interface.getPlanningFrame()]{moveit_msgs::msg::CollisionObjectcollision_object;collision_object.header.frame_id=frame_id;collision_object.id=\"box1\";shape_msgs::msg::SolidPrimitiveprimitive;// Define the size of the box in metersprimitive.type=primitive.BOX;primitive.dimensions.resize(3);primitive.dimensions[primitive.BOX_X]=0.5;primitive.dimensions[primitive.BOX_Y]=0.1;primitive.dimensions[primitive.BOX_Z]=0.5;// Define the pose of the box (relative to the frame_id)geometry_msgs::msg::Posebox_pose;box_pose.orientation.w=1.0;// We can leave out the x, y, and z components of the quaternion since they are initialized to 0box_pose.position.x=0.2;box_pose.position.y=0.2;box_pose.position.z=0.25;collision_object.primitives.push_back(primitive);collision_object.primitive_poses.push_back(box_pose);collision_object.operation=collision_object.ADD;returncollision_object;}();\n// Add the collision object to the scenemoveit::planning_interface::PlanningSceneInterfaceplanning_scene_interface;planning_scene_interface.applyCollisionObject(collision_object);\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=kinova_hello_moveit.rviz\nhello_moveit\nADD\nPlanningSceneInterface\nMoveGroup\ndemo.launch.py",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0d0"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/planning_around_objects/planning_around_objects.html#summary",
  "title": "Planning Around Objects — MoveIt Documentation: Rolling  documentation",
  "content": "Planning Around Objects\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to inserting objects into the planning scene and planning around them.\nIf you haven’t already done so, make sure you’ve completed the steps inVisualizing in RViz.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off. If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nAt the top of your source file, add this to the list of includes:\nFirst, update the target pose with the following change to make the robot plan to a different location:\nIn the next block of code, we create a collision object.\nThe first thing to notice is that it is being placed in the coordinate frame of the robot.\nIf we had a perception system that reported the location of obstacles in our scene, then this is the sort of message it would build.\nBecause this is just an example, we are creating it manually.\nOne thing to notice at the end of this block of code is that we set the operation on this message toADD.\nThis results in the object getting added to the collision scene.\nPlace this code block between setting the target pose from the previous step and creating a plan.\nFinally, we need to add this object to the collision scene.\nTo do this, we use an object called thePlanningSceneInterfacethat uses ROS interfaces to communicate changes to the planning scene toMoveGroup.\nThis code block should directly follow the code block that creates the collision object.\nJust as we did in the last tutorial, start RViz using thedemo.launch.pyscript and run our program. If you’re using one of the Docker tutorial containers, you can specify a different RViz configuration that already has the RvizVisualToolsGui panel added using:\nYou extended the program you wrote with MoveIt to plan around an object in the scene.\nHere is a copy of the full hello_moveit.cpp source.\nExamples of using the Planning Scene for collision and constraint checking.\nExamples of using the Planning Scene ROS API.\nExample of visualizing collision objects.\nExample of subframes used for planning with objects.\nIn the next tutorialPick and Place with MoveIt Task Constructor, you will be introduced to a higher layer tool designed to solve harder motion plans.\nIn this next tutorial, you will create a program to pick and place an object.\n© Copyright 2024, PickNik Robotics.\n#include<moveit/planning_scene_interface/planning_scene_interface.h>\n// Set a target Pose with updated values !!!autoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.y=0.8;msg.orientation.w=0.6;msg.position.x=0.1;msg.position.y=0.4;msg.position.z=0.4;returnmsg;}();move_group_interface.setPoseTarget(target_pose);\n// Create collision object for the robot to avoidautoconstcollision_object=[frame_id=move_group_interface.getPlanningFrame()]{moveit_msgs::msg::CollisionObjectcollision_object;collision_object.header.frame_id=frame_id;collision_object.id=\"box1\";shape_msgs::msg::SolidPrimitiveprimitive;// Define the size of the box in metersprimitive.type=primitive.BOX;primitive.dimensions.resize(3);primitive.dimensions[primitive.BOX_X]=0.5;primitive.dimensions[primitive.BOX_Y]=0.1;primitive.dimensions[primitive.BOX_Z]=0.5;// Define the pose of the box (relative to the frame_id)geometry_msgs::msg::Posebox_pose;box_pose.orientation.w=1.0;// We can leave out the x, y, and z components of the quaternion since they are initialized to 0box_pose.position.x=0.2;box_pose.position.y=0.2;box_pose.position.z=0.25;collision_object.primitives.push_back(primitive);collision_object.primitive_poses.push_back(box_pose);collision_object.operation=collision_object.ADD;returncollision_object;}();\n// Add the collision object to the scenemoveit::planning_interface::PlanningSceneInterfaceplanning_scene_interface;planning_scene_interface.applyCollisionObject(collision_object);\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=kinova_hello_moveit.rviz\nhello_moveit\nADD\nPlanningSceneInterface\nMoveGroup\ndemo.launch.py",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0d1"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/planning_around_objects/planning_around_objects.html#further-reading",
  "title": "Planning Around Objects — MoveIt Documentation: Rolling  documentation",
  "content": "Planning Around Objects\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to inserting objects into the planning scene and planning around them.\nIf you haven’t already done so, make sure you’ve completed the steps inVisualizing in RViz.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off. If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nAt the top of your source file, add this to the list of includes:\nFirst, update the target pose with the following change to make the robot plan to a different location:\nIn the next block of code, we create a collision object.\nThe first thing to notice is that it is being placed in the coordinate frame of the robot.\nIf we had a perception system that reported the location of obstacles in our scene, then this is the sort of message it would build.\nBecause this is just an example, we are creating it manually.\nOne thing to notice at the end of this block of code is that we set the operation on this message toADD.\nThis results in the object getting added to the collision scene.\nPlace this code block between setting the target pose from the previous step and creating a plan.\nFinally, we need to add this object to the collision scene.\nTo do this, we use an object called thePlanningSceneInterfacethat uses ROS interfaces to communicate changes to the planning scene toMoveGroup.\nThis code block should directly follow the code block that creates the collision object.\nJust as we did in the last tutorial, start RViz using thedemo.launch.pyscript and run our program. If you’re using one of the Docker tutorial containers, you can specify a different RViz configuration that already has the RvizVisualToolsGui panel added using:\nYou extended the program you wrote with MoveIt to plan around an object in the scene.\nHere is a copy of the full hello_moveit.cpp source.\nExamples of using the Planning Scene for collision and constraint checking.\nExamples of using the Planning Scene ROS API.\nExample of visualizing collision objects.\nExample of subframes used for planning with objects.\nIn the next tutorialPick and Place with MoveIt Task Constructor, you will be introduced to a higher layer tool designed to solve harder motion plans.\nIn this next tutorial, you will create a program to pick and place an object.\n© Copyright 2024, PickNik Robotics.\n#include<moveit/planning_scene_interface/planning_scene_interface.h>\n// Set a target Pose with updated values !!!autoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.y=0.8;msg.orientation.w=0.6;msg.position.x=0.1;msg.position.y=0.4;msg.position.z=0.4;returnmsg;}();move_group_interface.setPoseTarget(target_pose);\n// Create collision object for the robot to avoidautoconstcollision_object=[frame_id=move_group_interface.getPlanningFrame()]{moveit_msgs::msg::CollisionObjectcollision_object;collision_object.header.frame_id=frame_id;collision_object.id=\"box1\";shape_msgs::msg::SolidPrimitiveprimitive;// Define the size of the box in metersprimitive.type=primitive.BOX;primitive.dimensions.resize(3);primitive.dimensions[primitive.BOX_X]=0.5;primitive.dimensions[primitive.BOX_Y]=0.1;primitive.dimensions[primitive.BOX_Z]=0.5;// Define the pose of the box (relative to the frame_id)geometry_msgs::msg::Posebox_pose;box_pose.orientation.w=1.0;// We can leave out the x, y, and z components of the quaternion since they are initialized to 0box_pose.position.x=0.2;box_pose.position.y=0.2;box_pose.position.z=0.25;collision_object.primitives.push_back(primitive);collision_object.primitive_poses.push_back(box_pose);collision_object.operation=collision_object.ADD;returncollision_object;}();\n// Add the collision object to the scenemoveit::planning_interface::PlanningSceneInterfaceplanning_scene_interface;planning_scene_interface.applyCollisionObject(collision_object);\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=kinova_hello_moveit.rviz\nhello_moveit\nADD\nPlanningSceneInterface\nMoveGroup\ndemo.launch.py",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0d2"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/planning_around_objects/planning_around_objects.html#next-step",
  "title": "Planning Around Objects — MoveIt Documentation: Rolling  documentation",
  "content": "Planning Around Objects\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to inserting objects into the planning scene and planning around them.\nIf you haven’t already done so, make sure you’ve completed the steps inVisualizing in RViz.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off. If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nAt the top of your source file, add this to the list of includes:\nFirst, update the target pose with the following change to make the robot plan to a different location:\nIn the next block of code, we create a collision object.\nThe first thing to notice is that it is being placed in the coordinate frame of the robot.\nIf we had a perception system that reported the location of obstacles in our scene, then this is the sort of message it would build.\nBecause this is just an example, we are creating it manually.\nOne thing to notice at the end of this block of code is that we set the operation on this message toADD.\nThis results in the object getting added to the collision scene.\nPlace this code block between setting the target pose from the previous step and creating a plan.\nFinally, we need to add this object to the collision scene.\nTo do this, we use an object called thePlanningSceneInterfacethat uses ROS interfaces to communicate changes to the planning scene toMoveGroup.\nThis code block should directly follow the code block that creates the collision object.\nJust as we did in the last tutorial, start RViz using thedemo.launch.pyscript and run our program. If you’re using one of the Docker tutorial containers, you can specify a different RViz configuration that already has the RvizVisualToolsGui panel added using:\nYou extended the program you wrote with MoveIt to plan around an object in the scene.\nHere is a copy of the full hello_moveit.cpp source.\nExamples of using the Planning Scene for collision and constraint checking.\nExamples of using the Planning Scene ROS API.\nExample of visualizing collision objects.\nExample of subframes used for planning with objects.\nIn the next tutorialPick and Place with MoveIt Task Constructor, you will be introduced to a higher layer tool designed to solve harder motion plans.\nIn this next tutorial, you will create a program to pick and place an object.\n© Copyright 2024, PickNik Robotics.\n#include<moveit/planning_scene_interface/planning_scene_interface.h>\n// Set a target Pose with updated values !!!autoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.y=0.8;msg.orientation.w=0.6;msg.position.x=0.1;msg.position.y=0.4;msg.position.z=0.4;returnmsg;}();move_group_interface.setPoseTarget(target_pose);\n// Create collision object for the robot to avoidautoconstcollision_object=[frame_id=move_group_interface.getPlanningFrame()]{moveit_msgs::msg::CollisionObjectcollision_object;collision_object.header.frame_id=frame_id;collision_object.id=\"box1\";shape_msgs::msg::SolidPrimitiveprimitive;// Define the size of the box in metersprimitive.type=primitive.BOX;primitive.dimensions.resize(3);primitive.dimensions[primitive.BOX_X]=0.5;primitive.dimensions[primitive.BOX_Y]=0.1;primitive.dimensions[primitive.BOX_Z]=0.5;// Define the pose of the box (relative to the frame_id)geometry_msgs::msg::Posebox_pose;box_pose.orientation.w=1.0;// We can leave out the x, y, and z components of the quaternion since they are initialized to 0box_pose.position.x=0.2;box_pose.position.y=0.2;box_pose.position.z=0.25;collision_object.primitives.push_back(primitive);collision_object.primitive_poses.push_back(box_pose);collision_object.operation=collision_object.ADD;returncollision_object;}();\n// Add the collision object to the scenemoveit::planning_interface::PlanningSceneInterfaceplanning_scene_interface;planning_scene_interface.applyCollisionObject(collision_object);\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=kinova_hello_moveit.rviz\nhello_moveit\nADD\nPlanningSceneInterface\nMoveGroup\ndemo.launch.py",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0d3"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/planning_around_objects/planning_around_objects.html#planning-around-objects",
  "title": "Planning Around Objects — MoveIt Documentation: Rolling  documentation",
  "content": "Planning Around Objects\nPrerequisites\nSteps\nSummary\nFurther Reading\nNext Step\nThis tutorial will introduce you to inserting objects into the planning scene and planning around them.\nIf you haven’t already done so, make sure you’ve completed the steps inVisualizing in RViz.\nThis project assumes you are starting with thehello_moveitproject, where the previous tutorial left off. If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nAt the top of your source file, add this to the list of includes:\nFirst, update the target pose with the following change to make the robot plan to a different location:\nIn the next block of code, we create a collision object.\nThe first thing to notice is that it is being placed in the coordinate frame of the robot.\nIf we had a perception system that reported the location of obstacles in our scene, then this is the sort of message it would build.\nBecause this is just an example, we are creating it manually.\nOne thing to notice at the end of this block of code is that we set the operation on this message toADD.\nThis results in the object getting added to the collision scene.\nPlace this code block between setting the target pose from the previous step and creating a plan.\nFinally, we need to add this object to the collision scene.\nTo do this, we use an object called thePlanningSceneInterfacethat uses ROS interfaces to communicate changes to the planning scene toMoveGroup.\nThis code block should directly follow the code block that creates the collision object.\nJust as we did in the last tutorial, start RViz using thedemo.launch.pyscript and run our program. If you’re using one of the Docker tutorial containers, you can specify a different RViz configuration that already has the RvizVisualToolsGui panel added using:\nYou extended the program you wrote with MoveIt to plan around an object in the scene.\nHere is a copy of the full hello_moveit.cpp source.\nExamples of using the Planning Scene for collision and constraint checking.\nExamples of using the Planning Scene ROS API.\nExample of visualizing collision objects.\nExample of subframes used for planning with objects.\nIn the next tutorialPick and Place with MoveIt Task Constructor, you will be introduced to a higher layer tool designed to solve harder motion plans.\nIn this next tutorial, you will create a program to pick and place an object.\n© Copyright 2024, PickNik Robotics.\n#include<moveit/planning_scene_interface/planning_scene_interface.h>\n// Set a target Pose with updated values !!!autoconsttarget_pose=[]{geometry_msgs::msg::Posemsg;msg.orientation.y=0.8;msg.orientation.w=0.6;msg.position.x=0.1;msg.position.y=0.4;msg.position.z=0.4;returnmsg;}();move_group_interface.setPoseTarget(target_pose);\n// Create collision object for the robot to avoidautoconstcollision_object=[frame_id=move_group_interface.getPlanningFrame()]{moveit_msgs::msg::CollisionObjectcollision_object;collision_object.header.frame_id=frame_id;collision_object.id=\"box1\";shape_msgs::msg::SolidPrimitiveprimitive;// Define the size of the box in metersprimitive.type=primitive.BOX;primitive.dimensions.resize(3);primitive.dimensions[primitive.BOX_X]=0.5;primitive.dimensions[primitive.BOX_Y]=0.1;primitive.dimensions[primitive.BOX_Z]=0.5;// Define the pose of the box (relative to the frame_id)geometry_msgs::msg::Posebox_pose;box_pose.orientation.w=1.0;// We can leave out the x, y, and z components of the quaternion since they are initialized to 0box_pose.position.x=0.2;box_pose.position.y=0.2;box_pose.position.z=0.25;collision_object.primitives.push_back(primitive);collision_object.primitive_poses.push_back(box_pose);collision_object.operation=collision_object.ADD;returncollision_object;}();\n// Add the collision object to the scenemoveit::planning_interface::PlanningSceneInterfaceplanning_scene_interface;planning_scene_interface.applyCollisionObject(collision_object);\nros2launchmoveit2_tutorialsdemo.launch.pyrviz_config:=kinova_hello_moveit.rviz\nhello_moveit\nADD\nPlanningSceneInterface\nMoveGroup\ndemo.launch.py",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0d4"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/pick_and_place_with_moveit_task_constructor/pick_and_place_with_moveit_task_constructor.html",
  "title": "Pick and Place with MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "Pick and Place with MoveIt Task Constructor\n1 Basic Concepts\n2 Getting Started\n3 Trying It Out\n4 Setting up a Project with MoveIt Task Constructor\n5 Running the Demo\n6 Adding Stages\n7 Further Discussion\nThis tutorial will walk you through creating a package that plans a pick and place operation usingMoveIt Task Constructor. MoveIt Task Constructor provides a way to plan for tasks that consist of multiple different subtasks (known as stages). If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nThe fundamental idea of MTC is that complex motion planning problems can be composed into a set of simpler subproblems.\nThe top-level planning problem is specified as aTaskwhile all subproblems are specified byStages.\nStages can be arranged in any arbitrary order and hierarchy only limited by the individual stages types.\nThe order in which stages can be arranged is restricted by the direction in which results are passed.\nThere are three possible stages relating to the result flow: generator, propagator, and connector stages:\nGeneratorscompute their results independently of their neighbor stages and pass them in both directions, backwards and forwards.\nAn example is an IK sampler for geometric poses where approaching and departing motions (neighbor stages) depend on the solution.\nPropagatorsreceive the result of one neighbor stage, solve a subproblem and then propagate their result to the neighbor on the opposite site.\nDepending on the implementation, propagating stages can pass solutions forward, backward or in both directions separately.\nAn example is a stage that computes a Cartesian path based on either a start or a goal state.\nConnectorsdo not propagate any results, but rather attempt to bridge the gap between the resulting states of both neighbors.\nAn example is the computation of a free-motion plan from one given state to another.\nAdditional to the order types, there are different hierarchy types allowing to encapsulate subordinate stages.\nStages without subordinate stages are calledprimitive stages, higher-level stages are calledcontainer stages.\nThere are three container types:\nWrappersencapsulate a single subordinate stage and modify or filter the results.\nFor example, a filter stage that only accepts solutions of its child stage that satisfy a certain constraint can be realized as a wrapper.\nAnother standard use of this type includes the IK wrapper stage, which generates inverse kinematics solutions based on planning scenes annotated with a pose target property.\nSerial Containershold a sequence of subordinate stages and only consider end-to-end solutions as results.\nAn example is a picking motion that consists of a sequence of coherent steps.\nParallel Containerscombine set of subordinate stages and can be used for passing the best of alternative results, running fallback solvers or for merging multiple independent solutions.\nExamples are running alternative planners for a free-motion plan, picking objects with the right hand or with the left hand as a fallback, or moving the arm and opening the gripper at the same time.\nStages not only support solving motion planning problems.\nThey can also be used for all kinds of state transitions, as for instance modifying the planning scene.\nCombined with the possibility of using class inheritance it is possible to construct very complex behavior while only relying on a well-structured set of primitive stages.\nMore detailed information on MTC can be found in theMoveIt Task Constructor concepts page\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nMove into your colcon workspace and pull the MoveIt Task Constructor source, where<branch>can be e.g.humblefor ROS Humble, orros2for the latest version compatible with MoveIt 2main:\nInstall missing packages with rosdep:\nBuild the workspace:\nThe MoveIt Task Constructor package contains several basic examples and a pick-and-place demo.\nFor all demos you should launch the basic environment:\nSubsequently, you can run the individual demos:\nOn the right side you should see theMotion Planning Taskspanel outlining the hierarchical stage structure of the tasks.\nWhen you select a particular stage, the list of successful and failed solutions will be\nshown in the right-most window. Selecting one of those solutions will start its visualization.\nThis section walks through the steps required to build a simple task with MoveIt Task Constructor.\nCreate a new package with the following command:\nThis will create a new package and folder calledmtc_tutorialwith a dependency onmoveit_task_constructor_coreas well as a hello world example insrc/mtc_node.\nOpenmtc_node.cppin your editor of choice, and paste in the following code.\nThe top of the code includes the ROS and MoveIt Libraries that this package uses.\nrclcpp/rclcpp.hppincludes core ROS2 functionality\nmoveit/planning_scene/planning_scene.handmoveit/planning_scene_interface/planning_scene_interface.hinclude functionality to interface with the robot model and collision objects\nmoveit/task_constructor/task.h,moveit/task_constructor/solvers.h, andmoveit/task_constructor/stages.hinclude different components of MoveIt Task Constructor that are used in the example\ntf2_geometry_msgs/tf2_geometry_msgs.hppandtf2_eigen/tf2_eigen.hppwon’t be used in this initial example, but they will be used for pose generation when we add more stages to the MoveIt Task Constructor task.\nThe next line gets a logger for our new node. We also create a namespace alias formoveit::task_constructorfor convenience.\nWe start by defining a class that will contain the main MoveIt Task Constructor functionality.  We also declare the MoveIt Task Constructor task object as a member variable for our class: this isn’t strictly necessary for a given application, but it helps save the task for later visualization purposes. We will explore each function individually below.\nThese lines initialize the node with specified options (it is the constructor of ourMTCTaskNodeclass).\nThese next lines define a getter function to get the node base interface, which will be used for the executor later.\nThis class method is used to set up the planning scene that is used in the example. It creates a cylinder with dimensions specified byobject.primitives[0].dimensionsand position specified bypose.position.xandpose.position.y.\nYou can try changing these numbers to resize and move the cylinder around. If we move the cylinder out of the robot’s reach, planning will fail.\nThis function interfaces with the MoveIt Task Constructor task object. It first creates a task, which includes setting some properties and adding stages. This will be discussed further in thecreateTaskfunction definition. Next,task.init()initializes the task andtask.plan(5)generates a plan, stopping after 5 successful plans are found. The next line publishes the solution to be visualized in RViz - this line can be removed if you don’t care for visualization. Finally,task.execute()executes the plan. Execution occurs via an action server interface with the RViz plugin.\nAs mentioned above, this function creates a MoveIt Task Constructor object and sets some initial properties. In this case, we set the task name to “demo_task”, load the robot model, define the names of some useful frames, and set those frame names as properties of the task withtask.setProperty(property_name,value). The next few code blocks will fill out this function body.\nNow, we add an example stage to the node. The first line setscurrent_state_ptrtonullptr; this creates a pointer to a stage such that we can reuse stage information in specific scenarios. This line is not used at this moment, but will be used later when more stages are added to the task. Next, we make acurrent_statestage (a generator stage) and add it to our task - this starts the robot off in its current state. Now that we’ve created theCurrentStatestage, we save a pointer to it in thecurrent_state_ptrfor later use.\nSolvers are used to define the type of robot motion. MoveIt Task Constructor has three options for solvers:\nPipelinePlanneruses MoveIt’s planning pipeline, which typically defaults toOMPL.\nJointInterpolationis a simple planner that interpolates between the start and goal joint states. It is typically used for simple motions as it computes quickly but doesn’t support complex motions.\nCartesianPathis used to move the end effector in a straight line in Cartesian space.\nFeel free to try out the different solvers and see how the robot motion changes. For the first stage we will use the Cartesian planner, which requires the following properties to be set:\nNow that we added in the planners, we can add a stage that will move the robot.\nThe following lines use aMoveTostage (a propagator stage). Since opening the hand is a relatively simple movement, we can use the joint interpolation planner.\nThis stage plans a move to the “open hand” pose, which is a named pose defined in theSRDFfor the Panda robot.\nWe return the task and finish with thecreateTask()function.\nFinally, we havemain: the following lines create a node using the class defined above, and calls the class methods to set up and execute a basic MTC task. In this example, we do not cancel the executor once the task has finished executing to keep the node alive to inspect the solutions in RViz.\nWe will need a launch file to launch themove_group,ros2_control,static_tf,robot_state_publisher, andrviznodes that provide us the environment to run the demo. The one we will use for this example can be foundhere.\nTo run the MoveIt Task Constructor node, we will use a second launch file to start themtc_tutorialexecutable with the proper parameters. Here we can load URDF, SRDF, and OMPL parameters, or use MoveIt Configs Utils to do so. Your launch file should look something like the one found in this tutorial packagehere(pay close attention to thepackageandexecutablearguments below as they are different from the launch file linked) :\nSave a launch file aspick_place_demo.launch.pyor download one to the package’s launch directory. Make sure to edit theCMakeLists.txtso it includes the launch folder by adding the following lines:\nNow we can build and source the colcon workspace.\nStart by launching the first launch file. If you want to use the one provided by the tutorials:\nRViz will now load. If you’re using your own launch file and haven’t included an rviz configsuch as this, you will need to configure RViz before you see anything displayed. If you’re using the launch file from the tutorials package, RViz will already be configured for you and you can jump to the end of the next section.\nIf you are not using the RViz configuration provided, we’ll have to make some changes to the RViz configuration to see your robot and the MoveIt Task Constructor solutions. First, start RViz. The following steps will cover how to set up RViz for MoveIt Task Constructor solution visualization.\nIf theMotionPlanningdisplay is active, uncheck it to hide it for now.\nUnderGlobal Options, change theFixed Framefrommaptopanda_link0if not already done.\nOn the bottom left of the window, click theAddbutton.\nUndermoveit_task_constructor_visualizationselectMotion Planning Tasksand click OK. TheMotion Planning Tasksdisplay should appear on the bottom left.\nIn theDisplays, underMotion Planning Tasks,  changeTask Solution Topicto/solution\nYou should see the panda arm in the main view with Motion Planning Tasks display open in the bottom left and nothing in it. Your MTC task will show up in this panel once you launch themtc_tutorialnode. If you’re usingmtc_demo.launch.pyfrom the tutorials, jump back in here.\nLaunch themtc_tutorialnode with\nYou should see the arm execute the task with the single stage to open the hand, with the cylinder in green in front of it. It should look something like this:\nIf you haven’t made your own package, but still want to see what this looks like, you can launch this file from the tutorials:\nSo far, we’ve walked through creating and executing a simple task, which runs but does not do much. Now, we will start adding the pick-and-place stages to the task. The image below shows an outline of the stages we will use in our task.\nWe will start adding stages after our existing open hand stage. Openmtc_node.cppand locate the following lines:\nWe need to move the arm to a position where we can pick up our object. This is done with aConnectstage, which as its name implies, is a Connector stage. This means that it tries to bridge between the results of the stage before and after it. This stage is initialized with a name,move_to_pick, and aGroupPlannerVectorthat specifies the planning group and the planner. We then set a timeout for the stage, set the properties for the stage, and add it to our task.\nNext, we create a pointer to a MoveIt Task Constructor stage object, and set it tonullptrfor now. Later, we will use this to save a stage.\nThis next block of code creates aSerialContainer.\nThis is a container that can be added to our task and can hold several substages.\nIn this case, we create a serial container that will contain the stages relevant to the picking action.\nInstead of adding the stages to the task, we will add the relevant stages to the serial container. We useexposeTo()to declare the task properties from the parent task in the new serial container, and useconfigureInitFrom()to initialize them.\nThis allows the contained stages to access these properties.\nWe then create a stage to approach the object. This stage is aMoveRelativestage, which allows us to specify a relative movement from our current position.MoveRelativeis a propagator stage: it receives the solution from its neighbouring stages and propagates it to the next or previous stage. Usingcartesian_plannerfinds a solution that involves moving the end effector in a straight line. We set the properties, and set the minimum and maximum distance to move. Now we create aVector3Stampedmessage to indicate the direction we want to move - in this case, in the Z direction from the hand frame. Finally, we add this stage to our serial container\nNow, create a stage to generate the grasp pose.\nThis is a generator stage, so it computes its results without regard to the stages before and after it.\nThe first stage,CurrentStateis a generator stage as well - to connect the first stage and this stage, a connecting stage must be used, which we already created above.\nThis code sets the stage properties, sets the pose before grasping, the angle delta, and the monitored stage.\nAngle delta is a property of theGenerateGraspPosestage that is used to determine the number of poses to generate; when generating solutions, MoveIt Task Constructor will try to grasp the object from many different orientations, with the difference between the orientations specified by the angle delta. The smaller the delta, the closer together the grasp orientations will be. When defining the current stage, we setcurrent_state_ptr, which is now used to forward information about the object pose and shape to the inverse kinematics solver.\nThis stage won’t be directly added to the serial container like previously, as we still need to do inverse kinematics on the poses it generates.\nBefore we compute inverse kinematics for the poses generated above, we first need to define the frame. This can be done with aPoseStampedmessage fromgeometry_msgsor in this case, we define the transform using Eigen transformation matrix and the name of the relevant link. Here, we define the transformation matrix.\nNow, we create theComputeIKstage, and give it the namegenerateposeIKas well as thegenerategraspposestage defined above. Some robots have multiple inverse kinematics solutions for a given pose - we set the limit on the amount of solutions to solve for up to 8. We also set the minimum solution distance, which is a threshold on how different solutions must be: if the joint positions in a solution are too similar to a previous solution, it will be marked as invalid. Next, we configure some additional properties, and add theComputeIKstage to the serial container.\nTo pick up the object, we must allow collision between the hand and the object. This can be done with aModifyPlanningScenestage. TheallowCollisionsfunction lets us specify which collisions to disable.allowCollisionscan be used with a container of names, so we can usegetLinkModelNamesWithCollisionGeometryto get all the names of links with collision geometry in the hand group.\nWith collisions allowed, we now can close the hand. This is done with aMoveTostage, similarly to theopenhandstage from above, except moving to thecloseposition as defined in the SRDF.\nWe now use aModifyPlanningScenestage again, this time to attach the object to the hand usingattachObject. Similarly to what we did with thecurrent_state_ptr, we get a pointer to this stage for later use when generating the place pose for the object.\nNext, we lift the object with aMoveRelativestage, similarly to theapproach_objectstage.\nWith this, we have all the stages needed to pick the object. Now, we add the serial container (with all its substages) to the task. If you build the package as-is, you can see the robot plan to pick up the object.\nTo test out the newly created stage, build the code and execute:\nNow that the stages that define the pick are complete, we move on to defining the stages for placing the object. Picking up where we left off, we add aConnectstage to connect the two, as we will soon be using a generator stage to generate the pose for placing the object.\nWe also create a serial container for the place stages. This is done similarly to the pick serial container.\nThe next stages will be added to the serial container rather than the task.\nThis next stage generates the poses used to place the object and compute the inverse kinematics for those poses - it is somewhat similar to thegenerategraspposestage from the pick serial container.\nWe start by creating a stage to generate the poses and inheriting the task properties.\nWe specify the pose where we want to place the object with aPoseStampedmessage fromgeometry_msgs- in this case, we choosey=0.5in the\"object\"frame.\nWe then pass the target pose to the stage withsetPose.\nNext, we usesetMonitoredStageand pass it the pointer to theattach_objectstage from earlier.\nThis allows the stage to know how the object is attached.\nWe then create aComputeIKstage and pass it ourGeneratePlacePosestage - the rest follows the same logic as above with the pick stages.\nNow that we’re ready to place the object, we open the hand withMoveTostage and the joint interpolation planner.\nWe also can re-enable collisions with the object now that we no longer need to hold it.\nThis is done usingallowCollisionsalmost exactly the same way as disabling collisions, except setting the last argument tofalserather thantrue.\nNow, we can detach the object usingdetachObject.\nWe retreat from the object using aMoveRelativestage, which is done similarly to theapproachobjectandliftobjectstages.\nWe finish our place serial container and add it to the task.\nThe final step is to return home: we use aMoveTostage and pass it the goal pose ofready, which is a pose defined in the Panda SRDF.\nAll these stages should be added above these lines.\nCongratulations! You’ve now defined a pick and place task using MoveIt Task Constructor! To try it out, build the code and execute:\nThe task with each comprising stage is shown in the Motion Planning Tasks pane. Click on a stage and additional information about the stage will show up to the right. The right pane shows different solutions as well as their associated costs. Depending on the stage type and the robot configuration, there may only be one solution shown.\nClick one of the solution costs to see an animation of the robot following the plan for that stage. Click the “Exec” button in the upper-right portion of the pane to execute the motion.\nTo run the complete MoveIt Task Constructor example included with the MoveIt tutorials:\nAnd in a second terminal:\nWhen running MTC, it prints a diagram like this to terminal:\nThis example^ shows two stages. The first stage (“initial_state”) is aCurrentStatetype of stage, which initializes aPlanningSceneand captures any collision objects that are present at that moment.\nA pointer to this stage can be used to retrieve the state of the robot.\nSinceCurrentStateinherits fromGenerator, it propagates solutions both forward and backward.\nThis is denoted by the arrows in both directions.\nThe first1indicates that one solution was successfully propagated backwards to the previous stage.\nThe second1, between the arrows, indicates that one solution was generated.\nThe0indicates that a solution was not propagated forward successfully to the next stage, because the next stage failed.\nThe second stage (“move_to_home”) is aMoveTotype of stage. It inherits its propagation direction from the previous stage, so both arrows point forward. The0’s indicate that this stage failed completely. From left to right, the0’s mean:\nThe stage did not receive a solution from the previous stage\nThe stage did not generate a solution\nThe stage did not propagate a solution forward to the next stage\nIn this case, we could tell that “move_to_home” was the root cause of the failure. The problem was a home state that was in collision. Defining a new, collision-free home position fixed the issue.\nInformation about individual stages can be retrieved from the task. For example, here we retrieve the unique ID for a stage:\nACurrentStatetype stage does not just retrieve the current state of the robot.\nIt also initializes aPlanningSceneobject, capturing any collision objects that are present at that moment.\nMTC stages can be propagated in forward and backward order.\nYou can easily check which direction a stage propagates by the arrow in the RViz GUI.\nWhen propagating backwards, the logic of many operations is reversed.\nFor example, to allow collisions with an object in aModifyPlanningScenestage, you would callallowCollisions(false)rather thanallowCollisions(true). There is a discussion to be readhere.\n© Copyright 2024, PickNik Robotics.\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit_task_constructor.git\nrosdep install --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncd~/ws_moveitcolconbuild--mixinrelease\nros2launchmoveit_task_constructor_demodemo.launch.py\nros2launchmoveit_task_constructor_demorun.launch.pyexe:=cartesianros2launchmoveit_task_constructor_demorun.launch.pyexe:=modularros2launchmoveit_task_constructor_demorun.launch.pyexe:=pick_place_demo\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_task_constructor_corerclcpp\\--node-namemtc_nodemtc_tutorial\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endifstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;classMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};MTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}rclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}voidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;pose.orientation.w=1.0;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}voidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}mtc::TaskMTCTaskNode::createTask(){mtc::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);// Disable warnings for this line, as it's a variable that's set but not used in this example#pragma GCC diagnostic push#pragma GCC diagnostic ignored \"-Wunused-but-set-variable\"mtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generator#pragma GCC diagnostic popautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));autosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);autointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();autocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);autostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}intmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endif\nstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;\nclassMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};\nMTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}\nrclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}\nvoidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}\nvoidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}\nmtc::TaskMTCTaskNode::createTask(){moveit::task_constructor::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);\nmtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generatorautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));\nautosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);\nautointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}\nintmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\nfromlaunchimportLaunchDescriptionfromlaunch_ros.actionsimportNodefrommoveit_configs_utilsimportMoveItConfigsBuilderdefgenerate_launch_description():moveit_config=MoveItConfigsBuilder(\"moveit_resources_panda\").to_dict()# MTC Demo nodepick_place_demo=Node(package=\"mtc_tutorial\",executable=\"mtc_node\",output=\"screen\",parameters=[moveit_config,],)returnLaunchDescription([pick_place_demo])\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}\n  )\ncd~/ws_moveitcolconbuild--mixinreleasesource~/ws_moveit/install/setup.bash\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo_minimal.launch.py\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));// Add the next lines of codes to define more stages here\nautostage_move_to_pick=std::make_unique<mtc::stages::Connect>(\"move to pick\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner}});stage_move_to_pick->setTimeout(5.0);stage_move_to_pick->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_pick));\nmtc::Stage*attach_object_stage=nullptr;// Forward attach_object_stage to place pose generator\n{autograsp=std::make_unique<mtc::SerialContainer>(\"pick object\");task.properties().exposeTo(grasp->properties(),{\"eef\",\"group\",\"ik_frame\"});grasp->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"approach object\",cartesian_planner);stage->properties().set(\"marker_ns\",\"approach_object\");stage->properties().set(\"link\",hand_frame);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.15);// Set hand forward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=hand_frame;vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\n{// Sample grasp poseautostage=std::make_unique<mtc::stages::GenerateGraspPose>(\"generate grasp pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"grasp_pose\");stage->setPreGraspPose(\"open\");stage->setObject(\"object\");stage->setAngleDelta(M_PI/12);stage->setMonitoredStage(current_state_ptr);// Hook into current state\nEigen::Isometry3dgrasp_frame_transform;Eigen::Quaterniondq=Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitX())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitY())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitZ());grasp_frame_transform.linear()=q.matrix();grasp_frame_transform.translation().z()=0.1;\n// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"grasp pose IK\",std::move(stage));wrapper->setMaxIKSolutions(8);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(grasp_frame_transform,hand_frame);wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});grasp->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"allow collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),true);grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"close hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"close\");grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"attach object\");stage->attachObject(\"object\",hand_frame);attach_object_stage=stage.get();grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"lift object\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"lift_object\");// Set upward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\ntask.add(std::move(grasp));}\nros2launchmtc_tutorialpick_place_demo.launch.py\n{autostage_move_to_place=std::make_unique<mtc::stages::Connect>(\"move to place\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner},{hand_group_name,interpolation_planner}});stage_move_to_place->setTimeout(5.0);stage_move_to_place->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_place));}\n{autoplace=std::make_unique<mtc::SerialContainer>(\"place object\");task.properties().exposeTo(place->properties(),{\"eef\",\"group\",\"ik_frame\"});place->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{// Sample place poseautostage=std::make_unique<mtc::stages::GeneratePlacePose>(\"generate place pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"place_pose\");stage->setObject(\"object\");geometry_msgs::msg::PoseStampedtarget_pose_msg;target_pose_msg.header.frame_id=\"object\";target_pose_msg.pose.position.y=0.5;target_pose_msg.pose.orientation.w=1.0;stage->setPose(target_pose_msg);stage->setMonitoredStage(attach_object_stage);// Hook into attach_object_stage// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"place pose IK\",std::move(stage));wrapper->setMaxIKSolutions(2);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(\"object\");wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});place->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"open\");place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"forbid collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),false);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"detach object\");stage->detachObject(\"object\",hand_frame);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"retreat\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"retreat\");// Set retreat directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.x=-0.5;stage->setDirection(vec);place->insert(std::move(stage));}\ntask.add(std::move(place));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"return home\",interpolation_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setGoal(\"ready\");task.add(std::move(stage));}\n// Stages all added to the task above this linereturntask;}\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmoveit2_tutorialspick_place_demo.launch.py\n[demo_node-1]1-←1→-0/initial_state[demo_node-1]-0→0→-0/move_to_home\nuint32_tconstunique_stage_id=task_.stages()->findChild(stage_name)->introspectionId();\n<branch>\nhumble\nros2\nmain\nmtc_tutorial\nmoveit_task_constructor_core\nsrc/mtc_node\nmtc_node.cpp\nrclcpp/rclcpp.hpp\nmoveit/planning_scene/planning_scene.h\nmoveit/planning_scene_interface/planning_scene_interface.h\nmoveit/task_constructor/task.h\nmoveit/task_constructor/solvers.h\nmoveit/task_constructor/stages.h\ntf2_geometry_msgs/tf2_geometry_msgs.hpp\ntf2_eigen/tf2_eigen.hpp\nmoveit::task_constructor\nMTCTaskNode\nobject.primitives[0].dimensions\npose.position.x\npose.position.y\ncreateTask\ntask.init()\ntask.plan(5)\ntask.execute()\ntask.setProperty(property_name,value)\ncurrent_state_ptr\nnullptr\ncurrent_state\nCurrentState\ncurrent_state_ptr\nMoveTo\ncreateTask()\nmain\nmove_group\nros2_control\nstatic_tf\nrobot_state_publisher\nrviz\nmtc_tutorial\npackage\nexecutable\npick_place_demo.launch.py\nCMakeLists.txt\nmap\npanda_link0\nmoveit_task_constructor_visualization\n/solution\nmtc_tutorial\nmtc_demo.launch.py\nmtc_tutorial\nmtc_node.cpp\nConnect\nmove_to_pick\nGroupPlannerVector\nnullptr\nSerialContainer\nexposeTo()\nconfigureInitFrom()\nMoveRelative\nMoveRelative\ncartesian_planner\nVector3Stamped\nCurrentState\nGenerateGraspPose\ncurrent_state_ptr\nPoseStamped\ngeometry_msgs\nComputeIK\ngenerateposeIK\ngenerategrasppose\nComputeIK\nModifyPlanningScene\nallowCollisions\nallowCollisions\ngetLinkModelNamesWithCollisionGeometry\nMoveTo\nopenhand\nclose\nModifyPlanningScene\nattachObject\ncurrent_state_ptr\nMoveRelative\napproach_object\nConnect\ngenerategrasppose\nPoseStamped\ngeometry_msgs\ny=0.5\n\"object\"\nsetPose\nsetMonitoredStage\nattach_object\nComputeIK\nGeneratePlacePose\nMoveTo\nallowCollisions\nfalse\ntrue\ndetachObject\nMoveRelative\napproachobject\nliftobject\nMoveTo\nready\nCurrentState\nPlanningScene\nCurrentState\nGenerator\n1\n1\n0\nMoveTo\n0\n0\nCurrentState\nPlanningScene\nModifyPlanningScene\nallowCollisions(false)\nallowCollisions(true)",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0d5"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/pick_and_place_with_moveit_task_constructor/pick_and_place_with_moveit_task_constructor.html#basic-concepts",
  "title": "Pick and Place with MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "Pick and Place with MoveIt Task Constructor\n1 Basic Concepts\n2 Getting Started\n3 Trying It Out\n4 Setting up a Project with MoveIt Task Constructor\n5 Running the Demo\n6 Adding Stages\n7 Further Discussion\nThis tutorial will walk you through creating a package that plans a pick and place operation usingMoveIt Task Constructor. MoveIt Task Constructor provides a way to plan for tasks that consist of multiple different subtasks (known as stages). If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nThe fundamental idea of MTC is that complex motion planning problems can be composed into a set of simpler subproblems.\nThe top-level planning problem is specified as aTaskwhile all subproblems are specified byStages.\nStages can be arranged in any arbitrary order and hierarchy only limited by the individual stages types.\nThe order in which stages can be arranged is restricted by the direction in which results are passed.\nThere are three possible stages relating to the result flow: generator, propagator, and connector stages:\nGeneratorscompute their results independently of their neighbor stages and pass them in both directions, backwards and forwards.\nAn example is an IK sampler for geometric poses where approaching and departing motions (neighbor stages) depend on the solution.\nPropagatorsreceive the result of one neighbor stage, solve a subproblem and then propagate their result to the neighbor on the opposite site.\nDepending on the implementation, propagating stages can pass solutions forward, backward or in both directions separately.\nAn example is a stage that computes a Cartesian path based on either a start or a goal state.\nConnectorsdo not propagate any results, but rather attempt to bridge the gap between the resulting states of both neighbors.\nAn example is the computation of a free-motion plan from one given state to another.\nAdditional to the order types, there are different hierarchy types allowing to encapsulate subordinate stages.\nStages without subordinate stages are calledprimitive stages, higher-level stages are calledcontainer stages.\nThere are three container types:\nWrappersencapsulate a single subordinate stage and modify or filter the results.\nFor example, a filter stage that only accepts solutions of its child stage that satisfy a certain constraint can be realized as a wrapper.\nAnother standard use of this type includes the IK wrapper stage, which generates inverse kinematics solutions based on planning scenes annotated with a pose target property.\nSerial Containershold a sequence of subordinate stages and only consider end-to-end solutions as results.\nAn example is a picking motion that consists of a sequence of coherent steps.\nParallel Containerscombine set of subordinate stages and can be used for passing the best of alternative results, running fallback solvers or for merging multiple independent solutions.\nExamples are running alternative planners for a free-motion plan, picking objects with the right hand or with the left hand as a fallback, or moving the arm and opening the gripper at the same time.\nStages not only support solving motion planning problems.\nThey can also be used for all kinds of state transitions, as for instance modifying the planning scene.\nCombined with the possibility of using class inheritance it is possible to construct very complex behavior while only relying on a well-structured set of primitive stages.\nMore detailed information on MTC can be found in theMoveIt Task Constructor concepts page\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nMove into your colcon workspace and pull the MoveIt Task Constructor source, where<branch>can be e.g.humblefor ROS Humble, orros2for the latest version compatible with MoveIt 2main:\nInstall missing packages with rosdep:\nBuild the workspace:\nThe MoveIt Task Constructor package contains several basic examples and a pick-and-place demo.\nFor all demos you should launch the basic environment:\nSubsequently, you can run the individual demos:\nOn the right side you should see theMotion Planning Taskspanel outlining the hierarchical stage structure of the tasks.\nWhen you select a particular stage, the list of successful and failed solutions will be\nshown in the right-most window. Selecting one of those solutions will start its visualization.\nThis section walks through the steps required to build a simple task with MoveIt Task Constructor.\nCreate a new package with the following command:\nThis will create a new package and folder calledmtc_tutorialwith a dependency onmoveit_task_constructor_coreas well as a hello world example insrc/mtc_node.\nOpenmtc_node.cppin your editor of choice, and paste in the following code.\nThe top of the code includes the ROS and MoveIt Libraries that this package uses.\nrclcpp/rclcpp.hppincludes core ROS2 functionality\nmoveit/planning_scene/planning_scene.handmoveit/planning_scene_interface/planning_scene_interface.hinclude functionality to interface with the robot model and collision objects\nmoveit/task_constructor/task.h,moveit/task_constructor/solvers.h, andmoveit/task_constructor/stages.hinclude different components of MoveIt Task Constructor that are used in the example\ntf2_geometry_msgs/tf2_geometry_msgs.hppandtf2_eigen/tf2_eigen.hppwon’t be used in this initial example, but they will be used for pose generation when we add more stages to the MoveIt Task Constructor task.\nThe next line gets a logger for our new node. We also create a namespace alias formoveit::task_constructorfor convenience.\nWe start by defining a class that will contain the main MoveIt Task Constructor functionality.  We also declare the MoveIt Task Constructor task object as a member variable for our class: this isn’t strictly necessary for a given application, but it helps save the task for later visualization purposes. We will explore each function individually below.\nThese lines initialize the node with specified options (it is the constructor of ourMTCTaskNodeclass).\nThese next lines define a getter function to get the node base interface, which will be used for the executor later.\nThis class method is used to set up the planning scene that is used in the example. It creates a cylinder with dimensions specified byobject.primitives[0].dimensionsand position specified bypose.position.xandpose.position.y.\nYou can try changing these numbers to resize and move the cylinder around. If we move the cylinder out of the robot’s reach, planning will fail.\nThis function interfaces with the MoveIt Task Constructor task object. It first creates a task, which includes setting some properties and adding stages. This will be discussed further in thecreateTaskfunction definition. Next,task.init()initializes the task andtask.plan(5)generates a plan, stopping after 5 successful plans are found. The next line publishes the solution to be visualized in RViz - this line can be removed if you don’t care for visualization. Finally,task.execute()executes the plan. Execution occurs via an action server interface with the RViz plugin.\nAs mentioned above, this function creates a MoveIt Task Constructor object and sets some initial properties. In this case, we set the task name to “demo_task”, load the robot model, define the names of some useful frames, and set those frame names as properties of the task withtask.setProperty(property_name,value). The next few code blocks will fill out this function body.\nNow, we add an example stage to the node. The first line setscurrent_state_ptrtonullptr; this creates a pointer to a stage such that we can reuse stage information in specific scenarios. This line is not used at this moment, but will be used later when more stages are added to the task. Next, we make acurrent_statestage (a generator stage) and add it to our task - this starts the robot off in its current state. Now that we’ve created theCurrentStatestage, we save a pointer to it in thecurrent_state_ptrfor later use.\nSolvers are used to define the type of robot motion. MoveIt Task Constructor has three options for solvers:\nPipelinePlanneruses MoveIt’s planning pipeline, which typically defaults toOMPL.\nJointInterpolationis a simple planner that interpolates between the start and goal joint states. It is typically used for simple motions as it computes quickly but doesn’t support complex motions.\nCartesianPathis used to move the end effector in a straight line in Cartesian space.\nFeel free to try out the different solvers and see how the robot motion changes. For the first stage we will use the Cartesian planner, which requires the following properties to be set:\nNow that we added in the planners, we can add a stage that will move the robot.\nThe following lines use aMoveTostage (a propagator stage). Since opening the hand is a relatively simple movement, we can use the joint interpolation planner.\nThis stage plans a move to the “open hand” pose, which is a named pose defined in theSRDFfor the Panda robot.\nWe return the task and finish with thecreateTask()function.\nFinally, we havemain: the following lines create a node using the class defined above, and calls the class methods to set up and execute a basic MTC task. In this example, we do not cancel the executor once the task has finished executing to keep the node alive to inspect the solutions in RViz.\nWe will need a launch file to launch themove_group,ros2_control,static_tf,robot_state_publisher, andrviznodes that provide us the environment to run the demo. The one we will use for this example can be foundhere.\nTo run the MoveIt Task Constructor node, we will use a second launch file to start themtc_tutorialexecutable with the proper parameters. Here we can load URDF, SRDF, and OMPL parameters, or use MoveIt Configs Utils to do so. Your launch file should look something like the one found in this tutorial packagehere(pay close attention to thepackageandexecutablearguments below as they are different from the launch file linked) :\nSave a launch file aspick_place_demo.launch.pyor download one to the package’s launch directory. Make sure to edit theCMakeLists.txtso it includes the launch folder by adding the following lines:\nNow we can build and source the colcon workspace.\nStart by launching the first launch file. If you want to use the one provided by the tutorials:\nRViz will now load. If you’re using your own launch file and haven’t included an rviz configsuch as this, you will need to configure RViz before you see anything displayed. If you’re using the launch file from the tutorials package, RViz will already be configured for you and you can jump to the end of the next section.\nIf you are not using the RViz configuration provided, we’ll have to make some changes to the RViz configuration to see your robot and the MoveIt Task Constructor solutions. First, start RViz. The following steps will cover how to set up RViz for MoveIt Task Constructor solution visualization.\nIf theMotionPlanningdisplay is active, uncheck it to hide it for now.\nUnderGlobal Options, change theFixed Framefrommaptopanda_link0if not already done.\nOn the bottom left of the window, click theAddbutton.\nUndermoveit_task_constructor_visualizationselectMotion Planning Tasksand click OK. TheMotion Planning Tasksdisplay should appear on the bottom left.\nIn theDisplays, underMotion Planning Tasks,  changeTask Solution Topicto/solution\nYou should see the panda arm in the main view with Motion Planning Tasks display open in the bottom left and nothing in it. Your MTC task will show up in this panel once you launch themtc_tutorialnode. If you’re usingmtc_demo.launch.pyfrom the tutorials, jump back in here.\nLaunch themtc_tutorialnode with\nYou should see the arm execute the task with the single stage to open the hand, with the cylinder in green in front of it. It should look something like this:\nIf you haven’t made your own package, but still want to see what this looks like, you can launch this file from the tutorials:\nSo far, we’ve walked through creating and executing a simple task, which runs but does not do much. Now, we will start adding the pick-and-place stages to the task. The image below shows an outline of the stages we will use in our task.\nWe will start adding stages after our existing open hand stage. Openmtc_node.cppand locate the following lines:\nWe need to move the arm to a position where we can pick up our object. This is done with aConnectstage, which as its name implies, is a Connector stage. This means that it tries to bridge between the results of the stage before and after it. This stage is initialized with a name,move_to_pick, and aGroupPlannerVectorthat specifies the planning group and the planner. We then set a timeout for the stage, set the properties for the stage, and add it to our task.\nNext, we create a pointer to a MoveIt Task Constructor stage object, and set it tonullptrfor now. Later, we will use this to save a stage.\nThis next block of code creates aSerialContainer.\nThis is a container that can be added to our task and can hold several substages.\nIn this case, we create a serial container that will contain the stages relevant to the picking action.\nInstead of adding the stages to the task, we will add the relevant stages to the serial container. We useexposeTo()to declare the task properties from the parent task in the new serial container, and useconfigureInitFrom()to initialize them.\nThis allows the contained stages to access these properties.\nWe then create a stage to approach the object. This stage is aMoveRelativestage, which allows us to specify a relative movement from our current position.MoveRelativeis a propagator stage: it receives the solution from its neighbouring stages and propagates it to the next or previous stage. Usingcartesian_plannerfinds a solution that involves moving the end effector in a straight line. We set the properties, and set the minimum and maximum distance to move. Now we create aVector3Stampedmessage to indicate the direction we want to move - in this case, in the Z direction from the hand frame. Finally, we add this stage to our serial container\nNow, create a stage to generate the grasp pose.\nThis is a generator stage, so it computes its results without regard to the stages before and after it.\nThe first stage,CurrentStateis a generator stage as well - to connect the first stage and this stage, a connecting stage must be used, which we already created above.\nThis code sets the stage properties, sets the pose before grasping, the angle delta, and the monitored stage.\nAngle delta is a property of theGenerateGraspPosestage that is used to determine the number of poses to generate; when generating solutions, MoveIt Task Constructor will try to grasp the object from many different orientations, with the difference between the orientations specified by the angle delta. The smaller the delta, the closer together the grasp orientations will be. When defining the current stage, we setcurrent_state_ptr, which is now used to forward information about the object pose and shape to the inverse kinematics solver.\nThis stage won’t be directly added to the serial container like previously, as we still need to do inverse kinematics on the poses it generates.\nBefore we compute inverse kinematics for the poses generated above, we first need to define the frame. This can be done with aPoseStampedmessage fromgeometry_msgsor in this case, we define the transform using Eigen transformation matrix and the name of the relevant link. Here, we define the transformation matrix.\nNow, we create theComputeIKstage, and give it the namegenerateposeIKas well as thegenerategraspposestage defined above. Some robots have multiple inverse kinematics solutions for a given pose - we set the limit on the amount of solutions to solve for up to 8. We also set the minimum solution distance, which is a threshold on how different solutions must be: if the joint positions in a solution are too similar to a previous solution, it will be marked as invalid. Next, we configure some additional properties, and add theComputeIKstage to the serial container.\nTo pick up the object, we must allow collision between the hand and the object. This can be done with aModifyPlanningScenestage. TheallowCollisionsfunction lets us specify which collisions to disable.allowCollisionscan be used with a container of names, so we can usegetLinkModelNamesWithCollisionGeometryto get all the names of links with collision geometry in the hand group.\nWith collisions allowed, we now can close the hand. This is done with aMoveTostage, similarly to theopenhandstage from above, except moving to thecloseposition as defined in the SRDF.\nWe now use aModifyPlanningScenestage again, this time to attach the object to the hand usingattachObject. Similarly to what we did with thecurrent_state_ptr, we get a pointer to this stage for later use when generating the place pose for the object.\nNext, we lift the object with aMoveRelativestage, similarly to theapproach_objectstage.\nWith this, we have all the stages needed to pick the object. Now, we add the serial container (with all its substages) to the task. If you build the package as-is, you can see the robot plan to pick up the object.\nTo test out the newly created stage, build the code and execute:\nNow that the stages that define the pick are complete, we move on to defining the stages for placing the object. Picking up where we left off, we add aConnectstage to connect the two, as we will soon be using a generator stage to generate the pose for placing the object.\nWe also create a serial container for the place stages. This is done similarly to the pick serial container.\nThe next stages will be added to the serial container rather than the task.\nThis next stage generates the poses used to place the object and compute the inverse kinematics for those poses - it is somewhat similar to thegenerategraspposestage from the pick serial container.\nWe start by creating a stage to generate the poses and inheriting the task properties.\nWe specify the pose where we want to place the object with aPoseStampedmessage fromgeometry_msgs- in this case, we choosey=0.5in the\"object\"frame.\nWe then pass the target pose to the stage withsetPose.\nNext, we usesetMonitoredStageand pass it the pointer to theattach_objectstage from earlier.\nThis allows the stage to know how the object is attached.\nWe then create aComputeIKstage and pass it ourGeneratePlacePosestage - the rest follows the same logic as above with the pick stages.\nNow that we’re ready to place the object, we open the hand withMoveTostage and the joint interpolation planner.\nWe also can re-enable collisions with the object now that we no longer need to hold it.\nThis is done usingallowCollisionsalmost exactly the same way as disabling collisions, except setting the last argument tofalserather thantrue.\nNow, we can detach the object usingdetachObject.\nWe retreat from the object using aMoveRelativestage, which is done similarly to theapproachobjectandliftobjectstages.\nWe finish our place serial container and add it to the task.\nThe final step is to return home: we use aMoveTostage and pass it the goal pose ofready, which is a pose defined in the Panda SRDF.\nAll these stages should be added above these lines.\nCongratulations! You’ve now defined a pick and place task using MoveIt Task Constructor! To try it out, build the code and execute:\nThe task with each comprising stage is shown in the Motion Planning Tasks pane. Click on a stage and additional information about the stage will show up to the right. The right pane shows different solutions as well as their associated costs. Depending on the stage type and the robot configuration, there may only be one solution shown.\nClick one of the solution costs to see an animation of the robot following the plan for that stage. Click the “Exec” button in the upper-right portion of the pane to execute the motion.\nTo run the complete MoveIt Task Constructor example included with the MoveIt tutorials:\nAnd in a second terminal:\nWhen running MTC, it prints a diagram like this to terminal:\nThis example^ shows two stages. The first stage (“initial_state”) is aCurrentStatetype of stage, which initializes aPlanningSceneand captures any collision objects that are present at that moment.\nA pointer to this stage can be used to retrieve the state of the robot.\nSinceCurrentStateinherits fromGenerator, it propagates solutions both forward and backward.\nThis is denoted by the arrows in both directions.\nThe first1indicates that one solution was successfully propagated backwards to the previous stage.\nThe second1, between the arrows, indicates that one solution was generated.\nThe0indicates that a solution was not propagated forward successfully to the next stage, because the next stage failed.\nThe second stage (“move_to_home”) is aMoveTotype of stage. It inherits its propagation direction from the previous stage, so both arrows point forward. The0’s indicate that this stage failed completely. From left to right, the0’s mean:\nThe stage did not receive a solution from the previous stage\nThe stage did not generate a solution\nThe stage did not propagate a solution forward to the next stage\nIn this case, we could tell that “move_to_home” was the root cause of the failure. The problem was a home state that was in collision. Defining a new, collision-free home position fixed the issue.\nInformation about individual stages can be retrieved from the task. For example, here we retrieve the unique ID for a stage:\nACurrentStatetype stage does not just retrieve the current state of the robot.\nIt also initializes aPlanningSceneobject, capturing any collision objects that are present at that moment.\nMTC stages can be propagated in forward and backward order.\nYou can easily check which direction a stage propagates by the arrow in the RViz GUI.\nWhen propagating backwards, the logic of many operations is reversed.\nFor example, to allow collisions with an object in aModifyPlanningScenestage, you would callallowCollisions(false)rather thanallowCollisions(true). There is a discussion to be readhere.\n© Copyright 2024, PickNik Robotics.\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit_task_constructor.git\nrosdep install --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncd~/ws_moveitcolconbuild--mixinrelease\nros2launchmoveit_task_constructor_demodemo.launch.py\nros2launchmoveit_task_constructor_demorun.launch.pyexe:=cartesianros2launchmoveit_task_constructor_demorun.launch.pyexe:=modularros2launchmoveit_task_constructor_demorun.launch.pyexe:=pick_place_demo\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_task_constructor_corerclcpp\\--node-namemtc_nodemtc_tutorial\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endifstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;classMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};MTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}rclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}voidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;pose.orientation.w=1.0;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}voidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}mtc::TaskMTCTaskNode::createTask(){mtc::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);// Disable warnings for this line, as it's a variable that's set but not used in this example#pragma GCC diagnostic push#pragma GCC diagnostic ignored \"-Wunused-but-set-variable\"mtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generator#pragma GCC diagnostic popautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));autosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);autointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();autocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);autostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}intmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endif\nstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;\nclassMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};\nMTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}\nrclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}\nvoidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}\nvoidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}\nmtc::TaskMTCTaskNode::createTask(){moveit::task_constructor::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);\nmtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generatorautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));\nautosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);\nautointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}\nintmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\nfromlaunchimportLaunchDescriptionfromlaunch_ros.actionsimportNodefrommoveit_configs_utilsimportMoveItConfigsBuilderdefgenerate_launch_description():moveit_config=MoveItConfigsBuilder(\"moveit_resources_panda\").to_dict()# MTC Demo nodepick_place_demo=Node(package=\"mtc_tutorial\",executable=\"mtc_node\",output=\"screen\",parameters=[moveit_config,],)returnLaunchDescription([pick_place_demo])\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}\n  )\ncd~/ws_moveitcolconbuild--mixinreleasesource~/ws_moveit/install/setup.bash\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo_minimal.launch.py\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));// Add the next lines of codes to define more stages here\nautostage_move_to_pick=std::make_unique<mtc::stages::Connect>(\"move to pick\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner}});stage_move_to_pick->setTimeout(5.0);stage_move_to_pick->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_pick));\nmtc::Stage*attach_object_stage=nullptr;// Forward attach_object_stage to place pose generator\n{autograsp=std::make_unique<mtc::SerialContainer>(\"pick object\");task.properties().exposeTo(grasp->properties(),{\"eef\",\"group\",\"ik_frame\"});grasp->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"approach object\",cartesian_planner);stage->properties().set(\"marker_ns\",\"approach_object\");stage->properties().set(\"link\",hand_frame);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.15);// Set hand forward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=hand_frame;vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\n{// Sample grasp poseautostage=std::make_unique<mtc::stages::GenerateGraspPose>(\"generate grasp pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"grasp_pose\");stage->setPreGraspPose(\"open\");stage->setObject(\"object\");stage->setAngleDelta(M_PI/12);stage->setMonitoredStage(current_state_ptr);// Hook into current state\nEigen::Isometry3dgrasp_frame_transform;Eigen::Quaterniondq=Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitX())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitY())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitZ());grasp_frame_transform.linear()=q.matrix();grasp_frame_transform.translation().z()=0.1;\n// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"grasp pose IK\",std::move(stage));wrapper->setMaxIKSolutions(8);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(grasp_frame_transform,hand_frame);wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});grasp->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"allow collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),true);grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"close hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"close\");grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"attach object\");stage->attachObject(\"object\",hand_frame);attach_object_stage=stage.get();grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"lift object\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"lift_object\");// Set upward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\ntask.add(std::move(grasp));}\nros2launchmtc_tutorialpick_place_demo.launch.py\n{autostage_move_to_place=std::make_unique<mtc::stages::Connect>(\"move to place\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner},{hand_group_name,interpolation_planner}});stage_move_to_place->setTimeout(5.0);stage_move_to_place->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_place));}\n{autoplace=std::make_unique<mtc::SerialContainer>(\"place object\");task.properties().exposeTo(place->properties(),{\"eef\",\"group\",\"ik_frame\"});place->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{// Sample place poseautostage=std::make_unique<mtc::stages::GeneratePlacePose>(\"generate place pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"place_pose\");stage->setObject(\"object\");geometry_msgs::msg::PoseStampedtarget_pose_msg;target_pose_msg.header.frame_id=\"object\";target_pose_msg.pose.position.y=0.5;target_pose_msg.pose.orientation.w=1.0;stage->setPose(target_pose_msg);stage->setMonitoredStage(attach_object_stage);// Hook into attach_object_stage// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"place pose IK\",std::move(stage));wrapper->setMaxIKSolutions(2);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(\"object\");wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});place->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"open\");place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"forbid collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),false);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"detach object\");stage->detachObject(\"object\",hand_frame);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"retreat\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"retreat\");// Set retreat directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.x=-0.5;stage->setDirection(vec);place->insert(std::move(stage));}\ntask.add(std::move(place));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"return home\",interpolation_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setGoal(\"ready\");task.add(std::move(stage));}\n// Stages all added to the task above this linereturntask;}\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmoveit2_tutorialspick_place_demo.launch.py\n[demo_node-1]1-←1→-0/initial_state[demo_node-1]-0→0→-0/move_to_home\nuint32_tconstunique_stage_id=task_.stages()->findChild(stage_name)->introspectionId();\n<branch>\nhumble\nros2\nmain\nmtc_tutorial\nmoveit_task_constructor_core\nsrc/mtc_node\nmtc_node.cpp\nrclcpp/rclcpp.hpp\nmoveit/planning_scene/planning_scene.h\nmoveit/planning_scene_interface/planning_scene_interface.h\nmoveit/task_constructor/task.h\nmoveit/task_constructor/solvers.h\nmoveit/task_constructor/stages.h\ntf2_geometry_msgs/tf2_geometry_msgs.hpp\ntf2_eigen/tf2_eigen.hpp\nmoveit::task_constructor\nMTCTaskNode\nobject.primitives[0].dimensions\npose.position.x\npose.position.y\ncreateTask\ntask.init()\ntask.plan(5)\ntask.execute()\ntask.setProperty(property_name,value)\ncurrent_state_ptr\nnullptr\ncurrent_state\nCurrentState\ncurrent_state_ptr\nMoveTo\ncreateTask()\nmain\nmove_group\nros2_control\nstatic_tf\nrobot_state_publisher\nrviz\nmtc_tutorial\npackage\nexecutable\npick_place_demo.launch.py\nCMakeLists.txt\nmap\npanda_link0\nmoveit_task_constructor_visualization\n/solution\nmtc_tutorial\nmtc_demo.launch.py\nmtc_tutorial\nmtc_node.cpp\nConnect\nmove_to_pick\nGroupPlannerVector\nnullptr\nSerialContainer\nexposeTo()\nconfigureInitFrom()\nMoveRelative\nMoveRelative\ncartesian_planner\nVector3Stamped\nCurrentState\nGenerateGraspPose\ncurrent_state_ptr\nPoseStamped\ngeometry_msgs\nComputeIK\ngenerateposeIK\ngenerategrasppose\nComputeIK\nModifyPlanningScene\nallowCollisions\nallowCollisions\ngetLinkModelNamesWithCollisionGeometry\nMoveTo\nopenhand\nclose\nModifyPlanningScene\nattachObject\ncurrent_state_ptr\nMoveRelative\napproach_object\nConnect\ngenerategrasppose\nPoseStamped\ngeometry_msgs\ny=0.5\n\"object\"\nsetPose\nsetMonitoredStage\nattach_object\nComputeIK\nGeneratePlacePose\nMoveTo\nallowCollisions\nfalse\ntrue\ndetachObject\nMoveRelative\napproachobject\nliftobject\nMoveTo\nready\nCurrentState\nPlanningScene\nCurrentState\nGenerator\n1\n1\n0\nMoveTo\n0\n0\nCurrentState\nPlanningScene\nModifyPlanningScene\nallowCollisions(false)\nallowCollisions(true)",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0d6"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/pick_and_place_with_moveit_task_constructor/pick_and_place_with_moveit_task_constructor.html#getting-started",
  "title": "Pick and Place with MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "Pick and Place with MoveIt Task Constructor\n1 Basic Concepts\n2 Getting Started\n3 Trying It Out\n4 Setting up a Project with MoveIt Task Constructor\n5 Running the Demo\n6 Adding Stages\n7 Further Discussion\nThis tutorial will walk you through creating a package that plans a pick and place operation usingMoveIt Task Constructor. MoveIt Task Constructor provides a way to plan for tasks that consist of multiple different subtasks (known as stages). If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nThe fundamental idea of MTC is that complex motion planning problems can be composed into a set of simpler subproblems.\nThe top-level planning problem is specified as aTaskwhile all subproblems are specified byStages.\nStages can be arranged in any arbitrary order and hierarchy only limited by the individual stages types.\nThe order in which stages can be arranged is restricted by the direction in which results are passed.\nThere are three possible stages relating to the result flow: generator, propagator, and connector stages:\nGeneratorscompute their results independently of their neighbor stages and pass them in both directions, backwards and forwards.\nAn example is an IK sampler for geometric poses where approaching and departing motions (neighbor stages) depend on the solution.\nPropagatorsreceive the result of one neighbor stage, solve a subproblem and then propagate their result to the neighbor on the opposite site.\nDepending on the implementation, propagating stages can pass solutions forward, backward or in both directions separately.\nAn example is a stage that computes a Cartesian path based on either a start or a goal state.\nConnectorsdo not propagate any results, but rather attempt to bridge the gap between the resulting states of both neighbors.\nAn example is the computation of a free-motion plan from one given state to another.\nAdditional to the order types, there are different hierarchy types allowing to encapsulate subordinate stages.\nStages without subordinate stages are calledprimitive stages, higher-level stages are calledcontainer stages.\nThere are three container types:\nWrappersencapsulate a single subordinate stage and modify or filter the results.\nFor example, a filter stage that only accepts solutions of its child stage that satisfy a certain constraint can be realized as a wrapper.\nAnother standard use of this type includes the IK wrapper stage, which generates inverse kinematics solutions based on planning scenes annotated with a pose target property.\nSerial Containershold a sequence of subordinate stages and only consider end-to-end solutions as results.\nAn example is a picking motion that consists of a sequence of coherent steps.\nParallel Containerscombine set of subordinate stages and can be used for passing the best of alternative results, running fallback solvers or for merging multiple independent solutions.\nExamples are running alternative planners for a free-motion plan, picking objects with the right hand or with the left hand as a fallback, or moving the arm and opening the gripper at the same time.\nStages not only support solving motion planning problems.\nThey can also be used for all kinds of state transitions, as for instance modifying the planning scene.\nCombined with the possibility of using class inheritance it is possible to construct very complex behavior while only relying on a well-structured set of primitive stages.\nMore detailed information on MTC can be found in theMoveIt Task Constructor concepts page\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nMove into your colcon workspace and pull the MoveIt Task Constructor source, where<branch>can be e.g.humblefor ROS Humble, orros2for the latest version compatible with MoveIt 2main:\nInstall missing packages with rosdep:\nBuild the workspace:\nThe MoveIt Task Constructor package contains several basic examples and a pick-and-place demo.\nFor all demos you should launch the basic environment:\nSubsequently, you can run the individual demos:\nOn the right side you should see theMotion Planning Taskspanel outlining the hierarchical stage structure of the tasks.\nWhen you select a particular stage, the list of successful and failed solutions will be\nshown in the right-most window. Selecting one of those solutions will start its visualization.\nThis section walks through the steps required to build a simple task with MoveIt Task Constructor.\nCreate a new package with the following command:\nThis will create a new package and folder calledmtc_tutorialwith a dependency onmoveit_task_constructor_coreas well as a hello world example insrc/mtc_node.\nOpenmtc_node.cppin your editor of choice, and paste in the following code.\nThe top of the code includes the ROS and MoveIt Libraries that this package uses.\nrclcpp/rclcpp.hppincludes core ROS2 functionality\nmoveit/planning_scene/planning_scene.handmoveit/planning_scene_interface/planning_scene_interface.hinclude functionality to interface with the robot model and collision objects\nmoveit/task_constructor/task.h,moveit/task_constructor/solvers.h, andmoveit/task_constructor/stages.hinclude different components of MoveIt Task Constructor that are used in the example\ntf2_geometry_msgs/tf2_geometry_msgs.hppandtf2_eigen/tf2_eigen.hppwon’t be used in this initial example, but they will be used for pose generation when we add more stages to the MoveIt Task Constructor task.\nThe next line gets a logger for our new node. We also create a namespace alias formoveit::task_constructorfor convenience.\nWe start by defining a class that will contain the main MoveIt Task Constructor functionality.  We also declare the MoveIt Task Constructor task object as a member variable for our class: this isn’t strictly necessary for a given application, but it helps save the task for later visualization purposes. We will explore each function individually below.\nThese lines initialize the node with specified options (it is the constructor of ourMTCTaskNodeclass).\nThese next lines define a getter function to get the node base interface, which will be used for the executor later.\nThis class method is used to set up the planning scene that is used in the example. It creates a cylinder with dimensions specified byobject.primitives[0].dimensionsand position specified bypose.position.xandpose.position.y.\nYou can try changing these numbers to resize and move the cylinder around. If we move the cylinder out of the robot’s reach, planning will fail.\nThis function interfaces with the MoveIt Task Constructor task object. It first creates a task, which includes setting some properties and adding stages. This will be discussed further in thecreateTaskfunction definition. Next,task.init()initializes the task andtask.plan(5)generates a plan, stopping after 5 successful plans are found. The next line publishes the solution to be visualized in RViz - this line can be removed if you don’t care for visualization. Finally,task.execute()executes the plan. Execution occurs via an action server interface with the RViz plugin.\nAs mentioned above, this function creates a MoveIt Task Constructor object and sets some initial properties. In this case, we set the task name to “demo_task”, load the robot model, define the names of some useful frames, and set those frame names as properties of the task withtask.setProperty(property_name,value). The next few code blocks will fill out this function body.\nNow, we add an example stage to the node. The first line setscurrent_state_ptrtonullptr; this creates a pointer to a stage such that we can reuse stage information in specific scenarios. This line is not used at this moment, but will be used later when more stages are added to the task. Next, we make acurrent_statestage (a generator stage) and add it to our task - this starts the robot off in its current state. Now that we’ve created theCurrentStatestage, we save a pointer to it in thecurrent_state_ptrfor later use.\nSolvers are used to define the type of robot motion. MoveIt Task Constructor has three options for solvers:\nPipelinePlanneruses MoveIt’s planning pipeline, which typically defaults toOMPL.\nJointInterpolationis a simple planner that interpolates between the start and goal joint states. It is typically used for simple motions as it computes quickly but doesn’t support complex motions.\nCartesianPathis used to move the end effector in a straight line in Cartesian space.\nFeel free to try out the different solvers and see how the robot motion changes. For the first stage we will use the Cartesian planner, which requires the following properties to be set:\nNow that we added in the planners, we can add a stage that will move the robot.\nThe following lines use aMoveTostage (a propagator stage). Since opening the hand is a relatively simple movement, we can use the joint interpolation planner.\nThis stage plans a move to the “open hand” pose, which is a named pose defined in theSRDFfor the Panda robot.\nWe return the task and finish with thecreateTask()function.\nFinally, we havemain: the following lines create a node using the class defined above, and calls the class methods to set up and execute a basic MTC task. In this example, we do not cancel the executor once the task has finished executing to keep the node alive to inspect the solutions in RViz.\nWe will need a launch file to launch themove_group,ros2_control,static_tf,robot_state_publisher, andrviznodes that provide us the environment to run the demo. The one we will use for this example can be foundhere.\nTo run the MoveIt Task Constructor node, we will use a second launch file to start themtc_tutorialexecutable with the proper parameters. Here we can load URDF, SRDF, and OMPL parameters, or use MoveIt Configs Utils to do so. Your launch file should look something like the one found in this tutorial packagehere(pay close attention to thepackageandexecutablearguments below as they are different from the launch file linked) :\nSave a launch file aspick_place_demo.launch.pyor download one to the package’s launch directory. Make sure to edit theCMakeLists.txtso it includes the launch folder by adding the following lines:\nNow we can build and source the colcon workspace.\nStart by launching the first launch file. If you want to use the one provided by the tutorials:\nRViz will now load. If you’re using your own launch file and haven’t included an rviz configsuch as this, you will need to configure RViz before you see anything displayed. If you’re using the launch file from the tutorials package, RViz will already be configured for you and you can jump to the end of the next section.\nIf you are not using the RViz configuration provided, we’ll have to make some changes to the RViz configuration to see your robot and the MoveIt Task Constructor solutions. First, start RViz. The following steps will cover how to set up RViz for MoveIt Task Constructor solution visualization.\nIf theMotionPlanningdisplay is active, uncheck it to hide it for now.\nUnderGlobal Options, change theFixed Framefrommaptopanda_link0if not already done.\nOn the bottom left of the window, click theAddbutton.\nUndermoveit_task_constructor_visualizationselectMotion Planning Tasksand click OK. TheMotion Planning Tasksdisplay should appear on the bottom left.\nIn theDisplays, underMotion Planning Tasks,  changeTask Solution Topicto/solution\nYou should see the panda arm in the main view with Motion Planning Tasks display open in the bottom left and nothing in it. Your MTC task will show up in this panel once you launch themtc_tutorialnode. If you’re usingmtc_demo.launch.pyfrom the tutorials, jump back in here.\nLaunch themtc_tutorialnode with\nYou should see the arm execute the task with the single stage to open the hand, with the cylinder in green in front of it. It should look something like this:\nIf you haven’t made your own package, but still want to see what this looks like, you can launch this file from the tutorials:\nSo far, we’ve walked through creating and executing a simple task, which runs but does not do much. Now, we will start adding the pick-and-place stages to the task. The image below shows an outline of the stages we will use in our task.\nWe will start adding stages after our existing open hand stage. Openmtc_node.cppand locate the following lines:\nWe need to move the arm to a position where we can pick up our object. This is done with aConnectstage, which as its name implies, is a Connector stage. This means that it tries to bridge between the results of the stage before and after it. This stage is initialized with a name,move_to_pick, and aGroupPlannerVectorthat specifies the planning group and the planner. We then set a timeout for the stage, set the properties for the stage, and add it to our task.\nNext, we create a pointer to a MoveIt Task Constructor stage object, and set it tonullptrfor now. Later, we will use this to save a stage.\nThis next block of code creates aSerialContainer.\nThis is a container that can be added to our task and can hold several substages.\nIn this case, we create a serial container that will contain the stages relevant to the picking action.\nInstead of adding the stages to the task, we will add the relevant stages to the serial container. We useexposeTo()to declare the task properties from the parent task in the new serial container, and useconfigureInitFrom()to initialize them.\nThis allows the contained stages to access these properties.\nWe then create a stage to approach the object. This stage is aMoveRelativestage, which allows us to specify a relative movement from our current position.MoveRelativeis a propagator stage: it receives the solution from its neighbouring stages and propagates it to the next or previous stage. Usingcartesian_plannerfinds a solution that involves moving the end effector in a straight line. We set the properties, and set the minimum and maximum distance to move. Now we create aVector3Stampedmessage to indicate the direction we want to move - in this case, in the Z direction from the hand frame. Finally, we add this stage to our serial container\nNow, create a stage to generate the grasp pose.\nThis is a generator stage, so it computes its results without regard to the stages before and after it.\nThe first stage,CurrentStateis a generator stage as well - to connect the first stage and this stage, a connecting stage must be used, which we already created above.\nThis code sets the stage properties, sets the pose before grasping, the angle delta, and the monitored stage.\nAngle delta is a property of theGenerateGraspPosestage that is used to determine the number of poses to generate; when generating solutions, MoveIt Task Constructor will try to grasp the object from many different orientations, with the difference between the orientations specified by the angle delta. The smaller the delta, the closer together the grasp orientations will be. When defining the current stage, we setcurrent_state_ptr, which is now used to forward information about the object pose and shape to the inverse kinematics solver.\nThis stage won’t be directly added to the serial container like previously, as we still need to do inverse kinematics on the poses it generates.\nBefore we compute inverse kinematics for the poses generated above, we first need to define the frame. This can be done with aPoseStampedmessage fromgeometry_msgsor in this case, we define the transform using Eigen transformation matrix and the name of the relevant link. Here, we define the transformation matrix.\nNow, we create theComputeIKstage, and give it the namegenerateposeIKas well as thegenerategraspposestage defined above. Some robots have multiple inverse kinematics solutions for a given pose - we set the limit on the amount of solutions to solve for up to 8. We also set the minimum solution distance, which is a threshold on how different solutions must be: if the joint positions in a solution are too similar to a previous solution, it will be marked as invalid. Next, we configure some additional properties, and add theComputeIKstage to the serial container.\nTo pick up the object, we must allow collision between the hand and the object. This can be done with aModifyPlanningScenestage. TheallowCollisionsfunction lets us specify which collisions to disable.allowCollisionscan be used with a container of names, so we can usegetLinkModelNamesWithCollisionGeometryto get all the names of links with collision geometry in the hand group.\nWith collisions allowed, we now can close the hand. This is done with aMoveTostage, similarly to theopenhandstage from above, except moving to thecloseposition as defined in the SRDF.\nWe now use aModifyPlanningScenestage again, this time to attach the object to the hand usingattachObject. Similarly to what we did with thecurrent_state_ptr, we get a pointer to this stage for later use when generating the place pose for the object.\nNext, we lift the object with aMoveRelativestage, similarly to theapproach_objectstage.\nWith this, we have all the stages needed to pick the object. Now, we add the serial container (with all its substages) to the task. If you build the package as-is, you can see the robot plan to pick up the object.\nTo test out the newly created stage, build the code and execute:\nNow that the stages that define the pick are complete, we move on to defining the stages for placing the object. Picking up where we left off, we add aConnectstage to connect the two, as we will soon be using a generator stage to generate the pose for placing the object.\nWe also create a serial container for the place stages. This is done similarly to the pick serial container.\nThe next stages will be added to the serial container rather than the task.\nThis next stage generates the poses used to place the object and compute the inverse kinematics for those poses - it is somewhat similar to thegenerategraspposestage from the pick serial container.\nWe start by creating a stage to generate the poses and inheriting the task properties.\nWe specify the pose where we want to place the object with aPoseStampedmessage fromgeometry_msgs- in this case, we choosey=0.5in the\"object\"frame.\nWe then pass the target pose to the stage withsetPose.\nNext, we usesetMonitoredStageand pass it the pointer to theattach_objectstage from earlier.\nThis allows the stage to know how the object is attached.\nWe then create aComputeIKstage and pass it ourGeneratePlacePosestage - the rest follows the same logic as above with the pick stages.\nNow that we’re ready to place the object, we open the hand withMoveTostage and the joint interpolation planner.\nWe also can re-enable collisions with the object now that we no longer need to hold it.\nThis is done usingallowCollisionsalmost exactly the same way as disabling collisions, except setting the last argument tofalserather thantrue.\nNow, we can detach the object usingdetachObject.\nWe retreat from the object using aMoveRelativestage, which is done similarly to theapproachobjectandliftobjectstages.\nWe finish our place serial container and add it to the task.\nThe final step is to return home: we use aMoveTostage and pass it the goal pose ofready, which is a pose defined in the Panda SRDF.\nAll these stages should be added above these lines.\nCongratulations! You’ve now defined a pick and place task using MoveIt Task Constructor! To try it out, build the code and execute:\nThe task with each comprising stage is shown in the Motion Planning Tasks pane. Click on a stage and additional information about the stage will show up to the right. The right pane shows different solutions as well as their associated costs. Depending on the stage type and the robot configuration, there may only be one solution shown.\nClick one of the solution costs to see an animation of the robot following the plan for that stage. Click the “Exec” button in the upper-right portion of the pane to execute the motion.\nTo run the complete MoveIt Task Constructor example included with the MoveIt tutorials:\nAnd in a second terminal:\nWhen running MTC, it prints a diagram like this to terminal:\nThis example^ shows two stages. The first stage (“initial_state”) is aCurrentStatetype of stage, which initializes aPlanningSceneand captures any collision objects that are present at that moment.\nA pointer to this stage can be used to retrieve the state of the robot.\nSinceCurrentStateinherits fromGenerator, it propagates solutions both forward and backward.\nThis is denoted by the arrows in both directions.\nThe first1indicates that one solution was successfully propagated backwards to the previous stage.\nThe second1, between the arrows, indicates that one solution was generated.\nThe0indicates that a solution was not propagated forward successfully to the next stage, because the next stage failed.\nThe second stage (“move_to_home”) is aMoveTotype of stage. It inherits its propagation direction from the previous stage, so both arrows point forward. The0’s indicate that this stage failed completely. From left to right, the0’s mean:\nThe stage did not receive a solution from the previous stage\nThe stage did not generate a solution\nThe stage did not propagate a solution forward to the next stage\nIn this case, we could tell that “move_to_home” was the root cause of the failure. The problem was a home state that was in collision. Defining a new, collision-free home position fixed the issue.\nInformation about individual stages can be retrieved from the task. For example, here we retrieve the unique ID for a stage:\nACurrentStatetype stage does not just retrieve the current state of the robot.\nIt also initializes aPlanningSceneobject, capturing any collision objects that are present at that moment.\nMTC stages can be propagated in forward and backward order.\nYou can easily check which direction a stage propagates by the arrow in the RViz GUI.\nWhen propagating backwards, the logic of many operations is reversed.\nFor example, to allow collisions with an object in aModifyPlanningScenestage, you would callallowCollisions(false)rather thanallowCollisions(true). There is a discussion to be readhere.\n© Copyright 2024, PickNik Robotics.\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit_task_constructor.git\nrosdep install --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncd~/ws_moveitcolconbuild--mixinrelease\nros2launchmoveit_task_constructor_demodemo.launch.py\nros2launchmoveit_task_constructor_demorun.launch.pyexe:=cartesianros2launchmoveit_task_constructor_demorun.launch.pyexe:=modularros2launchmoveit_task_constructor_demorun.launch.pyexe:=pick_place_demo\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_task_constructor_corerclcpp\\--node-namemtc_nodemtc_tutorial\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endifstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;classMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};MTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}rclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}voidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;pose.orientation.w=1.0;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}voidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}mtc::TaskMTCTaskNode::createTask(){mtc::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);// Disable warnings for this line, as it's a variable that's set but not used in this example#pragma GCC diagnostic push#pragma GCC diagnostic ignored \"-Wunused-but-set-variable\"mtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generator#pragma GCC diagnostic popautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));autosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);autointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();autocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);autostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}intmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endif\nstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;\nclassMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};\nMTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}\nrclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}\nvoidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}\nvoidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}\nmtc::TaskMTCTaskNode::createTask(){moveit::task_constructor::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);\nmtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generatorautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));\nautosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);\nautointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}\nintmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\nfromlaunchimportLaunchDescriptionfromlaunch_ros.actionsimportNodefrommoveit_configs_utilsimportMoveItConfigsBuilderdefgenerate_launch_description():moveit_config=MoveItConfigsBuilder(\"moveit_resources_panda\").to_dict()# MTC Demo nodepick_place_demo=Node(package=\"mtc_tutorial\",executable=\"mtc_node\",output=\"screen\",parameters=[moveit_config,],)returnLaunchDescription([pick_place_demo])\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}\n  )\ncd~/ws_moveitcolconbuild--mixinreleasesource~/ws_moveit/install/setup.bash\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo_minimal.launch.py\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));// Add the next lines of codes to define more stages here\nautostage_move_to_pick=std::make_unique<mtc::stages::Connect>(\"move to pick\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner}});stage_move_to_pick->setTimeout(5.0);stage_move_to_pick->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_pick));\nmtc::Stage*attach_object_stage=nullptr;// Forward attach_object_stage to place pose generator\n{autograsp=std::make_unique<mtc::SerialContainer>(\"pick object\");task.properties().exposeTo(grasp->properties(),{\"eef\",\"group\",\"ik_frame\"});grasp->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"approach object\",cartesian_planner);stage->properties().set(\"marker_ns\",\"approach_object\");stage->properties().set(\"link\",hand_frame);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.15);// Set hand forward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=hand_frame;vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\n{// Sample grasp poseautostage=std::make_unique<mtc::stages::GenerateGraspPose>(\"generate grasp pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"grasp_pose\");stage->setPreGraspPose(\"open\");stage->setObject(\"object\");stage->setAngleDelta(M_PI/12);stage->setMonitoredStage(current_state_ptr);// Hook into current state\nEigen::Isometry3dgrasp_frame_transform;Eigen::Quaterniondq=Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitX())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitY())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitZ());grasp_frame_transform.linear()=q.matrix();grasp_frame_transform.translation().z()=0.1;\n// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"grasp pose IK\",std::move(stage));wrapper->setMaxIKSolutions(8);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(grasp_frame_transform,hand_frame);wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});grasp->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"allow collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),true);grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"close hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"close\");grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"attach object\");stage->attachObject(\"object\",hand_frame);attach_object_stage=stage.get();grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"lift object\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"lift_object\");// Set upward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\ntask.add(std::move(grasp));}\nros2launchmtc_tutorialpick_place_demo.launch.py\n{autostage_move_to_place=std::make_unique<mtc::stages::Connect>(\"move to place\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner},{hand_group_name,interpolation_planner}});stage_move_to_place->setTimeout(5.0);stage_move_to_place->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_place));}\n{autoplace=std::make_unique<mtc::SerialContainer>(\"place object\");task.properties().exposeTo(place->properties(),{\"eef\",\"group\",\"ik_frame\"});place->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{// Sample place poseautostage=std::make_unique<mtc::stages::GeneratePlacePose>(\"generate place pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"place_pose\");stage->setObject(\"object\");geometry_msgs::msg::PoseStampedtarget_pose_msg;target_pose_msg.header.frame_id=\"object\";target_pose_msg.pose.position.y=0.5;target_pose_msg.pose.orientation.w=1.0;stage->setPose(target_pose_msg);stage->setMonitoredStage(attach_object_stage);// Hook into attach_object_stage// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"place pose IK\",std::move(stage));wrapper->setMaxIKSolutions(2);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(\"object\");wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});place->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"open\");place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"forbid collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),false);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"detach object\");stage->detachObject(\"object\",hand_frame);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"retreat\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"retreat\");// Set retreat directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.x=-0.5;stage->setDirection(vec);place->insert(std::move(stage));}\ntask.add(std::move(place));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"return home\",interpolation_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setGoal(\"ready\");task.add(std::move(stage));}\n// Stages all added to the task above this linereturntask;}\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmoveit2_tutorialspick_place_demo.launch.py\n[demo_node-1]1-←1→-0/initial_state[demo_node-1]-0→0→-0/move_to_home\nuint32_tconstunique_stage_id=task_.stages()->findChild(stage_name)->introspectionId();\n<branch>\nhumble\nros2\nmain\nmtc_tutorial\nmoveit_task_constructor_core\nsrc/mtc_node\nmtc_node.cpp\nrclcpp/rclcpp.hpp\nmoveit/planning_scene/planning_scene.h\nmoveit/planning_scene_interface/planning_scene_interface.h\nmoveit/task_constructor/task.h\nmoveit/task_constructor/solvers.h\nmoveit/task_constructor/stages.h\ntf2_geometry_msgs/tf2_geometry_msgs.hpp\ntf2_eigen/tf2_eigen.hpp\nmoveit::task_constructor\nMTCTaskNode\nobject.primitives[0].dimensions\npose.position.x\npose.position.y\ncreateTask\ntask.init()\ntask.plan(5)\ntask.execute()\ntask.setProperty(property_name,value)\ncurrent_state_ptr\nnullptr\ncurrent_state\nCurrentState\ncurrent_state_ptr\nMoveTo\ncreateTask()\nmain\nmove_group\nros2_control\nstatic_tf\nrobot_state_publisher\nrviz\nmtc_tutorial\npackage\nexecutable\npick_place_demo.launch.py\nCMakeLists.txt\nmap\npanda_link0\nmoveit_task_constructor_visualization\n/solution\nmtc_tutorial\nmtc_demo.launch.py\nmtc_tutorial\nmtc_node.cpp\nConnect\nmove_to_pick\nGroupPlannerVector\nnullptr\nSerialContainer\nexposeTo()\nconfigureInitFrom()\nMoveRelative\nMoveRelative\ncartesian_planner\nVector3Stamped\nCurrentState\nGenerateGraspPose\ncurrent_state_ptr\nPoseStamped\ngeometry_msgs\nComputeIK\ngenerateposeIK\ngenerategrasppose\nComputeIK\nModifyPlanningScene\nallowCollisions\nallowCollisions\ngetLinkModelNamesWithCollisionGeometry\nMoveTo\nopenhand\nclose\nModifyPlanningScene\nattachObject\ncurrent_state_ptr\nMoveRelative\napproach_object\nConnect\ngenerategrasppose\nPoseStamped\ngeometry_msgs\ny=0.5\n\"object\"\nsetPose\nsetMonitoredStage\nattach_object\nComputeIK\nGeneratePlacePose\nMoveTo\nallowCollisions\nfalse\ntrue\ndetachObject\nMoveRelative\napproachobject\nliftobject\nMoveTo\nready\nCurrentState\nPlanningScene\nCurrentState\nGenerator\n1\n1\n0\nMoveTo\n0\n0\nCurrentState\nPlanningScene\nModifyPlanningScene\nallowCollisions(false)\nallowCollisions(true)",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0d7"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/pick_and_place_with_moveit_task_constructor/pick_and_place_with_moveit_task_constructor.html#trying-it-out",
  "title": "Pick and Place with MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "Pick and Place with MoveIt Task Constructor\n1 Basic Concepts\n2 Getting Started\n3 Trying It Out\n4 Setting up a Project with MoveIt Task Constructor\n5 Running the Demo\n6 Adding Stages\n7 Further Discussion\nThis tutorial will walk you through creating a package that plans a pick and place operation usingMoveIt Task Constructor. MoveIt Task Constructor provides a way to plan for tasks that consist of multiple different subtasks (known as stages). If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nThe fundamental idea of MTC is that complex motion planning problems can be composed into a set of simpler subproblems.\nThe top-level planning problem is specified as aTaskwhile all subproblems are specified byStages.\nStages can be arranged in any arbitrary order and hierarchy only limited by the individual stages types.\nThe order in which stages can be arranged is restricted by the direction in which results are passed.\nThere are three possible stages relating to the result flow: generator, propagator, and connector stages:\nGeneratorscompute their results independently of their neighbor stages and pass them in both directions, backwards and forwards.\nAn example is an IK sampler for geometric poses where approaching and departing motions (neighbor stages) depend on the solution.\nPropagatorsreceive the result of one neighbor stage, solve a subproblem and then propagate their result to the neighbor on the opposite site.\nDepending on the implementation, propagating stages can pass solutions forward, backward or in both directions separately.\nAn example is a stage that computes a Cartesian path based on either a start or a goal state.\nConnectorsdo not propagate any results, but rather attempt to bridge the gap between the resulting states of both neighbors.\nAn example is the computation of a free-motion plan from one given state to another.\nAdditional to the order types, there are different hierarchy types allowing to encapsulate subordinate stages.\nStages without subordinate stages are calledprimitive stages, higher-level stages are calledcontainer stages.\nThere are three container types:\nWrappersencapsulate a single subordinate stage and modify or filter the results.\nFor example, a filter stage that only accepts solutions of its child stage that satisfy a certain constraint can be realized as a wrapper.\nAnother standard use of this type includes the IK wrapper stage, which generates inverse kinematics solutions based on planning scenes annotated with a pose target property.\nSerial Containershold a sequence of subordinate stages and only consider end-to-end solutions as results.\nAn example is a picking motion that consists of a sequence of coherent steps.\nParallel Containerscombine set of subordinate stages and can be used for passing the best of alternative results, running fallback solvers or for merging multiple independent solutions.\nExamples are running alternative planners for a free-motion plan, picking objects with the right hand or with the left hand as a fallback, or moving the arm and opening the gripper at the same time.\nStages not only support solving motion planning problems.\nThey can also be used for all kinds of state transitions, as for instance modifying the planning scene.\nCombined with the possibility of using class inheritance it is possible to construct very complex behavior while only relying on a well-structured set of primitive stages.\nMore detailed information on MTC can be found in theMoveIt Task Constructor concepts page\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nMove into your colcon workspace and pull the MoveIt Task Constructor source, where<branch>can be e.g.humblefor ROS Humble, orros2for the latest version compatible with MoveIt 2main:\nInstall missing packages with rosdep:\nBuild the workspace:\nThe MoveIt Task Constructor package contains several basic examples and a pick-and-place demo.\nFor all demos you should launch the basic environment:\nSubsequently, you can run the individual demos:\nOn the right side you should see theMotion Planning Taskspanel outlining the hierarchical stage structure of the tasks.\nWhen you select a particular stage, the list of successful and failed solutions will be\nshown in the right-most window. Selecting one of those solutions will start its visualization.\nThis section walks through the steps required to build a simple task with MoveIt Task Constructor.\nCreate a new package with the following command:\nThis will create a new package and folder calledmtc_tutorialwith a dependency onmoveit_task_constructor_coreas well as a hello world example insrc/mtc_node.\nOpenmtc_node.cppin your editor of choice, and paste in the following code.\nThe top of the code includes the ROS and MoveIt Libraries that this package uses.\nrclcpp/rclcpp.hppincludes core ROS2 functionality\nmoveit/planning_scene/planning_scene.handmoveit/planning_scene_interface/planning_scene_interface.hinclude functionality to interface with the robot model and collision objects\nmoveit/task_constructor/task.h,moveit/task_constructor/solvers.h, andmoveit/task_constructor/stages.hinclude different components of MoveIt Task Constructor that are used in the example\ntf2_geometry_msgs/tf2_geometry_msgs.hppandtf2_eigen/tf2_eigen.hppwon’t be used in this initial example, but they will be used for pose generation when we add more stages to the MoveIt Task Constructor task.\nThe next line gets a logger for our new node. We also create a namespace alias formoveit::task_constructorfor convenience.\nWe start by defining a class that will contain the main MoveIt Task Constructor functionality.  We also declare the MoveIt Task Constructor task object as a member variable for our class: this isn’t strictly necessary for a given application, but it helps save the task for later visualization purposes. We will explore each function individually below.\nThese lines initialize the node with specified options (it is the constructor of ourMTCTaskNodeclass).\nThese next lines define a getter function to get the node base interface, which will be used for the executor later.\nThis class method is used to set up the planning scene that is used in the example. It creates a cylinder with dimensions specified byobject.primitives[0].dimensionsand position specified bypose.position.xandpose.position.y.\nYou can try changing these numbers to resize and move the cylinder around. If we move the cylinder out of the robot’s reach, planning will fail.\nThis function interfaces with the MoveIt Task Constructor task object. It first creates a task, which includes setting some properties and adding stages. This will be discussed further in thecreateTaskfunction definition. Next,task.init()initializes the task andtask.plan(5)generates a plan, stopping after 5 successful plans are found. The next line publishes the solution to be visualized in RViz - this line can be removed if you don’t care for visualization. Finally,task.execute()executes the plan. Execution occurs via an action server interface with the RViz plugin.\nAs mentioned above, this function creates a MoveIt Task Constructor object and sets some initial properties. In this case, we set the task name to “demo_task”, load the robot model, define the names of some useful frames, and set those frame names as properties of the task withtask.setProperty(property_name,value). The next few code blocks will fill out this function body.\nNow, we add an example stage to the node. The first line setscurrent_state_ptrtonullptr; this creates a pointer to a stage such that we can reuse stage information in specific scenarios. This line is not used at this moment, but will be used later when more stages are added to the task. Next, we make acurrent_statestage (a generator stage) and add it to our task - this starts the robot off in its current state. Now that we’ve created theCurrentStatestage, we save a pointer to it in thecurrent_state_ptrfor later use.\nSolvers are used to define the type of robot motion. MoveIt Task Constructor has three options for solvers:\nPipelinePlanneruses MoveIt’s planning pipeline, which typically defaults toOMPL.\nJointInterpolationis a simple planner that interpolates between the start and goal joint states. It is typically used for simple motions as it computes quickly but doesn’t support complex motions.\nCartesianPathis used to move the end effector in a straight line in Cartesian space.\nFeel free to try out the different solvers and see how the robot motion changes. For the first stage we will use the Cartesian planner, which requires the following properties to be set:\nNow that we added in the planners, we can add a stage that will move the robot.\nThe following lines use aMoveTostage (a propagator stage). Since opening the hand is a relatively simple movement, we can use the joint interpolation planner.\nThis stage plans a move to the “open hand” pose, which is a named pose defined in theSRDFfor the Panda robot.\nWe return the task and finish with thecreateTask()function.\nFinally, we havemain: the following lines create a node using the class defined above, and calls the class methods to set up and execute a basic MTC task. In this example, we do not cancel the executor once the task has finished executing to keep the node alive to inspect the solutions in RViz.\nWe will need a launch file to launch themove_group,ros2_control,static_tf,robot_state_publisher, andrviznodes that provide us the environment to run the demo. The one we will use for this example can be foundhere.\nTo run the MoveIt Task Constructor node, we will use a second launch file to start themtc_tutorialexecutable with the proper parameters. Here we can load URDF, SRDF, and OMPL parameters, or use MoveIt Configs Utils to do so. Your launch file should look something like the one found in this tutorial packagehere(pay close attention to thepackageandexecutablearguments below as they are different from the launch file linked) :\nSave a launch file aspick_place_demo.launch.pyor download one to the package’s launch directory. Make sure to edit theCMakeLists.txtso it includes the launch folder by adding the following lines:\nNow we can build and source the colcon workspace.\nStart by launching the first launch file. If you want to use the one provided by the tutorials:\nRViz will now load. If you’re using your own launch file and haven’t included an rviz configsuch as this, you will need to configure RViz before you see anything displayed. If you’re using the launch file from the tutorials package, RViz will already be configured for you and you can jump to the end of the next section.\nIf you are not using the RViz configuration provided, we’ll have to make some changes to the RViz configuration to see your robot and the MoveIt Task Constructor solutions. First, start RViz. The following steps will cover how to set up RViz for MoveIt Task Constructor solution visualization.\nIf theMotionPlanningdisplay is active, uncheck it to hide it for now.\nUnderGlobal Options, change theFixed Framefrommaptopanda_link0if not already done.\nOn the bottom left of the window, click theAddbutton.\nUndermoveit_task_constructor_visualizationselectMotion Planning Tasksand click OK. TheMotion Planning Tasksdisplay should appear on the bottom left.\nIn theDisplays, underMotion Planning Tasks,  changeTask Solution Topicto/solution\nYou should see the panda arm in the main view with Motion Planning Tasks display open in the bottom left and nothing in it. Your MTC task will show up in this panel once you launch themtc_tutorialnode. If you’re usingmtc_demo.launch.pyfrom the tutorials, jump back in here.\nLaunch themtc_tutorialnode with\nYou should see the arm execute the task with the single stage to open the hand, with the cylinder in green in front of it. It should look something like this:\nIf you haven’t made your own package, but still want to see what this looks like, you can launch this file from the tutorials:\nSo far, we’ve walked through creating and executing a simple task, which runs but does not do much. Now, we will start adding the pick-and-place stages to the task. The image below shows an outline of the stages we will use in our task.\nWe will start adding stages after our existing open hand stage. Openmtc_node.cppand locate the following lines:\nWe need to move the arm to a position where we can pick up our object. This is done with aConnectstage, which as its name implies, is a Connector stage. This means that it tries to bridge between the results of the stage before and after it. This stage is initialized with a name,move_to_pick, and aGroupPlannerVectorthat specifies the planning group and the planner. We then set a timeout for the stage, set the properties for the stage, and add it to our task.\nNext, we create a pointer to a MoveIt Task Constructor stage object, and set it tonullptrfor now. Later, we will use this to save a stage.\nThis next block of code creates aSerialContainer.\nThis is a container that can be added to our task and can hold several substages.\nIn this case, we create a serial container that will contain the stages relevant to the picking action.\nInstead of adding the stages to the task, we will add the relevant stages to the serial container. We useexposeTo()to declare the task properties from the parent task in the new serial container, and useconfigureInitFrom()to initialize them.\nThis allows the contained stages to access these properties.\nWe then create a stage to approach the object. This stage is aMoveRelativestage, which allows us to specify a relative movement from our current position.MoveRelativeis a propagator stage: it receives the solution from its neighbouring stages and propagates it to the next or previous stage. Usingcartesian_plannerfinds a solution that involves moving the end effector in a straight line. We set the properties, and set the minimum and maximum distance to move. Now we create aVector3Stampedmessage to indicate the direction we want to move - in this case, in the Z direction from the hand frame. Finally, we add this stage to our serial container\nNow, create a stage to generate the grasp pose.\nThis is a generator stage, so it computes its results without regard to the stages before and after it.\nThe first stage,CurrentStateis a generator stage as well - to connect the first stage and this stage, a connecting stage must be used, which we already created above.\nThis code sets the stage properties, sets the pose before grasping, the angle delta, and the monitored stage.\nAngle delta is a property of theGenerateGraspPosestage that is used to determine the number of poses to generate; when generating solutions, MoveIt Task Constructor will try to grasp the object from many different orientations, with the difference between the orientations specified by the angle delta. The smaller the delta, the closer together the grasp orientations will be. When defining the current stage, we setcurrent_state_ptr, which is now used to forward information about the object pose and shape to the inverse kinematics solver.\nThis stage won’t be directly added to the serial container like previously, as we still need to do inverse kinematics on the poses it generates.\nBefore we compute inverse kinematics for the poses generated above, we first need to define the frame. This can be done with aPoseStampedmessage fromgeometry_msgsor in this case, we define the transform using Eigen transformation matrix and the name of the relevant link. Here, we define the transformation matrix.\nNow, we create theComputeIKstage, and give it the namegenerateposeIKas well as thegenerategraspposestage defined above. Some robots have multiple inverse kinematics solutions for a given pose - we set the limit on the amount of solutions to solve for up to 8. We also set the minimum solution distance, which is a threshold on how different solutions must be: if the joint positions in a solution are too similar to a previous solution, it will be marked as invalid. Next, we configure some additional properties, and add theComputeIKstage to the serial container.\nTo pick up the object, we must allow collision between the hand and the object. This can be done with aModifyPlanningScenestage. TheallowCollisionsfunction lets us specify which collisions to disable.allowCollisionscan be used with a container of names, so we can usegetLinkModelNamesWithCollisionGeometryto get all the names of links with collision geometry in the hand group.\nWith collisions allowed, we now can close the hand. This is done with aMoveTostage, similarly to theopenhandstage from above, except moving to thecloseposition as defined in the SRDF.\nWe now use aModifyPlanningScenestage again, this time to attach the object to the hand usingattachObject. Similarly to what we did with thecurrent_state_ptr, we get a pointer to this stage for later use when generating the place pose for the object.\nNext, we lift the object with aMoveRelativestage, similarly to theapproach_objectstage.\nWith this, we have all the stages needed to pick the object. Now, we add the serial container (with all its substages) to the task. If you build the package as-is, you can see the robot plan to pick up the object.\nTo test out the newly created stage, build the code and execute:\nNow that the stages that define the pick are complete, we move on to defining the stages for placing the object. Picking up where we left off, we add aConnectstage to connect the two, as we will soon be using a generator stage to generate the pose for placing the object.\nWe also create a serial container for the place stages. This is done similarly to the pick serial container.\nThe next stages will be added to the serial container rather than the task.\nThis next stage generates the poses used to place the object and compute the inverse kinematics for those poses - it is somewhat similar to thegenerategraspposestage from the pick serial container.\nWe start by creating a stage to generate the poses and inheriting the task properties.\nWe specify the pose where we want to place the object with aPoseStampedmessage fromgeometry_msgs- in this case, we choosey=0.5in the\"object\"frame.\nWe then pass the target pose to the stage withsetPose.\nNext, we usesetMonitoredStageand pass it the pointer to theattach_objectstage from earlier.\nThis allows the stage to know how the object is attached.\nWe then create aComputeIKstage and pass it ourGeneratePlacePosestage - the rest follows the same logic as above with the pick stages.\nNow that we’re ready to place the object, we open the hand withMoveTostage and the joint interpolation planner.\nWe also can re-enable collisions with the object now that we no longer need to hold it.\nThis is done usingallowCollisionsalmost exactly the same way as disabling collisions, except setting the last argument tofalserather thantrue.\nNow, we can detach the object usingdetachObject.\nWe retreat from the object using aMoveRelativestage, which is done similarly to theapproachobjectandliftobjectstages.\nWe finish our place serial container and add it to the task.\nThe final step is to return home: we use aMoveTostage and pass it the goal pose ofready, which is a pose defined in the Panda SRDF.\nAll these stages should be added above these lines.\nCongratulations! You’ve now defined a pick and place task using MoveIt Task Constructor! To try it out, build the code and execute:\nThe task with each comprising stage is shown in the Motion Planning Tasks pane. Click on a stage and additional information about the stage will show up to the right. The right pane shows different solutions as well as their associated costs. Depending on the stage type and the robot configuration, there may only be one solution shown.\nClick one of the solution costs to see an animation of the robot following the plan for that stage. Click the “Exec” button in the upper-right portion of the pane to execute the motion.\nTo run the complete MoveIt Task Constructor example included with the MoveIt tutorials:\nAnd in a second terminal:\nWhen running MTC, it prints a diagram like this to terminal:\nThis example^ shows two stages. The first stage (“initial_state”) is aCurrentStatetype of stage, which initializes aPlanningSceneand captures any collision objects that are present at that moment.\nA pointer to this stage can be used to retrieve the state of the robot.\nSinceCurrentStateinherits fromGenerator, it propagates solutions both forward and backward.\nThis is denoted by the arrows in both directions.\nThe first1indicates that one solution was successfully propagated backwards to the previous stage.\nThe second1, between the arrows, indicates that one solution was generated.\nThe0indicates that a solution was not propagated forward successfully to the next stage, because the next stage failed.\nThe second stage (“move_to_home”) is aMoveTotype of stage. It inherits its propagation direction from the previous stage, so both arrows point forward. The0’s indicate that this stage failed completely. From left to right, the0’s mean:\nThe stage did not receive a solution from the previous stage\nThe stage did not generate a solution\nThe stage did not propagate a solution forward to the next stage\nIn this case, we could tell that “move_to_home” was the root cause of the failure. The problem was a home state that was in collision. Defining a new, collision-free home position fixed the issue.\nInformation about individual stages can be retrieved from the task. For example, here we retrieve the unique ID for a stage:\nACurrentStatetype stage does not just retrieve the current state of the robot.\nIt also initializes aPlanningSceneobject, capturing any collision objects that are present at that moment.\nMTC stages can be propagated in forward and backward order.\nYou can easily check which direction a stage propagates by the arrow in the RViz GUI.\nWhen propagating backwards, the logic of many operations is reversed.\nFor example, to allow collisions with an object in aModifyPlanningScenestage, you would callallowCollisions(false)rather thanallowCollisions(true). There is a discussion to be readhere.\n© Copyright 2024, PickNik Robotics.\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit_task_constructor.git\nrosdep install --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncd~/ws_moveitcolconbuild--mixinrelease\nros2launchmoveit_task_constructor_demodemo.launch.py\nros2launchmoveit_task_constructor_demorun.launch.pyexe:=cartesianros2launchmoveit_task_constructor_demorun.launch.pyexe:=modularros2launchmoveit_task_constructor_demorun.launch.pyexe:=pick_place_demo\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_task_constructor_corerclcpp\\--node-namemtc_nodemtc_tutorial\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endifstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;classMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};MTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}rclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}voidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;pose.orientation.w=1.0;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}voidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}mtc::TaskMTCTaskNode::createTask(){mtc::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);// Disable warnings for this line, as it's a variable that's set but not used in this example#pragma GCC diagnostic push#pragma GCC diagnostic ignored \"-Wunused-but-set-variable\"mtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generator#pragma GCC diagnostic popautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));autosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);autointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();autocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);autostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}intmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endif\nstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;\nclassMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};\nMTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}\nrclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}\nvoidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}\nvoidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}\nmtc::TaskMTCTaskNode::createTask(){moveit::task_constructor::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);\nmtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generatorautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));\nautosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);\nautointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}\nintmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\nfromlaunchimportLaunchDescriptionfromlaunch_ros.actionsimportNodefrommoveit_configs_utilsimportMoveItConfigsBuilderdefgenerate_launch_description():moveit_config=MoveItConfigsBuilder(\"moveit_resources_panda\").to_dict()# MTC Demo nodepick_place_demo=Node(package=\"mtc_tutorial\",executable=\"mtc_node\",output=\"screen\",parameters=[moveit_config,],)returnLaunchDescription([pick_place_demo])\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}\n  )\ncd~/ws_moveitcolconbuild--mixinreleasesource~/ws_moveit/install/setup.bash\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo_minimal.launch.py\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));// Add the next lines of codes to define more stages here\nautostage_move_to_pick=std::make_unique<mtc::stages::Connect>(\"move to pick\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner}});stage_move_to_pick->setTimeout(5.0);stage_move_to_pick->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_pick));\nmtc::Stage*attach_object_stage=nullptr;// Forward attach_object_stage to place pose generator\n{autograsp=std::make_unique<mtc::SerialContainer>(\"pick object\");task.properties().exposeTo(grasp->properties(),{\"eef\",\"group\",\"ik_frame\"});grasp->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"approach object\",cartesian_planner);stage->properties().set(\"marker_ns\",\"approach_object\");stage->properties().set(\"link\",hand_frame);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.15);// Set hand forward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=hand_frame;vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\n{// Sample grasp poseautostage=std::make_unique<mtc::stages::GenerateGraspPose>(\"generate grasp pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"grasp_pose\");stage->setPreGraspPose(\"open\");stage->setObject(\"object\");stage->setAngleDelta(M_PI/12);stage->setMonitoredStage(current_state_ptr);// Hook into current state\nEigen::Isometry3dgrasp_frame_transform;Eigen::Quaterniondq=Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitX())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitY())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitZ());grasp_frame_transform.linear()=q.matrix();grasp_frame_transform.translation().z()=0.1;\n// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"grasp pose IK\",std::move(stage));wrapper->setMaxIKSolutions(8);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(grasp_frame_transform,hand_frame);wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});grasp->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"allow collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),true);grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"close hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"close\");grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"attach object\");stage->attachObject(\"object\",hand_frame);attach_object_stage=stage.get();grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"lift object\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"lift_object\");// Set upward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\ntask.add(std::move(grasp));}\nros2launchmtc_tutorialpick_place_demo.launch.py\n{autostage_move_to_place=std::make_unique<mtc::stages::Connect>(\"move to place\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner},{hand_group_name,interpolation_planner}});stage_move_to_place->setTimeout(5.0);stage_move_to_place->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_place));}\n{autoplace=std::make_unique<mtc::SerialContainer>(\"place object\");task.properties().exposeTo(place->properties(),{\"eef\",\"group\",\"ik_frame\"});place->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{// Sample place poseautostage=std::make_unique<mtc::stages::GeneratePlacePose>(\"generate place pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"place_pose\");stage->setObject(\"object\");geometry_msgs::msg::PoseStampedtarget_pose_msg;target_pose_msg.header.frame_id=\"object\";target_pose_msg.pose.position.y=0.5;target_pose_msg.pose.orientation.w=1.0;stage->setPose(target_pose_msg);stage->setMonitoredStage(attach_object_stage);// Hook into attach_object_stage// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"place pose IK\",std::move(stage));wrapper->setMaxIKSolutions(2);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(\"object\");wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});place->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"open\");place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"forbid collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),false);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"detach object\");stage->detachObject(\"object\",hand_frame);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"retreat\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"retreat\");// Set retreat directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.x=-0.5;stage->setDirection(vec);place->insert(std::move(stage));}\ntask.add(std::move(place));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"return home\",interpolation_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setGoal(\"ready\");task.add(std::move(stage));}\n// Stages all added to the task above this linereturntask;}\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmoveit2_tutorialspick_place_demo.launch.py\n[demo_node-1]1-←1→-0/initial_state[demo_node-1]-0→0→-0/move_to_home\nuint32_tconstunique_stage_id=task_.stages()->findChild(stage_name)->introspectionId();\n<branch>\nhumble\nros2\nmain\nmtc_tutorial\nmoveit_task_constructor_core\nsrc/mtc_node\nmtc_node.cpp\nrclcpp/rclcpp.hpp\nmoveit/planning_scene/planning_scene.h\nmoveit/planning_scene_interface/planning_scene_interface.h\nmoveit/task_constructor/task.h\nmoveit/task_constructor/solvers.h\nmoveit/task_constructor/stages.h\ntf2_geometry_msgs/tf2_geometry_msgs.hpp\ntf2_eigen/tf2_eigen.hpp\nmoveit::task_constructor\nMTCTaskNode\nobject.primitives[0].dimensions\npose.position.x\npose.position.y\ncreateTask\ntask.init()\ntask.plan(5)\ntask.execute()\ntask.setProperty(property_name,value)\ncurrent_state_ptr\nnullptr\ncurrent_state\nCurrentState\ncurrent_state_ptr\nMoveTo\ncreateTask()\nmain\nmove_group\nros2_control\nstatic_tf\nrobot_state_publisher\nrviz\nmtc_tutorial\npackage\nexecutable\npick_place_demo.launch.py\nCMakeLists.txt\nmap\npanda_link0\nmoveit_task_constructor_visualization\n/solution\nmtc_tutorial\nmtc_demo.launch.py\nmtc_tutorial\nmtc_node.cpp\nConnect\nmove_to_pick\nGroupPlannerVector\nnullptr\nSerialContainer\nexposeTo()\nconfigureInitFrom()\nMoveRelative\nMoveRelative\ncartesian_planner\nVector3Stamped\nCurrentState\nGenerateGraspPose\ncurrent_state_ptr\nPoseStamped\ngeometry_msgs\nComputeIK\ngenerateposeIK\ngenerategrasppose\nComputeIK\nModifyPlanningScene\nallowCollisions\nallowCollisions\ngetLinkModelNamesWithCollisionGeometry\nMoveTo\nopenhand\nclose\nModifyPlanningScene\nattachObject\ncurrent_state_ptr\nMoveRelative\napproach_object\nConnect\ngenerategrasppose\nPoseStamped\ngeometry_msgs\ny=0.5\n\"object\"\nsetPose\nsetMonitoredStage\nattach_object\nComputeIK\nGeneratePlacePose\nMoveTo\nallowCollisions\nfalse\ntrue\ndetachObject\nMoveRelative\napproachobject\nliftobject\nMoveTo\nready\nCurrentState\nPlanningScene\nCurrentState\nGenerator\n1\n1\n0\nMoveTo\n0\n0\nCurrentState\nPlanningScene\nModifyPlanningScene\nallowCollisions(false)\nallowCollisions(true)",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0d8"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/pick_and_place_with_moveit_task_constructor/pick_and_place_with_moveit_task_constructor.html#setting-up-a-project-with-moveit-task-constructor",
  "title": "Pick and Place with MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "Pick and Place with MoveIt Task Constructor\n1 Basic Concepts\n2 Getting Started\n3 Trying It Out\n4 Setting up a Project with MoveIt Task Constructor\n5 Running the Demo\n6 Adding Stages\n7 Further Discussion\nThis tutorial will walk you through creating a package that plans a pick and place operation usingMoveIt Task Constructor. MoveIt Task Constructor provides a way to plan for tasks that consist of multiple different subtasks (known as stages). If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nThe fundamental idea of MTC is that complex motion planning problems can be composed into a set of simpler subproblems.\nThe top-level planning problem is specified as aTaskwhile all subproblems are specified byStages.\nStages can be arranged in any arbitrary order and hierarchy only limited by the individual stages types.\nThe order in which stages can be arranged is restricted by the direction in which results are passed.\nThere are three possible stages relating to the result flow: generator, propagator, and connector stages:\nGeneratorscompute their results independently of their neighbor stages and pass them in both directions, backwards and forwards.\nAn example is an IK sampler for geometric poses where approaching and departing motions (neighbor stages) depend on the solution.\nPropagatorsreceive the result of one neighbor stage, solve a subproblem and then propagate their result to the neighbor on the opposite site.\nDepending on the implementation, propagating stages can pass solutions forward, backward or in both directions separately.\nAn example is a stage that computes a Cartesian path based on either a start or a goal state.\nConnectorsdo not propagate any results, but rather attempt to bridge the gap between the resulting states of both neighbors.\nAn example is the computation of a free-motion plan from one given state to another.\nAdditional to the order types, there are different hierarchy types allowing to encapsulate subordinate stages.\nStages without subordinate stages are calledprimitive stages, higher-level stages are calledcontainer stages.\nThere are three container types:\nWrappersencapsulate a single subordinate stage and modify or filter the results.\nFor example, a filter stage that only accepts solutions of its child stage that satisfy a certain constraint can be realized as a wrapper.\nAnother standard use of this type includes the IK wrapper stage, which generates inverse kinematics solutions based on planning scenes annotated with a pose target property.\nSerial Containershold a sequence of subordinate stages and only consider end-to-end solutions as results.\nAn example is a picking motion that consists of a sequence of coherent steps.\nParallel Containerscombine set of subordinate stages and can be used for passing the best of alternative results, running fallback solvers or for merging multiple independent solutions.\nExamples are running alternative planners for a free-motion plan, picking objects with the right hand or with the left hand as a fallback, or moving the arm and opening the gripper at the same time.\nStages not only support solving motion planning problems.\nThey can also be used for all kinds of state transitions, as for instance modifying the planning scene.\nCombined with the possibility of using class inheritance it is possible to construct very complex behavior while only relying on a well-structured set of primitive stages.\nMore detailed information on MTC can be found in theMoveIt Task Constructor concepts page\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nMove into your colcon workspace and pull the MoveIt Task Constructor source, where<branch>can be e.g.humblefor ROS Humble, orros2for the latest version compatible with MoveIt 2main:\nInstall missing packages with rosdep:\nBuild the workspace:\nThe MoveIt Task Constructor package contains several basic examples and a pick-and-place demo.\nFor all demos you should launch the basic environment:\nSubsequently, you can run the individual demos:\nOn the right side you should see theMotion Planning Taskspanel outlining the hierarchical stage structure of the tasks.\nWhen you select a particular stage, the list of successful and failed solutions will be\nshown in the right-most window. Selecting one of those solutions will start its visualization.\nThis section walks through the steps required to build a simple task with MoveIt Task Constructor.\nCreate a new package with the following command:\nThis will create a new package and folder calledmtc_tutorialwith a dependency onmoveit_task_constructor_coreas well as a hello world example insrc/mtc_node.\nOpenmtc_node.cppin your editor of choice, and paste in the following code.\nThe top of the code includes the ROS and MoveIt Libraries that this package uses.\nrclcpp/rclcpp.hppincludes core ROS2 functionality\nmoveit/planning_scene/planning_scene.handmoveit/planning_scene_interface/planning_scene_interface.hinclude functionality to interface with the robot model and collision objects\nmoveit/task_constructor/task.h,moveit/task_constructor/solvers.h, andmoveit/task_constructor/stages.hinclude different components of MoveIt Task Constructor that are used in the example\ntf2_geometry_msgs/tf2_geometry_msgs.hppandtf2_eigen/tf2_eigen.hppwon’t be used in this initial example, but they will be used for pose generation when we add more stages to the MoveIt Task Constructor task.\nThe next line gets a logger for our new node. We also create a namespace alias formoveit::task_constructorfor convenience.\nWe start by defining a class that will contain the main MoveIt Task Constructor functionality.  We also declare the MoveIt Task Constructor task object as a member variable for our class: this isn’t strictly necessary for a given application, but it helps save the task for later visualization purposes. We will explore each function individually below.\nThese lines initialize the node with specified options (it is the constructor of ourMTCTaskNodeclass).\nThese next lines define a getter function to get the node base interface, which will be used for the executor later.\nThis class method is used to set up the planning scene that is used in the example. It creates a cylinder with dimensions specified byobject.primitives[0].dimensionsand position specified bypose.position.xandpose.position.y.\nYou can try changing these numbers to resize and move the cylinder around. If we move the cylinder out of the robot’s reach, planning will fail.\nThis function interfaces with the MoveIt Task Constructor task object. It first creates a task, which includes setting some properties and adding stages. This will be discussed further in thecreateTaskfunction definition. Next,task.init()initializes the task andtask.plan(5)generates a plan, stopping after 5 successful plans are found. The next line publishes the solution to be visualized in RViz - this line can be removed if you don’t care for visualization. Finally,task.execute()executes the plan. Execution occurs via an action server interface with the RViz plugin.\nAs mentioned above, this function creates a MoveIt Task Constructor object and sets some initial properties. In this case, we set the task name to “demo_task”, load the robot model, define the names of some useful frames, and set those frame names as properties of the task withtask.setProperty(property_name,value). The next few code blocks will fill out this function body.\nNow, we add an example stage to the node. The first line setscurrent_state_ptrtonullptr; this creates a pointer to a stage such that we can reuse stage information in specific scenarios. This line is not used at this moment, but will be used later when more stages are added to the task. Next, we make acurrent_statestage (a generator stage) and add it to our task - this starts the robot off in its current state. Now that we’ve created theCurrentStatestage, we save a pointer to it in thecurrent_state_ptrfor later use.\nSolvers are used to define the type of robot motion. MoveIt Task Constructor has three options for solvers:\nPipelinePlanneruses MoveIt’s planning pipeline, which typically defaults toOMPL.\nJointInterpolationis a simple planner that interpolates between the start and goal joint states. It is typically used for simple motions as it computes quickly but doesn’t support complex motions.\nCartesianPathis used to move the end effector in a straight line in Cartesian space.\nFeel free to try out the different solvers and see how the robot motion changes. For the first stage we will use the Cartesian planner, which requires the following properties to be set:\nNow that we added in the planners, we can add a stage that will move the robot.\nThe following lines use aMoveTostage (a propagator stage). Since opening the hand is a relatively simple movement, we can use the joint interpolation planner.\nThis stage plans a move to the “open hand” pose, which is a named pose defined in theSRDFfor the Panda robot.\nWe return the task and finish with thecreateTask()function.\nFinally, we havemain: the following lines create a node using the class defined above, and calls the class methods to set up and execute a basic MTC task. In this example, we do not cancel the executor once the task has finished executing to keep the node alive to inspect the solutions in RViz.\nWe will need a launch file to launch themove_group,ros2_control,static_tf,robot_state_publisher, andrviznodes that provide us the environment to run the demo. The one we will use for this example can be foundhere.\nTo run the MoveIt Task Constructor node, we will use a second launch file to start themtc_tutorialexecutable with the proper parameters. Here we can load URDF, SRDF, and OMPL parameters, or use MoveIt Configs Utils to do so. Your launch file should look something like the one found in this tutorial packagehere(pay close attention to thepackageandexecutablearguments below as they are different from the launch file linked) :\nSave a launch file aspick_place_demo.launch.pyor download one to the package’s launch directory. Make sure to edit theCMakeLists.txtso it includes the launch folder by adding the following lines:\nNow we can build and source the colcon workspace.\nStart by launching the first launch file. If you want to use the one provided by the tutorials:\nRViz will now load. If you’re using your own launch file and haven’t included an rviz configsuch as this, you will need to configure RViz before you see anything displayed. If you’re using the launch file from the tutorials package, RViz will already be configured for you and you can jump to the end of the next section.\nIf you are not using the RViz configuration provided, we’ll have to make some changes to the RViz configuration to see your robot and the MoveIt Task Constructor solutions. First, start RViz. The following steps will cover how to set up RViz for MoveIt Task Constructor solution visualization.\nIf theMotionPlanningdisplay is active, uncheck it to hide it for now.\nUnderGlobal Options, change theFixed Framefrommaptopanda_link0if not already done.\nOn the bottom left of the window, click theAddbutton.\nUndermoveit_task_constructor_visualizationselectMotion Planning Tasksand click OK. TheMotion Planning Tasksdisplay should appear on the bottom left.\nIn theDisplays, underMotion Planning Tasks,  changeTask Solution Topicto/solution\nYou should see the panda arm in the main view with Motion Planning Tasks display open in the bottom left and nothing in it. Your MTC task will show up in this panel once you launch themtc_tutorialnode. If you’re usingmtc_demo.launch.pyfrom the tutorials, jump back in here.\nLaunch themtc_tutorialnode with\nYou should see the arm execute the task with the single stage to open the hand, with the cylinder in green in front of it. It should look something like this:\nIf you haven’t made your own package, but still want to see what this looks like, you can launch this file from the tutorials:\nSo far, we’ve walked through creating and executing a simple task, which runs but does not do much. Now, we will start adding the pick-and-place stages to the task. The image below shows an outline of the stages we will use in our task.\nWe will start adding stages after our existing open hand stage. Openmtc_node.cppand locate the following lines:\nWe need to move the arm to a position where we can pick up our object. This is done with aConnectstage, which as its name implies, is a Connector stage. This means that it tries to bridge between the results of the stage before and after it. This stage is initialized with a name,move_to_pick, and aGroupPlannerVectorthat specifies the planning group and the planner. We then set a timeout for the stage, set the properties for the stage, and add it to our task.\nNext, we create a pointer to a MoveIt Task Constructor stage object, and set it tonullptrfor now. Later, we will use this to save a stage.\nThis next block of code creates aSerialContainer.\nThis is a container that can be added to our task and can hold several substages.\nIn this case, we create a serial container that will contain the stages relevant to the picking action.\nInstead of adding the stages to the task, we will add the relevant stages to the serial container. We useexposeTo()to declare the task properties from the parent task in the new serial container, and useconfigureInitFrom()to initialize them.\nThis allows the contained stages to access these properties.\nWe then create a stage to approach the object. This stage is aMoveRelativestage, which allows us to specify a relative movement from our current position.MoveRelativeis a propagator stage: it receives the solution from its neighbouring stages and propagates it to the next or previous stage. Usingcartesian_plannerfinds a solution that involves moving the end effector in a straight line. We set the properties, and set the minimum and maximum distance to move. Now we create aVector3Stampedmessage to indicate the direction we want to move - in this case, in the Z direction from the hand frame. Finally, we add this stage to our serial container\nNow, create a stage to generate the grasp pose.\nThis is a generator stage, so it computes its results without regard to the stages before and after it.\nThe first stage,CurrentStateis a generator stage as well - to connect the first stage and this stage, a connecting stage must be used, which we already created above.\nThis code sets the stage properties, sets the pose before grasping, the angle delta, and the monitored stage.\nAngle delta is a property of theGenerateGraspPosestage that is used to determine the number of poses to generate; when generating solutions, MoveIt Task Constructor will try to grasp the object from many different orientations, with the difference between the orientations specified by the angle delta. The smaller the delta, the closer together the grasp orientations will be. When defining the current stage, we setcurrent_state_ptr, which is now used to forward information about the object pose and shape to the inverse kinematics solver.\nThis stage won’t be directly added to the serial container like previously, as we still need to do inverse kinematics on the poses it generates.\nBefore we compute inverse kinematics for the poses generated above, we first need to define the frame. This can be done with aPoseStampedmessage fromgeometry_msgsor in this case, we define the transform using Eigen transformation matrix and the name of the relevant link. Here, we define the transformation matrix.\nNow, we create theComputeIKstage, and give it the namegenerateposeIKas well as thegenerategraspposestage defined above. Some robots have multiple inverse kinematics solutions for a given pose - we set the limit on the amount of solutions to solve for up to 8. We also set the minimum solution distance, which is a threshold on how different solutions must be: if the joint positions in a solution are too similar to a previous solution, it will be marked as invalid. Next, we configure some additional properties, and add theComputeIKstage to the serial container.\nTo pick up the object, we must allow collision between the hand and the object. This can be done with aModifyPlanningScenestage. TheallowCollisionsfunction lets us specify which collisions to disable.allowCollisionscan be used with a container of names, so we can usegetLinkModelNamesWithCollisionGeometryto get all the names of links with collision geometry in the hand group.\nWith collisions allowed, we now can close the hand. This is done with aMoveTostage, similarly to theopenhandstage from above, except moving to thecloseposition as defined in the SRDF.\nWe now use aModifyPlanningScenestage again, this time to attach the object to the hand usingattachObject. Similarly to what we did with thecurrent_state_ptr, we get a pointer to this stage for later use when generating the place pose for the object.\nNext, we lift the object with aMoveRelativestage, similarly to theapproach_objectstage.\nWith this, we have all the stages needed to pick the object. Now, we add the serial container (with all its substages) to the task. If you build the package as-is, you can see the robot plan to pick up the object.\nTo test out the newly created stage, build the code and execute:\nNow that the stages that define the pick are complete, we move on to defining the stages for placing the object. Picking up where we left off, we add aConnectstage to connect the two, as we will soon be using a generator stage to generate the pose for placing the object.\nWe also create a serial container for the place stages. This is done similarly to the pick serial container.\nThe next stages will be added to the serial container rather than the task.\nThis next stage generates the poses used to place the object and compute the inverse kinematics for those poses - it is somewhat similar to thegenerategraspposestage from the pick serial container.\nWe start by creating a stage to generate the poses and inheriting the task properties.\nWe specify the pose where we want to place the object with aPoseStampedmessage fromgeometry_msgs- in this case, we choosey=0.5in the\"object\"frame.\nWe then pass the target pose to the stage withsetPose.\nNext, we usesetMonitoredStageand pass it the pointer to theattach_objectstage from earlier.\nThis allows the stage to know how the object is attached.\nWe then create aComputeIKstage and pass it ourGeneratePlacePosestage - the rest follows the same logic as above with the pick stages.\nNow that we’re ready to place the object, we open the hand withMoveTostage and the joint interpolation planner.\nWe also can re-enable collisions with the object now that we no longer need to hold it.\nThis is done usingallowCollisionsalmost exactly the same way as disabling collisions, except setting the last argument tofalserather thantrue.\nNow, we can detach the object usingdetachObject.\nWe retreat from the object using aMoveRelativestage, which is done similarly to theapproachobjectandliftobjectstages.\nWe finish our place serial container and add it to the task.\nThe final step is to return home: we use aMoveTostage and pass it the goal pose ofready, which is a pose defined in the Panda SRDF.\nAll these stages should be added above these lines.\nCongratulations! You’ve now defined a pick and place task using MoveIt Task Constructor! To try it out, build the code and execute:\nThe task with each comprising stage is shown in the Motion Planning Tasks pane. Click on a stage and additional information about the stage will show up to the right. The right pane shows different solutions as well as their associated costs. Depending on the stage type and the robot configuration, there may only be one solution shown.\nClick one of the solution costs to see an animation of the robot following the plan for that stage. Click the “Exec” button in the upper-right portion of the pane to execute the motion.\nTo run the complete MoveIt Task Constructor example included with the MoveIt tutorials:\nAnd in a second terminal:\nWhen running MTC, it prints a diagram like this to terminal:\nThis example^ shows two stages. The first stage (“initial_state”) is aCurrentStatetype of stage, which initializes aPlanningSceneand captures any collision objects that are present at that moment.\nA pointer to this stage can be used to retrieve the state of the robot.\nSinceCurrentStateinherits fromGenerator, it propagates solutions both forward and backward.\nThis is denoted by the arrows in both directions.\nThe first1indicates that one solution was successfully propagated backwards to the previous stage.\nThe second1, between the arrows, indicates that one solution was generated.\nThe0indicates that a solution was not propagated forward successfully to the next stage, because the next stage failed.\nThe second stage (“move_to_home”) is aMoveTotype of stage. It inherits its propagation direction from the previous stage, so both arrows point forward. The0’s indicate that this stage failed completely. From left to right, the0’s mean:\nThe stage did not receive a solution from the previous stage\nThe stage did not generate a solution\nThe stage did not propagate a solution forward to the next stage\nIn this case, we could tell that “move_to_home” was the root cause of the failure. The problem was a home state that was in collision. Defining a new, collision-free home position fixed the issue.\nInformation about individual stages can be retrieved from the task. For example, here we retrieve the unique ID for a stage:\nACurrentStatetype stage does not just retrieve the current state of the robot.\nIt also initializes aPlanningSceneobject, capturing any collision objects that are present at that moment.\nMTC stages can be propagated in forward and backward order.\nYou can easily check which direction a stage propagates by the arrow in the RViz GUI.\nWhen propagating backwards, the logic of many operations is reversed.\nFor example, to allow collisions with an object in aModifyPlanningScenestage, you would callallowCollisions(false)rather thanallowCollisions(true). There is a discussion to be readhere.\n© Copyright 2024, PickNik Robotics.\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit_task_constructor.git\nrosdep install --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncd~/ws_moveitcolconbuild--mixinrelease\nros2launchmoveit_task_constructor_demodemo.launch.py\nros2launchmoveit_task_constructor_demorun.launch.pyexe:=cartesianros2launchmoveit_task_constructor_demorun.launch.pyexe:=modularros2launchmoveit_task_constructor_demorun.launch.pyexe:=pick_place_demo\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_task_constructor_corerclcpp\\--node-namemtc_nodemtc_tutorial\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endifstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;classMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};MTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}rclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}voidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;pose.orientation.w=1.0;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}voidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}mtc::TaskMTCTaskNode::createTask(){mtc::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);// Disable warnings for this line, as it's a variable that's set but not used in this example#pragma GCC diagnostic push#pragma GCC diagnostic ignored \"-Wunused-but-set-variable\"mtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generator#pragma GCC diagnostic popautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));autosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);autointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();autocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);autostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}intmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endif\nstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;\nclassMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};\nMTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}\nrclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}\nvoidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}\nvoidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}\nmtc::TaskMTCTaskNode::createTask(){moveit::task_constructor::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);\nmtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generatorautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));\nautosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);\nautointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}\nintmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\nfromlaunchimportLaunchDescriptionfromlaunch_ros.actionsimportNodefrommoveit_configs_utilsimportMoveItConfigsBuilderdefgenerate_launch_description():moveit_config=MoveItConfigsBuilder(\"moveit_resources_panda\").to_dict()# MTC Demo nodepick_place_demo=Node(package=\"mtc_tutorial\",executable=\"mtc_node\",output=\"screen\",parameters=[moveit_config,],)returnLaunchDescription([pick_place_demo])\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}\n  )\ncd~/ws_moveitcolconbuild--mixinreleasesource~/ws_moveit/install/setup.bash\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo_minimal.launch.py\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));// Add the next lines of codes to define more stages here\nautostage_move_to_pick=std::make_unique<mtc::stages::Connect>(\"move to pick\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner}});stage_move_to_pick->setTimeout(5.0);stage_move_to_pick->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_pick));\nmtc::Stage*attach_object_stage=nullptr;// Forward attach_object_stage to place pose generator\n{autograsp=std::make_unique<mtc::SerialContainer>(\"pick object\");task.properties().exposeTo(grasp->properties(),{\"eef\",\"group\",\"ik_frame\"});grasp->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"approach object\",cartesian_planner);stage->properties().set(\"marker_ns\",\"approach_object\");stage->properties().set(\"link\",hand_frame);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.15);// Set hand forward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=hand_frame;vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\n{// Sample grasp poseautostage=std::make_unique<mtc::stages::GenerateGraspPose>(\"generate grasp pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"grasp_pose\");stage->setPreGraspPose(\"open\");stage->setObject(\"object\");stage->setAngleDelta(M_PI/12);stage->setMonitoredStage(current_state_ptr);// Hook into current state\nEigen::Isometry3dgrasp_frame_transform;Eigen::Quaterniondq=Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitX())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitY())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitZ());grasp_frame_transform.linear()=q.matrix();grasp_frame_transform.translation().z()=0.1;\n// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"grasp pose IK\",std::move(stage));wrapper->setMaxIKSolutions(8);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(grasp_frame_transform,hand_frame);wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});grasp->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"allow collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),true);grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"close hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"close\");grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"attach object\");stage->attachObject(\"object\",hand_frame);attach_object_stage=stage.get();grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"lift object\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"lift_object\");// Set upward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\ntask.add(std::move(grasp));}\nros2launchmtc_tutorialpick_place_demo.launch.py\n{autostage_move_to_place=std::make_unique<mtc::stages::Connect>(\"move to place\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner},{hand_group_name,interpolation_planner}});stage_move_to_place->setTimeout(5.0);stage_move_to_place->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_place));}\n{autoplace=std::make_unique<mtc::SerialContainer>(\"place object\");task.properties().exposeTo(place->properties(),{\"eef\",\"group\",\"ik_frame\"});place->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{// Sample place poseautostage=std::make_unique<mtc::stages::GeneratePlacePose>(\"generate place pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"place_pose\");stage->setObject(\"object\");geometry_msgs::msg::PoseStampedtarget_pose_msg;target_pose_msg.header.frame_id=\"object\";target_pose_msg.pose.position.y=0.5;target_pose_msg.pose.orientation.w=1.0;stage->setPose(target_pose_msg);stage->setMonitoredStage(attach_object_stage);// Hook into attach_object_stage// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"place pose IK\",std::move(stage));wrapper->setMaxIKSolutions(2);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(\"object\");wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});place->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"open\");place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"forbid collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),false);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"detach object\");stage->detachObject(\"object\",hand_frame);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"retreat\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"retreat\");// Set retreat directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.x=-0.5;stage->setDirection(vec);place->insert(std::move(stage));}\ntask.add(std::move(place));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"return home\",interpolation_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setGoal(\"ready\");task.add(std::move(stage));}\n// Stages all added to the task above this linereturntask;}\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmoveit2_tutorialspick_place_demo.launch.py\n[demo_node-1]1-←1→-0/initial_state[demo_node-1]-0→0→-0/move_to_home\nuint32_tconstunique_stage_id=task_.stages()->findChild(stage_name)->introspectionId();\n<branch>\nhumble\nros2\nmain\nmtc_tutorial\nmoveit_task_constructor_core\nsrc/mtc_node\nmtc_node.cpp\nrclcpp/rclcpp.hpp\nmoveit/planning_scene/planning_scene.h\nmoveit/planning_scene_interface/planning_scene_interface.h\nmoveit/task_constructor/task.h\nmoveit/task_constructor/solvers.h\nmoveit/task_constructor/stages.h\ntf2_geometry_msgs/tf2_geometry_msgs.hpp\ntf2_eigen/tf2_eigen.hpp\nmoveit::task_constructor\nMTCTaskNode\nobject.primitives[0].dimensions\npose.position.x\npose.position.y\ncreateTask\ntask.init()\ntask.plan(5)\ntask.execute()\ntask.setProperty(property_name,value)\ncurrent_state_ptr\nnullptr\ncurrent_state\nCurrentState\ncurrent_state_ptr\nMoveTo\ncreateTask()\nmain\nmove_group\nros2_control\nstatic_tf\nrobot_state_publisher\nrviz\nmtc_tutorial\npackage\nexecutable\npick_place_demo.launch.py\nCMakeLists.txt\nmap\npanda_link0\nmoveit_task_constructor_visualization\n/solution\nmtc_tutorial\nmtc_demo.launch.py\nmtc_tutorial\nmtc_node.cpp\nConnect\nmove_to_pick\nGroupPlannerVector\nnullptr\nSerialContainer\nexposeTo()\nconfigureInitFrom()\nMoveRelative\nMoveRelative\ncartesian_planner\nVector3Stamped\nCurrentState\nGenerateGraspPose\ncurrent_state_ptr\nPoseStamped\ngeometry_msgs\nComputeIK\ngenerateposeIK\ngenerategrasppose\nComputeIK\nModifyPlanningScene\nallowCollisions\nallowCollisions\ngetLinkModelNamesWithCollisionGeometry\nMoveTo\nopenhand\nclose\nModifyPlanningScene\nattachObject\ncurrent_state_ptr\nMoveRelative\napproach_object\nConnect\ngenerategrasppose\nPoseStamped\ngeometry_msgs\ny=0.5\n\"object\"\nsetPose\nsetMonitoredStage\nattach_object\nComputeIK\nGeneratePlacePose\nMoveTo\nallowCollisions\nfalse\ntrue\ndetachObject\nMoveRelative\napproachobject\nliftobject\nMoveTo\nready\nCurrentState\nPlanningScene\nCurrentState\nGenerator\n1\n1\n0\nMoveTo\n0\n0\nCurrentState\nPlanningScene\nModifyPlanningScene\nallowCollisions(false)\nallowCollisions(true)",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0d9"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/pick_and_place_with_moveit_task_constructor/pick_and_place_with_moveit_task_constructor.html#create-a-new-package",
  "title": "Pick and Place with MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "Pick and Place with MoveIt Task Constructor\n1 Basic Concepts\n2 Getting Started\n3 Trying It Out\n4 Setting up a Project with MoveIt Task Constructor\n5 Running the Demo\n6 Adding Stages\n7 Further Discussion\nThis tutorial will walk you through creating a package that plans a pick and place operation usingMoveIt Task Constructor. MoveIt Task Constructor provides a way to plan for tasks that consist of multiple different subtasks (known as stages). If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nThe fundamental idea of MTC is that complex motion planning problems can be composed into a set of simpler subproblems.\nThe top-level planning problem is specified as aTaskwhile all subproblems are specified byStages.\nStages can be arranged in any arbitrary order and hierarchy only limited by the individual stages types.\nThe order in which stages can be arranged is restricted by the direction in which results are passed.\nThere are three possible stages relating to the result flow: generator, propagator, and connector stages:\nGeneratorscompute their results independently of their neighbor stages and pass them in both directions, backwards and forwards.\nAn example is an IK sampler for geometric poses where approaching and departing motions (neighbor stages) depend on the solution.\nPropagatorsreceive the result of one neighbor stage, solve a subproblem and then propagate their result to the neighbor on the opposite site.\nDepending on the implementation, propagating stages can pass solutions forward, backward or in both directions separately.\nAn example is a stage that computes a Cartesian path based on either a start or a goal state.\nConnectorsdo not propagate any results, but rather attempt to bridge the gap between the resulting states of both neighbors.\nAn example is the computation of a free-motion plan from one given state to another.\nAdditional to the order types, there are different hierarchy types allowing to encapsulate subordinate stages.\nStages without subordinate stages are calledprimitive stages, higher-level stages are calledcontainer stages.\nThere are three container types:\nWrappersencapsulate a single subordinate stage and modify or filter the results.\nFor example, a filter stage that only accepts solutions of its child stage that satisfy a certain constraint can be realized as a wrapper.\nAnother standard use of this type includes the IK wrapper stage, which generates inverse kinematics solutions based on planning scenes annotated with a pose target property.\nSerial Containershold a sequence of subordinate stages and only consider end-to-end solutions as results.\nAn example is a picking motion that consists of a sequence of coherent steps.\nParallel Containerscombine set of subordinate stages and can be used for passing the best of alternative results, running fallback solvers or for merging multiple independent solutions.\nExamples are running alternative planners for a free-motion plan, picking objects with the right hand or with the left hand as a fallback, or moving the arm and opening the gripper at the same time.\nStages not only support solving motion planning problems.\nThey can also be used for all kinds of state transitions, as for instance modifying the planning scene.\nCombined with the possibility of using class inheritance it is possible to construct very complex behavior while only relying on a well-structured set of primitive stages.\nMore detailed information on MTC can be found in theMoveIt Task Constructor concepts page\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nMove into your colcon workspace and pull the MoveIt Task Constructor source, where<branch>can be e.g.humblefor ROS Humble, orros2for the latest version compatible with MoveIt 2main:\nInstall missing packages with rosdep:\nBuild the workspace:\nThe MoveIt Task Constructor package contains several basic examples and a pick-and-place demo.\nFor all demos you should launch the basic environment:\nSubsequently, you can run the individual demos:\nOn the right side you should see theMotion Planning Taskspanel outlining the hierarchical stage structure of the tasks.\nWhen you select a particular stage, the list of successful and failed solutions will be\nshown in the right-most window. Selecting one of those solutions will start its visualization.\nThis section walks through the steps required to build a simple task with MoveIt Task Constructor.\nCreate a new package with the following command:\nThis will create a new package and folder calledmtc_tutorialwith a dependency onmoveit_task_constructor_coreas well as a hello world example insrc/mtc_node.\nOpenmtc_node.cppin your editor of choice, and paste in the following code.\nThe top of the code includes the ROS and MoveIt Libraries that this package uses.\nrclcpp/rclcpp.hppincludes core ROS2 functionality\nmoveit/planning_scene/planning_scene.handmoveit/planning_scene_interface/planning_scene_interface.hinclude functionality to interface with the robot model and collision objects\nmoveit/task_constructor/task.h,moveit/task_constructor/solvers.h, andmoveit/task_constructor/stages.hinclude different components of MoveIt Task Constructor that are used in the example\ntf2_geometry_msgs/tf2_geometry_msgs.hppandtf2_eigen/tf2_eigen.hppwon’t be used in this initial example, but they will be used for pose generation when we add more stages to the MoveIt Task Constructor task.\nThe next line gets a logger for our new node. We also create a namespace alias formoveit::task_constructorfor convenience.\nWe start by defining a class that will contain the main MoveIt Task Constructor functionality.  We also declare the MoveIt Task Constructor task object as a member variable for our class: this isn’t strictly necessary for a given application, but it helps save the task for later visualization purposes. We will explore each function individually below.\nThese lines initialize the node with specified options (it is the constructor of ourMTCTaskNodeclass).\nThese next lines define a getter function to get the node base interface, which will be used for the executor later.\nThis class method is used to set up the planning scene that is used in the example. It creates a cylinder with dimensions specified byobject.primitives[0].dimensionsand position specified bypose.position.xandpose.position.y.\nYou can try changing these numbers to resize and move the cylinder around. If we move the cylinder out of the robot’s reach, planning will fail.\nThis function interfaces with the MoveIt Task Constructor task object. It first creates a task, which includes setting some properties and adding stages. This will be discussed further in thecreateTaskfunction definition. Next,task.init()initializes the task andtask.plan(5)generates a plan, stopping after 5 successful plans are found. The next line publishes the solution to be visualized in RViz - this line can be removed if you don’t care for visualization. Finally,task.execute()executes the plan. Execution occurs via an action server interface with the RViz plugin.\nAs mentioned above, this function creates a MoveIt Task Constructor object and sets some initial properties. In this case, we set the task name to “demo_task”, load the robot model, define the names of some useful frames, and set those frame names as properties of the task withtask.setProperty(property_name,value). The next few code blocks will fill out this function body.\nNow, we add an example stage to the node. The first line setscurrent_state_ptrtonullptr; this creates a pointer to a stage such that we can reuse stage information in specific scenarios. This line is not used at this moment, but will be used later when more stages are added to the task. Next, we make acurrent_statestage (a generator stage) and add it to our task - this starts the robot off in its current state. Now that we’ve created theCurrentStatestage, we save a pointer to it in thecurrent_state_ptrfor later use.\nSolvers are used to define the type of robot motion. MoveIt Task Constructor has three options for solvers:\nPipelinePlanneruses MoveIt’s planning pipeline, which typically defaults toOMPL.\nJointInterpolationis a simple planner that interpolates between the start and goal joint states. It is typically used for simple motions as it computes quickly but doesn’t support complex motions.\nCartesianPathis used to move the end effector in a straight line in Cartesian space.\nFeel free to try out the different solvers and see how the robot motion changes. For the first stage we will use the Cartesian planner, which requires the following properties to be set:\nNow that we added in the planners, we can add a stage that will move the robot.\nThe following lines use aMoveTostage (a propagator stage). Since opening the hand is a relatively simple movement, we can use the joint interpolation planner.\nThis stage plans a move to the “open hand” pose, which is a named pose defined in theSRDFfor the Panda robot.\nWe return the task and finish with thecreateTask()function.\nFinally, we havemain: the following lines create a node using the class defined above, and calls the class methods to set up and execute a basic MTC task. In this example, we do not cancel the executor once the task has finished executing to keep the node alive to inspect the solutions in RViz.\nWe will need a launch file to launch themove_group,ros2_control,static_tf,robot_state_publisher, andrviznodes that provide us the environment to run the demo. The one we will use for this example can be foundhere.\nTo run the MoveIt Task Constructor node, we will use a second launch file to start themtc_tutorialexecutable with the proper parameters. Here we can load URDF, SRDF, and OMPL parameters, or use MoveIt Configs Utils to do so. Your launch file should look something like the one found in this tutorial packagehere(pay close attention to thepackageandexecutablearguments below as they are different from the launch file linked) :\nSave a launch file aspick_place_demo.launch.pyor download one to the package’s launch directory. Make sure to edit theCMakeLists.txtso it includes the launch folder by adding the following lines:\nNow we can build and source the colcon workspace.\nStart by launching the first launch file. If you want to use the one provided by the tutorials:\nRViz will now load. If you’re using your own launch file and haven’t included an rviz configsuch as this, you will need to configure RViz before you see anything displayed. If you’re using the launch file from the tutorials package, RViz will already be configured for you and you can jump to the end of the next section.\nIf you are not using the RViz configuration provided, we’ll have to make some changes to the RViz configuration to see your robot and the MoveIt Task Constructor solutions. First, start RViz. The following steps will cover how to set up RViz for MoveIt Task Constructor solution visualization.\nIf theMotionPlanningdisplay is active, uncheck it to hide it for now.\nUnderGlobal Options, change theFixed Framefrommaptopanda_link0if not already done.\nOn the bottom left of the window, click theAddbutton.\nUndermoveit_task_constructor_visualizationselectMotion Planning Tasksand click OK. TheMotion Planning Tasksdisplay should appear on the bottom left.\nIn theDisplays, underMotion Planning Tasks,  changeTask Solution Topicto/solution\nYou should see the panda arm in the main view with Motion Planning Tasks display open in the bottom left and nothing in it. Your MTC task will show up in this panel once you launch themtc_tutorialnode. If you’re usingmtc_demo.launch.pyfrom the tutorials, jump back in here.\nLaunch themtc_tutorialnode with\nYou should see the arm execute the task with the single stage to open the hand, with the cylinder in green in front of it. It should look something like this:\nIf you haven’t made your own package, but still want to see what this looks like, you can launch this file from the tutorials:\nSo far, we’ve walked through creating and executing a simple task, which runs but does not do much. Now, we will start adding the pick-and-place stages to the task. The image below shows an outline of the stages we will use in our task.\nWe will start adding stages after our existing open hand stage. Openmtc_node.cppand locate the following lines:\nWe need to move the arm to a position where we can pick up our object. This is done with aConnectstage, which as its name implies, is a Connector stage. This means that it tries to bridge between the results of the stage before and after it. This stage is initialized with a name,move_to_pick, and aGroupPlannerVectorthat specifies the planning group and the planner. We then set a timeout for the stage, set the properties for the stage, and add it to our task.\nNext, we create a pointer to a MoveIt Task Constructor stage object, and set it tonullptrfor now. Later, we will use this to save a stage.\nThis next block of code creates aSerialContainer.\nThis is a container that can be added to our task and can hold several substages.\nIn this case, we create a serial container that will contain the stages relevant to the picking action.\nInstead of adding the stages to the task, we will add the relevant stages to the serial container. We useexposeTo()to declare the task properties from the parent task in the new serial container, and useconfigureInitFrom()to initialize them.\nThis allows the contained stages to access these properties.\nWe then create a stage to approach the object. This stage is aMoveRelativestage, which allows us to specify a relative movement from our current position.MoveRelativeis a propagator stage: it receives the solution from its neighbouring stages and propagates it to the next or previous stage. Usingcartesian_plannerfinds a solution that involves moving the end effector in a straight line. We set the properties, and set the minimum and maximum distance to move. Now we create aVector3Stampedmessage to indicate the direction we want to move - in this case, in the Z direction from the hand frame. Finally, we add this stage to our serial container\nNow, create a stage to generate the grasp pose.\nThis is a generator stage, so it computes its results without regard to the stages before and after it.\nThe first stage,CurrentStateis a generator stage as well - to connect the first stage and this stage, a connecting stage must be used, which we already created above.\nThis code sets the stage properties, sets the pose before grasping, the angle delta, and the monitored stage.\nAngle delta is a property of theGenerateGraspPosestage that is used to determine the number of poses to generate; when generating solutions, MoveIt Task Constructor will try to grasp the object from many different orientations, with the difference between the orientations specified by the angle delta. The smaller the delta, the closer together the grasp orientations will be. When defining the current stage, we setcurrent_state_ptr, which is now used to forward information about the object pose and shape to the inverse kinematics solver.\nThis stage won’t be directly added to the serial container like previously, as we still need to do inverse kinematics on the poses it generates.\nBefore we compute inverse kinematics for the poses generated above, we first need to define the frame. This can be done with aPoseStampedmessage fromgeometry_msgsor in this case, we define the transform using Eigen transformation matrix and the name of the relevant link. Here, we define the transformation matrix.\nNow, we create theComputeIKstage, and give it the namegenerateposeIKas well as thegenerategraspposestage defined above. Some robots have multiple inverse kinematics solutions for a given pose - we set the limit on the amount of solutions to solve for up to 8. We also set the minimum solution distance, which is a threshold on how different solutions must be: if the joint positions in a solution are too similar to a previous solution, it will be marked as invalid. Next, we configure some additional properties, and add theComputeIKstage to the serial container.\nTo pick up the object, we must allow collision between the hand and the object. This can be done with aModifyPlanningScenestage. TheallowCollisionsfunction lets us specify which collisions to disable.allowCollisionscan be used with a container of names, so we can usegetLinkModelNamesWithCollisionGeometryto get all the names of links with collision geometry in the hand group.\nWith collisions allowed, we now can close the hand. This is done with aMoveTostage, similarly to theopenhandstage from above, except moving to thecloseposition as defined in the SRDF.\nWe now use aModifyPlanningScenestage again, this time to attach the object to the hand usingattachObject. Similarly to what we did with thecurrent_state_ptr, we get a pointer to this stage for later use when generating the place pose for the object.\nNext, we lift the object with aMoveRelativestage, similarly to theapproach_objectstage.\nWith this, we have all the stages needed to pick the object. Now, we add the serial container (with all its substages) to the task. If you build the package as-is, you can see the robot plan to pick up the object.\nTo test out the newly created stage, build the code and execute:\nNow that the stages that define the pick are complete, we move on to defining the stages for placing the object. Picking up where we left off, we add aConnectstage to connect the two, as we will soon be using a generator stage to generate the pose for placing the object.\nWe also create a serial container for the place stages. This is done similarly to the pick serial container.\nThe next stages will be added to the serial container rather than the task.\nThis next stage generates the poses used to place the object and compute the inverse kinematics for those poses - it is somewhat similar to thegenerategraspposestage from the pick serial container.\nWe start by creating a stage to generate the poses and inheriting the task properties.\nWe specify the pose where we want to place the object with aPoseStampedmessage fromgeometry_msgs- in this case, we choosey=0.5in the\"object\"frame.\nWe then pass the target pose to the stage withsetPose.\nNext, we usesetMonitoredStageand pass it the pointer to theattach_objectstage from earlier.\nThis allows the stage to know how the object is attached.\nWe then create aComputeIKstage and pass it ourGeneratePlacePosestage - the rest follows the same logic as above with the pick stages.\nNow that we’re ready to place the object, we open the hand withMoveTostage and the joint interpolation planner.\nWe also can re-enable collisions with the object now that we no longer need to hold it.\nThis is done usingallowCollisionsalmost exactly the same way as disabling collisions, except setting the last argument tofalserather thantrue.\nNow, we can detach the object usingdetachObject.\nWe retreat from the object using aMoveRelativestage, which is done similarly to theapproachobjectandliftobjectstages.\nWe finish our place serial container and add it to the task.\nThe final step is to return home: we use aMoveTostage and pass it the goal pose ofready, which is a pose defined in the Panda SRDF.\nAll these stages should be added above these lines.\nCongratulations! You’ve now defined a pick and place task using MoveIt Task Constructor! To try it out, build the code and execute:\nThe task with each comprising stage is shown in the Motion Planning Tasks pane. Click on a stage and additional information about the stage will show up to the right. The right pane shows different solutions as well as their associated costs. Depending on the stage type and the robot configuration, there may only be one solution shown.\nClick one of the solution costs to see an animation of the robot following the plan for that stage. Click the “Exec” button in the upper-right portion of the pane to execute the motion.\nTo run the complete MoveIt Task Constructor example included with the MoveIt tutorials:\nAnd in a second terminal:\nWhen running MTC, it prints a diagram like this to terminal:\nThis example^ shows two stages. The first stage (“initial_state”) is aCurrentStatetype of stage, which initializes aPlanningSceneand captures any collision objects that are present at that moment.\nA pointer to this stage can be used to retrieve the state of the robot.\nSinceCurrentStateinherits fromGenerator, it propagates solutions both forward and backward.\nThis is denoted by the arrows in both directions.\nThe first1indicates that one solution was successfully propagated backwards to the previous stage.\nThe second1, between the arrows, indicates that one solution was generated.\nThe0indicates that a solution was not propagated forward successfully to the next stage, because the next stage failed.\nThe second stage (“move_to_home”) is aMoveTotype of stage. It inherits its propagation direction from the previous stage, so both arrows point forward. The0’s indicate that this stage failed completely. From left to right, the0’s mean:\nThe stage did not receive a solution from the previous stage\nThe stage did not generate a solution\nThe stage did not propagate a solution forward to the next stage\nIn this case, we could tell that “move_to_home” was the root cause of the failure. The problem was a home state that was in collision. Defining a new, collision-free home position fixed the issue.\nInformation about individual stages can be retrieved from the task. For example, here we retrieve the unique ID for a stage:\nACurrentStatetype stage does not just retrieve the current state of the robot.\nIt also initializes aPlanningSceneobject, capturing any collision objects that are present at that moment.\nMTC stages can be propagated in forward and backward order.\nYou can easily check which direction a stage propagates by the arrow in the RViz GUI.\nWhen propagating backwards, the logic of many operations is reversed.\nFor example, to allow collisions with an object in aModifyPlanningScenestage, you would callallowCollisions(false)rather thanallowCollisions(true). There is a discussion to be readhere.\n© Copyright 2024, PickNik Robotics.\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit_task_constructor.git\nrosdep install --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncd~/ws_moveitcolconbuild--mixinrelease\nros2launchmoveit_task_constructor_demodemo.launch.py\nros2launchmoveit_task_constructor_demorun.launch.pyexe:=cartesianros2launchmoveit_task_constructor_demorun.launch.pyexe:=modularros2launchmoveit_task_constructor_demorun.launch.pyexe:=pick_place_demo\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_task_constructor_corerclcpp\\--node-namemtc_nodemtc_tutorial\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endifstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;classMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};MTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}rclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}voidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;pose.orientation.w=1.0;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}voidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}mtc::TaskMTCTaskNode::createTask(){mtc::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);// Disable warnings for this line, as it's a variable that's set but not used in this example#pragma GCC diagnostic push#pragma GCC diagnostic ignored \"-Wunused-but-set-variable\"mtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generator#pragma GCC diagnostic popautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));autosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);autointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();autocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);autostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}intmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endif\nstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;\nclassMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};\nMTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}\nrclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}\nvoidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}\nvoidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}\nmtc::TaskMTCTaskNode::createTask(){moveit::task_constructor::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);\nmtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generatorautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));\nautosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);\nautointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}\nintmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\nfromlaunchimportLaunchDescriptionfromlaunch_ros.actionsimportNodefrommoveit_configs_utilsimportMoveItConfigsBuilderdefgenerate_launch_description():moveit_config=MoveItConfigsBuilder(\"moveit_resources_panda\").to_dict()# MTC Demo nodepick_place_demo=Node(package=\"mtc_tutorial\",executable=\"mtc_node\",output=\"screen\",parameters=[moveit_config,],)returnLaunchDescription([pick_place_demo])\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}\n  )\ncd~/ws_moveitcolconbuild--mixinreleasesource~/ws_moveit/install/setup.bash\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo_minimal.launch.py\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));// Add the next lines of codes to define more stages here\nautostage_move_to_pick=std::make_unique<mtc::stages::Connect>(\"move to pick\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner}});stage_move_to_pick->setTimeout(5.0);stage_move_to_pick->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_pick));\nmtc::Stage*attach_object_stage=nullptr;// Forward attach_object_stage to place pose generator\n{autograsp=std::make_unique<mtc::SerialContainer>(\"pick object\");task.properties().exposeTo(grasp->properties(),{\"eef\",\"group\",\"ik_frame\"});grasp->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"approach object\",cartesian_planner);stage->properties().set(\"marker_ns\",\"approach_object\");stage->properties().set(\"link\",hand_frame);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.15);// Set hand forward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=hand_frame;vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\n{// Sample grasp poseautostage=std::make_unique<mtc::stages::GenerateGraspPose>(\"generate grasp pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"grasp_pose\");stage->setPreGraspPose(\"open\");stage->setObject(\"object\");stage->setAngleDelta(M_PI/12);stage->setMonitoredStage(current_state_ptr);// Hook into current state\nEigen::Isometry3dgrasp_frame_transform;Eigen::Quaterniondq=Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitX())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitY())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitZ());grasp_frame_transform.linear()=q.matrix();grasp_frame_transform.translation().z()=0.1;\n// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"grasp pose IK\",std::move(stage));wrapper->setMaxIKSolutions(8);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(grasp_frame_transform,hand_frame);wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});grasp->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"allow collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),true);grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"close hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"close\");grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"attach object\");stage->attachObject(\"object\",hand_frame);attach_object_stage=stage.get();grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"lift object\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"lift_object\");// Set upward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\ntask.add(std::move(grasp));}\nros2launchmtc_tutorialpick_place_demo.launch.py\n{autostage_move_to_place=std::make_unique<mtc::stages::Connect>(\"move to place\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner},{hand_group_name,interpolation_planner}});stage_move_to_place->setTimeout(5.0);stage_move_to_place->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_place));}\n{autoplace=std::make_unique<mtc::SerialContainer>(\"place object\");task.properties().exposeTo(place->properties(),{\"eef\",\"group\",\"ik_frame\"});place->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{// Sample place poseautostage=std::make_unique<mtc::stages::GeneratePlacePose>(\"generate place pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"place_pose\");stage->setObject(\"object\");geometry_msgs::msg::PoseStampedtarget_pose_msg;target_pose_msg.header.frame_id=\"object\";target_pose_msg.pose.position.y=0.5;target_pose_msg.pose.orientation.w=1.0;stage->setPose(target_pose_msg);stage->setMonitoredStage(attach_object_stage);// Hook into attach_object_stage// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"place pose IK\",std::move(stage));wrapper->setMaxIKSolutions(2);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(\"object\");wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});place->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"open\");place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"forbid collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),false);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"detach object\");stage->detachObject(\"object\",hand_frame);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"retreat\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"retreat\");// Set retreat directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.x=-0.5;stage->setDirection(vec);place->insert(std::move(stage));}\ntask.add(std::move(place));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"return home\",interpolation_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setGoal(\"ready\");task.add(std::move(stage));}\n// Stages all added to the task above this linereturntask;}\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmoveit2_tutorialspick_place_demo.launch.py\n[demo_node-1]1-←1→-0/initial_state[demo_node-1]-0→0→-0/move_to_home\nuint32_tconstunique_stage_id=task_.stages()->findChild(stage_name)->introspectionId();\n<branch>\nhumble\nros2\nmain\nmtc_tutorial\nmoveit_task_constructor_core\nsrc/mtc_node\nmtc_node.cpp\nrclcpp/rclcpp.hpp\nmoveit/planning_scene/planning_scene.h\nmoveit/planning_scene_interface/planning_scene_interface.h\nmoveit/task_constructor/task.h\nmoveit/task_constructor/solvers.h\nmoveit/task_constructor/stages.h\ntf2_geometry_msgs/tf2_geometry_msgs.hpp\ntf2_eigen/tf2_eigen.hpp\nmoveit::task_constructor\nMTCTaskNode\nobject.primitives[0].dimensions\npose.position.x\npose.position.y\ncreateTask\ntask.init()\ntask.plan(5)\ntask.execute()\ntask.setProperty(property_name,value)\ncurrent_state_ptr\nnullptr\ncurrent_state\nCurrentState\ncurrent_state_ptr\nMoveTo\ncreateTask()\nmain\nmove_group\nros2_control\nstatic_tf\nrobot_state_publisher\nrviz\nmtc_tutorial\npackage\nexecutable\npick_place_demo.launch.py\nCMakeLists.txt\nmap\npanda_link0\nmoveit_task_constructor_visualization\n/solution\nmtc_tutorial\nmtc_demo.launch.py\nmtc_tutorial\nmtc_node.cpp\nConnect\nmove_to_pick\nGroupPlannerVector\nnullptr\nSerialContainer\nexposeTo()\nconfigureInitFrom()\nMoveRelative\nMoveRelative\ncartesian_planner\nVector3Stamped\nCurrentState\nGenerateGraspPose\ncurrent_state_ptr\nPoseStamped\ngeometry_msgs\nComputeIK\ngenerateposeIK\ngenerategrasppose\nComputeIK\nModifyPlanningScene\nallowCollisions\nallowCollisions\ngetLinkModelNamesWithCollisionGeometry\nMoveTo\nopenhand\nclose\nModifyPlanningScene\nattachObject\ncurrent_state_ptr\nMoveRelative\napproach_object\nConnect\ngenerategrasppose\nPoseStamped\ngeometry_msgs\ny=0.5\n\"object\"\nsetPose\nsetMonitoredStage\nattach_object\nComputeIK\nGeneratePlacePose\nMoveTo\nallowCollisions\nfalse\ntrue\ndetachObject\nMoveRelative\napproachobject\nliftobject\nMoveTo\nready\nCurrentState\nPlanningScene\nCurrentState\nGenerator\n1\n1\n0\nMoveTo\n0\n0\nCurrentState\nPlanningScene\nModifyPlanningScene\nallowCollisions(false)\nallowCollisions(true)",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0da"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/pick_and_place_with_moveit_task_constructor/pick_and_place_with_moveit_task_constructor.html#the-code",
  "title": "Pick and Place with MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "Pick and Place with MoveIt Task Constructor\n1 Basic Concepts\n2 Getting Started\n3 Trying It Out\n4 Setting up a Project with MoveIt Task Constructor\n5 Running the Demo\n6 Adding Stages\n7 Further Discussion\nThis tutorial will walk you through creating a package that plans a pick and place operation usingMoveIt Task Constructor. MoveIt Task Constructor provides a way to plan for tasks that consist of multiple different subtasks (known as stages). If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nThe fundamental idea of MTC is that complex motion planning problems can be composed into a set of simpler subproblems.\nThe top-level planning problem is specified as aTaskwhile all subproblems are specified byStages.\nStages can be arranged in any arbitrary order and hierarchy only limited by the individual stages types.\nThe order in which stages can be arranged is restricted by the direction in which results are passed.\nThere are three possible stages relating to the result flow: generator, propagator, and connector stages:\nGeneratorscompute their results independently of their neighbor stages and pass them in both directions, backwards and forwards.\nAn example is an IK sampler for geometric poses where approaching and departing motions (neighbor stages) depend on the solution.\nPropagatorsreceive the result of one neighbor stage, solve a subproblem and then propagate their result to the neighbor on the opposite site.\nDepending on the implementation, propagating stages can pass solutions forward, backward or in both directions separately.\nAn example is a stage that computes a Cartesian path based on either a start or a goal state.\nConnectorsdo not propagate any results, but rather attempt to bridge the gap between the resulting states of both neighbors.\nAn example is the computation of a free-motion plan from one given state to another.\nAdditional to the order types, there are different hierarchy types allowing to encapsulate subordinate stages.\nStages without subordinate stages are calledprimitive stages, higher-level stages are calledcontainer stages.\nThere are three container types:\nWrappersencapsulate a single subordinate stage and modify or filter the results.\nFor example, a filter stage that only accepts solutions of its child stage that satisfy a certain constraint can be realized as a wrapper.\nAnother standard use of this type includes the IK wrapper stage, which generates inverse kinematics solutions based on planning scenes annotated with a pose target property.\nSerial Containershold a sequence of subordinate stages and only consider end-to-end solutions as results.\nAn example is a picking motion that consists of a sequence of coherent steps.\nParallel Containerscombine set of subordinate stages and can be used for passing the best of alternative results, running fallback solvers or for merging multiple independent solutions.\nExamples are running alternative planners for a free-motion plan, picking objects with the right hand or with the left hand as a fallback, or moving the arm and opening the gripper at the same time.\nStages not only support solving motion planning problems.\nThey can also be used for all kinds of state transitions, as for instance modifying the planning scene.\nCombined with the possibility of using class inheritance it is possible to construct very complex behavior while only relying on a well-structured set of primitive stages.\nMore detailed information on MTC can be found in theMoveIt Task Constructor concepts page\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nMove into your colcon workspace and pull the MoveIt Task Constructor source, where<branch>can be e.g.humblefor ROS Humble, orros2for the latest version compatible with MoveIt 2main:\nInstall missing packages with rosdep:\nBuild the workspace:\nThe MoveIt Task Constructor package contains several basic examples and a pick-and-place demo.\nFor all demos you should launch the basic environment:\nSubsequently, you can run the individual demos:\nOn the right side you should see theMotion Planning Taskspanel outlining the hierarchical stage structure of the tasks.\nWhen you select a particular stage, the list of successful and failed solutions will be\nshown in the right-most window. Selecting one of those solutions will start its visualization.\nThis section walks through the steps required to build a simple task with MoveIt Task Constructor.\nCreate a new package with the following command:\nThis will create a new package and folder calledmtc_tutorialwith a dependency onmoveit_task_constructor_coreas well as a hello world example insrc/mtc_node.\nOpenmtc_node.cppin your editor of choice, and paste in the following code.\nThe top of the code includes the ROS and MoveIt Libraries that this package uses.\nrclcpp/rclcpp.hppincludes core ROS2 functionality\nmoveit/planning_scene/planning_scene.handmoveit/planning_scene_interface/planning_scene_interface.hinclude functionality to interface with the robot model and collision objects\nmoveit/task_constructor/task.h,moveit/task_constructor/solvers.h, andmoveit/task_constructor/stages.hinclude different components of MoveIt Task Constructor that are used in the example\ntf2_geometry_msgs/tf2_geometry_msgs.hppandtf2_eigen/tf2_eigen.hppwon’t be used in this initial example, but they will be used for pose generation when we add more stages to the MoveIt Task Constructor task.\nThe next line gets a logger for our new node. We also create a namespace alias formoveit::task_constructorfor convenience.\nWe start by defining a class that will contain the main MoveIt Task Constructor functionality.  We also declare the MoveIt Task Constructor task object as a member variable for our class: this isn’t strictly necessary for a given application, but it helps save the task for later visualization purposes. We will explore each function individually below.\nThese lines initialize the node with specified options (it is the constructor of ourMTCTaskNodeclass).\nThese next lines define a getter function to get the node base interface, which will be used for the executor later.\nThis class method is used to set up the planning scene that is used in the example. It creates a cylinder with dimensions specified byobject.primitives[0].dimensionsand position specified bypose.position.xandpose.position.y.\nYou can try changing these numbers to resize and move the cylinder around. If we move the cylinder out of the robot’s reach, planning will fail.\nThis function interfaces with the MoveIt Task Constructor task object. It first creates a task, which includes setting some properties and adding stages. This will be discussed further in thecreateTaskfunction definition. Next,task.init()initializes the task andtask.plan(5)generates a plan, stopping after 5 successful plans are found. The next line publishes the solution to be visualized in RViz - this line can be removed if you don’t care for visualization. Finally,task.execute()executes the plan. Execution occurs via an action server interface with the RViz plugin.\nAs mentioned above, this function creates a MoveIt Task Constructor object and sets some initial properties. In this case, we set the task name to “demo_task”, load the robot model, define the names of some useful frames, and set those frame names as properties of the task withtask.setProperty(property_name,value). The next few code blocks will fill out this function body.\nNow, we add an example stage to the node. The first line setscurrent_state_ptrtonullptr; this creates a pointer to a stage such that we can reuse stage information in specific scenarios. This line is not used at this moment, but will be used later when more stages are added to the task. Next, we make acurrent_statestage (a generator stage) and add it to our task - this starts the robot off in its current state. Now that we’ve created theCurrentStatestage, we save a pointer to it in thecurrent_state_ptrfor later use.\nSolvers are used to define the type of robot motion. MoveIt Task Constructor has three options for solvers:\nPipelinePlanneruses MoveIt’s planning pipeline, which typically defaults toOMPL.\nJointInterpolationis a simple planner that interpolates between the start and goal joint states. It is typically used for simple motions as it computes quickly but doesn’t support complex motions.\nCartesianPathis used to move the end effector in a straight line in Cartesian space.\nFeel free to try out the different solvers and see how the robot motion changes. For the first stage we will use the Cartesian planner, which requires the following properties to be set:\nNow that we added in the planners, we can add a stage that will move the robot.\nThe following lines use aMoveTostage (a propagator stage). Since opening the hand is a relatively simple movement, we can use the joint interpolation planner.\nThis stage plans a move to the “open hand” pose, which is a named pose defined in theSRDFfor the Panda robot.\nWe return the task and finish with thecreateTask()function.\nFinally, we havemain: the following lines create a node using the class defined above, and calls the class methods to set up and execute a basic MTC task. In this example, we do not cancel the executor once the task has finished executing to keep the node alive to inspect the solutions in RViz.\nWe will need a launch file to launch themove_group,ros2_control,static_tf,robot_state_publisher, andrviznodes that provide us the environment to run the demo. The one we will use for this example can be foundhere.\nTo run the MoveIt Task Constructor node, we will use a second launch file to start themtc_tutorialexecutable with the proper parameters. Here we can load URDF, SRDF, and OMPL parameters, or use MoveIt Configs Utils to do so. Your launch file should look something like the one found in this tutorial packagehere(pay close attention to thepackageandexecutablearguments below as they are different from the launch file linked) :\nSave a launch file aspick_place_demo.launch.pyor download one to the package’s launch directory. Make sure to edit theCMakeLists.txtso it includes the launch folder by adding the following lines:\nNow we can build and source the colcon workspace.\nStart by launching the first launch file. If you want to use the one provided by the tutorials:\nRViz will now load. If you’re using your own launch file and haven’t included an rviz configsuch as this, you will need to configure RViz before you see anything displayed. If you’re using the launch file from the tutorials package, RViz will already be configured for you and you can jump to the end of the next section.\nIf you are not using the RViz configuration provided, we’ll have to make some changes to the RViz configuration to see your robot and the MoveIt Task Constructor solutions. First, start RViz. The following steps will cover how to set up RViz for MoveIt Task Constructor solution visualization.\nIf theMotionPlanningdisplay is active, uncheck it to hide it for now.\nUnderGlobal Options, change theFixed Framefrommaptopanda_link0if not already done.\nOn the bottom left of the window, click theAddbutton.\nUndermoveit_task_constructor_visualizationselectMotion Planning Tasksand click OK. TheMotion Planning Tasksdisplay should appear on the bottom left.\nIn theDisplays, underMotion Planning Tasks,  changeTask Solution Topicto/solution\nYou should see the panda arm in the main view with Motion Planning Tasks display open in the bottom left and nothing in it. Your MTC task will show up in this panel once you launch themtc_tutorialnode. If you’re usingmtc_demo.launch.pyfrom the tutorials, jump back in here.\nLaunch themtc_tutorialnode with\nYou should see the arm execute the task with the single stage to open the hand, with the cylinder in green in front of it. It should look something like this:\nIf you haven’t made your own package, but still want to see what this looks like, you can launch this file from the tutorials:\nSo far, we’ve walked through creating and executing a simple task, which runs but does not do much. Now, we will start adding the pick-and-place stages to the task. The image below shows an outline of the stages we will use in our task.\nWe will start adding stages after our existing open hand stage. Openmtc_node.cppand locate the following lines:\nWe need to move the arm to a position where we can pick up our object. This is done with aConnectstage, which as its name implies, is a Connector stage. This means that it tries to bridge between the results of the stage before and after it. This stage is initialized with a name,move_to_pick, and aGroupPlannerVectorthat specifies the planning group and the planner. We then set a timeout for the stage, set the properties for the stage, and add it to our task.\nNext, we create a pointer to a MoveIt Task Constructor stage object, and set it tonullptrfor now. Later, we will use this to save a stage.\nThis next block of code creates aSerialContainer.\nThis is a container that can be added to our task and can hold several substages.\nIn this case, we create a serial container that will contain the stages relevant to the picking action.\nInstead of adding the stages to the task, we will add the relevant stages to the serial container. We useexposeTo()to declare the task properties from the parent task in the new serial container, and useconfigureInitFrom()to initialize them.\nThis allows the contained stages to access these properties.\nWe then create a stage to approach the object. This stage is aMoveRelativestage, which allows us to specify a relative movement from our current position.MoveRelativeis a propagator stage: it receives the solution from its neighbouring stages and propagates it to the next or previous stage. Usingcartesian_plannerfinds a solution that involves moving the end effector in a straight line. We set the properties, and set the minimum and maximum distance to move. Now we create aVector3Stampedmessage to indicate the direction we want to move - in this case, in the Z direction from the hand frame. Finally, we add this stage to our serial container\nNow, create a stage to generate the grasp pose.\nThis is a generator stage, so it computes its results without regard to the stages before and after it.\nThe first stage,CurrentStateis a generator stage as well - to connect the first stage and this stage, a connecting stage must be used, which we already created above.\nThis code sets the stage properties, sets the pose before grasping, the angle delta, and the monitored stage.\nAngle delta is a property of theGenerateGraspPosestage that is used to determine the number of poses to generate; when generating solutions, MoveIt Task Constructor will try to grasp the object from many different orientations, with the difference between the orientations specified by the angle delta. The smaller the delta, the closer together the grasp orientations will be. When defining the current stage, we setcurrent_state_ptr, which is now used to forward information about the object pose and shape to the inverse kinematics solver.\nThis stage won’t be directly added to the serial container like previously, as we still need to do inverse kinematics on the poses it generates.\nBefore we compute inverse kinematics for the poses generated above, we first need to define the frame. This can be done with aPoseStampedmessage fromgeometry_msgsor in this case, we define the transform using Eigen transformation matrix and the name of the relevant link. Here, we define the transformation matrix.\nNow, we create theComputeIKstage, and give it the namegenerateposeIKas well as thegenerategraspposestage defined above. Some robots have multiple inverse kinematics solutions for a given pose - we set the limit on the amount of solutions to solve for up to 8. We also set the minimum solution distance, which is a threshold on how different solutions must be: if the joint positions in a solution are too similar to a previous solution, it will be marked as invalid. Next, we configure some additional properties, and add theComputeIKstage to the serial container.\nTo pick up the object, we must allow collision between the hand and the object. This can be done with aModifyPlanningScenestage. TheallowCollisionsfunction lets us specify which collisions to disable.allowCollisionscan be used with a container of names, so we can usegetLinkModelNamesWithCollisionGeometryto get all the names of links with collision geometry in the hand group.\nWith collisions allowed, we now can close the hand. This is done with aMoveTostage, similarly to theopenhandstage from above, except moving to thecloseposition as defined in the SRDF.\nWe now use aModifyPlanningScenestage again, this time to attach the object to the hand usingattachObject. Similarly to what we did with thecurrent_state_ptr, we get a pointer to this stage for later use when generating the place pose for the object.\nNext, we lift the object with aMoveRelativestage, similarly to theapproach_objectstage.\nWith this, we have all the stages needed to pick the object. Now, we add the serial container (with all its substages) to the task. If you build the package as-is, you can see the robot plan to pick up the object.\nTo test out the newly created stage, build the code and execute:\nNow that the stages that define the pick are complete, we move on to defining the stages for placing the object. Picking up where we left off, we add aConnectstage to connect the two, as we will soon be using a generator stage to generate the pose for placing the object.\nWe also create a serial container for the place stages. This is done similarly to the pick serial container.\nThe next stages will be added to the serial container rather than the task.\nThis next stage generates the poses used to place the object and compute the inverse kinematics for those poses - it is somewhat similar to thegenerategraspposestage from the pick serial container.\nWe start by creating a stage to generate the poses and inheriting the task properties.\nWe specify the pose where we want to place the object with aPoseStampedmessage fromgeometry_msgs- in this case, we choosey=0.5in the\"object\"frame.\nWe then pass the target pose to the stage withsetPose.\nNext, we usesetMonitoredStageand pass it the pointer to theattach_objectstage from earlier.\nThis allows the stage to know how the object is attached.\nWe then create aComputeIKstage and pass it ourGeneratePlacePosestage - the rest follows the same logic as above with the pick stages.\nNow that we’re ready to place the object, we open the hand withMoveTostage and the joint interpolation planner.\nWe also can re-enable collisions with the object now that we no longer need to hold it.\nThis is done usingallowCollisionsalmost exactly the same way as disabling collisions, except setting the last argument tofalserather thantrue.\nNow, we can detach the object usingdetachObject.\nWe retreat from the object using aMoveRelativestage, which is done similarly to theapproachobjectandliftobjectstages.\nWe finish our place serial container and add it to the task.\nThe final step is to return home: we use aMoveTostage and pass it the goal pose ofready, which is a pose defined in the Panda SRDF.\nAll these stages should be added above these lines.\nCongratulations! You’ve now defined a pick and place task using MoveIt Task Constructor! To try it out, build the code and execute:\nThe task with each comprising stage is shown in the Motion Planning Tasks pane. Click on a stage and additional information about the stage will show up to the right. The right pane shows different solutions as well as their associated costs. Depending on the stage type and the robot configuration, there may only be one solution shown.\nClick one of the solution costs to see an animation of the robot following the plan for that stage. Click the “Exec” button in the upper-right portion of the pane to execute the motion.\nTo run the complete MoveIt Task Constructor example included with the MoveIt tutorials:\nAnd in a second terminal:\nWhen running MTC, it prints a diagram like this to terminal:\nThis example^ shows two stages. The first stage (“initial_state”) is aCurrentStatetype of stage, which initializes aPlanningSceneand captures any collision objects that are present at that moment.\nA pointer to this stage can be used to retrieve the state of the robot.\nSinceCurrentStateinherits fromGenerator, it propagates solutions both forward and backward.\nThis is denoted by the arrows in both directions.\nThe first1indicates that one solution was successfully propagated backwards to the previous stage.\nThe second1, between the arrows, indicates that one solution was generated.\nThe0indicates that a solution was not propagated forward successfully to the next stage, because the next stage failed.\nThe second stage (“move_to_home”) is aMoveTotype of stage. It inherits its propagation direction from the previous stage, so both arrows point forward. The0’s indicate that this stage failed completely. From left to right, the0’s mean:\nThe stage did not receive a solution from the previous stage\nThe stage did not generate a solution\nThe stage did not propagate a solution forward to the next stage\nIn this case, we could tell that “move_to_home” was the root cause of the failure. The problem was a home state that was in collision. Defining a new, collision-free home position fixed the issue.\nInformation about individual stages can be retrieved from the task. For example, here we retrieve the unique ID for a stage:\nACurrentStatetype stage does not just retrieve the current state of the robot.\nIt also initializes aPlanningSceneobject, capturing any collision objects that are present at that moment.\nMTC stages can be propagated in forward and backward order.\nYou can easily check which direction a stage propagates by the arrow in the RViz GUI.\nWhen propagating backwards, the logic of many operations is reversed.\nFor example, to allow collisions with an object in aModifyPlanningScenestage, you would callallowCollisions(false)rather thanallowCollisions(true). There is a discussion to be readhere.\n© Copyright 2024, PickNik Robotics.\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit_task_constructor.git\nrosdep install --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncd~/ws_moveitcolconbuild--mixinrelease\nros2launchmoveit_task_constructor_demodemo.launch.py\nros2launchmoveit_task_constructor_demorun.launch.pyexe:=cartesianros2launchmoveit_task_constructor_demorun.launch.pyexe:=modularros2launchmoveit_task_constructor_demorun.launch.pyexe:=pick_place_demo\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_task_constructor_corerclcpp\\--node-namemtc_nodemtc_tutorial\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endifstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;classMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};MTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}rclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}voidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;pose.orientation.w=1.0;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}voidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}mtc::TaskMTCTaskNode::createTask(){mtc::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);// Disable warnings for this line, as it's a variable that's set but not used in this example#pragma GCC diagnostic push#pragma GCC diagnostic ignored \"-Wunused-but-set-variable\"mtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generator#pragma GCC diagnostic popautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));autosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);autointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();autocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);autostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}intmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endif\nstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;\nclassMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};\nMTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}\nrclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}\nvoidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}\nvoidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}\nmtc::TaskMTCTaskNode::createTask(){moveit::task_constructor::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);\nmtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generatorautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));\nautosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);\nautointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}\nintmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\nfromlaunchimportLaunchDescriptionfromlaunch_ros.actionsimportNodefrommoveit_configs_utilsimportMoveItConfigsBuilderdefgenerate_launch_description():moveit_config=MoveItConfigsBuilder(\"moveit_resources_panda\").to_dict()# MTC Demo nodepick_place_demo=Node(package=\"mtc_tutorial\",executable=\"mtc_node\",output=\"screen\",parameters=[moveit_config,],)returnLaunchDescription([pick_place_demo])\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}\n  )\ncd~/ws_moveitcolconbuild--mixinreleasesource~/ws_moveit/install/setup.bash\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo_minimal.launch.py\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));// Add the next lines of codes to define more stages here\nautostage_move_to_pick=std::make_unique<mtc::stages::Connect>(\"move to pick\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner}});stage_move_to_pick->setTimeout(5.0);stage_move_to_pick->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_pick));\nmtc::Stage*attach_object_stage=nullptr;// Forward attach_object_stage to place pose generator\n{autograsp=std::make_unique<mtc::SerialContainer>(\"pick object\");task.properties().exposeTo(grasp->properties(),{\"eef\",\"group\",\"ik_frame\"});grasp->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"approach object\",cartesian_planner);stage->properties().set(\"marker_ns\",\"approach_object\");stage->properties().set(\"link\",hand_frame);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.15);// Set hand forward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=hand_frame;vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\n{// Sample grasp poseautostage=std::make_unique<mtc::stages::GenerateGraspPose>(\"generate grasp pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"grasp_pose\");stage->setPreGraspPose(\"open\");stage->setObject(\"object\");stage->setAngleDelta(M_PI/12);stage->setMonitoredStage(current_state_ptr);// Hook into current state\nEigen::Isometry3dgrasp_frame_transform;Eigen::Quaterniondq=Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitX())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitY())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitZ());grasp_frame_transform.linear()=q.matrix();grasp_frame_transform.translation().z()=0.1;\n// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"grasp pose IK\",std::move(stage));wrapper->setMaxIKSolutions(8);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(grasp_frame_transform,hand_frame);wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});grasp->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"allow collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),true);grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"close hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"close\");grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"attach object\");stage->attachObject(\"object\",hand_frame);attach_object_stage=stage.get();grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"lift object\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"lift_object\");// Set upward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\ntask.add(std::move(grasp));}\nros2launchmtc_tutorialpick_place_demo.launch.py\n{autostage_move_to_place=std::make_unique<mtc::stages::Connect>(\"move to place\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner},{hand_group_name,interpolation_planner}});stage_move_to_place->setTimeout(5.0);stage_move_to_place->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_place));}\n{autoplace=std::make_unique<mtc::SerialContainer>(\"place object\");task.properties().exposeTo(place->properties(),{\"eef\",\"group\",\"ik_frame\"});place->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{// Sample place poseautostage=std::make_unique<mtc::stages::GeneratePlacePose>(\"generate place pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"place_pose\");stage->setObject(\"object\");geometry_msgs::msg::PoseStampedtarget_pose_msg;target_pose_msg.header.frame_id=\"object\";target_pose_msg.pose.position.y=0.5;target_pose_msg.pose.orientation.w=1.0;stage->setPose(target_pose_msg);stage->setMonitoredStage(attach_object_stage);// Hook into attach_object_stage// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"place pose IK\",std::move(stage));wrapper->setMaxIKSolutions(2);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(\"object\");wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});place->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"open\");place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"forbid collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),false);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"detach object\");stage->detachObject(\"object\",hand_frame);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"retreat\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"retreat\");// Set retreat directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.x=-0.5;stage->setDirection(vec);place->insert(std::move(stage));}\ntask.add(std::move(place));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"return home\",interpolation_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setGoal(\"ready\");task.add(std::move(stage));}\n// Stages all added to the task above this linereturntask;}\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmoveit2_tutorialspick_place_demo.launch.py\n[demo_node-1]1-←1→-0/initial_state[demo_node-1]-0→0→-0/move_to_home\nuint32_tconstunique_stage_id=task_.stages()->findChild(stage_name)->introspectionId();\n<branch>\nhumble\nros2\nmain\nmtc_tutorial\nmoveit_task_constructor_core\nsrc/mtc_node\nmtc_node.cpp\nrclcpp/rclcpp.hpp\nmoveit/planning_scene/planning_scene.h\nmoveit/planning_scene_interface/planning_scene_interface.h\nmoveit/task_constructor/task.h\nmoveit/task_constructor/solvers.h\nmoveit/task_constructor/stages.h\ntf2_geometry_msgs/tf2_geometry_msgs.hpp\ntf2_eigen/tf2_eigen.hpp\nmoveit::task_constructor\nMTCTaskNode\nobject.primitives[0].dimensions\npose.position.x\npose.position.y\ncreateTask\ntask.init()\ntask.plan(5)\ntask.execute()\ntask.setProperty(property_name,value)\ncurrent_state_ptr\nnullptr\ncurrent_state\nCurrentState\ncurrent_state_ptr\nMoveTo\ncreateTask()\nmain\nmove_group\nros2_control\nstatic_tf\nrobot_state_publisher\nrviz\nmtc_tutorial\npackage\nexecutable\npick_place_demo.launch.py\nCMakeLists.txt\nmap\npanda_link0\nmoveit_task_constructor_visualization\n/solution\nmtc_tutorial\nmtc_demo.launch.py\nmtc_tutorial\nmtc_node.cpp\nConnect\nmove_to_pick\nGroupPlannerVector\nnullptr\nSerialContainer\nexposeTo()\nconfigureInitFrom()\nMoveRelative\nMoveRelative\ncartesian_planner\nVector3Stamped\nCurrentState\nGenerateGraspPose\ncurrent_state_ptr\nPoseStamped\ngeometry_msgs\nComputeIK\ngenerateposeIK\ngenerategrasppose\nComputeIK\nModifyPlanningScene\nallowCollisions\nallowCollisions\ngetLinkModelNamesWithCollisionGeometry\nMoveTo\nopenhand\nclose\nModifyPlanningScene\nattachObject\ncurrent_state_ptr\nMoveRelative\napproach_object\nConnect\ngenerategrasppose\nPoseStamped\ngeometry_msgs\ny=0.5\n\"object\"\nsetPose\nsetMonitoredStage\nattach_object\nComputeIK\nGeneratePlacePose\nMoveTo\nallowCollisions\nfalse\ntrue\ndetachObject\nMoveRelative\napproachobject\nliftobject\nMoveTo\nready\nCurrentState\nPlanningScene\nCurrentState\nGenerator\n1\n1\n0\nMoveTo\n0\n0\nCurrentState\nPlanningScene\nModifyPlanningScene\nallowCollisions(false)\nallowCollisions(true)",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0db"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/pick_and_place_with_moveit_task_constructor/pick_and_place_with_moveit_task_constructor.html#code-breakdown",
  "title": "Pick and Place with MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "Pick and Place with MoveIt Task Constructor\n1 Basic Concepts\n2 Getting Started\n3 Trying It Out\n4 Setting up a Project with MoveIt Task Constructor\n5 Running the Demo\n6 Adding Stages\n7 Further Discussion\nThis tutorial will walk you through creating a package that plans a pick and place operation usingMoveIt Task Constructor. MoveIt Task Constructor provides a way to plan for tasks that consist of multiple different subtasks (known as stages). If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nThe fundamental idea of MTC is that complex motion planning problems can be composed into a set of simpler subproblems.\nThe top-level planning problem is specified as aTaskwhile all subproblems are specified byStages.\nStages can be arranged in any arbitrary order and hierarchy only limited by the individual stages types.\nThe order in which stages can be arranged is restricted by the direction in which results are passed.\nThere are three possible stages relating to the result flow: generator, propagator, and connector stages:\nGeneratorscompute their results independently of their neighbor stages and pass them in both directions, backwards and forwards.\nAn example is an IK sampler for geometric poses where approaching and departing motions (neighbor stages) depend on the solution.\nPropagatorsreceive the result of one neighbor stage, solve a subproblem and then propagate their result to the neighbor on the opposite site.\nDepending on the implementation, propagating stages can pass solutions forward, backward or in both directions separately.\nAn example is a stage that computes a Cartesian path based on either a start or a goal state.\nConnectorsdo not propagate any results, but rather attempt to bridge the gap between the resulting states of both neighbors.\nAn example is the computation of a free-motion plan from one given state to another.\nAdditional to the order types, there are different hierarchy types allowing to encapsulate subordinate stages.\nStages without subordinate stages are calledprimitive stages, higher-level stages are calledcontainer stages.\nThere are three container types:\nWrappersencapsulate a single subordinate stage and modify or filter the results.\nFor example, a filter stage that only accepts solutions of its child stage that satisfy a certain constraint can be realized as a wrapper.\nAnother standard use of this type includes the IK wrapper stage, which generates inverse kinematics solutions based on planning scenes annotated with a pose target property.\nSerial Containershold a sequence of subordinate stages and only consider end-to-end solutions as results.\nAn example is a picking motion that consists of a sequence of coherent steps.\nParallel Containerscombine set of subordinate stages and can be used for passing the best of alternative results, running fallback solvers or for merging multiple independent solutions.\nExamples are running alternative planners for a free-motion plan, picking objects with the right hand or with the left hand as a fallback, or moving the arm and opening the gripper at the same time.\nStages not only support solving motion planning problems.\nThey can also be used for all kinds of state transitions, as for instance modifying the planning scene.\nCombined with the possibility of using class inheritance it is possible to construct very complex behavior while only relying on a well-structured set of primitive stages.\nMore detailed information on MTC can be found in theMoveIt Task Constructor concepts page\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nMove into your colcon workspace and pull the MoveIt Task Constructor source, where<branch>can be e.g.humblefor ROS Humble, orros2for the latest version compatible with MoveIt 2main:\nInstall missing packages with rosdep:\nBuild the workspace:\nThe MoveIt Task Constructor package contains several basic examples and a pick-and-place demo.\nFor all demos you should launch the basic environment:\nSubsequently, you can run the individual demos:\nOn the right side you should see theMotion Planning Taskspanel outlining the hierarchical stage structure of the tasks.\nWhen you select a particular stage, the list of successful and failed solutions will be\nshown in the right-most window. Selecting one of those solutions will start its visualization.\nThis section walks through the steps required to build a simple task with MoveIt Task Constructor.\nCreate a new package with the following command:\nThis will create a new package and folder calledmtc_tutorialwith a dependency onmoveit_task_constructor_coreas well as a hello world example insrc/mtc_node.\nOpenmtc_node.cppin your editor of choice, and paste in the following code.\nThe top of the code includes the ROS and MoveIt Libraries that this package uses.\nrclcpp/rclcpp.hppincludes core ROS2 functionality\nmoveit/planning_scene/planning_scene.handmoveit/planning_scene_interface/planning_scene_interface.hinclude functionality to interface with the robot model and collision objects\nmoveit/task_constructor/task.h,moveit/task_constructor/solvers.h, andmoveit/task_constructor/stages.hinclude different components of MoveIt Task Constructor that are used in the example\ntf2_geometry_msgs/tf2_geometry_msgs.hppandtf2_eigen/tf2_eigen.hppwon’t be used in this initial example, but they will be used for pose generation when we add more stages to the MoveIt Task Constructor task.\nThe next line gets a logger for our new node. We also create a namespace alias formoveit::task_constructorfor convenience.\nWe start by defining a class that will contain the main MoveIt Task Constructor functionality.  We also declare the MoveIt Task Constructor task object as a member variable for our class: this isn’t strictly necessary for a given application, but it helps save the task for later visualization purposes. We will explore each function individually below.\nThese lines initialize the node with specified options (it is the constructor of ourMTCTaskNodeclass).\nThese next lines define a getter function to get the node base interface, which will be used for the executor later.\nThis class method is used to set up the planning scene that is used in the example. It creates a cylinder with dimensions specified byobject.primitives[0].dimensionsand position specified bypose.position.xandpose.position.y.\nYou can try changing these numbers to resize and move the cylinder around. If we move the cylinder out of the robot’s reach, planning will fail.\nThis function interfaces with the MoveIt Task Constructor task object. It first creates a task, which includes setting some properties and adding stages. This will be discussed further in thecreateTaskfunction definition. Next,task.init()initializes the task andtask.plan(5)generates a plan, stopping after 5 successful plans are found. The next line publishes the solution to be visualized in RViz - this line can be removed if you don’t care for visualization. Finally,task.execute()executes the plan. Execution occurs via an action server interface with the RViz plugin.\nAs mentioned above, this function creates a MoveIt Task Constructor object and sets some initial properties. In this case, we set the task name to “demo_task”, load the robot model, define the names of some useful frames, and set those frame names as properties of the task withtask.setProperty(property_name,value). The next few code blocks will fill out this function body.\nNow, we add an example stage to the node. The first line setscurrent_state_ptrtonullptr; this creates a pointer to a stage such that we can reuse stage information in specific scenarios. This line is not used at this moment, but will be used later when more stages are added to the task. Next, we make acurrent_statestage (a generator stage) and add it to our task - this starts the robot off in its current state. Now that we’ve created theCurrentStatestage, we save a pointer to it in thecurrent_state_ptrfor later use.\nSolvers are used to define the type of robot motion. MoveIt Task Constructor has three options for solvers:\nPipelinePlanneruses MoveIt’s planning pipeline, which typically defaults toOMPL.\nJointInterpolationis a simple planner that interpolates between the start and goal joint states. It is typically used for simple motions as it computes quickly but doesn’t support complex motions.\nCartesianPathis used to move the end effector in a straight line in Cartesian space.\nFeel free to try out the different solvers and see how the robot motion changes. For the first stage we will use the Cartesian planner, which requires the following properties to be set:\nNow that we added in the planners, we can add a stage that will move the robot.\nThe following lines use aMoveTostage (a propagator stage). Since opening the hand is a relatively simple movement, we can use the joint interpolation planner.\nThis stage plans a move to the “open hand” pose, which is a named pose defined in theSRDFfor the Panda robot.\nWe return the task and finish with thecreateTask()function.\nFinally, we havemain: the following lines create a node using the class defined above, and calls the class methods to set up and execute a basic MTC task. In this example, we do not cancel the executor once the task has finished executing to keep the node alive to inspect the solutions in RViz.\nWe will need a launch file to launch themove_group,ros2_control,static_tf,robot_state_publisher, andrviznodes that provide us the environment to run the demo. The one we will use for this example can be foundhere.\nTo run the MoveIt Task Constructor node, we will use a second launch file to start themtc_tutorialexecutable with the proper parameters. Here we can load URDF, SRDF, and OMPL parameters, or use MoveIt Configs Utils to do so. Your launch file should look something like the one found in this tutorial packagehere(pay close attention to thepackageandexecutablearguments below as they are different from the launch file linked) :\nSave a launch file aspick_place_demo.launch.pyor download one to the package’s launch directory. Make sure to edit theCMakeLists.txtso it includes the launch folder by adding the following lines:\nNow we can build and source the colcon workspace.\nStart by launching the first launch file. If you want to use the one provided by the tutorials:\nRViz will now load. If you’re using your own launch file and haven’t included an rviz configsuch as this, you will need to configure RViz before you see anything displayed. If you’re using the launch file from the tutorials package, RViz will already be configured for you and you can jump to the end of the next section.\nIf you are not using the RViz configuration provided, we’ll have to make some changes to the RViz configuration to see your robot and the MoveIt Task Constructor solutions. First, start RViz. The following steps will cover how to set up RViz for MoveIt Task Constructor solution visualization.\nIf theMotionPlanningdisplay is active, uncheck it to hide it for now.\nUnderGlobal Options, change theFixed Framefrommaptopanda_link0if not already done.\nOn the bottom left of the window, click theAddbutton.\nUndermoveit_task_constructor_visualizationselectMotion Planning Tasksand click OK. TheMotion Planning Tasksdisplay should appear on the bottom left.\nIn theDisplays, underMotion Planning Tasks,  changeTask Solution Topicto/solution\nYou should see the panda arm in the main view with Motion Planning Tasks display open in the bottom left and nothing in it. Your MTC task will show up in this panel once you launch themtc_tutorialnode. If you’re usingmtc_demo.launch.pyfrom the tutorials, jump back in here.\nLaunch themtc_tutorialnode with\nYou should see the arm execute the task with the single stage to open the hand, with the cylinder in green in front of it. It should look something like this:\nIf you haven’t made your own package, but still want to see what this looks like, you can launch this file from the tutorials:\nSo far, we’ve walked through creating and executing a simple task, which runs but does not do much. Now, we will start adding the pick-and-place stages to the task. The image below shows an outline of the stages we will use in our task.\nWe will start adding stages after our existing open hand stage. Openmtc_node.cppand locate the following lines:\nWe need to move the arm to a position where we can pick up our object. This is done with aConnectstage, which as its name implies, is a Connector stage. This means that it tries to bridge between the results of the stage before and after it. This stage is initialized with a name,move_to_pick, and aGroupPlannerVectorthat specifies the planning group and the planner. We then set a timeout for the stage, set the properties for the stage, and add it to our task.\nNext, we create a pointer to a MoveIt Task Constructor stage object, and set it tonullptrfor now. Later, we will use this to save a stage.\nThis next block of code creates aSerialContainer.\nThis is a container that can be added to our task and can hold several substages.\nIn this case, we create a serial container that will contain the stages relevant to the picking action.\nInstead of adding the stages to the task, we will add the relevant stages to the serial container. We useexposeTo()to declare the task properties from the parent task in the new serial container, and useconfigureInitFrom()to initialize them.\nThis allows the contained stages to access these properties.\nWe then create a stage to approach the object. This stage is aMoveRelativestage, which allows us to specify a relative movement from our current position.MoveRelativeis a propagator stage: it receives the solution from its neighbouring stages and propagates it to the next or previous stage. Usingcartesian_plannerfinds a solution that involves moving the end effector in a straight line. We set the properties, and set the minimum and maximum distance to move. Now we create aVector3Stampedmessage to indicate the direction we want to move - in this case, in the Z direction from the hand frame. Finally, we add this stage to our serial container\nNow, create a stage to generate the grasp pose.\nThis is a generator stage, so it computes its results without regard to the stages before and after it.\nThe first stage,CurrentStateis a generator stage as well - to connect the first stage and this stage, a connecting stage must be used, which we already created above.\nThis code sets the stage properties, sets the pose before grasping, the angle delta, and the monitored stage.\nAngle delta is a property of theGenerateGraspPosestage that is used to determine the number of poses to generate; when generating solutions, MoveIt Task Constructor will try to grasp the object from many different orientations, with the difference between the orientations specified by the angle delta. The smaller the delta, the closer together the grasp orientations will be. When defining the current stage, we setcurrent_state_ptr, which is now used to forward information about the object pose and shape to the inverse kinematics solver.\nThis stage won’t be directly added to the serial container like previously, as we still need to do inverse kinematics on the poses it generates.\nBefore we compute inverse kinematics for the poses generated above, we first need to define the frame. This can be done with aPoseStampedmessage fromgeometry_msgsor in this case, we define the transform using Eigen transformation matrix and the name of the relevant link. Here, we define the transformation matrix.\nNow, we create theComputeIKstage, and give it the namegenerateposeIKas well as thegenerategraspposestage defined above. Some robots have multiple inverse kinematics solutions for a given pose - we set the limit on the amount of solutions to solve for up to 8. We also set the minimum solution distance, which is a threshold on how different solutions must be: if the joint positions in a solution are too similar to a previous solution, it will be marked as invalid. Next, we configure some additional properties, and add theComputeIKstage to the serial container.\nTo pick up the object, we must allow collision between the hand and the object. This can be done with aModifyPlanningScenestage. TheallowCollisionsfunction lets us specify which collisions to disable.allowCollisionscan be used with a container of names, so we can usegetLinkModelNamesWithCollisionGeometryto get all the names of links with collision geometry in the hand group.\nWith collisions allowed, we now can close the hand. This is done with aMoveTostage, similarly to theopenhandstage from above, except moving to thecloseposition as defined in the SRDF.\nWe now use aModifyPlanningScenestage again, this time to attach the object to the hand usingattachObject. Similarly to what we did with thecurrent_state_ptr, we get a pointer to this stage for later use when generating the place pose for the object.\nNext, we lift the object with aMoveRelativestage, similarly to theapproach_objectstage.\nWith this, we have all the stages needed to pick the object. Now, we add the serial container (with all its substages) to the task. If you build the package as-is, you can see the robot plan to pick up the object.\nTo test out the newly created stage, build the code and execute:\nNow that the stages that define the pick are complete, we move on to defining the stages for placing the object. Picking up where we left off, we add aConnectstage to connect the two, as we will soon be using a generator stage to generate the pose for placing the object.\nWe also create a serial container for the place stages. This is done similarly to the pick serial container.\nThe next stages will be added to the serial container rather than the task.\nThis next stage generates the poses used to place the object and compute the inverse kinematics for those poses - it is somewhat similar to thegenerategraspposestage from the pick serial container.\nWe start by creating a stage to generate the poses and inheriting the task properties.\nWe specify the pose where we want to place the object with aPoseStampedmessage fromgeometry_msgs- in this case, we choosey=0.5in the\"object\"frame.\nWe then pass the target pose to the stage withsetPose.\nNext, we usesetMonitoredStageand pass it the pointer to theattach_objectstage from earlier.\nThis allows the stage to know how the object is attached.\nWe then create aComputeIKstage and pass it ourGeneratePlacePosestage - the rest follows the same logic as above with the pick stages.\nNow that we’re ready to place the object, we open the hand withMoveTostage and the joint interpolation planner.\nWe also can re-enable collisions with the object now that we no longer need to hold it.\nThis is done usingallowCollisionsalmost exactly the same way as disabling collisions, except setting the last argument tofalserather thantrue.\nNow, we can detach the object usingdetachObject.\nWe retreat from the object using aMoveRelativestage, which is done similarly to theapproachobjectandliftobjectstages.\nWe finish our place serial container and add it to the task.\nThe final step is to return home: we use aMoveTostage and pass it the goal pose ofready, which is a pose defined in the Panda SRDF.\nAll these stages should be added above these lines.\nCongratulations! You’ve now defined a pick and place task using MoveIt Task Constructor! To try it out, build the code and execute:\nThe task with each comprising stage is shown in the Motion Planning Tasks pane. Click on a stage and additional information about the stage will show up to the right. The right pane shows different solutions as well as their associated costs. Depending on the stage type and the robot configuration, there may only be one solution shown.\nClick one of the solution costs to see an animation of the robot following the plan for that stage. Click the “Exec” button in the upper-right portion of the pane to execute the motion.\nTo run the complete MoveIt Task Constructor example included with the MoveIt tutorials:\nAnd in a second terminal:\nWhen running MTC, it prints a diagram like this to terminal:\nThis example^ shows two stages. The first stage (“initial_state”) is aCurrentStatetype of stage, which initializes aPlanningSceneand captures any collision objects that are present at that moment.\nA pointer to this stage can be used to retrieve the state of the robot.\nSinceCurrentStateinherits fromGenerator, it propagates solutions both forward and backward.\nThis is denoted by the arrows in both directions.\nThe first1indicates that one solution was successfully propagated backwards to the previous stage.\nThe second1, between the arrows, indicates that one solution was generated.\nThe0indicates that a solution was not propagated forward successfully to the next stage, because the next stage failed.\nThe second stage (“move_to_home”) is aMoveTotype of stage. It inherits its propagation direction from the previous stage, so both arrows point forward. The0’s indicate that this stage failed completely. From left to right, the0’s mean:\nThe stage did not receive a solution from the previous stage\nThe stage did not generate a solution\nThe stage did not propagate a solution forward to the next stage\nIn this case, we could tell that “move_to_home” was the root cause of the failure. The problem was a home state that was in collision. Defining a new, collision-free home position fixed the issue.\nInformation about individual stages can be retrieved from the task. For example, here we retrieve the unique ID for a stage:\nACurrentStatetype stage does not just retrieve the current state of the robot.\nIt also initializes aPlanningSceneobject, capturing any collision objects that are present at that moment.\nMTC stages can be propagated in forward and backward order.\nYou can easily check which direction a stage propagates by the arrow in the RViz GUI.\nWhen propagating backwards, the logic of many operations is reversed.\nFor example, to allow collisions with an object in aModifyPlanningScenestage, you would callallowCollisions(false)rather thanallowCollisions(true). There is a discussion to be readhere.\n© Copyright 2024, PickNik Robotics.\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit_task_constructor.git\nrosdep install --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncd~/ws_moveitcolconbuild--mixinrelease\nros2launchmoveit_task_constructor_demodemo.launch.py\nros2launchmoveit_task_constructor_demorun.launch.pyexe:=cartesianros2launchmoveit_task_constructor_demorun.launch.pyexe:=modularros2launchmoveit_task_constructor_demorun.launch.pyexe:=pick_place_demo\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_task_constructor_corerclcpp\\--node-namemtc_nodemtc_tutorial\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endifstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;classMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};MTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}rclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}voidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;pose.orientation.w=1.0;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}voidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}mtc::TaskMTCTaskNode::createTask(){mtc::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);// Disable warnings for this line, as it's a variable that's set but not used in this example#pragma GCC diagnostic push#pragma GCC diagnostic ignored \"-Wunused-but-set-variable\"mtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generator#pragma GCC diagnostic popautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));autosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);autointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();autocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);autostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}intmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endif\nstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;\nclassMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};\nMTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}\nrclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}\nvoidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}\nvoidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}\nmtc::TaskMTCTaskNode::createTask(){moveit::task_constructor::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);\nmtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generatorautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));\nautosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);\nautointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}\nintmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\nfromlaunchimportLaunchDescriptionfromlaunch_ros.actionsimportNodefrommoveit_configs_utilsimportMoveItConfigsBuilderdefgenerate_launch_description():moveit_config=MoveItConfigsBuilder(\"moveit_resources_panda\").to_dict()# MTC Demo nodepick_place_demo=Node(package=\"mtc_tutorial\",executable=\"mtc_node\",output=\"screen\",parameters=[moveit_config,],)returnLaunchDescription([pick_place_demo])\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}\n  )\ncd~/ws_moveitcolconbuild--mixinreleasesource~/ws_moveit/install/setup.bash\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo_minimal.launch.py\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));// Add the next lines of codes to define more stages here\nautostage_move_to_pick=std::make_unique<mtc::stages::Connect>(\"move to pick\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner}});stage_move_to_pick->setTimeout(5.0);stage_move_to_pick->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_pick));\nmtc::Stage*attach_object_stage=nullptr;// Forward attach_object_stage to place pose generator\n{autograsp=std::make_unique<mtc::SerialContainer>(\"pick object\");task.properties().exposeTo(grasp->properties(),{\"eef\",\"group\",\"ik_frame\"});grasp->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"approach object\",cartesian_planner);stage->properties().set(\"marker_ns\",\"approach_object\");stage->properties().set(\"link\",hand_frame);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.15);// Set hand forward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=hand_frame;vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\n{// Sample grasp poseautostage=std::make_unique<mtc::stages::GenerateGraspPose>(\"generate grasp pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"grasp_pose\");stage->setPreGraspPose(\"open\");stage->setObject(\"object\");stage->setAngleDelta(M_PI/12);stage->setMonitoredStage(current_state_ptr);// Hook into current state\nEigen::Isometry3dgrasp_frame_transform;Eigen::Quaterniondq=Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitX())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitY())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitZ());grasp_frame_transform.linear()=q.matrix();grasp_frame_transform.translation().z()=0.1;\n// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"grasp pose IK\",std::move(stage));wrapper->setMaxIKSolutions(8);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(grasp_frame_transform,hand_frame);wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});grasp->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"allow collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),true);grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"close hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"close\");grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"attach object\");stage->attachObject(\"object\",hand_frame);attach_object_stage=stage.get();grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"lift object\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"lift_object\");// Set upward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\ntask.add(std::move(grasp));}\nros2launchmtc_tutorialpick_place_demo.launch.py\n{autostage_move_to_place=std::make_unique<mtc::stages::Connect>(\"move to place\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner},{hand_group_name,interpolation_planner}});stage_move_to_place->setTimeout(5.0);stage_move_to_place->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_place));}\n{autoplace=std::make_unique<mtc::SerialContainer>(\"place object\");task.properties().exposeTo(place->properties(),{\"eef\",\"group\",\"ik_frame\"});place->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{// Sample place poseautostage=std::make_unique<mtc::stages::GeneratePlacePose>(\"generate place pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"place_pose\");stage->setObject(\"object\");geometry_msgs::msg::PoseStampedtarget_pose_msg;target_pose_msg.header.frame_id=\"object\";target_pose_msg.pose.position.y=0.5;target_pose_msg.pose.orientation.w=1.0;stage->setPose(target_pose_msg);stage->setMonitoredStage(attach_object_stage);// Hook into attach_object_stage// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"place pose IK\",std::move(stage));wrapper->setMaxIKSolutions(2);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(\"object\");wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});place->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"open\");place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"forbid collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),false);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"detach object\");stage->detachObject(\"object\",hand_frame);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"retreat\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"retreat\");// Set retreat directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.x=-0.5;stage->setDirection(vec);place->insert(std::move(stage));}\ntask.add(std::move(place));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"return home\",interpolation_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setGoal(\"ready\");task.add(std::move(stage));}\n// Stages all added to the task above this linereturntask;}\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmoveit2_tutorialspick_place_demo.launch.py\n[demo_node-1]1-←1→-0/initial_state[demo_node-1]-0→0→-0/move_to_home\nuint32_tconstunique_stage_id=task_.stages()->findChild(stage_name)->introspectionId();\n<branch>\nhumble\nros2\nmain\nmtc_tutorial\nmoveit_task_constructor_core\nsrc/mtc_node\nmtc_node.cpp\nrclcpp/rclcpp.hpp\nmoveit/planning_scene/planning_scene.h\nmoveit/planning_scene_interface/planning_scene_interface.h\nmoveit/task_constructor/task.h\nmoveit/task_constructor/solvers.h\nmoveit/task_constructor/stages.h\ntf2_geometry_msgs/tf2_geometry_msgs.hpp\ntf2_eigen/tf2_eigen.hpp\nmoveit::task_constructor\nMTCTaskNode\nobject.primitives[0].dimensions\npose.position.x\npose.position.y\ncreateTask\ntask.init()\ntask.plan(5)\ntask.execute()\ntask.setProperty(property_name,value)\ncurrent_state_ptr\nnullptr\ncurrent_state\nCurrentState\ncurrent_state_ptr\nMoveTo\ncreateTask()\nmain\nmove_group\nros2_control\nstatic_tf\nrobot_state_publisher\nrviz\nmtc_tutorial\npackage\nexecutable\npick_place_demo.launch.py\nCMakeLists.txt\nmap\npanda_link0\nmoveit_task_constructor_visualization\n/solution\nmtc_tutorial\nmtc_demo.launch.py\nmtc_tutorial\nmtc_node.cpp\nConnect\nmove_to_pick\nGroupPlannerVector\nnullptr\nSerialContainer\nexposeTo()\nconfigureInitFrom()\nMoveRelative\nMoveRelative\ncartesian_planner\nVector3Stamped\nCurrentState\nGenerateGraspPose\ncurrent_state_ptr\nPoseStamped\ngeometry_msgs\nComputeIK\ngenerateposeIK\ngenerategrasppose\nComputeIK\nModifyPlanningScene\nallowCollisions\nallowCollisions\ngetLinkModelNamesWithCollisionGeometry\nMoveTo\nopenhand\nclose\nModifyPlanningScene\nattachObject\ncurrent_state_ptr\nMoveRelative\napproach_object\nConnect\ngenerategrasppose\nPoseStamped\ngeometry_msgs\ny=0.5\n\"object\"\nsetPose\nsetMonitoredStage\nattach_object\nComputeIK\nGeneratePlacePose\nMoveTo\nallowCollisions\nfalse\ntrue\ndetachObject\nMoveRelative\napproachobject\nliftobject\nMoveTo\nready\nCurrentState\nPlanningScene\nCurrentState\nGenerator\n1\n1\n0\nMoveTo\n0\n0\nCurrentState\nPlanningScene\nModifyPlanningScene\nallowCollisions(false)\nallowCollisions(true)",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0dc"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/pick_and_place_with_moveit_task_constructor/pick_and_place_with_moveit_task_constructor.html#running-the-demo",
  "title": "Pick and Place with MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "Pick and Place with MoveIt Task Constructor\n1 Basic Concepts\n2 Getting Started\n3 Trying It Out\n4 Setting up a Project with MoveIt Task Constructor\n5 Running the Demo\n6 Adding Stages\n7 Further Discussion\nThis tutorial will walk you through creating a package that plans a pick and place operation usingMoveIt Task Constructor. MoveIt Task Constructor provides a way to plan for tasks that consist of multiple different subtasks (known as stages). If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nThe fundamental idea of MTC is that complex motion planning problems can be composed into a set of simpler subproblems.\nThe top-level planning problem is specified as aTaskwhile all subproblems are specified byStages.\nStages can be arranged in any arbitrary order and hierarchy only limited by the individual stages types.\nThe order in which stages can be arranged is restricted by the direction in which results are passed.\nThere are three possible stages relating to the result flow: generator, propagator, and connector stages:\nGeneratorscompute their results independently of their neighbor stages and pass them in both directions, backwards and forwards.\nAn example is an IK sampler for geometric poses where approaching and departing motions (neighbor stages) depend on the solution.\nPropagatorsreceive the result of one neighbor stage, solve a subproblem and then propagate their result to the neighbor on the opposite site.\nDepending on the implementation, propagating stages can pass solutions forward, backward or in both directions separately.\nAn example is a stage that computes a Cartesian path based on either a start or a goal state.\nConnectorsdo not propagate any results, but rather attempt to bridge the gap between the resulting states of both neighbors.\nAn example is the computation of a free-motion plan from one given state to another.\nAdditional to the order types, there are different hierarchy types allowing to encapsulate subordinate stages.\nStages without subordinate stages are calledprimitive stages, higher-level stages are calledcontainer stages.\nThere are three container types:\nWrappersencapsulate a single subordinate stage and modify or filter the results.\nFor example, a filter stage that only accepts solutions of its child stage that satisfy a certain constraint can be realized as a wrapper.\nAnother standard use of this type includes the IK wrapper stage, which generates inverse kinematics solutions based on planning scenes annotated with a pose target property.\nSerial Containershold a sequence of subordinate stages and only consider end-to-end solutions as results.\nAn example is a picking motion that consists of a sequence of coherent steps.\nParallel Containerscombine set of subordinate stages and can be used for passing the best of alternative results, running fallback solvers or for merging multiple independent solutions.\nExamples are running alternative planners for a free-motion plan, picking objects with the right hand or with the left hand as a fallback, or moving the arm and opening the gripper at the same time.\nStages not only support solving motion planning problems.\nThey can also be used for all kinds of state transitions, as for instance modifying the planning scene.\nCombined with the possibility of using class inheritance it is possible to construct very complex behavior while only relying on a well-structured set of primitive stages.\nMore detailed information on MTC can be found in theMoveIt Task Constructor concepts page\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nMove into your colcon workspace and pull the MoveIt Task Constructor source, where<branch>can be e.g.humblefor ROS Humble, orros2for the latest version compatible with MoveIt 2main:\nInstall missing packages with rosdep:\nBuild the workspace:\nThe MoveIt Task Constructor package contains several basic examples and a pick-and-place demo.\nFor all demos you should launch the basic environment:\nSubsequently, you can run the individual demos:\nOn the right side you should see theMotion Planning Taskspanel outlining the hierarchical stage structure of the tasks.\nWhen you select a particular stage, the list of successful and failed solutions will be\nshown in the right-most window. Selecting one of those solutions will start its visualization.\nThis section walks through the steps required to build a simple task with MoveIt Task Constructor.\nCreate a new package with the following command:\nThis will create a new package and folder calledmtc_tutorialwith a dependency onmoveit_task_constructor_coreas well as a hello world example insrc/mtc_node.\nOpenmtc_node.cppin your editor of choice, and paste in the following code.\nThe top of the code includes the ROS and MoveIt Libraries that this package uses.\nrclcpp/rclcpp.hppincludes core ROS2 functionality\nmoveit/planning_scene/planning_scene.handmoveit/planning_scene_interface/planning_scene_interface.hinclude functionality to interface with the robot model and collision objects\nmoveit/task_constructor/task.h,moveit/task_constructor/solvers.h, andmoveit/task_constructor/stages.hinclude different components of MoveIt Task Constructor that are used in the example\ntf2_geometry_msgs/tf2_geometry_msgs.hppandtf2_eigen/tf2_eigen.hppwon’t be used in this initial example, but they will be used for pose generation when we add more stages to the MoveIt Task Constructor task.\nThe next line gets a logger for our new node. We also create a namespace alias formoveit::task_constructorfor convenience.\nWe start by defining a class that will contain the main MoveIt Task Constructor functionality.  We also declare the MoveIt Task Constructor task object as a member variable for our class: this isn’t strictly necessary for a given application, but it helps save the task for later visualization purposes. We will explore each function individually below.\nThese lines initialize the node with specified options (it is the constructor of ourMTCTaskNodeclass).\nThese next lines define a getter function to get the node base interface, which will be used for the executor later.\nThis class method is used to set up the planning scene that is used in the example. It creates a cylinder with dimensions specified byobject.primitives[0].dimensionsand position specified bypose.position.xandpose.position.y.\nYou can try changing these numbers to resize and move the cylinder around. If we move the cylinder out of the robot’s reach, planning will fail.\nThis function interfaces with the MoveIt Task Constructor task object. It first creates a task, which includes setting some properties and adding stages. This will be discussed further in thecreateTaskfunction definition. Next,task.init()initializes the task andtask.plan(5)generates a plan, stopping after 5 successful plans are found. The next line publishes the solution to be visualized in RViz - this line can be removed if you don’t care for visualization. Finally,task.execute()executes the plan. Execution occurs via an action server interface with the RViz plugin.\nAs mentioned above, this function creates a MoveIt Task Constructor object and sets some initial properties. In this case, we set the task name to “demo_task”, load the robot model, define the names of some useful frames, and set those frame names as properties of the task withtask.setProperty(property_name,value). The next few code blocks will fill out this function body.\nNow, we add an example stage to the node. The first line setscurrent_state_ptrtonullptr; this creates a pointer to a stage such that we can reuse stage information in specific scenarios. This line is not used at this moment, but will be used later when more stages are added to the task. Next, we make acurrent_statestage (a generator stage) and add it to our task - this starts the robot off in its current state. Now that we’ve created theCurrentStatestage, we save a pointer to it in thecurrent_state_ptrfor later use.\nSolvers are used to define the type of robot motion. MoveIt Task Constructor has three options for solvers:\nPipelinePlanneruses MoveIt’s planning pipeline, which typically defaults toOMPL.\nJointInterpolationis a simple planner that interpolates between the start and goal joint states. It is typically used for simple motions as it computes quickly but doesn’t support complex motions.\nCartesianPathis used to move the end effector in a straight line in Cartesian space.\nFeel free to try out the different solvers and see how the robot motion changes. For the first stage we will use the Cartesian planner, which requires the following properties to be set:\nNow that we added in the planners, we can add a stage that will move the robot.\nThe following lines use aMoveTostage (a propagator stage). Since opening the hand is a relatively simple movement, we can use the joint interpolation planner.\nThis stage plans a move to the “open hand” pose, which is a named pose defined in theSRDFfor the Panda robot.\nWe return the task and finish with thecreateTask()function.\nFinally, we havemain: the following lines create a node using the class defined above, and calls the class methods to set up and execute a basic MTC task. In this example, we do not cancel the executor once the task has finished executing to keep the node alive to inspect the solutions in RViz.\nWe will need a launch file to launch themove_group,ros2_control,static_tf,robot_state_publisher, andrviznodes that provide us the environment to run the demo. The one we will use for this example can be foundhere.\nTo run the MoveIt Task Constructor node, we will use a second launch file to start themtc_tutorialexecutable with the proper parameters. Here we can load URDF, SRDF, and OMPL parameters, or use MoveIt Configs Utils to do so. Your launch file should look something like the one found in this tutorial packagehere(pay close attention to thepackageandexecutablearguments below as they are different from the launch file linked) :\nSave a launch file aspick_place_demo.launch.pyor download one to the package’s launch directory. Make sure to edit theCMakeLists.txtso it includes the launch folder by adding the following lines:\nNow we can build and source the colcon workspace.\nStart by launching the first launch file. If you want to use the one provided by the tutorials:\nRViz will now load. If you’re using your own launch file and haven’t included an rviz configsuch as this, you will need to configure RViz before you see anything displayed. If you’re using the launch file from the tutorials package, RViz will already be configured for you and you can jump to the end of the next section.\nIf you are not using the RViz configuration provided, we’ll have to make some changes to the RViz configuration to see your robot and the MoveIt Task Constructor solutions. First, start RViz. The following steps will cover how to set up RViz for MoveIt Task Constructor solution visualization.\nIf theMotionPlanningdisplay is active, uncheck it to hide it for now.\nUnderGlobal Options, change theFixed Framefrommaptopanda_link0if not already done.\nOn the bottom left of the window, click theAddbutton.\nUndermoveit_task_constructor_visualizationselectMotion Planning Tasksand click OK. TheMotion Planning Tasksdisplay should appear on the bottom left.\nIn theDisplays, underMotion Planning Tasks,  changeTask Solution Topicto/solution\nYou should see the panda arm in the main view with Motion Planning Tasks display open in the bottom left and nothing in it. Your MTC task will show up in this panel once you launch themtc_tutorialnode. If you’re usingmtc_demo.launch.pyfrom the tutorials, jump back in here.\nLaunch themtc_tutorialnode with\nYou should see the arm execute the task with the single stage to open the hand, with the cylinder in green in front of it. It should look something like this:\nIf you haven’t made your own package, but still want to see what this looks like, you can launch this file from the tutorials:\nSo far, we’ve walked through creating and executing a simple task, which runs but does not do much. Now, we will start adding the pick-and-place stages to the task. The image below shows an outline of the stages we will use in our task.\nWe will start adding stages after our existing open hand stage. Openmtc_node.cppand locate the following lines:\nWe need to move the arm to a position where we can pick up our object. This is done with aConnectstage, which as its name implies, is a Connector stage. This means that it tries to bridge between the results of the stage before and after it. This stage is initialized with a name,move_to_pick, and aGroupPlannerVectorthat specifies the planning group and the planner. We then set a timeout for the stage, set the properties for the stage, and add it to our task.\nNext, we create a pointer to a MoveIt Task Constructor stage object, and set it tonullptrfor now. Later, we will use this to save a stage.\nThis next block of code creates aSerialContainer.\nThis is a container that can be added to our task and can hold several substages.\nIn this case, we create a serial container that will contain the stages relevant to the picking action.\nInstead of adding the stages to the task, we will add the relevant stages to the serial container. We useexposeTo()to declare the task properties from the parent task in the new serial container, and useconfigureInitFrom()to initialize them.\nThis allows the contained stages to access these properties.\nWe then create a stage to approach the object. This stage is aMoveRelativestage, which allows us to specify a relative movement from our current position.MoveRelativeis a propagator stage: it receives the solution from its neighbouring stages and propagates it to the next or previous stage. Usingcartesian_plannerfinds a solution that involves moving the end effector in a straight line. We set the properties, and set the minimum and maximum distance to move. Now we create aVector3Stampedmessage to indicate the direction we want to move - in this case, in the Z direction from the hand frame. Finally, we add this stage to our serial container\nNow, create a stage to generate the grasp pose.\nThis is a generator stage, so it computes its results without regard to the stages before and after it.\nThe first stage,CurrentStateis a generator stage as well - to connect the first stage and this stage, a connecting stage must be used, which we already created above.\nThis code sets the stage properties, sets the pose before grasping, the angle delta, and the monitored stage.\nAngle delta is a property of theGenerateGraspPosestage that is used to determine the number of poses to generate; when generating solutions, MoveIt Task Constructor will try to grasp the object from many different orientations, with the difference between the orientations specified by the angle delta. The smaller the delta, the closer together the grasp orientations will be. When defining the current stage, we setcurrent_state_ptr, which is now used to forward information about the object pose and shape to the inverse kinematics solver.\nThis stage won’t be directly added to the serial container like previously, as we still need to do inverse kinematics on the poses it generates.\nBefore we compute inverse kinematics for the poses generated above, we first need to define the frame. This can be done with aPoseStampedmessage fromgeometry_msgsor in this case, we define the transform using Eigen transformation matrix and the name of the relevant link. Here, we define the transformation matrix.\nNow, we create theComputeIKstage, and give it the namegenerateposeIKas well as thegenerategraspposestage defined above. Some robots have multiple inverse kinematics solutions for a given pose - we set the limit on the amount of solutions to solve for up to 8. We also set the minimum solution distance, which is a threshold on how different solutions must be: if the joint positions in a solution are too similar to a previous solution, it will be marked as invalid. Next, we configure some additional properties, and add theComputeIKstage to the serial container.\nTo pick up the object, we must allow collision between the hand and the object. This can be done with aModifyPlanningScenestage. TheallowCollisionsfunction lets us specify which collisions to disable.allowCollisionscan be used with a container of names, so we can usegetLinkModelNamesWithCollisionGeometryto get all the names of links with collision geometry in the hand group.\nWith collisions allowed, we now can close the hand. This is done with aMoveTostage, similarly to theopenhandstage from above, except moving to thecloseposition as defined in the SRDF.\nWe now use aModifyPlanningScenestage again, this time to attach the object to the hand usingattachObject. Similarly to what we did with thecurrent_state_ptr, we get a pointer to this stage for later use when generating the place pose for the object.\nNext, we lift the object with aMoveRelativestage, similarly to theapproach_objectstage.\nWith this, we have all the stages needed to pick the object. Now, we add the serial container (with all its substages) to the task. If you build the package as-is, you can see the robot plan to pick up the object.\nTo test out the newly created stage, build the code and execute:\nNow that the stages that define the pick are complete, we move on to defining the stages for placing the object. Picking up where we left off, we add aConnectstage to connect the two, as we will soon be using a generator stage to generate the pose for placing the object.\nWe also create a serial container for the place stages. This is done similarly to the pick serial container.\nThe next stages will be added to the serial container rather than the task.\nThis next stage generates the poses used to place the object and compute the inverse kinematics for those poses - it is somewhat similar to thegenerategraspposestage from the pick serial container.\nWe start by creating a stage to generate the poses and inheriting the task properties.\nWe specify the pose where we want to place the object with aPoseStampedmessage fromgeometry_msgs- in this case, we choosey=0.5in the\"object\"frame.\nWe then pass the target pose to the stage withsetPose.\nNext, we usesetMonitoredStageand pass it the pointer to theattach_objectstage from earlier.\nThis allows the stage to know how the object is attached.\nWe then create aComputeIKstage and pass it ourGeneratePlacePosestage - the rest follows the same logic as above with the pick stages.\nNow that we’re ready to place the object, we open the hand withMoveTostage and the joint interpolation planner.\nWe also can re-enable collisions with the object now that we no longer need to hold it.\nThis is done usingallowCollisionsalmost exactly the same way as disabling collisions, except setting the last argument tofalserather thantrue.\nNow, we can detach the object usingdetachObject.\nWe retreat from the object using aMoveRelativestage, which is done similarly to theapproachobjectandliftobjectstages.\nWe finish our place serial container and add it to the task.\nThe final step is to return home: we use aMoveTostage and pass it the goal pose ofready, which is a pose defined in the Panda SRDF.\nAll these stages should be added above these lines.\nCongratulations! You’ve now defined a pick and place task using MoveIt Task Constructor! To try it out, build the code and execute:\nThe task with each comprising stage is shown in the Motion Planning Tasks pane. Click on a stage and additional information about the stage will show up to the right. The right pane shows different solutions as well as their associated costs. Depending on the stage type and the robot configuration, there may only be one solution shown.\nClick one of the solution costs to see an animation of the robot following the plan for that stage. Click the “Exec” button in the upper-right portion of the pane to execute the motion.\nTo run the complete MoveIt Task Constructor example included with the MoveIt tutorials:\nAnd in a second terminal:\nWhen running MTC, it prints a diagram like this to terminal:\nThis example^ shows two stages. The first stage (“initial_state”) is aCurrentStatetype of stage, which initializes aPlanningSceneand captures any collision objects that are present at that moment.\nA pointer to this stage can be used to retrieve the state of the robot.\nSinceCurrentStateinherits fromGenerator, it propagates solutions both forward and backward.\nThis is denoted by the arrows in both directions.\nThe first1indicates that one solution was successfully propagated backwards to the previous stage.\nThe second1, between the arrows, indicates that one solution was generated.\nThe0indicates that a solution was not propagated forward successfully to the next stage, because the next stage failed.\nThe second stage (“move_to_home”) is aMoveTotype of stage. It inherits its propagation direction from the previous stage, so both arrows point forward. The0’s indicate that this stage failed completely. From left to right, the0’s mean:\nThe stage did not receive a solution from the previous stage\nThe stage did not generate a solution\nThe stage did not propagate a solution forward to the next stage\nIn this case, we could tell that “move_to_home” was the root cause of the failure. The problem was a home state that was in collision. Defining a new, collision-free home position fixed the issue.\nInformation about individual stages can be retrieved from the task. For example, here we retrieve the unique ID for a stage:\nACurrentStatetype stage does not just retrieve the current state of the robot.\nIt also initializes aPlanningSceneobject, capturing any collision objects that are present at that moment.\nMTC stages can be propagated in forward and backward order.\nYou can easily check which direction a stage propagates by the arrow in the RViz GUI.\nWhen propagating backwards, the logic of many operations is reversed.\nFor example, to allow collisions with an object in aModifyPlanningScenestage, you would callallowCollisions(false)rather thanallowCollisions(true). There is a discussion to be readhere.\n© Copyright 2024, PickNik Robotics.\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit_task_constructor.git\nrosdep install --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncd~/ws_moveitcolconbuild--mixinrelease\nros2launchmoveit_task_constructor_demodemo.launch.py\nros2launchmoveit_task_constructor_demorun.launch.pyexe:=cartesianros2launchmoveit_task_constructor_demorun.launch.pyexe:=modularros2launchmoveit_task_constructor_demorun.launch.pyexe:=pick_place_demo\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_task_constructor_corerclcpp\\--node-namemtc_nodemtc_tutorial\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endifstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;classMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};MTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}rclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}voidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;pose.orientation.w=1.0;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}voidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}mtc::TaskMTCTaskNode::createTask(){mtc::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);// Disable warnings for this line, as it's a variable that's set but not used in this example#pragma GCC diagnostic push#pragma GCC diagnostic ignored \"-Wunused-but-set-variable\"mtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generator#pragma GCC diagnostic popautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));autosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);autointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();autocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);autostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}intmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endif\nstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;\nclassMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};\nMTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}\nrclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}\nvoidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}\nvoidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}\nmtc::TaskMTCTaskNode::createTask(){moveit::task_constructor::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);\nmtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generatorautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));\nautosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);\nautointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}\nintmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\nfromlaunchimportLaunchDescriptionfromlaunch_ros.actionsimportNodefrommoveit_configs_utilsimportMoveItConfigsBuilderdefgenerate_launch_description():moveit_config=MoveItConfigsBuilder(\"moveit_resources_panda\").to_dict()# MTC Demo nodepick_place_demo=Node(package=\"mtc_tutorial\",executable=\"mtc_node\",output=\"screen\",parameters=[moveit_config,],)returnLaunchDescription([pick_place_demo])\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}\n  )\ncd~/ws_moveitcolconbuild--mixinreleasesource~/ws_moveit/install/setup.bash\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo_minimal.launch.py\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));// Add the next lines of codes to define more stages here\nautostage_move_to_pick=std::make_unique<mtc::stages::Connect>(\"move to pick\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner}});stage_move_to_pick->setTimeout(5.0);stage_move_to_pick->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_pick));\nmtc::Stage*attach_object_stage=nullptr;// Forward attach_object_stage to place pose generator\n{autograsp=std::make_unique<mtc::SerialContainer>(\"pick object\");task.properties().exposeTo(grasp->properties(),{\"eef\",\"group\",\"ik_frame\"});grasp->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"approach object\",cartesian_planner);stage->properties().set(\"marker_ns\",\"approach_object\");stage->properties().set(\"link\",hand_frame);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.15);// Set hand forward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=hand_frame;vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\n{// Sample grasp poseautostage=std::make_unique<mtc::stages::GenerateGraspPose>(\"generate grasp pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"grasp_pose\");stage->setPreGraspPose(\"open\");stage->setObject(\"object\");stage->setAngleDelta(M_PI/12);stage->setMonitoredStage(current_state_ptr);// Hook into current state\nEigen::Isometry3dgrasp_frame_transform;Eigen::Quaterniondq=Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitX())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitY())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitZ());grasp_frame_transform.linear()=q.matrix();grasp_frame_transform.translation().z()=0.1;\n// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"grasp pose IK\",std::move(stage));wrapper->setMaxIKSolutions(8);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(grasp_frame_transform,hand_frame);wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});grasp->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"allow collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),true);grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"close hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"close\");grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"attach object\");stage->attachObject(\"object\",hand_frame);attach_object_stage=stage.get();grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"lift object\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"lift_object\");// Set upward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\ntask.add(std::move(grasp));}\nros2launchmtc_tutorialpick_place_demo.launch.py\n{autostage_move_to_place=std::make_unique<mtc::stages::Connect>(\"move to place\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner},{hand_group_name,interpolation_planner}});stage_move_to_place->setTimeout(5.0);stage_move_to_place->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_place));}\n{autoplace=std::make_unique<mtc::SerialContainer>(\"place object\");task.properties().exposeTo(place->properties(),{\"eef\",\"group\",\"ik_frame\"});place->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{// Sample place poseautostage=std::make_unique<mtc::stages::GeneratePlacePose>(\"generate place pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"place_pose\");stage->setObject(\"object\");geometry_msgs::msg::PoseStampedtarget_pose_msg;target_pose_msg.header.frame_id=\"object\";target_pose_msg.pose.position.y=0.5;target_pose_msg.pose.orientation.w=1.0;stage->setPose(target_pose_msg);stage->setMonitoredStage(attach_object_stage);// Hook into attach_object_stage// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"place pose IK\",std::move(stage));wrapper->setMaxIKSolutions(2);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(\"object\");wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});place->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"open\");place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"forbid collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),false);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"detach object\");stage->detachObject(\"object\",hand_frame);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"retreat\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"retreat\");// Set retreat directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.x=-0.5;stage->setDirection(vec);place->insert(std::move(stage));}\ntask.add(std::move(place));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"return home\",interpolation_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setGoal(\"ready\");task.add(std::move(stage));}\n// Stages all added to the task above this linereturntask;}\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmoveit2_tutorialspick_place_demo.launch.py\n[demo_node-1]1-←1→-0/initial_state[demo_node-1]-0→0→-0/move_to_home\nuint32_tconstunique_stage_id=task_.stages()->findChild(stage_name)->introspectionId();\n<branch>\nhumble\nros2\nmain\nmtc_tutorial\nmoveit_task_constructor_core\nsrc/mtc_node\nmtc_node.cpp\nrclcpp/rclcpp.hpp\nmoveit/planning_scene/planning_scene.h\nmoveit/planning_scene_interface/planning_scene_interface.h\nmoveit/task_constructor/task.h\nmoveit/task_constructor/solvers.h\nmoveit/task_constructor/stages.h\ntf2_geometry_msgs/tf2_geometry_msgs.hpp\ntf2_eigen/tf2_eigen.hpp\nmoveit::task_constructor\nMTCTaskNode\nobject.primitives[0].dimensions\npose.position.x\npose.position.y\ncreateTask\ntask.init()\ntask.plan(5)\ntask.execute()\ntask.setProperty(property_name,value)\ncurrent_state_ptr\nnullptr\ncurrent_state\nCurrentState\ncurrent_state_ptr\nMoveTo\ncreateTask()\nmain\nmove_group\nros2_control\nstatic_tf\nrobot_state_publisher\nrviz\nmtc_tutorial\npackage\nexecutable\npick_place_demo.launch.py\nCMakeLists.txt\nmap\npanda_link0\nmoveit_task_constructor_visualization\n/solution\nmtc_tutorial\nmtc_demo.launch.py\nmtc_tutorial\nmtc_node.cpp\nConnect\nmove_to_pick\nGroupPlannerVector\nnullptr\nSerialContainer\nexposeTo()\nconfigureInitFrom()\nMoveRelative\nMoveRelative\ncartesian_planner\nVector3Stamped\nCurrentState\nGenerateGraspPose\ncurrent_state_ptr\nPoseStamped\ngeometry_msgs\nComputeIK\ngenerateposeIK\ngenerategrasppose\nComputeIK\nModifyPlanningScene\nallowCollisions\nallowCollisions\ngetLinkModelNamesWithCollisionGeometry\nMoveTo\nopenhand\nclose\nModifyPlanningScene\nattachObject\ncurrent_state_ptr\nMoveRelative\napproach_object\nConnect\ngenerategrasppose\nPoseStamped\ngeometry_msgs\ny=0.5\n\"object\"\nsetPose\nsetMonitoredStage\nattach_object\nComputeIK\nGeneratePlacePose\nMoveTo\nallowCollisions\nfalse\ntrue\ndetachObject\nMoveRelative\napproachobject\nliftobject\nMoveTo\nready\nCurrentState\nPlanningScene\nCurrentState\nGenerator\n1\n1\n0\nMoveTo\n0\n0\nCurrentState\nPlanningScene\nModifyPlanningScene\nallowCollisions(false)\nallowCollisions(true)",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0dd"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/pick_and_place_with_moveit_task_constructor/pick_and_place_with_moveit_task_constructor.html#launch-files",
  "title": "Pick and Place with MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "Pick and Place with MoveIt Task Constructor\n1 Basic Concepts\n2 Getting Started\n3 Trying It Out\n4 Setting up a Project with MoveIt Task Constructor\n5 Running the Demo\n6 Adding Stages\n7 Further Discussion\nThis tutorial will walk you through creating a package that plans a pick and place operation usingMoveIt Task Constructor. MoveIt Task Constructor provides a way to plan for tasks that consist of multiple different subtasks (known as stages). If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nThe fundamental idea of MTC is that complex motion planning problems can be composed into a set of simpler subproblems.\nThe top-level planning problem is specified as aTaskwhile all subproblems are specified byStages.\nStages can be arranged in any arbitrary order and hierarchy only limited by the individual stages types.\nThe order in which stages can be arranged is restricted by the direction in which results are passed.\nThere are three possible stages relating to the result flow: generator, propagator, and connector stages:\nGeneratorscompute their results independently of their neighbor stages and pass them in both directions, backwards and forwards.\nAn example is an IK sampler for geometric poses where approaching and departing motions (neighbor stages) depend on the solution.\nPropagatorsreceive the result of one neighbor stage, solve a subproblem and then propagate their result to the neighbor on the opposite site.\nDepending on the implementation, propagating stages can pass solutions forward, backward or in both directions separately.\nAn example is a stage that computes a Cartesian path based on either a start or a goal state.\nConnectorsdo not propagate any results, but rather attempt to bridge the gap between the resulting states of both neighbors.\nAn example is the computation of a free-motion plan from one given state to another.\nAdditional to the order types, there are different hierarchy types allowing to encapsulate subordinate stages.\nStages without subordinate stages are calledprimitive stages, higher-level stages are calledcontainer stages.\nThere are three container types:\nWrappersencapsulate a single subordinate stage and modify or filter the results.\nFor example, a filter stage that only accepts solutions of its child stage that satisfy a certain constraint can be realized as a wrapper.\nAnother standard use of this type includes the IK wrapper stage, which generates inverse kinematics solutions based on planning scenes annotated with a pose target property.\nSerial Containershold a sequence of subordinate stages and only consider end-to-end solutions as results.\nAn example is a picking motion that consists of a sequence of coherent steps.\nParallel Containerscombine set of subordinate stages and can be used for passing the best of alternative results, running fallback solvers or for merging multiple independent solutions.\nExamples are running alternative planners for a free-motion plan, picking objects with the right hand or with the left hand as a fallback, or moving the arm and opening the gripper at the same time.\nStages not only support solving motion planning problems.\nThey can also be used for all kinds of state transitions, as for instance modifying the planning scene.\nCombined with the possibility of using class inheritance it is possible to construct very complex behavior while only relying on a well-structured set of primitive stages.\nMore detailed information on MTC can be found in theMoveIt Task Constructor concepts page\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nMove into your colcon workspace and pull the MoveIt Task Constructor source, where<branch>can be e.g.humblefor ROS Humble, orros2for the latest version compatible with MoveIt 2main:\nInstall missing packages with rosdep:\nBuild the workspace:\nThe MoveIt Task Constructor package contains several basic examples and a pick-and-place demo.\nFor all demos you should launch the basic environment:\nSubsequently, you can run the individual demos:\nOn the right side you should see theMotion Planning Taskspanel outlining the hierarchical stage structure of the tasks.\nWhen you select a particular stage, the list of successful and failed solutions will be\nshown in the right-most window. Selecting one of those solutions will start its visualization.\nThis section walks through the steps required to build a simple task with MoveIt Task Constructor.\nCreate a new package with the following command:\nThis will create a new package and folder calledmtc_tutorialwith a dependency onmoveit_task_constructor_coreas well as a hello world example insrc/mtc_node.\nOpenmtc_node.cppin your editor of choice, and paste in the following code.\nThe top of the code includes the ROS and MoveIt Libraries that this package uses.\nrclcpp/rclcpp.hppincludes core ROS2 functionality\nmoveit/planning_scene/planning_scene.handmoveit/planning_scene_interface/planning_scene_interface.hinclude functionality to interface with the robot model and collision objects\nmoveit/task_constructor/task.h,moveit/task_constructor/solvers.h, andmoveit/task_constructor/stages.hinclude different components of MoveIt Task Constructor that are used in the example\ntf2_geometry_msgs/tf2_geometry_msgs.hppandtf2_eigen/tf2_eigen.hppwon’t be used in this initial example, but they will be used for pose generation when we add more stages to the MoveIt Task Constructor task.\nThe next line gets a logger for our new node. We also create a namespace alias formoveit::task_constructorfor convenience.\nWe start by defining a class that will contain the main MoveIt Task Constructor functionality.  We also declare the MoveIt Task Constructor task object as a member variable for our class: this isn’t strictly necessary for a given application, but it helps save the task for later visualization purposes. We will explore each function individually below.\nThese lines initialize the node with specified options (it is the constructor of ourMTCTaskNodeclass).\nThese next lines define a getter function to get the node base interface, which will be used for the executor later.\nThis class method is used to set up the planning scene that is used in the example. It creates a cylinder with dimensions specified byobject.primitives[0].dimensionsand position specified bypose.position.xandpose.position.y.\nYou can try changing these numbers to resize and move the cylinder around. If we move the cylinder out of the robot’s reach, planning will fail.\nThis function interfaces with the MoveIt Task Constructor task object. It first creates a task, which includes setting some properties and adding stages. This will be discussed further in thecreateTaskfunction definition. Next,task.init()initializes the task andtask.plan(5)generates a plan, stopping after 5 successful plans are found. The next line publishes the solution to be visualized in RViz - this line can be removed if you don’t care for visualization. Finally,task.execute()executes the plan. Execution occurs via an action server interface with the RViz plugin.\nAs mentioned above, this function creates a MoveIt Task Constructor object and sets some initial properties. In this case, we set the task name to “demo_task”, load the robot model, define the names of some useful frames, and set those frame names as properties of the task withtask.setProperty(property_name,value). The next few code blocks will fill out this function body.\nNow, we add an example stage to the node. The first line setscurrent_state_ptrtonullptr; this creates a pointer to a stage such that we can reuse stage information in specific scenarios. This line is not used at this moment, but will be used later when more stages are added to the task. Next, we make acurrent_statestage (a generator stage) and add it to our task - this starts the robot off in its current state. Now that we’ve created theCurrentStatestage, we save a pointer to it in thecurrent_state_ptrfor later use.\nSolvers are used to define the type of robot motion. MoveIt Task Constructor has three options for solvers:\nPipelinePlanneruses MoveIt’s planning pipeline, which typically defaults toOMPL.\nJointInterpolationis a simple planner that interpolates between the start and goal joint states. It is typically used for simple motions as it computes quickly but doesn’t support complex motions.\nCartesianPathis used to move the end effector in a straight line in Cartesian space.\nFeel free to try out the different solvers and see how the robot motion changes. For the first stage we will use the Cartesian planner, which requires the following properties to be set:\nNow that we added in the planners, we can add a stage that will move the robot.\nThe following lines use aMoveTostage (a propagator stage). Since opening the hand is a relatively simple movement, we can use the joint interpolation planner.\nThis stage plans a move to the “open hand” pose, which is a named pose defined in theSRDFfor the Panda robot.\nWe return the task and finish with thecreateTask()function.\nFinally, we havemain: the following lines create a node using the class defined above, and calls the class methods to set up and execute a basic MTC task. In this example, we do not cancel the executor once the task has finished executing to keep the node alive to inspect the solutions in RViz.\nWe will need a launch file to launch themove_group,ros2_control,static_tf,robot_state_publisher, andrviznodes that provide us the environment to run the demo. The one we will use for this example can be foundhere.\nTo run the MoveIt Task Constructor node, we will use a second launch file to start themtc_tutorialexecutable with the proper parameters. Here we can load URDF, SRDF, and OMPL parameters, or use MoveIt Configs Utils to do so. Your launch file should look something like the one found in this tutorial packagehere(pay close attention to thepackageandexecutablearguments below as they are different from the launch file linked) :\nSave a launch file aspick_place_demo.launch.pyor download one to the package’s launch directory. Make sure to edit theCMakeLists.txtso it includes the launch folder by adding the following lines:\nNow we can build and source the colcon workspace.\nStart by launching the first launch file. If you want to use the one provided by the tutorials:\nRViz will now load. If you’re using your own launch file and haven’t included an rviz configsuch as this, you will need to configure RViz before you see anything displayed. If you’re using the launch file from the tutorials package, RViz will already be configured for you and you can jump to the end of the next section.\nIf you are not using the RViz configuration provided, we’ll have to make some changes to the RViz configuration to see your robot and the MoveIt Task Constructor solutions. First, start RViz. The following steps will cover how to set up RViz for MoveIt Task Constructor solution visualization.\nIf theMotionPlanningdisplay is active, uncheck it to hide it for now.\nUnderGlobal Options, change theFixed Framefrommaptopanda_link0if not already done.\nOn the bottom left of the window, click theAddbutton.\nUndermoveit_task_constructor_visualizationselectMotion Planning Tasksand click OK. TheMotion Planning Tasksdisplay should appear on the bottom left.\nIn theDisplays, underMotion Planning Tasks,  changeTask Solution Topicto/solution\nYou should see the panda arm in the main view with Motion Planning Tasks display open in the bottom left and nothing in it. Your MTC task will show up in this panel once you launch themtc_tutorialnode. If you’re usingmtc_demo.launch.pyfrom the tutorials, jump back in here.\nLaunch themtc_tutorialnode with\nYou should see the arm execute the task with the single stage to open the hand, with the cylinder in green in front of it. It should look something like this:\nIf you haven’t made your own package, but still want to see what this looks like, you can launch this file from the tutorials:\nSo far, we’ve walked through creating and executing a simple task, which runs but does not do much. Now, we will start adding the pick-and-place stages to the task. The image below shows an outline of the stages we will use in our task.\nWe will start adding stages after our existing open hand stage. Openmtc_node.cppand locate the following lines:\nWe need to move the arm to a position where we can pick up our object. This is done with aConnectstage, which as its name implies, is a Connector stage. This means that it tries to bridge between the results of the stage before and after it. This stage is initialized with a name,move_to_pick, and aGroupPlannerVectorthat specifies the planning group and the planner. We then set a timeout for the stage, set the properties for the stage, and add it to our task.\nNext, we create a pointer to a MoveIt Task Constructor stage object, and set it tonullptrfor now. Later, we will use this to save a stage.\nThis next block of code creates aSerialContainer.\nThis is a container that can be added to our task and can hold several substages.\nIn this case, we create a serial container that will contain the stages relevant to the picking action.\nInstead of adding the stages to the task, we will add the relevant stages to the serial container. We useexposeTo()to declare the task properties from the parent task in the new serial container, and useconfigureInitFrom()to initialize them.\nThis allows the contained stages to access these properties.\nWe then create a stage to approach the object. This stage is aMoveRelativestage, which allows us to specify a relative movement from our current position.MoveRelativeis a propagator stage: it receives the solution from its neighbouring stages and propagates it to the next or previous stage. Usingcartesian_plannerfinds a solution that involves moving the end effector in a straight line. We set the properties, and set the minimum and maximum distance to move. Now we create aVector3Stampedmessage to indicate the direction we want to move - in this case, in the Z direction from the hand frame. Finally, we add this stage to our serial container\nNow, create a stage to generate the grasp pose.\nThis is a generator stage, so it computes its results without regard to the stages before and after it.\nThe first stage,CurrentStateis a generator stage as well - to connect the first stage and this stage, a connecting stage must be used, which we already created above.\nThis code sets the stage properties, sets the pose before grasping, the angle delta, and the monitored stage.\nAngle delta is a property of theGenerateGraspPosestage that is used to determine the number of poses to generate; when generating solutions, MoveIt Task Constructor will try to grasp the object from many different orientations, with the difference between the orientations specified by the angle delta. The smaller the delta, the closer together the grasp orientations will be. When defining the current stage, we setcurrent_state_ptr, which is now used to forward information about the object pose and shape to the inverse kinematics solver.\nThis stage won’t be directly added to the serial container like previously, as we still need to do inverse kinematics on the poses it generates.\nBefore we compute inverse kinematics for the poses generated above, we first need to define the frame. This can be done with aPoseStampedmessage fromgeometry_msgsor in this case, we define the transform using Eigen transformation matrix and the name of the relevant link. Here, we define the transformation matrix.\nNow, we create theComputeIKstage, and give it the namegenerateposeIKas well as thegenerategraspposestage defined above. Some robots have multiple inverse kinematics solutions for a given pose - we set the limit on the amount of solutions to solve for up to 8. We also set the minimum solution distance, which is a threshold on how different solutions must be: if the joint positions in a solution are too similar to a previous solution, it will be marked as invalid. Next, we configure some additional properties, and add theComputeIKstage to the serial container.\nTo pick up the object, we must allow collision between the hand and the object. This can be done with aModifyPlanningScenestage. TheallowCollisionsfunction lets us specify which collisions to disable.allowCollisionscan be used with a container of names, so we can usegetLinkModelNamesWithCollisionGeometryto get all the names of links with collision geometry in the hand group.\nWith collisions allowed, we now can close the hand. This is done with aMoveTostage, similarly to theopenhandstage from above, except moving to thecloseposition as defined in the SRDF.\nWe now use aModifyPlanningScenestage again, this time to attach the object to the hand usingattachObject. Similarly to what we did with thecurrent_state_ptr, we get a pointer to this stage for later use when generating the place pose for the object.\nNext, we lift the object with aMoveRelativestage, similarly to theapproach_objectstage.\nWith this, we have all the stages needed to pick the object. Now, we add the serial container (with all its substages) to the task. If you build the package as-is, you can see the robot plan to pick up the object.\nTo test out the newly created stage, build the code and execute:\nNow that the stages that define the pick are complete, we move on to defining the stages for placing the object. Picking up where we left off, we add aConnectstage to connect the two, as we will soon be using a generator stage to generate the pose for placing the object.\nWe also create a serial container for the place stages. This is done similarly to the pick serial container.\nThe next stages will be added to the serial container rather than the task.\nThis next stage generates the poses used to place the object and compute the inverse kinematics for those poses - it is somewhat similar to thegenerategraspposestage from the pick serial container.\nWe start by creating a stage to generate the poses and inheriting the task properties.\nWe specify the pose where we want to place the object with aPoseStampedmessage fromgeometry_msgs- in this case, we choosey=0.5in the\"object\"frame.\nWe then pass the target pose to the stage withsetPose.\nNext, we usesetMonitoredStageand pass it the pointer to theattach_objectstage from earlier.\nThis allows the stage to know how the object is attached.\nWe then create aComputeIKstage and pass it ourGeneratePlacePosestage - the rest follows the same logic as above with the pick stages.\nNow that we’re ready to place the object, we open the hand withMoveTostage and the joint interpolation planner.\nWe also can re-enable collisions with the object now that we no longer need to hold it.\nThis is done usingallowCollisionsalmost exactly the same way as disabling collisions, except setting the last argument tofalserather thantrue.\nNow, we can detach the object usingdetachObject.\nWe retreat from the object using aMoveRelativestage, which is done similarly to theapproachobjectandliftobjectstages.\nWe finish our place serial container and add it to the task.\nThe final step is to return home: we use aMoveTostage and pass it the goal pose ofready, which is a pose defined in the Panda SRDF.\nAll these stages should be added above these lines.\nCongratulations! You’ve now defined a pick and place task using MoveIt Task Constructor! To try it out, build the code and execute:\nThe task with each comprising stage is shown in the Motion Planning Tasks pane. Click on a stage and additional information about the stage will show up to the right. The right pane shows different solutions as well as their associated costs. Depending on the stage type and the robot configuration, there may only be one solution shown.\nClick one of the solution costs to see an animation of the robot following the plan for that stage. Click the “Exec” button in the upper-right portion of the pane to execute the motion.\nTo run the complete MoveIt Task Constructor example included with the MoveIt tutorials:\nAnd in a second terminal:\nWhen running MTC, it prints a diagram like this to terminal:\nThis example^ shows two stages. The first stage (“initial_state”) is aCurrentStatetype of stage, which initializes aPlanningSceneand captures any collision objects that are present at that moment.\nA pointer to this stage can be used to retrieve the state of the robot.\nSinceCurrentStateinherits fromGenerator, it propagates solutions both forward and backward.\nThis is denoted by the arrows in both directions.\nThe first1indicates that one solution was successfully propagated backwards to the previous stage.\nThe second1, between the arrows, indicates that one solution was generated.\nThe0indicates that a solution was not propagated forward successfully to the next stage, because the next stage failed.\nThe second stage (“move_to_home”) is aMoveTotype of stage. It inherits its propagation direction from the previous stage, so both arrows point forward. The0’s indicate that this stage failed completely. From left to right, the0’s mean:\nThe stage did not receive a solution from the previous stage\nThe stage did not generate a solution\nThe stage did not propagate a solution forward to the next stage\nIn this case, we could tell that “move_to_home” was the root cause of the failure. The problem was a home state that was in collision. Defining a new, collision-free home position fixed the issue.\nInformation about individual stages can be retrieved from the task. For example, here we retrieve the unique ID for a stage:\nACurrentStatetype stage does not just retrieve the current state of the robot.\nIt also initializes aPlanningSceneobject, capturing any collision objects that are present at that moment.\nMTC stages can be propagated in forward and backward order.\nYou can easily check which direction a stage propagates by the arrow in the RViz GUI.\nWhen propagating backwards, the logic of many operations is reversed.\nFor example, to allow collisions with an object in aModifyPlanningScenestage, you would callallowCollisions(false)rather thanallowCollisions(true). There is a discussion to be readhere.\n© Copyright 2024, PickNik Robotics.\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit_task_constructor.git\nrosdep install --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncd~/ws_moveitcolconbuild--mixinrelease\nros2launchmoveit_task_constructor_demodemo.launch.py\nros2launchmoveit_task_constructor_demorun.launch.pyexe:=cartesianros2launchmoveit_task_constructor_demorun.launch.pyexe:=modularros2launchmoveit_task_constructor_demorun.launch.pyexe:=pick_place_demo\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_task_constructor_corerclcpp\\--node-namemtc_nodemtc_tutorial\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endifstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;classMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};MTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}rclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}voidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;pose.orientation.w=1.0;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}voidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}mtc::TaskMTCTaskNode::createTask(){mtc::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);// Disable warnings for this line, as it's a variable that's set but not used in this example#pragma GCC diagnostic push#pragma GCC diagnostic ignored \"-Wunused-but-set-variable\"mtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generator#pragma GCC diagnostic popautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));autosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);autointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();autocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);autostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}intmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endif\nstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;\nclassMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};\nMTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}\nrclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}\nvoidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}\nvoidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}\nmtc::TaskMTCTaskNode::createTask(){moveit::task_constructor::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);\nmtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generatorautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));\nautosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);\nautointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}\nintmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\nfromlaunchimportLaunchDescriptionfromlaunch_ros.actionsimportNodefrommoveit_configs_utilsimportMoveItConfigsBuilderdefgenerate_launch_description():moveit_config=MoveItConfigsBuilder(\"moveit_resources_panda\").to_dict()# MTC Demo nodepick_place_demo=Node(package=\"mtc_tutorial\",executable=\"mtc_node\",output=\"screen\",parameters=[moveit_config,],)returnLaunchDescription([pick_place_demo])\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}\n  )\ncd~/ws_moveitcolconbuild--mixinreleasesource~/ws_moveit/install/setup.bash\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo_minimal.launch.py\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));// Add the next lines of codes to define more stages here\nautostage_move_to_pick=std::make_unique<mtc::stages::Connect>(\"move to pick\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner}});stage_move_to_pick->setTimeout(5.0);stage_move_to_pick->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_pick));\nmtc::Stage*attach_object_stage=nullptr;// Forward attach_object_stage to place pose generator\n{autograsp=std::make_unique<mtc::SerialContainer>(\"pick object\");task.properties().exposeTo(grasp->properties(),{\"eef\",\"group\",\"ik_frame\"});grasp->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"approach object\",cartesian_planner);stage->properties().set(\"marker_ns\",\"approach_object\");stage->properties().set(\"link\",hand_frame);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.15);// Set hand forward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=hand_frame;vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\n{// Sample grasp poseautostage=std::make_unique<mtc::stages::GenerateGraspPose>(\"generate grasp pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"grasp_pose\");stage->setPreGraspPose(\"open\");stage->setObject(\"object\");stage->setAngleDelta(M_PI/12);stage->setMonitoredStage(current_state_ptr);// Hook into current state\nEigen::Isometry3dgrasp_frame_transform;Eigen::Quaterniondq=Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitX())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitY())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitZ());grasp_frame_transform.linear()=q.matrix();grasp_frame_transform.translation().z()=0.1;\n// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"grasp pose IK\",std::move(stage));wrapper->setMaxIKSolutions(8);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(grasp_frame_transform,hand_frame);wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});grasp->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"allow collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),true);grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"close hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"close\");grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"attach object\");stage->attachObject(\"object\",hand_frame);attach_object_stage=stage.get();grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"lift object\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"lift_object\");// Set upward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\ntask.add(std::move(grasp));}\nros2launchmtc_tutorialpick_place_demo.launch.py\n{autostage_move_to_place=std::make_unique<mtc::stages::Connect>(\"move to place\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner},{hand_group_name,interpolation_planner}});stage_move_to_place->setTimeout(5.0);stage_move_to_place->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_place));}\n{autoplace=std::make_unique<mtc::SerialContainer>(\"place object\");task.properties().exposeTo(place->properties(),{\"eef\",\"group\",\"ik_frame\"});place->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{// Sample place poseautostage=std::make_unique<mtc::stages::GeneratePlacePose>(\"generate place pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"place_pose\");stage->setObject(\"object\");geometry_msgs::msg::PoseStampedtarget_pose_msg;target_pose_msg.header.frame_id=\"object\";target_pose_msg.pose.position.y=0.5;target_pose_msg.pose.orientation.w=1.0;stage->setPose(target_pose_msg);stage->setMonitoredStage(attach_object_stage);// Hook into attach_object_stage// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"place pose IK\",std::move(stage));wrapper->setMaxIKSolutions(2);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(\"object\");wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});place->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"open\");place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"forbid collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),false);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"detach object\");stage->detachObject(\"object\",hand_frame);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"retreat\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"retreat\");// Set retreat directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.x=-0.5;stage->setDirection(vec);place->insert(std::move(stage));}\ntask.add(std::move(place));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"return home\",interpolation_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setGoal(\"ready\");task.add(std::move(stage));}\n// Stages all added to the task above this linereturntask;}\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmoveit2_tutorialspick_place_demo.launch.py\n[demo_node-1]1-←1→-0/initial_state[demo_node-1]-0→0→-0/move_to_home\nuint32_tconstunique_stage_id=task_.stages()->findChild(stage_name)->introspectionId();\n<branch>\nhumble\nros2\nmain\nmtc_tutorial\nmoveit_task_constructor_core\nsrc/mtc_node\nmtc_node.cpp\nrclcpp/rclcpp.hpp\nmoveit/planning_scene/planning_scene.h\nmoveit/planning_scene_interface/planning_scene_interface.h\nmoveit/task_constructor/task.h\nmoveit/task_constructor/solvers.h\nmoveit/task_constructor/stages.h\ntf2_geometry_msgs/tf2_geometry_msgs.hpp\ntf2_eigen/tf2_eigen.hpp\nmoveit::task_constructor\nMTCTaskNode\nobject.primitives[0].dimensions\npose.position.x\npose.position.y\ncreateTask\ntask.init()\ntask.plan(5)\ntask.execute()\ntask.setProperty(property_name,value)\ncurrent_state_ptr\nnullptr\ncurrent_state\nCurrentState\ncurrent_state_ptr\nMoveTo\ncreateTask()\nmain\nmove_group\nros2_control\nstatic_tf\nrobot_state_publisher\nrviz\nmtc_tutorial\npackage\nexecutable\npick_place_demo.launch.py\nCMakeLists.txt\nmap\npanda_link0\nmoveit_task_constructor_visualization\n/solution\nmtc_tutorial\nmtc_demo.launch.py\nmtc_tutorial\nmtc_node.cpp\nConnect\nmove_to_pick\nGroupPlannerVector\nnullptr\nSerialContainer\nexposeTo()\nconfigureInitFrom()\nMoveRelative\nMoveRelative\ncartesian_planner\nVector3Stamped\nCurrentState\nGenerateGraspPose\ncurrent_state_ptr\nPoseStamped\ngeometry_msgs\nComputeIK\ngenerateposeIK\ngenerategrasppose\nComputeIK\nModifyPlanningScene\nallowCollisions\nallowCollisions\ngetLinkModelNamesWithCollisionGeometry\nMoveTo\nopenhand\nclose\nModifyPlanningScene\nattachObject\ncurrent_state_ptr\nMoveRelative\napproach_object\nConnect\ngenerategrasppose\nPoseStamped\ngeometry_msgs\ny=0.5\n\"object\"\nsetPose\nsetMonitoredStage\nattach_object\nComputeIK\nGeneratePlacePose\nMoveTo\nallowCollisions\nfalse\ntrue\ndetachObject\nMoveRelative\napproachobject\nliftobject\nMoveTo\nready\nCurrentState\nPlanningScene\nCurrentState\nGenerator\n1\n1\n0\nMoveTo\n0\n0\nCurrentState\nPlanningScene\nModifyPlanningScene\nallowCollisions(false)\nallowCollisions(true)",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0de"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/pick_and_place_with_moveit_task_constructor/pick_and_place_with_moveit_task_constructor.html#rviz-configuration",
  "title": "Pick and Place with MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "Pick and Place with MoveIt Task Constructor\n1 Basic Concepts\n2 Getting Started\n3 Trying It Out\n4 Setting up a Project with MoveIt Task Constructor\n5 Running the Demo\n6 Adding Stages\n7 Further Discussion\nThis tutorial will walk you through creating a package that plans a pick and place operation usingMoveIt Task Constructor. MoveIt Task Constructor provides a way to plan for tasks that consist of multiple different subtasks (known as stages). If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nThe fundamental idea of MTC is that complex motion planning problems can be composed into a set of simpler subproblems.\nThe top-level planning problem is specified as aTaskwhile all subproblems are specified byStages.\nStages can be arranged in any arbitrary order and hierarchy only limited by the individual stages types.\nThe order in which stages can be arranged is restricted by the direction in which results are passed.\nThere are three possible stages relating to the result flow: generator, propagator, and connector stages:\nGeneratorscompute their results independently of their neighbor stages and pass them in both directions, backwards and forwards.\nAn example is an IK sampler for geometric poses where approaching and departing motions (neighbor stages) depend on the solution.\nPropagatorsreceive the result of one neighbor stage, solve a subproblem and then propagate their result to the neighbor on the opposite site.\nDepending on the implementation, propagating stages can pass solutions forward, backward or in both directions separately.\nAn example is a stage that computes a Cartesian path based on either a start or a goal state.\nConnectorsdo not propagate any results, but rather attempt to bridge the gap between the resulting states of both neighbors.\nAn example is the computation of a free-motion plan from one given state to another.\nAdditional to the order types, there are different hierarchy types allowing to encapsulate subordinate stages.\nStages without subordinate stages are calledprimitive stages, higher-level stages are calledcontainer stages.\nThere are three container types:\nWrappersencapsulate a single subordinate stage and modify or filter the results.\nFor example, a filter stage that only accepts solutions of its child stage that satisfy a certain constraint can be realized as a wrapper.\nAnother standard use of this type includes the IK wrapper stage, which generates inverse kinematics solutions based on planning scenes annotated with a pose target property.\nSerial Containershold a sequence of subordinate stages and only consider end-to-end solutions as results.\nAn example is a picking motion that consists of a sequence of coherent steps.\nParallel Containerscombine set of subordinate stages and can be used for passing the best of alternative results, running fallback solvers or for merging multiple independent solutions.\nExamples are running alternative planners for a free-motion plan, picking objects with the right hand or with the left hand as a fallback, or moving the arm and opening the gripper at the same time.\nStages not only support solving motion planning problems.\nThey can also be used for all kinds of state transitions, as for instance modifying the planning scene.\nCombined with the possibility of using class inheritance it is possible to construct very complex behavior while only relying on a well-structured set of primitive stages.\nMore detailed information on MTC can be found in theMoveIt Task Constructor concepts page\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nMove into your colcon workspace and pull the MoveIt Task Constructor source, where<branch>can be e.g.humblefor ROS Humble, orros2for the latest version compatible with MoveIt 2main:\nInstall missing packages with rosdep:\nBuild the workspace:\nThe MoveIt Task Constructor package contains several basic examples and a pick-and-place demo.\nFor all demos you should launch the basic environment:\nSubsequently, you can run the individual demos:\nOn the right side you should see theMotion Planning Taskspanel outlining the hierarchical stage structure of the tasks.\nWhen you select a particular stage, the list of successful and failed solutions will be\nshown in the right-most window. Selecting one of those solutions will start its visualization.\nThis section walks through the steps required to build a simple task with MoveIt Task Constructor.\nCreate a new package with the following command:\nThis will create a new package and folder calledmtc_tutorialwith a dependency onmoveit_task_constructor_coreas well as a hello world example insrc/mtc_node.\nOpenmtc_node.cppin your editor of choice, and paste in the following code.\nThe top of the code includes the ROS and MoveIt Libraries that this package uses.\nrclcpp/rclcpp.hppincludes core ROS2 functionality\nmoveit/planning_scene/planning_scene.handmoveit/planning_scene_interface/planning_scene_interface.hinclude functionality to interface with the robot model and collision objects\nmoveit/task_constructor/task.h,moveit/task_constructor/solvers.h, andmoveit/task_constructor/stages.hinclude different components of MoveIt Task Constructor that are used in the example\ntf2_geometry_msgs/tf2_geometry_msgs.hppandtf2_eigen/tf2_eigen.hppwon’t be used in this initial example, but they will be used for pose generation when we add more stages to the MoveIt Task Constructor task.\nThe next line gets a logger for our new node. We also create a namespace alias formoveit::task_constructorfor convenience.\nWe start by defining a class that will contain the main MoveIt Task Constructor functionality.  We also declare the MoveIt Task Constructor task object as a member variable for our class: this isn’t strictly necessary for a given application, but it helps save the task for later visualization purposes. We will explore each function individually below.\nThese lines initialize the node with specified options (it is the constructor of ourMTCTaskNodeclass).\nThese next lines define a getter function to get the node base interface, which will be used for the executor later.\nThis class method is used to set up the planning scene that is used in the example. It creates a cylinder with dimensions specified byobject.primitives[0].dimensionsand position specified bypose.position.xandpose.position.y.\nYou can try changing these numbers to resize and move the cylinder around. If we move the cylinder out of the robot’s reach, planning will fail.\nThis function interfaces with the MoveIt Task Constructor task object. It first creates a task, which includes setting some properties and adding stages. This will be discussed further in thecreateTaskfunction definition. Next,task.init()initializes the task andtask.plan(5)generates a plan, stopping after 5 successful plans are found. The next line publishes the solution to be visualized in RViz - this line can be removed if you don’t care for visualization. Finally,task.execute()executes the plan. Execution occurs via an action server interface with the RViz plugin.\nAs mentioned above, this function creates a MoveIt Task Constructor object and sets some initial properties. In this case, we set the task name to “demo_task”, load the robot model, define the names of some useful frames, and set those frame names as properties of the task withtask.setProperty(property_name,value). The next few code blocks will fill out this function body.\nNow, we add an example stage to the node. The first line setscurrent_state_ptrtonullptr; this creates a pointer to a stage such that we can reuse stage information in specific scenarios. This line is not used at this moment, but will be used later when more stages are added to the task. Next, we make acurrent_statestage (a generator stage) and add it to our task - this starts the robot off in its current state. Now that we’ve created theCurrentStatestage, we save a pointer to it in thecurrent_state_ptrfor later use.\nSolvers are used to define the type of robot motion. MoveIt Task Constructor has three options for solvers:\nPipelinePlanneruses MoveIt’s planning pipeline, which typically defaults toOMPL.\nJointInterpolationis a simple planner that interpolates between the start and goal joint states. It is typically used for simple motions as it computes quickly but doesn’t support complex motions.\nCartesianPathis used to move the end effector in a straight line in Cartesian space.\nFeel free to try out the different solvers and see how the robot motion changes. For the first stage we will use the Cartesian planner, which requires the following properties to be set:\nNow that we added in the planners, we can add a stage that will move the robot.\nThe following lines use aMoveTostage (a propagator stage). Since opening the hand is a relatively simple movement, we can use the joint interpolation planner.\nThis stage plans a move to the “open hand” pose, which is a named pose defined in theSRDFfor the Panda robot.\nWe return the task and finish with thecreateTask()function.\nFinally, we havemain: the following lines create a node using the class defined above, and calls the class methods to set up and execute a basic MTC task. In this example, we do not cancel the executor once the task has finished executing to keep the node alive to inspect the solutions in RViz.\nWe will need a launch file to launch themove_group,ros2_control,static_tf,robot_state_publisher, andrviznodes that provide us the environment to run the demo. The one we will use for this example can be foundhere.\nTo run the MoveIt Task Constructor node, we will use a second launch file to start themtc_tutorialexecutable with the proper parameters. Here we can load URDF, SRDF, and OMPL parameters, or use MoveIt Configs Utils to do so. Your launch file should look something like the one found in this tutorial packagehere(pay close attention to thepackageandexecutablearguments below as they are different from the launch file linked) :\nSave a launch file aspick_place_demo.launch.pyor download one to the package’s launch directory. Make sure to edit theCMakeLists.txtso it includes the launch folder by adding the following lines:\nNow we can build and source the colcon workspace.\nStart by launching the first launch file. If you want to use the one provided by the tutorials:\nRViz will now load. If you’re using your own launch file and haven’t included an rviz configsuch as this, you will need to configure RViz before you see anything displayed. If you’re using the launch file from the tutorials package, RViz will already be configured for you and you can jump to the end of the next section.\nIf you are not using the RViz configuration provided, we’ll have to make some changes to the RViz configuration to see your robot and the MoveIt Task Constructor solutions. First, start RViz. The following steps will cover how to set up RViz for MoveIt Task Constructor solution visualization.\nIf theMotionPlanningdisplay is active, uncheck it to hide it for now.\nUnderGlobal Options, change theFixed Framefrommaptopanda_link0if not already done.\nOn the bottom left of the window, click theAddbutton.\nUndermoveit_task_constructor_visualizationselectMotion Planning Tasksand click OK. TheMotion Planning Tasksdisplay should appear on the bottom left.\nIn theDisplays, underMotion Planning Tasks,  changeTask Solution Topicto/solution\nYou should see the panda arm in the main view with Motion Planning Tasks display open in the bottom left and nothing in it. Your MTC task will show up in this panel once you launch themtc_tutorialnode. If you’re usingmtc_demo.launch.pyfrom the tutorials, jump back in here.\nLaunch themtc_tutorialnode with\nYou should see the arm execute the task with the single stage to open the hand, with the cylinder in green in front of it. It should look something like this:\nIf you haven’t made your own package, but still want to see what this looks like, you can launch this file from the tutorials:\nSo far, we’ve walked through creating and executing a simple task, which runs but does not do much. Now, we will start adding the pick-and-place stages to the task. The image below shows an outline of the stages we will use in our task.\nWe will start adding stages after our existing open hand stage. Openmtc_node.cppand locate the following lines:\nWe need to move the arm to a position where we can pick up our object. This is done with aConnectstage, which as its name implies, is a Connector stage. This means that it tries to bridge between the results of the stage before and after it. This stage is initialized with a name,move_to_pick, and aGroupPlannerVectorthat specifies the planning group and the planner. We then set a timeout for the stage, set the properties for the stage, and add it to our task.\nNext, we create a pointer to a MoveIt Task Constructor stage object, and set it tonullptrfor now. Later, we will use this to save a stage.\nThis next block of code creates aSerialContainer.\nThis is a container that can be added to our task and can hold several substages.\nIn this case, we create a serial container that will contain the stages relevant to the picking action.\nInstead of adding the stages to the task, we will add the relevant stages to the serial container. We useexposeTo()to declare the task properties from the parent task in the new serial container, and useconfigureInitFrom()to initialize them.\nThis allows the contained stages to access these properties.\nWe then create a stage to approach the object. This stage is aMoveRelativestage, which allows us to specify a relative movement from our current position.MoveRelativeis a propagator stage: it receives the solution from its neighbouring stages and propagates it to the next or previous stage. Usingcartesian_plannerfinds a solution that involves moving the end effector in a straight line. We set the properties, and set the minimum and maximum distance to move. Now we create aVector3Stampedmessage to indicate the direction we want to move - in this case, in the Z direction from the hand frame. Finally, we add this stage to our serial container\nNow, create a stage to generate the grasp pose.\nThis is a generator stage, so it computes its results without regard to the stages before and after it.\nThe first stage,CurrentStateis a generator stage as well - to connect the first stage and this stage, a connecting stage must be used, which we already created above.\nThis code sets the stage properties, sets the pose before grasping, the angle delta, and the monitored stage.\nAngle delta is a property of theGenerateGraspPosestage that is used to determine the number of poses to generate; when generating solutions, MoveIt Task Constructor will try to grasp the object from many different orientations, with the difference between the orientations specified by the angle delta. The smaller the delta, the closer together the grasp orientations will be. When defining the current stage, we setcurrent_state_ptr, which is now used to forward information about the object pose and shape to the inverse kinematics solver.\nThis stage won’t be directly added to the serial container like previously, as we still need to do inverse kinematics on the poses it generates.\nBefore we compute inverse kinematics for the poses generated above, we first need to define the frame. This can be done with aPoseStampedmessage fromgeometry_msgsor in this case, we define the transform using Eigen transformation matrix and the name of the relevant link. Here, we define the transformation matrix.\nNow, we create theComputeIKstage, and give it the namegenerateposeIKas well as thegenerategraspposestage defined above. Some robots have multiple inverse kinematics solutions for a given pose - we set the limit on the amount of solutions to solve for up to 8. We also set the minimum solution distance, which is a threshold on how different solutions must be: if the joint positions in a solution are too similar to a previous solution, it will be marked as invalid. Next, we configure some additional properties, and add theComputeIKstage to the serial container.\nTo pick up the object, we must allow collision between the hand and the object. This can be done with aModifyPlanningScenestage. TheallowCollisionsfunction lets us specify which collisions to disable.allowCollisionscan be used with a container of names, so we can usegetLinkModelNamesWithCollisionGeometryto get all the names of links with collision geometry in the hand group.\nWith collisions allowed, we now can close the hand. This is done with aMoveTostage, similarly to theopenhandstage from above, except moving to thecloseposition as defined in the SRDF.\nWe now use aModifyPlanningScenestage again, this time to attach the object to the hand usingattachObject. Similarly to what we did with thecurrent_state_ptr, we get a pointer to this stage for later use when generating the place pose for the object.\nNext, we lift the object with aMoveRelativestage, similarly to theapproach_objectstage.\nWith this, we have all the stages needed to pick the object. Now, we add the serial container (with all its substages) to the task. If you build the package as-is, you can see the robot plan to pick up the object.\nTo test out the newly created stage, build the code and execute:\nNow that the stages that define the pick are complete, we move on to defining the stages for placing the object. Picking up where we left off, we add aConnectstage to connect the two, as we will soon be using a generator stage to generate the pose for placing the object.\nWe also create a serial container for the place stages. This is done similarly to the pick serial container.\nThe next stages will be added to the serial container rather than the task.\nThis next stage generates the poses used to place the object and compute the inverse kinematics for those poses - it is somewhat similar to thegenerategraspposestage from the pick serial container.\nWe start by creating a stage to generate the poses and inheriting the task properties.\nWe specify the pose where we want to place the object with aPoseStampedmessage fromgeometry_msgs- in this case, we choosey=0.5in the\"object\"frame.\nWe then pass the target pose to the stage withsetPose.\nNext, we usesetMonitoredStageand pass it the pointer to theattach_objectstage from earlier.\nThis allows the stage to know how the object is attached.\nWe then create aComputeIKstage and pass it ourGeneratePlacePosestage - the rest follows the same logic as above with the pick stages.\nNow that we’re ready to place the object, we open the hand withMoveTostage and the joint interpolation planner.\nWe also can re-enable collisions with the object now that we no longer need to hold it.\nThis is done usingallowCollisionsalmost exactly the same way as disabling collisions, except setting the last argument tofalserather thantrue.\nNow, we can detach the object usingdetachObject.\nWe retreat from the object using aMoveRelativestage, which is done similarly to theapproachobjectandliftobjectstages.\nWe finish our place serial container and add it to the task.\nThe final step is to return home: we use aMoveTostage and pass it the goal pose ofready, which is a pose defined in the Panda SRDF.\nAll these stages should be added above these lines.\nCongratulations! You’ve now defined a pick and place task using MoveIt Task Constructor! To try it out, build the code and execute:\nThe task with each comprising stage is shown in the Motion Planning Tasks pane. Click on a stage and additional information about the stage will show up to the right. The right pane shows different solutions as well as their associated costs. Depending on the stage type and the robot configuration, there may only be one solution shown.\nClick one of the solution costs to see an animation of the robot following the plan for that stage. Click the “Exec” button in the upper-right portion of the pane to execute the motion.\nTo run the complete MoveIt Task Constructor example included with the MoveIt tutorials:\nAnd in a second terminal:\nWhen running MTC, it prints a diagram like this to terminal:\nThis example^ shows two stages. The first stage (“initial_state”) is aCurrentStatetype of stage, which initializes aPlanningSceneand captures any collision objects that are present at that moment.\nA pointer to this stage can be used to retrieve the state of the robot.\nSinceCurrentStateinherits fromGenerator, it propagates solutions both forward and backward.\nThis is denoted by the arrows in both directions.\nThe first1indicates that one solution was successfully propagated backwards to the previous stage.\nThe second1, between the arrows, indicates that one solution was generated.\nThe0indicates that a solution was not propagated forward successfully to the next stage, because the next stage failed.\nThe second stage (“move_to_home”) is aMoveTotype of stage. It inherits its propagation direction from the previous stage, so both arrows point forward. The0’s indicate that this stage failed completely. From left to right, the0’s mean:\nThe stage did not receive a solution from the previous stage\nThe stage did not generate a solution\nThe stage did not propagate a solution forward to the next stage\nIn this case, we could tell that “move_to_home” was the root cause of the failure. The problem was a home state that was in collision. Defining a new, collision-free home position fixed the issue.\nInformation about individual stages can be retrieved from the task. For example, here we retrieve the unique ID for a stage:\nACurrentStatetype stage does not just retrieve the current state of the robot.\nIt also initializes aPlanningSceneobject, capturing any collision objects that are present at that moment.\nMTC stages can be propagated in forward and backward order.\nYou can easily check which direction a stage propagates by the arrow in the RViz GUI.\nWhen propagating backwards, the logic of many operations is reversed.\nFor example, to allow collisions with an object in aModifyPlanningScenestage, you would callallowCollisions(false)rather thanallowCollisions(true). There is a discussion to be readhere.\n© Copyright 2024, PickNik Robotics.\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit_task_constructor.git\nrosdep install --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncd~/ws_moveitcolconbuild--mixinrelease\nros2launchmoveit_task_constructor_demodemo.launch.py\nros2launchmoveit_task_constructor_demorun.launch.pyexe:=cartesianros2launchmoveit_task_constructor_demorun.launch.pyexe:=modularros2launchmoveit_task_constructor_demorun.launch.pyexe:=pick_place_demo\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_task_constructor_corerclcpp\\--node-namemtc_nodemtc_tutorial\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endifstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;classMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};MTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}rclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}voidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;pose.orientation.w=1.0;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}voidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}mtc::TaskMTCTaskNode::createTask(){mtc::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);// Disable warnings for this line, as it's a variable that's set but not used in this example#pragma GCC diagnostic push#pragma GCC diagnostic ignored \"-Wunused-but-set-variable\"mtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generator#pragma GCC diagnostic popautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));autosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);autointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();autocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);autostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}intmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endif\nstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;\nclassMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};\nMTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}\nrclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}\nvoidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}\nvoidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}\nmtc::TaskMTCTaskNode::createTask(){moveit::task_constructor::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);\nmtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generatorautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));\nautosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);\nautointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}\nintmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\nfromlaunchimportLaunchDescriptionfromlaunch_ros.actionsimportNodefrommoveit_configs_utilsimportMoveItConfigsBuilderdefgenerate_launch_description():moveit_config=MoveItConfigsBuilder(\"moveit_resources_panda\").to_dict()# MTC Demo nodepick_place_demo=Node(package=\"mtc_tutorial\",executable=\"mtc_node\",output=\"screen\",parameters=[moveit_config,],)returnLaunchDescription([pick_place_demo])\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}\n  )\ncd~/ws_moveitcolconbuild--mixinreleasesource~/ws_moveit/install/setup.bash\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo_minimal.launch.py\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));// Add the next lines of codes to define more stages here\nautostage_move_to_pick=std::make_unique<mtc::stages::Connect>(\"move to pick\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner}});stage_move_to_pick->setTimeout(5.0);stage_move_to_pick->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_pick));\nmtc::Stage*attach_object_stage=nullptr;// Forward attach_object_stage to place pose generator\n{autograsp=std::make_unique<mtc::SerialContainer>(\"pick object\");task.properties().exposeTo(grasp->properties(),{\"eef\",\"group\",\"ik_frame\"});grasp->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"approach object\",cartesian_planner);stage->properties().set(\"marker_ns\",\"approach_object\");stage->properties().set(\"link\",hand_frame);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.15);// Set hand forward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=hand_frame;vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\n{// Sample grasp poseautostage=std::make_unique<mtc::stages::GenerateGraspPose>(\"generate grasp pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"grasp_pose\");stage->setPreGraspPose(\"open\");stage->setObject(\"object\");stage->setAngleDelta(M_PI/12);stage->setMonitoredStage(current_state_ptr);// Hook into current state\nEigen::Isometry3dgrasp_frame_transform;Eigen::Quaterniondq=Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitX())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitY())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitZ());grasp_frame_transform.linear()=q.matrix();grasp_frame_transform.translation().z()=0.1;\n// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"grasp pose IK\",std::move(stage));wrapper->setMaxIKSolutions(8);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(grasp_frame_transform,hand_frame);wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});grasp->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"allow collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),true);grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"close hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"close\");grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"attach object\");stage->attachObject(\"object\",hand_frame);attach_object_stage=stage.get();grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"lift object\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"lift_object\");// Set upward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\ntask.add(std::move(grasp));}\nros2launchmtc_tutorialpick_place_demo.launch.py\n{autostage_move_to_place=std::make_unique<mtc::stages::Connect>(\"move to place\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner},{hand_group_name,interpolation_planner}});stage_move_to_place->setTimeout(5.0);stage_move_to_place->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_place));}\n{autoplace=std::make_unique<mtc::SerialContainer>(\"place object\");task.properties().exposeTo(place->properties(),{\"eef\",\"group\",\"ik_frame\"});place->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{// Sample place poseautostage=std::make_unique<mtc::stages::GeneratePlacePose>(\"generate place pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"place_pose\");stage->setObject(\"object\");geometry_msgs::msg::PoseStampedtarget_pose_msg;target_pose_msg.header.frame_id=\"object\";target_pose_msg.pose.position.y=0.5;target_pose_msg.pose.orientation.w=1.0;stage->setPose(target_pose_msg);stage->setMonitoredStage(attach_object_stage);// Hook into attach_object_stage// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"place pose IK\",std::move(stage));wrapper->setMaxIKSolutions(2);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(\"object\");wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});place->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"open\");place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"forbid collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),false);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"detach object\");stage->detachObject(\"object\",hand_frame);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"retreat\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"retreat\");// Set retreat directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.x=-0.5;stage->setDirection(vec);place->insert(std::move(stage));}\ntask.add(std::move(place));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"return home\",interpolation_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setGoal(\"ready\");task.add(std::move(stage));}\n// Stages all added to the task above this linereturntask;}\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmoveit2_tutorialspick_place_demo.launch.py\n[demo_node-1]1-←1→-0/initial_state[demo_node-1]-0→0→-0/move_to_home\nuint32_tconstunique_stage_id=task_.stages()->findChild(stage_name)->introspectionId();\n<branch>\nhumble\nros2\nmain\nmtc_tutorial\nmoveit_task_constructor_core\nsrc/mtc_node\nmtc_node.cpp\nrclcpp/rclcpp.hpp\nmoveit/planning_scene/planning_scene.h\nmoveit/planning_scene_interface/planning_scene_interface.h\nmoveit/task_constructor/task.h\nmoveit/task_constructor/solvers.h\nmoveit/task_constructor/stages.h\ntf2_geometry_msgs/tf2_geometry_msgs.hpp\ntf2_eigen/tf2_eigen.hpp\nmoveit::task_constructor\nMTCTaskNode\nobject.primitives[0].dimensions\npose.position.x\npose.position.y\ncreateTask\ntask.init()\ntask.plan(5)\ntask.execute()\ntask.setProperty(property_name,value)\ncurrent_state_ptr\nnullptr\ncurrent_state\nCurrentState\ncurrent_state_ptr\nMoveTo\ncreateTask()\nmain\nmove_group\nros2_control\nstatic_tf\nrobot_state_publisher\nrviz\nmtc_tutorial\npackage\nexecutable\npick_place_demo.launch.py\nCMakeLists.txt\nmap\npanda_link0\nmoveit_task_constructor_visualization\n/solution\nmtc_tutorial\nmtc_demo.launch.py\nmtc_tutorial\nmtc_node.cpp\nConnect\nmove_to_pick\nGroupPlannerVector\nnullptr\nSerialContainer\nexposeTo()\nconfigureInitFrom()\nMoveRelative\nMoveRelative\ncartesian_planner\nVector3Stamped\nCurrentState\nGenerateGraspPose\ncurrent_state_ptr\nPoseStamped\ngeometry_msgs\nComputeIK\ngenerateposeIK\ngenerategrasppose\nComputeIK\nModifyPlanningScene\nallowCollisions\nallowCollisions\ngetLinkModelNamesWithCollisionGeometry\nMoveTo\nopenhand\nclose\nModifyPlanningScene\nattachObject\ncurrent_state_ptr\nMoveRelative\napproach_object\nConnect\ngenerategrasppose\nPoseStamped\ngeometry_msgs\ny=0.5\n\"object\"\nsetPose\nsetMonitoredStage\nattach_object\nComputeIK\nGeneratePlacePose\nMoveTo\nallowCollisions\nfalse\ntrue\ndetachObject\nMoveRelative\napproachobject\nliftobject\nMoveTo\nready\nCurrentState\nPlanningScene\nCurrentState\nGenerator\n1\n1\n0\nMoveTo\n0\n0\nCurrentState\nPlanningScene\nModifyPlanningScene\nallowCollisions(false)\nallowCollisions(true)",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0df"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/pick_and_place_with_moveit_task_constructor/pick_and_place_with_moveit_task_constructor.html#launching-the-demo",
  "title": "Pick and Place with MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "Pick and Place with MoveIt Task Constructor\n1 Basic Concepts\n2 Getting Started\n3 Trying It Out\n4 Setting up a Project with MoveIt Task Constructor\n5 Running the Demo\n6 Adding Stages\n7 Further Discussion\nThis tutorial will walk you through creating a package that plans a pick and place operation usingMoveIt Task Constructor. MoveIt Task Constructor provides a way to plan for tasks that consist of multiple different subtasks (known as stages). If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nThe fundamental idea of MTC is that complex motion planning problems can be composed into a set of simpler subproblems.\nThe top-level planning problem is specified as aTaskwhile all subproblems are specified byStages.\nStages can be arranged in any arbitrary order and hierarchy only limited by the individual stages types.\nThe order in which stages can be arranged is restricted by the direction in which results are passed.\nThere are three possible stages relating to the result flow: generator, propagator, and connector stages:\nGeneratorscompute their results independently of their neighbor stages and pass them in both directions, backwards and forwards.\nAn example is an IK sampler for geometric poses where approaching and departing motions (neighbor stages) depend on the solution.\nPropagatorsreceive the result of one neighbor stage, solve a subproblem and then propagate their result to the neighbor on the opposite site.\nDepending on the implementation, propagating stages can pass solutions forward, backward or in both directions separately.\nAn example is a stage that computes a Cartesian path based on either a start or a goal state.\nConnectorsdo not propagate any results, but rather attempt to bridge the gap between the resulting states of both neighbors.\nAn example is the computation of a free-motion plan from one given state to another.\nAdditional to the order types, there are different hierarchy types allowing to encapsulate subordinate stages.\nStages without subordinate stages are calledprimitive stages, higher-level stages are calledcontainer stages.\nThere are three container types:\nWrappersencapsulate a single subordinate stage and modify or filter the results.\nFor example, a filter stage that only accepts solutions of its child stage that satisfy a certain constraint can be realized as a wrapper.\nAnother standard use of this type includes the IK wrapper stage, which generates inverse kinematics solutions based on planning scenes annotated with a pose target property.\nSerial Containershold a sequence of subordinate stages and only consider end-to-end solutions as results.\nAn example is a picking motion that consists of a sequence of coherent steps.\nParallel Containerscombine set of subordinate stages and can be used for passing the best of alternative results, running fallback solvers or for merging multiple independent solutions.\nExamples are running alternative planners for a free-motion plan, picking objects with the right hand or with the left hand as a fallback, or moving the arm and opening the gripper at the same time.\nStages not only support solving motion planning problems.\nThey can also be used for all kinds of state transitions, as for instance modifying the planning scene.\nCombined with the possibility of using class inheritance it is possible to construct very complex behavior while only relying on a well-structured set of primitive stages.\nMore detailed information on MTC can be found in theMoveIt Task Constructor concepts page\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nMove into your colcon workspace and pull the MoveIt Task Constructor source, where<branch>can be e.g.humblefor ROS Humble, orros2for the latest version compatible with MoveIt 2main:\nInstall missing packages with rosdep:\nBuild the workspace:\nThe MoveIt Task Constructor package contains several basic examples and a pick-and-place demo.\nFor all demos you should launch the basic environment:\nSubsequently, you can run the individual demos:\nOn the right side you should see theMotion Planning Taskspanel outlining the hierarchical stage structure of the tasks.\nWhen you select a particular stage, the list of successful and failed solutions will be\nshown in the right-most window. Selecting one of those solutions will start its visualization.\nThis section walks through the steps required to build a simple task with MoveIt Task Constructor.\nCreate a new package with the following command:\nThis will create a new package and folder calledmtc_tutorialwith a dependency onmoveit_task_constructor_coreas well as a hello world example insrc/mtc_node.\nOpenmtc_node.cppin your editor of choice, and paste in the following code.\nThe top of the code includes the ROS and MoveIt Libraries that this package uses.\nrclcpp/rclcpp.hppincludes core ROS2 functionality\nmoveit/planning_scene/planning_scene.handmoveit/planning_scene_interface/planning_scene_interface.hinclude functionality to interface with the robot model and collision objects\nmoveit/task_constructor/task.h,moveit/task_constructor/solvers.h, andmoveit/task_constructor/stages.hinclude different components of MoveIt Task Constructor that are used in the example\ntf2_geometry_msgs/tf2_geometry_msgs.hppandtf2_eigen/tf2_eigen.hppwon’t be used in this initial example, but they will be used for pose generation when we add more stages to the MoveIt Task Constructor task.\nThe next line gets a logger for our new node. We also create a namespace alias formoveit::task_constructorfor convenience.\nWe start by defining a class that will contain the main MoveIt Task Constructor functionality.  We also declare the MoveIt Task Constructor task object as a member variable for our class: this isn’t strictly necessary for a given application, but it helps save the task for later visualization purposes. We will explore each function individually below.\nThese lines initialize the node with specified options (it is the constructor of ourMTCTaskNodeclass).\nThese next lines define a getter function to get the node base interface, which will be used for the executor later.\nThis class method is used to set up the planning scene that is used in the example. It creates a cylinder with dimensions specified byobject.primitives[0].dimensionsand position specified bypose.position.xandpose.position.y.\nYou can try changing these numbers to resize and move the cylinder around. If we move the cylinder out of the robot’s reach, planning will fail.\nThis function interfaces with the MoveIt Task Constructor task object. It first creates a task, which includes setting some properties and adding stages. This will be discussed further in thecreateTaskfunction definition. Next,task.init()initializes the task andtask.plan(5)generates a plan, stopping after 5 successful plans are found. The next line publishes the solution to be visualized in RViz - this line can be removed if you don’t care for visualization. Finally,task.execute()executes the plan. Execution occurs via an action server interface with the RViz plugin.\nAs mentioned above, this function creates a MoveIt Task Constructor object and sets some initial properties. In this case, we set the task name to “demo_task”, load the robot model, define the names of some useful frames, and set those frame names as properties of the task withtask.setProperty(property_name,value). The next few code blocks will fill out this function body.\nNow, we add an example stage to the node. The first line setscurrent_state_ptrtonullptr; this creates a pointer to a stage such that we can reuse stage information in specific scenarios. This line is not used at this moment, but will be used later when more stages are added to the task. Next, we make acurrent_statestage (a generator stage) and add it to our task - this starts the robot off in its current state. Now that we’ve created theCurrentStatestage, we save a pointer to it in thecurrent_state_ptrfor later use.\nSolvers are used to define the type of robot motion. MoveIt Task Constructor has three options for solvers:\nPipelinePlanneruses MoveIt’s planning pipeline, which typically defaults toOMPL.\nJointInterpolationis a simple planner that interpolates between the start and goal joint states. It is typically used for simple motions as it computes quickly but doesn’t support complex motions.\nCartesianPathis used to move the end effector in a straight line in Cartesian space.\nFeel free to try out the different solvers and see how the robot motion changes. For the first stage we will use the Cartesian planner, which requires the following properties to be set:\nNow that we added in the planners, we can add a stage that will move the robot.\nThe following lines use aMoveTostage (a propagator stage). Since opening the hand is a relatively simple movement, we can use the joint interpolation planner.\nThis stage plans a move to the “open hand” pose, which is a named pose defined in theSRDFfor the Panda robot.\nWe return the task and finish with thecreateTask()function.\nFinally, we havemain: the following lines create a node using the class defined above, and calls the class methods to set up and execute a basic MTC task. In this example, we do not cancel the executor once the task has finished executing to keep the node alive to inspect the solutions in RViz.\nWe will need a launch file to launch themove_group,ros2_control,static_tf,robot_state_publisher, andrviznodes that provide us the environment to run the demo. The one we will use for this example can be foundhere.\nTo run the MoveIt Task Constructor node, we will use a second launch file to start themtc_tutorialexecutable with the proper parameters. Here we can load URDF, SRDF, and OMPL parameters, or use MoveIt Configs Utils to do so. Your launch file should look something like the one found in this tutorial packagehere(pay close attention to thepackageandexecutablearguments below as they are different from the launch file linked) :\nSave a launch file aspick_place_demo.launch.pyor download one to the package’s launch directory. Make sure to edit theCMakeLists.txtso it includes the launch folder by adding the following lines:\nNow we can build and source the colcon workspace.\nStart by launching the first launch file. If you want to use the one provided by the tutorials:\nRViz will now load. If you’re using your own launch file and haven’t included an rviz configsuch as this, you will need to configure RViz before you see anything displayed. If you’re using the launch file from the tutorials package, RViz will already be configured for you and you can jump to the end of the next section.\nIf you are not using the RViz configuration provided, we’ll have to make some changes to the RViz configuration to see your robot and the MoveIt Task Constructor solutions. First, start RViz. The following steps will cover how to set up RViz for MoveIt Task Constructor solution visualization.\nIf theMotionPlanningdisplay is active, uncheck it to hide it for now.\nUnderGlobal Options, change theFixed Framefrommaptopanda_link0if not already done.\nOn the bottom left of the window, click theAddbutton.\nUndermoveit_task_constructor_visualizationselectMotion Planning Tasksand click OK. TheMotion Planning Tasksdisplay should appear on the bottom left.\nIn theDisplays, underMotion Planning Tasks,  changeTask Solution Topicto/solution\nYou should see the panda arm in the main view with Motion Planning Tasks display open in the bottom left and nothing in it. Your MTC task will show up in this panel once you launch themtc_tutorialnode. If you’re usingmtc_demo.launch.pyfrom the tutorials, jump back in here.\nLaunch themtc_tutorialnode with\nYou should see the arm execute the task with the single stage to open the hand, with the cylinder in green in front of it. It should look something like this:\nIf you haven’t made your own package, but still want to see what this looks like, you can launch this file from the tutorials:\nSo far, we’ve walked through creating and executing a simple task, which runs but does not do much. Now, we will start adding the pick-and-place stages to the task. The image below shows an outline of the stages we will use in our task.\nWe will start adding stages after our existing open hand stage. Openmtc_node.cppand locate the following lines:\nWe need to move the arm to a position where we can pick up our object. This is done with aConnectstage, which as its name implies, is a Connector stage. This means that it tries to bridge between the results of the stage before and after it. This stage is initialized with a name,move_to_pick, and aGroupPlannerVectorthat specifies the planning group and the planner. We then set a timeout for the stage, set the properties for the stage, and add it to our task.\nNext, we create a pointer to a MoveIt Task Constructor stage object, and set it tonullptrfor now. Later, we will use this to save a stage.\nThis next block of code creates aSerialContainer.\nThis is a container that can be added to our task and can hold several substages.\nIn this case, we create a serial container that will contain the stages relevant to the picking action.\nInstead of adding the stages to the task, we will add the relevant stages to the serial container. We useexposeTo()to declare the task properties from the parent task in the new serial container, and useconfigureInitFrom()to initialize them.\nThis allows the contained stages to access these properties.\nWe then create a stage to approach the object. This stage is aMoveRelativestage, which allows us to specify a relative movement from our current position.MoveRelativeis a propagator stage: it receives the solution from its neighbouring stages and propagates it to the next or previous stage. Usingcartesian_plannerfinds a solution that involves moving the end effector in a straight line. We set the properties, and set the minimum and maximum distance to move. Now we create aVector3Stampedmessage to indicate the direction we want to move - in this case, in the Z direction from the hand frame. Finally, we add this stage to our serial container\nNow, create a stage to generate the grasp pose.\nThis is a generator stage, so it computes its results without regard to the stages before and after it.\nThe first stage,CurrentStateis a generator stage as well - to connect the first stage and this stage, a connecting stage must be used, which we already created above.\nThis code sets the stage properties, sets the pose before grasping, the angle delta, and the monitored stage.\nAngle delta is a property of theGenerateGraspPosestage that is used to determine the number of poses to generate; when generating solutions, MoveIt Task Constructor will try to grasp the object from many different orientations, with the difference between the orientations specified by the angle delta. The smaller the delta, the closer together the grasp orientations will be. When defining the current stage, we setcurrent_state_ptr, which is now used to forward information about the object pose and shape to the inverse kinematics solver.\nThis stage won’t be directly added to the serial container like previously, as we still need to do inverse kinematics on the poses it generates.\nBefore we compute inverse kinematics for the poses generated above, we first need to define the frame. This can be done with aPoseStampedmessage fromgeometry_msgsor in this case, we define the transform using Eigen transformation matrix and the name of the relevant link. Here, we define the transformation matrix.\nNow, we create theComputeIKstage, and give it the namegenerateposeIKas well as thegenerategraspposestage defined above. Some robots have multiple inverse kinematics solutions for a given pose - we set the limit on the amount of solutions to solve for up to 8. We also set the minimum solution distance, which is a threshold on how different solutions must be: if the joint positions in a solution are too similar to a previous solution, it will be marked as invalid. Next, we configure some additional properties, and add theComputeIKstage to the serial container.\nTo pick up the object, we must allow collision between the hand and the object. This can be done with aModifyPlanningScenestage. TheallowCollisionsfunction lets us specify which collisions to disable.allowCollisionscan be used with a container of names, so we can usegetLinkModelNamesWithCollisionGeometryto get all the names of links with collision geometry in the hand group.\nWith collisions allowed, we now can close the hand. This is done with aMoveTostage, similarly to theopenhandstage from above, except moving to thecloseposition as defined in the SRDF.\nWe now use aModifyPlanningScenestage again, this time to attach the object to the hand usingattachObject. Similarly to what we did with thecurrent_state_ptr, we get a pointer to this stage for later use when generating the place pose for the object.\nNext, we lift the object with aMoveRelativestage, similarly to theapproach_objectstage.\nWith this, we have all the stages needed to pick the object. Now, we add the serial container (with all its substages) to the task. If you build the package as-is, you can see the robot plan to pick up the object.\nTo test out the newly created stage, build the code and execute:\nNow that the stages that define the pick are complete, we move on to defining the stages for placing the object. Picking up where we left off, we add aConnectstage to connect the two, as we will soon be using a generator stage to generate the pose for placing the object.\nWe also create a serial container for the place stages. This is done similarly to the pick serial container.\nThe next stages will be added to the serial container rather than the task.\nThis next stage generates the poses used to place the object and compute the inverse kinematics for those poses - it is somewhat similar to thegenerategraspposestage from the pick serial container.\nWe start by creating a stage to generate the poses and inheriting the task properties.\nWe specify the pose where we want to place the object with aPoseStampedmessage fromgeometry_msgs- in this case, we choosey=0.5in the\"object\"frame.\nWe then pass the target pose to the stage withsetPose.\nNext, we usesetMonitoredStageand pass it the pointer to theattach_objectstage from earlier.\nThis allows the stage to know how the object is attached.\nWe then create aComputeIKstage and pass it ourGeneratePlacePosestage - the rest follows the same logic as above with the pick stages.\nNow that we’re ready to place the object, we open the hand withMoveTostage and the joint interpolation planner.\nWe also can re-enable collisions with the object now that we no longer need to hold it.\nThis is done usingallowCollisionsalmost exactly the same way as disabling collisions, except setting the last argument tofalserather thantrue.\nNow, we can detach the object usingdetachObject.\nWe retreat from the object using aMoveRelativestage, which is done similarly to theapproachobjectandliftobjectstages.\nWe finish our place serial container and add it to the task.\nThe final step is to return home: we use aMoveTostage and pass it the goal pose ofready, which is a pose defined in the Panda SRDF.\nAll these stages should be added above these lines.\nCongratulations! You’ve now defined a pick and place task using MoveIt Task Constructor! To try it out, build the code and execute:\nThe task with each comprising stage is shown in the Motion Planning Tasks pane. Click on a stage and additional information about the stage will show up to the right. The right pane shows different solutions as well as their associated costs. Depending on the stage type and the robot configuration, there may only be one solution shown.\nClick one of the solution costs to see an animation of the robot following the plan for that stage. Click the “Exec” button in the upper-right portion of the pane to execute the motion.\nTo run the complete MoveIt Task Constructor example included with the MoveIt tutorials:\nAnd in a second terminal:\nWhen running MTC, it prints a diagram like this to terminal:\nThis example^ shows two stages. The first stage (“initial_state”) is aCurrentStatetype of stage, which initializes aPlanningSceneand captures any collision objects that are present at that moment.\nA pointer to this stage can be used to retrieve the state of the robot.\nSinceCurrentStateinherits fromGenerator, it propagates solutions both forward and backward.\nThis is denoted by the arrows in both directions.\nThe first1indicates that one solution was successfully propagated backwards to the previous stage.\nThe second1, between the arrows, indicates that one solution was generated.\nThe0indicates that a solution was not propagated forward successfully to the next stage, because the next stage failed.\nThe second stage (“move_to_home”) is aMoveTotype of stage. It inherits its propagation direction from the previous stage, so both arrows point forward. The0’s indicate that this stage failed completely. From left to right, the0’s mean:\nThe stage did not receive a solution from the previous stage\nThe stage did not generate a solution\nThe stage did not propagate a solution forward to the next stage\nIn this case, we could tell that “move_to_home” was the root cause of the failure. The problem was a home state that was in collision. Defining a new, collision-free home position fixed the issue.\nInformation about individual stages can be retrieved from the task. For example, here we retrieve the unique ID for a stage:\nACurrentStatetype stage does not just retrieve the current state of the robot.\nIt also initializes aPlanningSceneobject, capturing any collision objects that are present at that moment.\nMTC stages can be propagated in forward and backward order.\nYou can easily check which direction a stage propagates by the arrow in the RViz GUI.\nWhen propagating backwards, the logic of many operations is reversed.\nFor example, to allow collisions with an object in aModifyPlanningScenestage, you would callallowCollisions(false)rather thanallowCollisions(true). There is a discussion to be readhere.\n© Copyright 2024, PickNik Robotics.\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit_task_constructor.git\nrosdep install --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncd~/ws_moveitcolconbuild--mixinrelease\nros2launchmoveit_task_constructor_demodemo.launch.py\nros2launchmoveit_task_constructor_demorun.launch.pyexe:=cartesianros2launchmoveit_task_constructor_demorun.launch.pyexe:=modularros2launchmoveit_task_constructor_demorun.launch.pyexe:=pick_place_demo\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_task_constructor_corerclcpp\\--node-namemtc_nodemtc_tutorial\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endifstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;classMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};MTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}rclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}voidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;pose.orientation.w=1.0;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}voidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}mtc::TaskMTCTaskNode::createTask(){mtc::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);// Disable warnings for this line, as it's a variable that's set but not used in this example#pragma GCC diagnostic push#pragma GCC diagnostic ignored \"-Wunused-but-set-variable\"mtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generator#pragma GCC diagnostic popautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));autosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);autointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();autocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);autostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}intmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endif\nstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;\nclassMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};\nMTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}\nrclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}\nvoidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}\nvoidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}\nmtc::TaskMTCTaskNode::createTask(){moveit::task_constructor::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);\nmtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generatorautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));\nautosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);\nautointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}\nintmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\nfromlaunchimportLaunchDescriptionfromlaunch_ros.actionsimportNodefrommoveit_configs_utilsimportMoveItConfigsBuilderdefgenerate_launch_description():moveit_config=MoveItConfigsBuilder(\"moveit_resources_panda\").to_dict()# MTC Demo nodepick_place_demo=Node(package=\"mtc_tutorial\",executable=\"mtc_node\",output=\"screen\",parameters=[moveit_config,],)returnLaunchDescription([pick_place_demo])\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}\n  )\ncd~/ws_moveitcolconbuild--mixinreleasesource~/ws_moveit/install/setup.bash\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo_minimal.launch.py\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));// Add the next lines of codes to define more stages here\nautostage_move_to_pick=std::make_unique<mtc::stages::Connect>(\"move to pick\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner}});stage_move_to_pick->setTimeout(5.0);stage_move_to_pick->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_pick));\nmtc::Stage*attach_object_stage=nullptr;// Forward attach_object_stage to place pose generator\n{autograsp=std::make_unique<mtc::SerialContainer>(\"pick object\");task.properties().exposeTo(grasp->properties(),{\"eef\",\"group\",\"ik_frame\"});grasp->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"approach object\",cartesian_planner);stage->properties().set(\"marker_ns\",\"approach_object\");stage->properties().set(\"link\",hand_frame);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.15);// Set hand forward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=hand_frame;vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\n{// Sample grasp poseautostage=std::make_unique<mtc::stages::GenerateGraspPose>(\"generate grasp pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"grasp_pose\");stage->setPreGraspPose(\"open\");stage->setObject(\"object\");stage->setAngleDelta(M_PI/12);stage->setMonitoredStage(current_state_ptr);// Hook into current state\nEigen::Isometry3dgrasp_frame_transform;Eigen::Quaterniondq=Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitX())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitY())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitZ());grasp_frame_transform.linear()=q.matrix();grasp_frame_transform.translation().z()=0.1;\n// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"grasp pose IK\",std::move(stage));wrapper->setMaxIKSolutions(8);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(grasp_frame_transform,hand_frame);wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});grasp->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"allow collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),true);grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"close hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"close\");grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"attach object\");stage->attachObject(\"object\",hand_frame);attach_object_stage=stage.get();grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"lift object\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"lift_object\");// Set upward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\ntask.add(std::move(grasp));}\nros2launchmtc_tutorialpick_place_demo.launch.py\n{autostage_move_to_place=std::make_unique<mtc::stages::Connect>(\"move to place\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner},{hand_group_name,interpolation_planner}});stage_move_to_place->setTimeout(5.0);stage_move_to_place->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_place));}\n{autoplace=std::make_unique<mtc::SerialContainer>(\"place object\");task.properties().exposeTo(place->properties(),{\"eef\",\"group\",\"ik_frame\"});place->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{// Sample place poseautostage=std::make_unique<mtc::stages::GeneratePlacePose>(\"generate place pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"place_pose\");stage->setObject(\"object\");geometry_msgs::msg::PoseStampedtarget_pose_msg;target_pose_msg.header.frame_id=\"object\";target_pose_msg.pose.position.y=0.5;target_pose_msg.pose.orientation.w=1.0;stage->setPose(target_pose_msg);stage->setMonitoredStage(attach_object_stage);// Hook into attach_object_stage// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"place pose IK\",std::move(stage));wrapper->setMaxIKSolutions(2);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(\"object\");wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});place->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"open\");place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"forbid collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),false);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"detach object\");stage->detachObject(\"object\",hand_frame);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"retreat\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"retreat\");// Set retreat directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.x=-0.5;stage->setDirection(vec);place->insert(std::move(stage));}\ntask.add(std::move(place));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"return home\",interpolation_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setGoal(\"ready\");task.add(std::move(stage));}\n// Stages all added to the task above this linereturntask;}\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmoveit2_tutorialspick_place_demo.launch.py\n[demo_node-1]1-←1→-0/initial_state[demo_node-1]-0→0→-0/move_to_home\nuint32_tconstunique_stage_id=task_.stages()->findChild(stage_name)->introspectionId();\n<branch>\nhumble\nros2\nmain\nmtc_tutorial\nmoveit_task_constructor_core\nsrc/mtc_node\nmtc_node.cpp\nrclcpp/rclcpp.hpp\nmoveit/planning_scene/planning_scene.h\nmoveit/planning_scene_interface/planning_scene_interface.h\nmoveit/task_constructor/task.h\nmoveit/task_constructor/solvers.h\nmoveit/task_constructor/stages.h\ntf2_geometry_msgs/tf2_geometry_msgs.hpp\ntf2_eigen/tf2_eigen.hpp\nmoveit::task_constructor\nMTCTaskNode\nobject.primitives[0].dimensions\npose.position.x\npose.position.y\ncreateTask\ntask.init()\ntask.plan(5)\ntask.execute()\ntask.setProperty(property_name,value)\ncurrent_state_ptr\nnullptr\ncurrent_state\nCurrentState\ncurrent_state_ptr\nMoveTo\ncreateTask()\nmain\nmove_group\nros2_control\nstatic_tf\nrobot_state_publisher\nrviz\nmtc_tutorial\npackage\nexecutable\npick_place_demo.launch.py\nCMakeLists.txt\nmap\npanda_link0\nmoveit_task_constructor_visualization\n/solution\nmtc_tutorial\nmtc_demo.launch.py\nmtc_tutorial\nmtc_node.cpp\nConnect\nmove_to_pick\nGroupPlannerVector\nnullptr\nSerialContainer\nexposeTo()\nconfigureInitFrom()\nMoveRelative\nMoveRelative\ncartesian_planner\nVector3Stamped\nCurrentState\nGenerateGraspPose\ncurrent_state_ptr\nPoseStamped\ngeometry_msgs\nComputeIK\ngenerateposeIK\ngenerategrasppose\nComputeIK\nModifyPlanningScene\nallowCollisions\nallowCollisions\ngetLinkModelNamesWithCollisionGeometry\nMoveTo\nopenhand\nclose\nModifyPlanningScene\nattachObject\ncurrent_state_ptr\nMoveRelative\napproach_object\nConnect\ngenerategrasppose\nPoseStamped\ngeometry_msgs\ny=0.5\n\"object\"\nsetPose\nsetMonitoredStage\nattach_object\nComputeIK\nGeneratePlacePose\nMoveTo\nallowCollisions\nfalse\ntrue\ndetachObject\nMoveRelative\napproachobject\nliftobject\nMoveTo\nready\nCurrentState\nPlanningScene\nCurrentState\nGenerator\n1\n1\n0\nMoveTo\n0\n0\nCurrentState\nPlanningScene\nModifyPlanningScene\nallowCollisions(false)\nallowCollisions(true)",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0e0"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/pick_and_place_with_moveit_task_constructor/pick_and_place_with_moveit_task_constructor.html#adding-stages",
  "title": "Pick and Place with MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "Pick and Place with MoveIt Task Constructor\n1 Basic Concepts\n2 Getting Started\n3 Trying It Out\n4 Setting up a Project with MoveIt Task Constructor\n5 Running the Demo\n6 Adding Stages\n7 Further Discussion\nThis tutorial will walk you through creating a package that plans a pick and place operation usingMoveIt Task Constructor. MoveIt Task Constructor provides a way to plan for tasks that consist of multiple different subtasks (known as stages). If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nThe fundamental idea of MTC is that complex motion planning problems can be composed into a set of simpler subproblems.\nThe top-level planning problem is specified as aTaskwhile all subproblems are specified byStages.\nStages can be arranged in any arbitrary order and hierarchy only limited by the individual stages types.\nThe order in which stages can be arranged is restricted by the direction in which results are passed.\nThere are three possible stages relating to the result flow: generator, propagator, and connector stages:\nGeneratorscompute their results independently of their neighbor stages and pass them in both directions, backwards and forwards.\nAn example is an IK sampler for geometric poses where approaching and departing motions (neighbor stages) depend on the solution.\nPropagatorsreceive the result of one neighbor stage, solve a subproblem and then propagate their result to the neighbor on the opposite site.\nDepending on the implementation, propagating stages can pass solutions forward, backward or in both directions separately.\nAn example is a stage that computes a Cartesian path based on either a start or a goal state.\nConnectorsdo not propagate any results, but rather attempt to bridge the gap between the resulting states of both neighbors.\nAn example is the computation of a free-motion plan from one given state to another.\nAdditional to the order types, there are different hierarchy types allowing to encapsulate subordinate stages.\nStages without subordinate stages are calledprimitive stages, higher-level stages are calledcontainer stages.\nThere are three container types:\nWrappersencapsulate a single subordinate stage and modify or filter the results.\nFor example, a filter stage that only accepts solutions of its child stage that satisfy a certain constraint can be realized as a wrapper.\nAnother standard use of this type includes the IK wrapper stage, which generates inverse kinematics solutions based on planning scenes annotated with a pose target property.\nSerial Containershold a sequence of subordinate stages and only consider end-to-end solutions as results.\nAn example is a picking motion that consists of a sequence of coherent steps.\nParallel Containerscombine set of subordinate stages and can be used for passing the best of alternative results, running fallback solvers or for merging multiple independent solutions.\nExamples are running alternative planners for a free-motion plan, picking objects with the right hand or with the left hand as a fallback, or moving the arm and opening the gripper at the same time.\nStages not only support solving motion planning problems.\nThey can also be used for all kinds of state transitions, as for instance modifying the planning scene.\nCombined with the possibility of using class inheritance it is possible to construct very complex behavior while only relying on a well-structured set of primitive stages.\nMore detailed information on MTC can be found in theMoveIt Task Constructor concepts page\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nMove into your colcon workspace and pull the MoveIt Task Constructor source, where<branch>can be e.g.humblefor ROS Humble, orros2for the latest version compatible with MoveIt 2main:\nInstall missing packages with rosdep:\nBuild the workspace:\nThe MoveIt Task Constructor package contains several basic examples and a pick-and-place demo.\nFor all demos you should launch the basic environment:\nSubsequently, you can run the individual demos:\nOn the right side you should see theMotion Planning Taskspanel outlining the hierarchical stage structure of the tasks.\nWhen you select a particular stage, the list of successful and failed solutions will be\nshown in the right-most window. Selecting one of those solutions will start its visualization.\nThis section walks through the steps required to build a simple task with MoveIt Task Constructor.\nCreate a new package with the following command:\nThis will create a new package and folder calledmtc_tutorialwith a dependency onmoveit_task_constructor_coreas well as a hello world example insrc/mtc_node.\nOpenmtc_node.cppin your editor of choice, and paste in the following code.\nThe top of the code includes the ROS and MoveIt Libraries that this package uses.\nrclcpp/rclcpp.hppincludes core ROS2 functionality\nmoveit/planning_scene/planning_scene.handmoveit/planning_scene_interface/planning_scene_interface.hinclude functionality to interface with the robot model and collision objects\nmoveit/task_constructor/task.h,moveit/task_constructor/solvers.h, andmoveit/task_constructor/stages.hinclude different components of MoveIt Task Constructor that are used in the example\ntf2_geometry_msgs/tf2_geometry_msgs.hppandtf2_eigen/tf2_eigen.hppwon’t be used in this initial example, but they will be used for pose generation when we add more stages to the MoveIt Task Constructor task.\nThe next line gets a logger for our new node. We also create a namespace alias formoveit::task_constructorfor convenience.\nWe start by defining a class that will contain the main MoveIt Task Constructor functionality.  We also declare the MoveIt Task Constructor task object as a member variable for our class: this isn’t strictly necessary for a given application, but it helps save the task for later visualization purposes. We will explore each function individually below.\nThese lines initialize the node with specified options (it is the constructor of ourMTCTaskNodeclass).\nThese next lines define a getter function to get the node base interface, which will be used for the executor later.\nThis class method is used to set up the planning scene that is used in the example. It creates a cylinder with dimensions specified byobject.primitives[0].dimensionsand position specified bypose.position.xandpose.position.y.\nYou can try changing these numbers to resize and move the cylinder around. If we move the cylinder out of the robot’s reach, planning will fail.\nThis function interfaces with the MoveIt Task Constructor task object. It first creates a task, which includes setting some properties and adding stages. This will be discussed further in thecreateTaskfunction definition. Next,task.init()initializes the task andtask.plan(5)generates a plan, stopping after 5 successful plans are found. The next line publishes the solution to be visualized in RViz - this line can be removed if you don’t care for visualization. Finally,task.execute()executes the plan. Execution occurs via an action server interface with the RViz plugin.\nAs mentioned above, this function creates a MoveIt Task Constructor object and sets some initial properties. In this case, we set the task name to “demo_task”, load the robot model, define the names of some useful frames, and set those frame names as properties of the task withtask.setProperty(property_name,value). The next few code blocks will fill out this function body.\nNow, we add an example stage to the node. The first line setscurrent_state_ptrtonullptr; this creates a pointer to a stage such that we can reuse stage information in specific scenarios. This line is not used at this moment, but will be used later when more stages are added to the task. Next, we make acurrent_statestage (a generator stage) and add it to our task - this starts the robot off in its current state. Now that we’ve created theCurrentStatestage, we save a pointer to it in thecurrent_state_ptrfor later use.\nSolvers are used to define the type of robot motion. MoveIt Task Constructor has three options for solvers:\nPipelinePlanneruses MoveIt’s planning pipeline, which typically defaults toOMPL.\nJointInterpolationis a simple planner that interpolates between the start and goal joint states. It is typically used for simple motions as it computes quickly but doesn’t support complex motions.\nCartesianPathis used to move the end effector in a straight line in Cartesian space.\nFeel free to try out the different solvers and see how the robot motion changes. For the first stage we will use the Cartesian planner, which requires the following properties to be set:\nNow that we added in the planners, we can add a stage that will move the robot.\nThe following lines use aMoveTostage (a propagator stage). Since opening the hand is a relatively simple movement, we can use the joint interpolation planner.\nThis stage plans a move to the “open hand” pose, which is a named pose defined in theSRDFfor the Panda robot.\nWe return the task and finish with thecreateTask()function.\nFinally, we havemain: the following lines create a node using the class defined above, and calls the class methods to set up and execute a basic MTC task. In this example, we do not cancel the executor once the task has finished executing to keep the node alive to inspect the solutions in RViz.\nWe will need a launch file to launch themove_group,ros2_control,static_tf,robot_state_publisher, andrviznodes that provide us the environment to run the demo. The one we will use for this example can be foundhere.\nTo run the MoveIt Task Constructor node, we will use a second launch file to start themtc_tutorialexecutable with the proper parameters. Here we can load URDF, SRDF, and OMPL parameters, or use MoveIt Configs Utils to do so. Your launch file should look something like the one found in this tutorial packagehere(pay close attention to thepackageandexecutablearguments below as they are different from the launch file linked) :\nSave a launch file aspick_place_demo.launch.pyor download one to the package’s launch directory. Make sure to edit theCMakeLists.txtso it includes the launch folder by adding the following lines:\nNow we can build and source the colcon workspace.\nStart by launching the first launch file. If you want to use the one provided by the tutorials:\nRViz will now load. If you’re using your own launch file and haven’t included an rviz configsuch as this, you will need to configure RViz before you see anything displayed. If you’re using the launch file from the tutorials package, RViz will already be configured for you and you can jump to the end of the next section.\nIf you are not using the RViz configuration provided, we’ll have to make some changes to the RViz configuration to see your robot and the MoveIt Task Constructor solutions. First, start RViz. The following steps will cover how to set up RViz for MoveIt Task Constructor solution visualization.\nIf theMotionPlanningdisplay is active, uncheck it to hide it for now.\nUnderGlobal Options, change theFixed Framefrommaptopanda_link0if not already done.\nOn the bottom left of the window, click theAddbutton.\nUndermoveit_task_constructor_visualizationselectMotion Planning Tasksand click OK. TheMotion Planning Tasksdisplay should appear on the bottom left.\nIn theDisplays, underMotion Planning Tasks,  changeTask Solution Topicto/solution\nYou should see the panda arm in the main view with Motion Planning Tasks display open in the bottom left and nothing in it. Your MTC task will show up in this panel once you launch themtc_tutorialnode. If you’re usingmtc_demo.launch.pyfrom the tutorials, jump back in here.\nLaunch themtc_tutorialnode with\nYou should see the arm execute the task with the single stage to open the hand, with the cylinder in green in front of it. It should look something like this:\nIf you haven’t made your own package, but still want to see what this looks like, you can launch this file from the tutorials:\nSo far, we’ve walked through creating and executing a simple task, which runs but does not do much. Now, we will start adding the pick-and-place stages to the task. The image below shows an outline of the stages we will use in our task.\nWe will start adding stages after our existing open hand stage. Openmtc_node.cppand locate the following lines:\nWe need to move the arm to a position where we can pick up our object. This is done with aConnectstage, which as its name implies, is a Connector stage. This means that it tries to bridge between the results of the stage before and after it. This stage is initialized with a name,move_to_pick, and aGroupPlannerVectorthat specifies the planning group and the planner. We then set a timeout for the stage, set the properties for the stage, and add it to our task.\nNext, we create a pointer to a MoveIt Task Constructor stage object, and set it tonullptrfor now. Later, we will use this to save a stage.\nThis next block of code creates aSerialContainer.\nThis is a container that can be added to our task and can hold several substages.\nIn this case, we create a serial container that will contain the stages relevant to the picking action.\nInstead of adding the stages to the task, we will add the relevant stages to the serial container. We useexposeTo()to declare the task properties from the parent task in the new serial container, and useconfigureInitFrom()to initialize them.\nThis allows the contained stages to access these properties.\nWe then create a stage to approach the object. This stage is aMoveRelativestage, which allows us to specify a relative movement from our current position.MoveRelativeis a propagator stage: it receives the solution from its neighbouring stages and propagates it to the next or previous stage. Usingcartesian_plannerfinds a solution that involves moving the end effector in a straight line. We set the properties, and set the minimum and maximum distance to move. Now we create aVector3Stampedmessage to indicate the direction we want to move - in this case, in the Z direction from the hand frame. Finally, we add this stage to our serial container\nNow, create a stage to generate the grasp pose.\nThis is a generator stage, so it computes its results without regard to the stages before and after it.\nThe first stage,CurrentStateis a generator stage as well - to connect the first stage and this stage, a connecting stage must be used, which we already created above.\nThis code sets the stage properties, sets the pose before grasping, the angle delta, and the monitored stage.\nAngle delta is a property of theGenerateGraspPosestage that is used to determine the number of poses to generate; when generating solutions, MoveIt Task Constructor will try to grasp the object from many different orientations, with the difference between the orientations specified by the angle delta. The smaller the delta, the closer together the grasp orientations will be. When defining the current stage, we setcurrent_state_ptr, which is now used to forward information about the object pose and shape to the inverse kinematics solver.\nThis stage won’t be directly added to the serial container like previously, as we still need to do inverse kinematics on the poses it generates.\nBefore we compute inverse kinematics for the poses generated above, we first need to define the frame. This can be done with aPoseStampedmessage fromgeometry_msgsor in this case, we define the transform using Eigen transformation matrix and the name of the relevant link. Here, we define the transformation matrix.\nNow, we create theComputeIKstage, and give it the namegenerateposeIKas well as thegenerategraspposestage defined above. Some robots have multiple inverse kinematics solutions for a given pose - we set the limit on the amount of solutions to solve for up to 8. We also set the minimum solution distance, which is a threshold on how different solutions must be: if the joint positions in a solution are too similar to a previous solution, it will be marked as invalid. Next, we configure some additional properties, and add theComputeIKstage to the serial container.\nTo pick up the object, we must allow collision between the hand and the object. This can be done with aModifyPlanningScenestage. TheallowCollisionsfunction lets us specify which collisions to disable.allowCollisionscan be used with a container of names, so we can usegetLinkModelNamesWithCollisionGeometryto get all the names of links with collision geometry in the hand group.\nWith collisions allowed, we now can close the hand. This is done with aMoveTostage, similarly to theopenhandstage from above, except moving to thecloseposition as defined in the SRDF.\nWe now use aModifyPlanningScenestage again, this time to attach the object to the hand usingattachObject. Similarly to what we did with thecurrent_state_ptr, we get a pointer to this stage for later use when generating the place pose for the object.\nNext, we lift the object with aMoveRelativestage, similarly to theapproach_objectstage.\nWith this, we have all the stages needed to pick the object. Now, we add the serial container (with all its substages) to the task. If you build the package as-is, you can see the robot plan to pick up the object.\nTo test out the newly created stage, build the code and execute:\nNow that the stages that define the pick are complete, we move on to defining the stages for placing the object. Picking up where we left off, we add aConnectstage to connect the two, as we will soon be using a generator stage to generate the pose for placing the object.\nWe also create a serial container for the place stages. This is done similarly to the pick serial container.\nThe next stages will be added to the serial container rather than the task.\nThis next stage generates the poses used to place the object and compute the inverse kinematics for those poses - it is somewhat similar to thegenerategraspposestage from the pick serial container.\nWe start by creating a stage to generate the poses and inheriting the task properties.\nWe specify the pose where we want to place the object with aPoseStampedmessage fromgeometry_msgs- in this case, we choosey=0.5in the\"object\"frame.\nWe then pass the target pose to the stage withsetPose.\nNext, we usesetMonitoredStageand pass it the pointer to theattach_objectstage from earlier.\nThis allows the stage to know how the object is attached.\nWe then create aComputeIKstage and pass it ourGeneratePlacePosestage - the rest follows the same logic as above with the pick stages.\nNow that we’re ready to place the object, we open the hand withMoveTostage and the joint interpolation planner.\nWe also can re-enable collisions with the object now that we no longer need to hold it.\nThis is done usingallowCollisionsalmost exactly the same way as disabling collisions, except setting the last argument tofalserather thantrue.\nNow, we can detach the object usingdetachObject.\nWe retreat from the object using aMoveRelativestage, which is done similarly to theapproachobjectandliftobjectstages.\nWe finish our place serial container and add it to the task.\nThe final step is to return home: we use aMoveTostage and pass it the goal pose ofready, which is a pose defined in the Panda SRDF.\nAll these stages should be added above these lines.\nCongratulations! You’ve now defined a pick and place task using MoveIt Task Constructor! To try it out, build the code and execute:\nThe task with each comprising stage is shown in the Motion Planning Tasks pane. Click on a stage and additional information about the stage will show up to the right. The right pane shows different solutions as well as their associated costs. Depending on the stage type and the robot configuration, there may only be one solution shown.\nClick one of the solution costs to see an animation of the robot following the plan for that stage. Click the “Exec” button in the upper-right portion of the pane to execute the motion.\nTo run the complete MoveIt Task Constructor example included with the MoveIt tutorials:\nAnd in a second terminal:\nWhen running MTC, it prints a diagram like this to terminal:\nThis example^ shows two stages. The first stage (“initial_state”) is aCurrentStatetype of stage, which initializes aPlanningSceneand captures any collision objects that are present at that moment.\nA pointer to this stage can be used to retrieve the state of the robot.\nSinceCurrentStateinherits fromGenerator, it propagates solutions both forward and backward.\nThis is denoted by the arrows in both directions.\nThe first1indicates that one solution was successfully propagated backwards to the previous stage.\nThe second1, between the arrows, indicates that one solution was generated.\nThe0indicates that a solution was not propagated forward successfully to the next stage, because the next stage failed.\nThe second stage (“move_to_home”) is aMoveTotype of stage. It inherits its propagation direction from the previous stage, so both arrows point forward. The0’s indicate that this stage failed completely. From left to right, the0’s mean:\nThe stage did not receive a solution from the previous stage\nThe stage did not generate a solution\nThe stage did not propagate a solution forward to the next stage\nIn this case, we could tell that “move_to_home” was the root cause of the failure. The problem was a home state that was in collision. Defining a new, collision-free home position fixed the issue.\nInformation about individual stages can be retrieved from the task. For example, here we retrieve the unique ID for a stage:\nACurrentStatetype stage does not just retrieve the current state of the robot.\nIt also initializes aPlanningSceneobject, capturing any collision objects that are present at that moment.\nMTC stages can be propagated in forward and backward order.\nYou can easily check which direction a stage propagates by the arrow in the RViz GUI.\nWhen propagating backwards, the logic of many operations is reversed.\nFor example, to allow collisions with an object in aModifyPlanningScenestage, you would callallowCollisions(false)rather thanallowCollisions(true). There is a discussion to be readhere.\n© Copyright 2024, PickNik Robotics.\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit_task_constructor.git\nrosdep install --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncd~/ws_moveitcolconbuild--mixinrelease\nros2launchmoveit_task_constructor_demodemo.launch.py\nros2launchmoveit_task_constructor_demorun.launch.pyexe:=cartesianros2launchmoveit_task_constructor_demorun.launch.pyexe:=modularros2launchmoveit_task_constructor_demorun.launch.pyexe:=pick_place_demo\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_task_constructor_corerclcpp\\--node-namemtc_nodemtc_tutorial\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endifstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;classMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};MTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}rclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}voidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;pose.orientation.w=1.0;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}voidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}mtc::TaskMTCTaskNode::createTask(){mtc::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);// Disable warnings for this line, as it's a variable that's set but not used in this example#pragma GCC diagnostic push#pragma GCC diagnostic ignored \"-Wunused-but-set-variable\"mtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generator#pragma GCC diagnostic popautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));autosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);autointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();autocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);autostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}intmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endif\nstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;\nclassMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};\nMTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}\nrclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}\nvoidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}\nvoidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}\nmtc::TaskMTCTaskNode::createTask(){moveit::task_constructor::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);\nmtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generatorautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));\nautosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);\nautointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}\nintmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\nfromlaunchimportLaunchDescriptionfromlaunch_ros.actionsimportNodefrommoveit_configs_utilsimportMoveItConfigsBuilderdefgenerate_launch_description():moveit_config=MoveItConfigsBuilder(\"moveit_resources_panda\").to_dict()# MTC Demo nodepick_place_demo=Node(package=\"mtc_tutorial\",executable=\"mtc_node\",output=\"screen\",parameters=[moveit_config,],)returnLaunchDescription([pick_place_demo])\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}\n  )\ncd~/ws_moveitcolconbuild--mixinreleasesource~/ws_moveit/install/setup.bash\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo_minimal.launch.py\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));// Add the next lines of codes to define more stages here\nautostage_move_to_pick=std::make_unique<mtc::stages::Connect>(\"move to pick\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner}});stage_move_to_pick->setTimeout(5.0);stage_move_to_pick->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_pick));\nmtc::Stage*attach_object_stage=nullptr;// Forward attach_object_stage to place pose generator\n{autograsp=std::make_unique<mtc::SerialContainer>(\"pick object\");task.properties().exposeTo(grasp->properties(),{\"eef\",\"group\",\"ik_frame\"});grasp->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"approach object\",cartesian_planner);stage->properties().set(\"marker_ns\",\"approach_object\");stage->properties().set(\"link\",hand_frame);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.15);// Set hand forward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=hand_frame;vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\n{// Sample grasp poseautostage=std::make_unique<mtc::stages::GenerateGraspPose>(\"generate grasp pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"grasp_pose\");stage->setPreGraspPose(\"open\");stage->setObject(\"object\");stage->setAngleDelta(M_PI/12);stage->setMonitoredStage(current_state_ptr);// Hook into current state\nEigen::Isometry3dgrasp_frame_transform;Eigen::Quaterniondq=Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitX())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitY())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitZ());grasp_frame_transform.linear()=q.matrix();grasp_frame_transform.translation().z()=0.1;\n// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"grasp pose IK\",std::move(stage));wrapper->setMaxIKSolutions(8);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(grasp_frame_transform,hand_frame);wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});grasp->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"allow collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),true);grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"close hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"close\");grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"attach object\");stage->attachObject(\"object\",hand_frame);attach_object_stage=stage.get();grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"lift object\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"lift_object\");// Set upward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\ntask.add(std::move(grasp));}\nros2launchmtc_tutorialpick_place_demo.launch.py\n{autostage_move_to_place=std::make_unique<mtc::stages::Connect>(\"move to place\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner},{hand_group_name,interpolation_planner}});stage_move_to_place->setTimeout(5.0);stage_move_to_place->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_place));}\n{autoplace=std::make_unique<mtc::SerialContainer>(\"place object\");task.properties().exposeTo(place->properties(),{\"eef\",\"group\",\"ik_frame\"});place->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{// Sample place poseautostage=std::make_unique<mtc::stages::GeneratePlacePose>(\"generate place pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"place_pose\");stage->setObject(\"object\");geometry_msgs::msg::PoseStampedtarget_pose_msg;target_pose_msg.header.frame_id=\"object\";target_pose_msg.pose.position.y=0.5;target_pose_msg.pose.orientation.w=1.0;stage->setPose(target_pose_msg);stage->setMonitoredStage(attach_object_stage);// Hook into attach_object_stage// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"place pose IK\",std::move(stage));wrapper->setMaxIKSolutions(2);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(\"object\");wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});place->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"open\");place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"forbid collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),false);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"detach object\");stage->detachObject(\"object\",hand_frame);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"retreat\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"retreat\");// Set retreat directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.x=-0.5;stage->setDirection(vec);place->insert(std::move(stage));}\ntask.add(std::move(place));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"return home\",interpolation_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setGoal(\"ready\");task.add(std::move(stage));}\n// Stages all added to the task above this linereturntask;}\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmoveit2_tutorialspick_place_demo.launch.py\n[demo_node-1]1-←1→-0/initial_state[demo_node-1]-0→0→-0/move_to_home\nuint32_tconstunique_stage_id=task_.stages()->findChild(stage_name)->introspectionId();\n<branch>\nhumble\nros2\nmain\nmtc_tutorial\nmoveit_task_constructor_core\nsrc/mtc_node\nmtc_node.cpp\nrclcpp/rclcpp.hpp\nmoveit/planning_scene/planning_scene.h\nmoveit/planning_scene_interface/planning_scene_interface.h\nmoveit/task_constructor/task.h\nmoveit/task_constructor/solvers.h\nmoveit/task_constructor/stages.h\ntf2_geometry_msgs/tf2_geometry_msgs.hpp\ntf2_eigen/tf2_eigen.hpp\nmoveit::task_constructor\nMTCTaskNode\nobject.primitives[0].dimensions\npose.position.x\npose.position.y\ncreateTask\ntask.init()\ntask.plan(5)\ntask.execute()\ntask.setProperty(property_name,value)\ncurrent_state_ptr\nnullptr\ncurrent_state\nCurrentState\ncurrent_state_ptr\nMoveTo\ncreateTask()\nmain\nmove_group\nros2_control\nstatic_tf\nrobot_state_publisher\nrviz\nmtc_tutorial\npackage\nexecutable\npick_place_demo.launch.py\nCMakeLists.txt\nmap\npanda_link0\nmoveit_task_constructor_visualization\n/solution\nmtc_tutorial\nmtc_demo.launch.py\nmtc_tutorial\nmtc_node.cpp\nConnect\nmove_to_pick\nGroupPlannerVector\nnullptr\nSerialContainer\nexposeTo()\nconfigureInitFrom()\nMoveRelative\nMoveRelative\ncartesian_planner\nVector3Stamped\nCurrentState\nGenerateGraspPose\ncurrent_state_ptr\nPoseStamped\ngeometry_msgs\nComputeIK\ngenerateposeIK\ngenerategrasppose\nComputeIK\nModifyPlanningScene\nallowCollisions\nallowCollisions\ngetLinkModelNamesWithCollisionGeometry\nMoveTo\nopenhand\nclose\nModifyPlanningScene\nattachObject\ncurrent_state_ptr\nMoveRelative\napproach_object\nConnect\ngenerategrasppose\nPoseStamped\ngeometry_msgs\ny=0.5\n\"object\"\nsetPose\nsetMonitoredStage\nattach_object\nComputeIK\nGeneratePlacePose\nMoveTo\nallowCollisions\nfalse\ntrue\ndetachObject\nMoveRelative\napproachobject\nliftobject\nMoveTo\nready\nCurrentState\nPlanningScene\nCurrentState\nGenerator\n1\n1\n0\nMoveTo\n0\n0\nCurrentState\nPlanningScene\nModifyPlanningScene\nallowCollisions(false)\nallowCollisions(true)",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0e1"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/pick_and_place_with_moveit_task_constructor/pick_and_place_with_moveit_task_constructor.html#pick-stages",
  "title": "Pick and Place with MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "Pick and Place with MoveIt Task Constructor\n1 Basic Concepts\n2 Getting Started\n3 Trying It Out\n4 Setting up a Project with MoveIt Task Constructor\n5 Running the Demo\n6 Adding Stages\n7 Further Discussion\nThis tutorial will walk you through creating a package that plans a pick and place operation usingMoveIt Task Constructor. MoveIt Task Constructor provides a way to plan for tasks that consist of multiple different subtasks (known as stages). If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nThe fundamental idea of MTC is that complex motion planning problems can be composed into a set of simpler subproblems.\nThe top-level planning problem is specified as aTaskwhile all subproblems are specified byStages.\nStages can be arranged in any arbitrary order and hierarchy only limited by the individual stages types.\nThe order in which stages can be arranged is restricted by the direction in which results are passed.\nThere are three possible stages relating to the result flow: generator, propagator, and connector stages:\nGeneratorscompute their results independently of their neighbor stages and pass them in both directions, backwards and forwards.\nAn example is an IK sampler for geometric poses where approaching and departing motions (neighbor stages) depend on the solution.\nPropagatorsreceive the result of one neighbor stage, solve a subproblem and then propagate their result to the neighbor on the opposite site.\nDepending on the implementation, propagating stages can pass solutions forward, backward or in both directions separately.\nAn example is a stage that computes a Cartesian path based on either a start or a goal state.\nConnectorsdo not propagate any results, but rather attempt to bridge the gap between the resulting states of both neighbors.\nAn example is the computation of a free-motion plan from one given state to another.\nAdditional to the order types, there are different hierarchy types allowing to encapsulate subordinate stages.\nStages without subordinate stages are calledprimitive stages, higher-level stages are calledcontainer stages.\nThere are three container types:\nWrappersencapsulate a single subordinate stage and modify or filter the results.\nFor example, a filter stage that only accepts solutions of its child stage that satisfy a certain constraint can be realized as a wrapper.\nAnother standard use of this type includes the IK wrapper stage, which generates inverse kinematics solutions based on planning scenes annotated with a pose target property.\nSerial Containershold a sequence of subordinate stages and only consider end-to-end solutions as results.\nAn example is a picking motion that consists of a sequence of coherent steps.\nParallel Containerscombine set of subordinate stages and can be used for passing the best of alternative results, running fallback solvers or for merging multiple independent solutions.\nExamples are running alternative planners for a free-motion plan, picking objects with the right hand or with the left hand as a fallback, or moving the arm and opening the gripper at the same time.\nStages not only support solving motion planning problems.\nThey can also be used for all kinds of state transitions, as for instance modifying the planning scene.\nCombined with the possibility of using class inheritance it is possible to construct very complex behavior while only relying on a well-structured set of primitive stages.\nMore detailed information on MTC can be found in theMoveIt Task Constructor concepts page\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nMove into your colcon workspace and pull the MoveIt Task Constructor source, where<branch>can be e.g.humblefor ROS Humble, orros2for the latest version compatible with MoveIt 2main:\nInstall missing packages with rosdep:\nBuild the workspace:\nThe MoveIt Task Constructor package contains several basic examples and a pick-and-place demo.\nFor all demos you should launch the basic environment:\nSubsequently, you can run the individual demos:\nOn the right side you should see theMotion Planning Taskspanel outlining the hierarchical stage structure of the tasks.\nWhen you select a particular stage, the list of successful and failed solutions will be\nshown in the right-most window. Selecting one of those solutions will start its visualization.\nThis section walks through the steps required to build a simple task with MoveIt Task Constructor.\nCreate a new package with the following command:\nThis will create a new package and folder calledmtc_tutorialwith a dependency onmoveit_task_constructor_coreas well as a hello world example insrc/mtc_node.\nOpenmtc_node.cppin your editor of choice, and paste in the following code.\nThe top of the code includes the ROS and MoveIt Libraries that this package uses.\nrclcpp/rclcpp.hppincludes core ROS2 functionality\nmoveit/planning_scene/planning_scene.handmoveit/planning_scene_interface/planning_scene_interface.hinclude functionality to interface with the robot model and collision objects\nmoveit/task_constructor/task.h,moveit/task_constructor/solvers.h, andmoveit/task_constructor/stages.hinclude different components of MoveIt Task Constructor that are used in the example\ntf2_geometry_msgs/tf2_geometry_msgs.hppandtf2_eigen/tf2_eigen.hppwon’t be used in this initial example, but they will be used for pose generation when we add more stages to the MoveIt Task Constructor task.\nThe next line gets a logger for our new node. We also create a namespace alias formoveit::task_constructorfor convenience.\nWe start by defining a class that will contain the main MoveIt Task Constructor functionality.  We also declare the MoveIt Task Constructor task object as a member variable for our class: this isn’t strictly necessary for a given application, but it helps save the task for later visualization purposes. We will explore each function individually below.\nThese lines initialize the node with specified options (it is the constructor of ourMTCTaskNodeclass).\nThese next lines define a getter function to get the node base interface, which will be used for the executor later.\nThis class method is used to set up the planning scene that is used in the example. It creates a cylinder with dimensions specified byobject.primitives[0].dimensionsand position specified bypose.position.xandpose.position.y.\nYou can try changing these numbers to resize and move the cylinder around. If we move the cylinder out of the robot’s reach, planning will fail.\nThis function interfaces with the MoveIt Task Constructor task object. It first creates a task, which includes setting some properties and adding stages. This will be discussed further in thecreateTaskfunction definition. Next,task.init()initializes the task andtask.plan(5)generates a plan, stopping after 5 successful plans are found. The next line publishes the solution to be visualized in RViz - this line can be removed if you don’t care for visualization. Finally,task.execute()executes the plan. Execution occurs via an action server interface with the RViz plugin.\nAs mentioned above, this function creates a MoveIt Task Constructor object and sets some initial properties. In this case, we set the task name to “demo_task”, load the robot model, define the names of some useful frames, and set those frame names as properties of the task withtask.setProperty(property_name,value). The next few code blocks will fill out this function body.\nNow, we add an example stage to the node. The first line setscurrent_state_ptrtonullptr; this creates a pointer to a stage such that we can reuse stage information in specific scenarios. This line is not used at this moment, but will be used later when more stages are added to the task. Next, we make acurrent_statestage (a generator stage) and add it to our task - this starts the robot off in its current state. Now that we’ve created theCurrentStatestage, we save a pointer to it in thecurrent_state_ptrfor later use.\nSolvers are used to define the type of robot motion. MoveIt Task Constructor has three options for solvers:\nPipelinePlanneruses MoveIt’s planning pipeline, which typically defaults toOMPL.\nJointInterpolationis a simple planner that interpolates between the start and goal joint states. It is typically used for simple motions as it computes quickly but doesn’t support complex motions.\nCartesianPathis used to move the end effector in a straight line in Cartesian space.\nFeel free to try out the different solvers and see how the robot motion changes. For the first stage we will use the Cartesian planner, which requires the following properties to be set:\nNow that we added in the planners, we can add a stage that will move the robot.\nThe following lines use aMoveTostage (a propagator stage). Since opening the hand is a relatively simple movement, we can use the joint interpolation planner.\nThis stage plans a move to the “open hand” pose, which is a named pose defined in theSRDFfor the Panda robot.\nWe return the task and finish with thecreateTask()function.\nFinally, we havemain: the following lines create a node using the class defined above, and calls the class methods to set up and execute a basic MTC task. In this example, we do not cancel the executor once the task has finished executing to keep the node alive to inspect the solutions in RViz.\nWe will need a launch file to launch themove_group,ros2_control,static_tf,robot_state_publisher, andrviznodes that provide us the environment to run the demo. The one we will use for this example can be foundhere.\nTo run the MoveIt Task Constructor node, we will use a second launch file to start themtc_tutorialexecutable with the proper parameters. Here we can load URDF, SRDF, and OMPL parameters, or use MoveIt Configs Utils to do so. Your launch file should look something like the one found in this tutorial packagehere(pay close attention to thepackageandexecutablearguments below as they are different from the launch file linked) :\nSave a launch file aspick_place_demo.launch.pyor download one to the package’s launch directory. Make sure to edit theCMakeLists.txtso it includes the launch folder by adding the following lines:\nNow we can build and source the colcon workspace.\nStart by launching the first launch file. If you want to use the one provided by the tutorials:\nRViz will now load. If you’re using your own launch file and haven’t included an rviz configsuch as this, you will need to configure RViz before you see anything displayed. If you’re using the launch file from the tutorials package, RViz will already be configured for you and you can jump to the end of the next section.\nIf you are not using the RViz configuration provided, we’ll have to make some changes to the RViz configuration to see your robot and the MoveIt Task Constructor solutions. First, start RViz. The following steps will cover how to set up RViz for MoveIt Task Constructor solution visualization.\nIf theMotionPlanningdisplay is active, uncheck it to hide it for now.\nUnderGlobal Options, change theFixed Framefrommaptopanda_link0if not already done.\nOn the bottom left of the window, click theAddbutton.\nUndermoveit_task_constructor_visualizationselectMotion Planning Tasksand click OK. TheMotion Planning Tasksdisplay should appear on the bottom left.\nIn theDisplays, underMotion Planning Tasks,  changeTask Solution Topicto/solution\nYou should see the panda arm in the main view with Motion Planning Tasks display open in the bottom left and nothing in it. Your MTC task will show up in this panel once you launch themtc_tutorialnode. If you’re usingmtc_demo.launch.pyfrom the tutorials, jump back in here.\nLaunch themtc_tutorialnode with\nYou should see the arm execute the task with the single stage to open the hand, with the cylinder in green in front of it. It should look something like this:\nIf you haven’t made your own package, but still want to see what this looks like, you can launch this file from the tutorials:\nSo far, we’ve walked through creating and executing a simple task, which runs but does not do much. Now, we will start adding the pick-and-place stages to the task. The image below shows an outline of the stages we will use in our task.\nWe will start adding stages after our existing open hand stage. Openmtc_node.cppand locate the following lines:\nWe need to move the arm to a position where we can pick up our object. This is done with aConnectstage, which as its name implies, is a Connector stage. This means that it tries to bridge between the results of the stage before and after it. This stage is initialized with a name,move_to_pick, and aGroupPlannerVectorthat specifies the planning group and the planner. We then set a timeout for the stage, set the properties for the stage, and add it to our task.\nNext, we create a pointer to a MoveIt Task Constructor stage object, and set it tonullptrfor now. Later, we will use this to save a stage.\nThis next block of code creates aSerialContainer.\nThis is a container that can be added to our task and can hold several substages.\nIn this case, we create a serial container that will contain the stages relevant to the picking action.\nInstead of adding the stages to the task, we will add the relevant stages to the serial container. We useexposeTo()to declare the task properties from the parent task in the new serial container, and useconfigureInitFrom()to initialize them.\nThis allows the contained stages to access these properties.\nWe then create a stage to approach the object. This stage is aMoveRelativestage, which allows us to specify a relative movement from our current position.MoveRelativeis a propagator stage: it receives the solution from its neighbouring stages and propagates it to the next or previous stage. Usingcartesian_plannerfinds a solution that involves moving the end effector in a straight line. We set the properties, and set the minimum and maximum distance to move. Now we create aVector3Stampedmessage to indicate the direction we want to move - in this case, in the Z direction from the hand frame. Finally, we add this stage to our serial container\nNow, create a stage to generate the grasp pose.\nThis is a generator stage, so it computes its results without regard to the stages before and after it.\nThe first stage,CurrentStateis a generator stage as well - to connect the first stage and this stage, a connecting stage must be used, which we already created above.\nThis code sets the stage properties, sets the pose before grasping, the angle delta, and the monitored stage.\nAngle delta is a property of theGenerateGraspPosestage that is used to determine the number of poses to generate; when generating solutions, MoveIt Task Constructor will try to grasp the object from many different orientations, with the difference between the orientations specified by the angle delta. The smaller the delta, the closer together the grasp orientations will be. When defining the current stage, we setcurrent_state_ptr, which is now used to forward information about the object pose and shape to the inverse kinematics solver.\nThis stage won’t be directly added to the serial container like previously, as we still need to do inverse kinematics on the poses it generates.\nBefore we compute inverse kinematics for the poses generated above, we first need to define the frame. This can be done with aPoseStampedmessage fromgeometry_msgsor in this case, we define the transform using Eigen transformation matrix and the name of the relevant link. Here, we define the transformation matrix.\nNow, we create theComputeIKstage, and give it the namegenerateposeIKas well as thegenerategraspposestage defined above. Some robots have multiple inverse kinematics solutions for a given pose - we set the limit on the amount of solutions to solve for up to 8. We also set the minimum solution distance, which is a threshold on how different solutions must be: if the joint positions in a solution are too similar to a previous solution, it will be marked as invalid. Next, we configure some additional properties, and add theComputeIKstage to the serial container.\nTo pick up the object, we must allow collision between the hand and the object. This can be done with aModifyPlanningScenestage. TheallowCollisionsfunction lets us specify which collisions to disable.allowCollisionscan be used with a container of names, so we can usegetLinkModelNamesWithCollisionGeometryto get all the names of links with collision geometry in the hand group.\nWith collisions allowed, we now can close the hand. This is done with aMoveTostage, similarly to theopenhandstage from above, except moving to thecloseposition as defined in the SRDF.\nWe now use aModifyPlanningScenestage again, this time to attach the object to the hand usingattachObject. Similarly to what we did with thecurrent_state_ptr, we get a pointer to this stage for later use when generating the place pose for the object.\nNext, we lift the object with aMoveRelativestage, similarly to theapproach_objectstage.\nWith this, we have all the stages needed to pick the object. Now, we add the serial container (with all its substages) to the task. If you build the package as-is, you can see the robot plan to pick up the object.\nTo test out the newly created stage, build the code and execute:\nNow that the stages that define the pick are complete, we move on to defining the stages for placing the object. Picking up where we left off, we add aConnectstage to connect the two, as we will soon be using a generator stage to generate the pose for placing the object.\nWe also create a serial container for the place stages. This is done similarly to the pick serial container.\nThe next stages will be added to the serial container rather than the task.\nThis next stage generates the poses used to place the object and compute the inverse kinematics for those poses - it is somewhat similar to thegenerategraspposestage from the pick serial container.\nWe start by creating a stage to generate the poses and inheriting the task properties.\nWe specify the pose where we want to place the object with aPoseStampedmessage fromgeometry_msgs- in this case, we choosey=0.5in the\"object\"frame.\nWe then pass the target pose to the stage withsetPose.\nNext, we usesetMonitoredStageand pass it the pointer to theattach_objectstage from earlier.\nThis allows the stage to know how the object is attached.\nWe then create aComputeIKstage and pass it ourGeneratePlacePosestage - the rest follows the same logic as above with the pick stages.\nNow that we’re ready to place the object, we open the hand withMoveTostage and the joint interpolation planner.\nWe also can re-enable collisions with the object now that we no longer need to hold it.\nThis is done usingallowCollisionsalmost exactly the same way as disabling collisions, except setting the last argument tofalserather thantrue.\nNow, we can detach the object usingdetachObject.\nWe retreat from the object using aMoveRelativestage, which is done similarly to theapproachobjectandliftobjectstages.\nWe finish our place serial container and add it to the task.\nThe final step is to return home: we use aMoveTostage and pass it the goal pose ofready, which is a pose defined in the Panda SRDF.\nAll these stages should be added above these lines.\nCongratulations! You’ve now defined a pick and place task using MoveIt Task Constructor! To try it out, build the code and execute:\nThe task with each comprising stage is shown in the Motion Planning Tasks pane. Click on a stage and additional information about the stage will show up to the right. The right pane shows different solutions as well as their associated costs. Depending on the stage type and the robot configuration, there may only be one solution shown.\nClick one of the solution costs to see an animation of the robot following the plan for that stage. Click the “Exec” button in the upper-right portion of the pane to execute the motion.\nTo run the complete MoveIt Task Constructor example included with the MoveIt tutorials:\nAnd in a second terminal:\nWhen running MTC, it prints a diagram like this to terminal:\nThis example^ shows two stages. The first stage (“initial_state”) is aCurrentStatetype of stage, which initializes aPlanningSceneand captures any collision objects that are present at that moment.\nA pointer to this stage can be used to retrieve the state of the robot.\nSinceCurrentStateinherits fromGenerator, it propagates solutions both forward and backward.\nThis is denoted by the arrows in both directions.\nThe first1indicates that one solution was successfully propagated backwards to the previous stage.\nThe second1, between the arrows, indicates that one solution was generated.\nThe0indicates that a solution was not propagated forward successfully to the next stage, because the next stage failed.\nThe second stage (“move_to_home”) is aMoveTotype of stage. It inherits its propagation direction from the previous stage, so both arrows point forward. The0’s indicate that this stage failed completely. From left to right, the0’s mean:\nThe stage did not receive a solution from the previous stage\nThe stage did not generate a solution\nThe stage did not propagate a solution forward to the next stage\nIn this case, we could tell that “move_to_home” was the root cause of the failure. The problem was a home state that was in collision. Defining a new, collision-free home position fixed the issue.\nInformation about individual stages can be retrieved from the task. For example, here we retrieve the unique ID for a stage:\nACurrentStatetype stage does not just retrieve the current state of the robot.\nIt also initializes aPlanningSceneobject, capturing any collision objects that are present at that moment.\nMTC stages can be propagated in forward and backward order.\nYou can easily check which direction a stage propagates by the arrow in the RViz GUI.\nWhen propagating backwards, the logic of many operations is reversed.\nFor example, to allow collisions with an object in aModifyPlanningScenestage, you would callallowCollisions(false)rather thanallowCollisions(true). There is a discussion to be readhere.\n© Copyright 2024, PickNik Robotics.\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit_task_constructor.git\nrosdep install --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncd~/ws_moveitcolconbuild--mixinrelease\nros2launchmoveit_task_constructor_demodemo.launch.py\nros2launchmoveit_task_constructor_demorun.launch.pyexe:=cartesianros2launchmoveit_task_constructor_demorun.launch.pyexe:=modularros2launchmoveit_task_constructor_demorun.launch.pyexe:=pick_place_demo\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_task_constructor_corerclcpp\\--node-namemtc_nodemtc_tutorial\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endifstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;classMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};MTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}rclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}voidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;pose.orientation.w=1.0;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}voidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}mtc::TaskMTCTaskNode::createTask(){mtc::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);// Disable warnings for this line, as it's a variable that's set but not used in this example#pragma GCC diagnostic push#pragma GCC diagnostic ignored \"-Wunused-but-set-variable\"mtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generator#pragma GCC diagnostic popautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));autosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);autointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();autocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);autostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}intmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endif\nstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;\nclassMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};\nMTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}\nrclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}\nvoidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}\nvoidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}\nmtc::TaskMTCTaskNode::createTask(){moveit::task_constructor::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);\nmtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generatorautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));\nautosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);\nautointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}\nintmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\nfromlaunchimportLaunchDescriptionfromlaunch_ros.actionsimportNodefrommoveit_configs_utilsimportMoveItConfigsBuilderdefgenerate_launch_description():moveit_config=MoveItConfigsBuilder(\"moveit_resources_panda\").to_dict()# MTC Demo nodepick_place_demo=Node(package=\"mtc_tutorial\",executable=\"mtc_node\",output=\"screen\",parameters=[moveit_config,],)returnLaunchDescription([pick_place_demo])\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}\n  )\ncd~/ws_moveitcolconbuild--mixinreleasesource~/ws_moveit/install/setup.bash\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo_minimal.launch.py\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));// Add the next lines of codes to define more stages here\nautostage_move_to_pick=std::make_unique<mtc::stages::Connect>(\"move to pick\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner}});stage_move_to_pick->setTimeout(5.0);stage_move_to_pick->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_pick));\nmtc::Stage*attach_object_stage=nullptr;// Forward attach_object_stage to place pose generator\n{autograsp=std::make_unique<mtc::SerialContainer>(\"pick object\");task.properties().exposeTo(grasp->properties(),{\"eef\",\"group\",\"ik_frame\"});grasp->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"approach object\",cartesian_planner);stage->properties().set(\"marker_ns\",\"approach_object\");stage->properties().set(\"link\",hand_frame);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.15);// Set hand forward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=hand_frame;vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\n{// Sample grasp poseautostage=std::make_unique<mtc::stages::GenerateGraspPose>(\"generate grasp pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"grasp_pose\");stage->setPreGraspPose(\"open\");stage->setObject(\"object\");stage->setAngleDelta(M_PI/12);stage->setMonitoredStage(current_state_ptr);// Hook into current state\nEigen::Isometry3dgrasp_frame_transform;Eigen::Quaterniondq=Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitX())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitY())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitZ());grasp_frame_transform.linear()=q.matrix();grasp_frame_transform.translation().z()=0.1;\n// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"grasp pose IK\",std::move(stage));wrapper->setMaxIKSolutions(8);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(grasp_frame_transform,hand_frame);wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});grasp->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"allow collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),true);grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"close hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"close\");grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"attach object\");stage->attachObject(\"object\",hand_frame);attach_object_stage=stage.get();grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"lift object\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"lift_object\");// Set upward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\ntask.add(std::move(grasp));}\nros2launchmtc_tutorialpick_place_demo.launch.py\n{autostage_move_to_place=std::make_unique<mtc::stages::Connect>(\"move to place\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner},{hand_group_name,interpolation_planner}});stage_move_to_place->setTimeout(5.0);stage_move_to_place->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_place));}\n{autoplace=std::make_unique<mtc::SerialContainer>(\"place object\");task.properties().exposeTo(place->properties(),{\"eef\",\"group\",\"ik_frame\"});place->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{// Sample place poseautostage=std::make_unique<mtc::stages::GeneratePlacePose>(\"generate place pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"place_pose\");stage->setObject(\"object\");geometry_msgs::msg::PoseStampedtarget_pose_msg;target_pose_msg.header.frame_id=\"object\";target_pose_msg.pose.position.y=0.5;target_pose_msg.pose.orientation.w=1.0;stage->setPose(target_pose_msg);stage->setMonitoredStage(attach_object_stage);// Hook into attach_object_stage// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"place pose IK\",std::move(stage));wrapper->setMaxIKSolutions(2);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(\"object\");wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});place->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"open\");place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"forbid collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),false);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"detach object\");stage->detachObject(\"object\",hand_frame);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"retreat\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"retreat\");// Set retreat directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.x=-0.5;stage->setDirection(vec);place->insert(std::move(stage));}\ntask.add(std::move(place));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"return home\",interpolation_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setGoal(\"ready\");task.add(std::move(stage));}\n// Stages all added to the task above this linereturntask;}\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmoveit2_tutorialspick_place_demo.launch.py\n[demo_node-1]1-←1→-0/initial_state[demo_node-1]-0→0→-0/move_to_home\nuint32_tconstunique_stage_id=task_.stages()->findChild(stage_name)->introspectionId();\n<branch>\nhumble\nros2\nmain\nmtc_tutorial\nmoveit_task_constructor_core\nsrc/mtc_node\nmtc_node.cpp\nrclcpp/rclcpp.hpp\nmoveit/planning_scene/planning_scene.h\nmoveit/planning_scene_interface/planning_scene_interface.h\nmoveit/task_constructor/task.h\nmoveit/task_constructor/solvers.h\nmoveit/task_constructor/stages.h\ntf2_geometry_msgs/tf2_geometry_msgs.hpp\ntf2_eigen/tf2_eigen.hpp\nmoveit::task_constructor\nMTCTaskNode\nobject.primitives[0].dimensions\npose.position.x\npose.position.y\ncreateTask\ntask.init()\ntask.plan(5)\ntask.execute()\ntask.setProperty(property_name,value)\ncurrent_state_ptr\nnullptr\ncurrent_state\nCurrentState\ncurrent_state_ptr\nMoveTo\ncreateTask()\nmain\nmove_group\nros2_control\nstatic_tf\nrobot_state_publisher\nrviz\nmtc_tutorial\npackage\nexecutable\npick_place_demo.launch.py\nCMakeLists.txt\nmap\npanda_link0\nmoveit_task_constructor_visualization\n/solution\nmtc_tutorial\nmtc_demo.launch.py\nmtc_tutorial\nmtc_node.cpp\nConnect\nmove_to_pick\nGroupPlannerVector\nnullptr\nSerialContainer\nexposeTo()\nconfigureInitFrom()\nMoveRelative\nMoveRelative\ncartesian_planner\nVector3Stamped\nCurrentState\nGenerateGraspPose\ncurrent_state_ptr\nPoseStamped\ngeometry_msgs\nComputeIK\ngenerateposeIK\ngenerategrasppose\nComputeIK\nModifyPlanningScene\nallowCollisions\nallowCollisions\ngetLinkModelNamesWithCollisionGeometry\nMoveTo\nopenhand\nclose\nModifyPlanningScene\nattachObject\ncurrent_state_ptr\nMoveRelative\napproach_object\nConnect\ngenerategrasppose\nPoseStamped\ngeometry_msgs\ny=0.5\n\"object\"\nsetPose\nsetMonitoredStage\nattach_object\nComputeIK\nGeneratePlacePose\nMoveTo\nallowCollisions\nfalse\ntrue\ndetachObject\nMoveRelative\napproachobject\nliftobject\nMoveTo\nready\nCurrentState\nPlanningScene\nCurrentState\nGenerator\n1\n1\n0\nMoveTo\n0\n0\nCurrentState\nPlanningScene\nModifyPlanningScene\nallowCollisions(false)\nallowCollisions(true)",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0e2"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/pick_and_place_with_moveit_task_constructor/pick_and_place_with_moveit_task_constructor.html#place-stages",
  "title": "Pick and Place with MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "Pick and Place with MoveIt Task Constructor\n1 Basic Concepts\n2 Getting Started\n3 Trying It Out\n4 Setting up a Project with MoveIt Task Constructor\n5 Running the Demo\n6 Adding Stages\n7 Further Discussion\nThis tutorial will walk you through creating a package that plans a pick and place operation usingMoveIt Task Constructor. MoveIt Task Constructor provides a way to plan for tasks that consist of multiple different subtasks (known as stages). If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nThe fundamental idea of MTC is that complex motion planning problems can be composed into a set of simpler subproblems.\nThe top-level planning problem is specified as aTaskwhile all subproblems are specified byStages.\nStages can be arranged in any arbitrary order and hierarchy only limited by the individual stages types.\nThe order in which stages can be arranged is restricted by the direction in which results are passed.\nThere are three possible stages relating to the result flow: generator, propagator, and connector stages:\nGeneratorscompute their results independently of their neighbor stages and pass them in both directions, backwards and forwards.\nAn example is an IK sampler for geometric poses where approaching and departing motions (neighbor stages) depend on the solution.\nPropagatorsreceive the result of one neighbor stage, solve a subproblem and then propagate their result to the neighbor on the opposite site.\nDepending on the implementation, propagating stages can pass solutions forward, backward or in both directions separately.\nAn example is a stage that computes a Cartesian path based on either a start or a goal state.\nConnectorsdo not propagate any results, but rather attempt to bridge the gap between the resulting states of both neighbors.\nAn example is the computation of a free-motion plan from one given state to another.\nAdditional to the order types, there are different hierarchy types allowing to encapsulate subordinate stages.\nStages without subordinate stages are calledprimitive stages, higher-level stages are calledcontainer stages.\nThere are three container types:\nWrappersencapsulate a single subordinate stage and modify or filter the results.\nFor example, a filter stage that only accepts solutions of its child stage that satisfy a certain constraint can be realized as a wrapper.\nAnother standard use of this type includes the IK wrapper stage, which generates inverse kinematics solutions based on planning scenes annotated with a pose target property.\nSerial Containershold a sequence of subordinate stages and only consider end-to-end solutions as results.\nAn example is a picking motion that consists of a sequence of coherent steps.\nParallel Containerscombine set of subordinate stages and can be used for passing the best of alternative results, running fallback solvers or for merging multiple independent solutions.\nExamples are running alternative planners for a free-motion plan, picking objects with the right hand or with the left hand as a fallback, or moving the arm and opening the gripper at the same time.\nStages not only support solving motion planning problems.\nThey can also be used for all kinds of state transitions, as for instance modifying the planning scene.\nCombined with the possibility of using class inheritance it is possible to construct very complex behavior while only relying on a well-structured set of primitive stages.\nMore detailed information on MTC can be found in theMoveIt Task Constructor concepts page\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nMove into your colcon workspace and pull the MoveIt Task Constructor source, where<branch>can be e.g.humblefor ROS Humble, orros2for the latest version compatible with MoveIt 2main:\nInstall missing packages with rosdep:\nBuild the workspace:\nThe MoveIt Task Constructor package contains several basic examples and a pick-and-place demo.\nFor all demos you should launch the basic environment:\nSubsequently, you can run the individual demos:\nOn the right side you should see theMotion Planning Taskspanel outlining the hierarchical stage structure of the tasks.\nWhen you select a particular stage, the list of successful and failed solutions will be\nshown in the right-most window. Selecting one of those solutions will start its visualization.\nThis section walks through the steps required to build a simple task with MoveIt Task Constructor.\nCreate a new package with the following command:\nThis will create a new package and folder calledmtc_tutorialwith a dependency onmoveit_task_constructor_coreas well as a hello world example insrc/mtc_node.\nOpenmtc_node.cppin your editor of choice, and paste in the following code.\nThe top of the code includes the ROS and MoveIt Libraries that this package uses.\nrclcpp/rclcpp.hppincludes core ROS2 functionality\nmoveit/planning_scene/planning_scene.handmoveit/planning_scene_interface/planning_scene_interface.hinclude functionality to interface with the robot model and collision objects\nmoveit/task_constructor/task.h,moveit/task_constructor/solvers.h, andmoveit/task_constructor/stages.hinclude different components of MoveIt Task Constructor that are used in the example\ntf2_geometry_msgs/tf2_geometry_msgs.hppandtf2_eigen/tf2_eigen.hppwon’t be used in this initial example, but they will be used for pose generation when we add more stages to the MoveIt Task Constructor task.\nThe next line gets a logger for our new node. We also create a namespace alias formoveit::task_constructorfor convenience.\nWe start by defining a class that will contain the main MoveIt Task Constructor functionality.  We also declare the MoveIt Task Constructor task object as a member variable for our class: this isn’t strictly necessary for a given application, but it helps save the task for later visualization purposes. We will explore each function individually below.\nThese lines initialize the node with specified options (it is the constructor of ourMTCTaskNodeclass).\nThese next lines define a getter function to get the node base interface, which will be used for the executor later.\nThis class method is used to set up the planning scene that is used in the example. It creates a cylinder with dimensions specified byobject.primitives[0].dimensionsand position specified bypose.position.xandpose.position.y.\nYou can try changing these numbers to resize and move the cylinder around. If we move the cylinder out of the robot’s reach, planning will fail.\nThis function interfaces with the MoveIt Task Constructor task object. It first creates a task, which includes setting some properties and adding stages. This will be discussed further in thecreateTaskfunction definition. Next,task.init()initializes the task andtask.plan(5)generates a plan, stopping after 5 successful plans are found. The next line publishes the solution to be visualized in RViz - this line can be removed if you don’t care for visualization. Finally,task.execute()executes the plan. Execution occurs via an action server interface with the RViz plugin.\nAs mentioned above, this function creates a MoveIt Task Constructor object and sets some initial properties. In this case, we set the task name to “demo_task”, load the robot model, define the names of some useful frames, and set those frame names as properties of the task withtask.setProperty(property_name,value). The next few code blocks will fill out this function body.\nNow, we add an example stage to the node. The first line setscurrent_state_ptrtonullptr; this creates a pointer to a stage such that we can reuse stage information in specific scenarios. This line is not used at this moment, but will be used later when more stages are added to the task. Next, we make acurrent_statestage (a generator stage) and add it to our task - this starts the robot off in its current state. Now that we’ve created theCurrentStatestage, we save a pointer to it in thecurrent_state_ptrfor later use.\nSolvers are used to define the type of robot motion. MoveIt Task Constructor has three options for solvers:\nPipelinePlanneruses MoveIt’s planning pipeline, which typically defaults toOMPL.\nJointInterpolationis a simple planner that interpolates between the start and goal joint states. It is typically used for simple motions as it computes quickly but doesn’t support complex motions.\nCartesianPathis used to move the end effector in a straight line in Cartesian space.\nFeel free to try out the different solvers and see how the robot motion changes. For the first stage we will use the Cartesian planner, which requires the following properties to be set:\nNow that we added in the planners, we can add a stage that will move the robot.\nThe following lines use aMoveTostage (a propagator stage). Since opening the hand is a relatively simple movement, we can use the joint interpolation planner.\nThis stage plans a move to the “open hand” pose, which is a named pose defined in theSRDFfor the Panda robot.\nWe return the task and finish with thecreateTask()function.\nFinally, we havemain: the following lines create a node using the class defined above, and calls the class methods to set up and execute a basic MTC task. In this example, we do not cancel the executor once the task has finished executing to keep the node alive to inspect the solutions in RViz.\nWe will need a launch file to launch themove_group,ros2_control,static_tf,robot_state_publisher, andrviznodes that provide us the environment to run the demo. The one we will use for this example can be foundhere.\nTo run the MoveIt Task Constructor node, we will use a second launch file to start themtc_tutorialexecutable with the proper parameters. Here we can load URDF, SRDF, and OMPL parameters, or use MoveIt Configs Utils to do so. Your launch file should look something like the one found in this tutorial packagehere(pay close attention to thepackageandexecutablearguments below as they are different from the launch file linked) :\nSave a launch file aspick_place_demo.launch.pyor download one to the package’s launch directory. Make sure to edit theCMakeLists.txtso it includes the launch folder by adding the following lines:\nNow we can build and source the colcon workspace.\nStart by launching the first launch file. If you want to use the one provided by the tutorials:\nRViz will now load. If you’re using your own launch file and haven’t included an rviz configsuch as this, you will need to configure RViz before you see anything displayed. If you’re using the launch file from the tutorials package, RViz will already be configured for you and you can jump to the end of the next section.\nIf you are not using the RViz configuration provided, we’ll have to make some changes to the RViz configuration to see your robot and the MoveIt Task Constructor solutions. First, start RViz. The following steps will cover how to set up RViz for MoveIt Task Constructor solution visualization.\nIf theMotionPlanningdisplay is active, uncheck it to hide it for now.\nUnderGlobal Options, change theFixed Framefrommaptopanda_link0if not already done.\nOn the bottom left of the window, click theAddbutton.\nUndermoveit_task_constructor_visualizationselectMotion Planning Tasksand click OK. TheMotion Planning Tasksdisplay should appear on the bottom left.\nIn theDisplays, underMotion Planning Tasks,  changeTask Solution Topicto/solution\nYou should see the panda arm in the main view with Motion Planning Tasks display open in the bottom left and nothing in it. Your MTC task will show up in this panel once you launch themtc_tutorialnode. If you’re usingmtc_demo.launch.pyfrom the tutorials, jump back in here.\nLaunch themtc_tutorialnode with\nYou should see the arm execute the task with the single stage to open the hand, with the cylinder in green in front of it. It should look something like this:\nIf you haven’t made your own package, but still want to see what this looks like, you can launch this file from the tutorials:\nSo far, we’ve walked through creating and executing a simple task, which runs but does not do much. Now, we will start adding the pick-and-place stages to the task. The image below shows an outline of the stages we will use in our task.\nWe will start adding stages after our existing open hand stage. Openmtc_node.cppand locate the following lines:\nWe need to move the arm to a position where we can pick up our object. This is done with aConnectstage, which as its name implies, is a Connector stage. This means that it tries to bridge between the results of the stage before and after it. This stage is initialized with a name,move_to_pick, and aGroupPlannerVectorthat specifies the planning group and the planner. We then set a timeout for the stage, set the properties for the stage, and add it to our task.\nNext, we create a pointer to a MoveIt Task Constructor stage object, and set it tonullptrfor now. Later, we will use this to save a stage.\nThis next block of code creates aSerialContainer.\nThis is a container that can be added to our task and can hold several substages.\nIn this case, we create a serial container that will contain the stages relevant to the picking action.\nInstead of adding the stages to the task, we will add the relevant stages to the serial container. We useexposeTo()to declare the task properties from the parent task in the new serial container, and useconfigureInitFrom()to initialize them.\nThis allows the contained stages to access these properties.\nWe then create a stage to approach the object. This stage is aMoveRelativestage, which allows us to specify a relative movement from our current position.MoveRelativeis a propagator stage: it receives the solution from its neighbouring stages and propagates it to the next or previous stage. Usingcartesian_plannerfinds a solution that involves moving the end effector in a straight line. We set the properties, and set the minimum and maximum distance to move. Now we create aVector3Stampedmessage to indicate the direction we want to move - in this case, in the Z direction from the hand frame. Finally, we add this stage to our serial container\nNow, create a stage to generate the grasp pose.\nThis is a generator stage, so it computes its results without regard to the stages before and after it.\nThe first stage,CurrentStateis a generator stage as well - to connect the first stage and this stage, a connecting stage must be used, which we already created above.\nThis code sets the stage properties, sets the pose before grasping, the angle delta, and the monitored stage.\nAngle delta is a property of theGenerateGraspPosestage that is used to determine the number of poses to generate; when generating solutions, MoveIt Task Constructor will try to grasp the object from many different orientations, with the difference between the orientations specified by the angle delta. The smaller the delta, the closer together the grasp orientations will be. When defining the current stage, we setcurrent_state_ptr, which is now used to forward information about the object pose and shape to the inverse kinematics solver.\nThis stage won’t be directly added to the serial container like previously, as we still need to do inverse kinematics on the poses it generates.\nBefore we compute inverse kinematics for the poses generated above, we first need to define the frame. This can be done with aPoseStampedmessage fromgeometry_msgsor in this case, we define the transform using Eigen transformation matrix and the name of the relevant link. Here, we define the transformation matrix.\nNow, we create theComputeIKstage, and give it the namegenerateposeIKas well as thegenerategraspposestage defined above. Some robots have multiple inverse kinematics solutions for a given pose - we set the limit on the amount of solutions to solve for up to 8. We also set the minimum solution distance, which is a threshold on how different solutions must be: if the joint positions in a solution are too similar to a previous solution, it will be marked as invalid. Next, we configure some additional properties, and add theComputeIKstage to the serial container.\nTo pick up the object, we must allow collision between the hand and the object. This can be done with aModifyPlanningScenestage. TheallowCollisionsfunction lets us specify which collisions to disable.allowCollisionscan be used with a container of names, so we can usegetLinkModelNamesWithCollisionGeometryto get all the names of links with collision geometry in the hand group.\nWith collisions allowed, we now can close the hand. This is done with aMoveTostage, similarly to theopenhandstage from above, except moving to thecloseposition as defined in the SRDF.\nWe now use aModifyPlanningScenestage again, this time to attach the object to the hand usingattachObject. Similarly to what we did with thecurrent_state_ptr, we get a pointer to this stage for later use when generating the place pose for the object.\nNext, we lift the object with aMoveRelativestage, similarly to theapproach_objectstage.\nWith this, we have all the stages needed to pick the object. Now, we add the serial container (with all its substages) to the task. If you build the package as-is, you can see the robot plan to pick up the object.\nTo test out the newly created stage, build the code and execute:\nNow that the stages that define the pick are complete, we move on to defining the stages for placing the object. Picking up where we left off, we add aConnectstage to connect the two, as we will soon be using a generator stage to generate the pose for placing the object.\nWe also create a serial container for the place stages. This is done similarly to the pick serial container.\nThe next stages will be added to the serial container rather than the task.\nThis next stage generates the poses used to place the object and compute the inverse kinematics for those poses - it is somewhat similar to thegenerategraspposestage from the pick serial container.\nWe start by creating a stage to generate the poses and inheriting the task properties.\nWe specify the pose where we want to place the object with aPoseStampedmessage fromgeometry_msgs- in this case, we choosey=0.5in the\"object\"frame.\nWe then pass the target pose to the stage withsetPose.\nNext, we usesetMonitoredStageand pass it the pointer to theattach_objectstage from earlier.\nThis allows the stage to know how the object is attached.\nWe then create aComputeIKstage and pass it ourGeneratePlacePosestage - the rest follows the same logic as above with the pick stages.\nNow that we’re ready to place the object, we open the hand withMoveTostage and the joint interpolation planner.\nWe also can re-enable collisions with the object now that we no longer need to hold it.\nThis is done usingallowCollisionsalmost exactly the same way as disabling collisions, except setting the last argument tofalserather thantrue.\nNow, we can detach the object usingdetachObject.\nWe retreat from the object using aMoveRelativestage, which is done similarly to theapproachobjectandliftobjectstages.\nWe finish our place serial container and add it to the task.\nThe final step is to return home: we use aMoveTostage and pass it the goal pose ofready, which is a pose defined in the Panda SRDF.\nAll these stages should be added above these lines.\nCongratulations! You’ve now defined a pick and place task using MoveIt Task Constructor! To try it out, build the code and execute:\nThe task with each comprising stage is shown in the Motion Planning Tasks pane. Click on a stage and additional information about the stage will show up to the right. The right pane shows different solutions as well as their associated costs. Depending on the stage type and the robot configuration, there may only be one solution shown.\nClick one of the solution costs to see an animation of the robot following the plan for that stage. Click the “Exec” button in the upper-right portion of the pane to execute the motion.\nTo run the complete MoveIt Task Constructor example included with the MoveIt tutorials:\nAnd in a second terminal:\nWhen running MTC, it prints a diagram like this to terminal:\nThis example^ shows two stages. The first stage (“initial_state”) is aCurrentStatetype of stage, which initializes aPlanningSceneand captures any collision objects that are present at that moment.\nA pointer to this stage can be used to retrieve the state of the robot.\nSinceCurrentStateinherits fromGenerator, it propagates solutions both forward and backward.\nThis is denoted by the arrows in both directions.\nThe first1indicates that one solution was successfully propagated backwards to the previous stage.\nThe second1, between the arrows, indicates that one solution was generated.\nThe0indicates that a solution was not propagated forward successfully to the next stage, because the next stage failed.\nThe second stage (“move_to_home”) is aMoveTotype of stage. It inherits its propagation direction from the previous stage, so both arrows point forward. The0’s indicate that this stage failed completely. From left to right, the0’s mean:\nThe stage did not receive a solution from the previous stage\nThe stage did not generate a solution\nThe stage did not propagate a solution forward to the next stage\nIn this case, we could tell that “move_to_home” was the root cause of the failure. The problem was a home state that was in collision. Defining a new, collision-free home position fixed the issue.\nInformation about individual stages can be retrieved from the task. For example, here we retrieve the unique ID for a stage:\nACurrentStatetype stage does not just retrieve the current state of the robot.\nIt also initializes aPlanningSceneobject, capturing any collision objects that are present at that moment.\nMTC stages can be propagated in forward and backward order.\nYou can easily check which direction a stage propagates by the arrow in the RViz GUI.\nWhen propagating backwards, the logic of many operations is reversed.\nFor example, to allow collisions with an object in aModifyPlanningScenestage, you would callallowCollisions(false)rather thanallowCollisions(true). There is a discussion to be readhere.\n© Copyright 2024, PickNik Robotics.\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit_task_constructor.git\nrosdep install --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncd~/ws_moveitcolconbuild--mixinrelease\nros2launchmoveit_task_constructor_demodemo.launch.py\nros2launchmoveit_task_constructor_demorun.launch.pyexe:=cartesianros2launchmoveit_task_constructor_demorun.launch.pyexe:=modularros2launchmoveit_task_constructor_demorun.launch.pyexe:=pick_place_demo\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_task_constructor_corerclcpp\\--node-namemtc_nodemtc_tutorial\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endifstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;classMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};MTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}rclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}voidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;pose.orientation.w=1.0;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}voidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}mtc::TaskMTCTaskNode::createTask(){mtc::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);// Disable warnings for this line, as it's a variable that's set but not used in this example#pragma GCC diagnostic push#pragma GCC diagnostic ignored \"-Wunused-but-set-variable\"mtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generator#pragma GCC diagnostic popautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));autosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);autointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();autocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);autostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}intmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endif\nstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;\nclassMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};\nMTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}\nrclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}\nvoidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}\nvoidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}\nmtc::TaskMTCTaskNode::createTask(){moveit::task_constructor::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);\nmtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generatorautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));\nautosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);\nautointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}\nintmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\nfromlaunchimportLaunchDescriptionfromlaunch_ros.actionsimportNodefrommoveit_configs_utilsimportMoveItConfigsBuilderdefgenerate_launch_description():moveit_config=MoveItConfigsBuilder(\"moveit_resources_panda\").to_dict()# MTC Demo nodepick_place_demo=Node(package=\"mtc_tutorial\",executable=\"mtc_node\",output=\"screen\",parameters=[moveit_config,],)returnLaunchDescription([pick_place_demo])\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}\n  )\ncd~/ws_moveitcolconbuild--mixinreleasesource~/ws_moveit/install/setup.bash\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo_minimal.launch.py\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));// Add the next lines of codes to define more stages here\nautostage_move_to_pick=std::make_unique<mtc::stages::Connect>(\"move to pick\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner}});stage_move_to_pick->setTimeout(5.0);stage_move_to_pick->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_pick));\nmtc::Stage*attach_object_stage=nullptr;// Forward attach_object_stage to place pose generator\n{autograsp=std::make_unique<mtc::SerialContainer>(\"pick object\");task.properties().exposeTo(grasp->properties(),{\"eef\",\"group\",\"ik_frame\"});grasp->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"approach object\",cartesian_planner);stage->properties().set(\"marker_ns\",\"approach_object\");stage->properties().set(\"link\",hand_frame);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.15);// Set hand forward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=hand_frame;vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\n{// Sample grasp poseautostage=std::make_unique<mtc::stages::GenerateGraspPose>(\"generate grasp pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"grasp_pose\");stage->setPreGraspPose(\"open\");stage->setObject(\"object\");stage->setAngleDelta(M_PI/12);stage->setMonitoredStage(current_state_ptr);// Hook into current state\nEigen::Isometry3dgrasp_frame_transform;Eigen::Quaterniondq=Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitX())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitY())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitZ());grasp_frame_transform.linear()=q.matrix();grasp_frame_transform.translation().z()=0.1;\n// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"grasp pose IK\",std::move(stage));wrapper->setMaxIKSolutions(8);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(grasp_frame_transform,hand_frame);wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});grasp->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"allow collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),true);grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"close hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"close\");grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"attach object\");stage->attachObject(\"object\",hand_frame);attach_object_stage=stage.get();grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"lift object\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"lift_object\");// Set upward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\ntask.add(std::move(grasp));}\nros2launchmtc_tutorialpick_place_demo.launch.py\n{autostage_move_to_place=std::make_unique<mtc::stages::Connect>(\"move to place\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner},{hand_group_name,interpolation_planner}});stage_move_to_place->setTimeout(5.0);stage_move_to_place->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_place));}\n{autoplace=std::make_unique<mtc::SerialContainer>(\"place object\");task.properties().exposeTo(place->properties(),{\"eef\",\"group\",\"ik_frame\"});place->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{// Sample place poseautostage=std::make_unique<mtc::stages::GeneratePlacePose>(\"generate place pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"place_pose\");stage->setObject(\"object\");geometry_msgs::msg::PoseStampedtarget_pose_msg;target_pose_msg.header.frame_id=\"object\";target_pose_msg.pose.position.y=0.5;target_pose_msg.pose.orientation.w=1.0;stage->setPose(target_pose_msg);stage->setMonitoredStage(attach_object_stage);// Hook into attach_object_stage// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"place pose IK\",std::move(stage));wrapper->setMaxIKSolutions(2);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(\"object\");wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});place->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"open\");place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"forbid collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),false);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"detach object\");stage->detachObject(\"object\",hand_frame);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"retreat\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"retreat\");// Set retreat directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.x=-0.5;stage->setDirection(vec);place->insert(std::move(stage));}\ntask.add(std::move(place));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"return home\",interpolation_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setGoal(\"ready\");task.add(std::move(stage));}\n// Stages all added to the task above this linereturntask;}\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmoveit2_tutorialspick_place_demo.launch.py\n[demo_node-1]1-←1→-0/initial_state[demo_node-1]-0→0→-0/move_to_home\nuint32_tconstunique_stage_id=task_.stages()->findChild(stage_name)->introspectionId();\n<branch>\nhumble\nros2\nmain\nmtc_tutorial\nmoveit_task_constructor_core\nsrc/mtc_node\nmtc_node.cpp\nrclcpp/rclcpp.hpp\nmoveit/planning_scene/planning_scene.h\nmoveit/planning_scene_interface/planning_scene_interface.h\nmoveit/task_constructor/task.h\nmoveit/task_constructor/solvers.h\nmoveit/task_constructor/stages.h\ntf2_geometry_msgs/tf2_geometry_msgs.hpp\ntf2_eigen/tf2_eigen.hpp\nmoveit::task_constructor\nMTCTaskNode\nobject.primitives[0].dimensions\npose.position.x\npose.position.y\ncreateTask\ntask.init()\ntask.plan(5)\ntask.execute()\ntask.setProperty(property_name,value)\ncurrent_state_ptr\nnullptr\ncurrent_state\nCurrentState\ncurrent_state_ptr\nMoveTo\ncreateTask()\nmain\nmove_group\nros2_control\nstatic_tf\nrobot_state_publisher\nrviz\nmtc_tutorial\npackage\nexecutable\npick_place_demo.launch.py\nCMakeLists.txt\nmap\npanda_link0\nmoveit_task_constructor_visualization\n/solution\nmtc_tutorial\nmtc_demo.launch.py\nmtc_tutorial\nmtc_node.cpp\nConnect\nmove_to_pick\nGroupPlannerVector\nnullptr\nSerialContainer\nexposeTo()\nconfigureInitFrom()\nMoveRelative\nMoveRelative\ncartesian_planner\nVector3Stamped\nCurrentState\nGenerateGraspPose\ncurrent_state_ptr\nPoseStamped\ngeometry_msgs\nComputeIK\ngenerateposeIK\ngenerategrasppose\nComputeIK\nModifyPlanningScene\nallowCollisions\nallowCollisions\ngetLinkModelNamesWithCollisionGeometry\nMoveTo\nopenhand\nclose\nModifyPlanningScene\nattachObject\ncurrent_state_ptr\nMoveRelative\napproach_object\nConnect\ngenerategrasppose\nPoseStamped\ngeometry_msgs\ny=0.5\n\"object\"\nsetPose\nsetMonitoredStage\nattach_object\nComputeIK\nGeneratePlacePose\nMoveTo\nallowCollisions\nfalse\ntrue\ndetachObject\nMoveRelative\napproachobject\nliftobject\nMoveTo\nready\nCurrentState\nPlanningScene\nCurrentState\nGenerator\n1\n1\n0\nMoveTo\n0\n0\nCurrentState\nPlanningScene\nModifyPlanningScene\nallowCollisions(false)\nallowCollisions(true)",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0e3"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/pick_and_place_with_moveit_task_constructor/pick_and_place_with_moveit_task_constructor.html#further-discussion",
  "title": "Pick and Place with MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "Pick and Place with MoveIt Task Constructor\n1 Basic Concepts\n2 Getting Started\n3 Trying It Out\n4 Setting up a Project with MoveIt Task Constructor\n5 Running the Demo\n6 Adding Stages\n7 Further Discussion\nThis tutorial will walk you through creating a package that plans a pick and place operation usingMoveIt Task Constructor. MoveIt Task Constructor provides a way to plan for tasks that consist of multiple different subtasks (known as stages). If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nThe fundamental idea of MTC is that complex motion planning problems can be composed into a set of simpler subproblems.\nThe top-level planning problem is specified as aTaskwhile all subproblems are specified byStages.\nStages can be arranged in any arbitrary order and hierarchy only limited by the individual stages types.\nThe order in which stages can be arranged is restricted by the direction in which results are passed.\nThere are three possible stages relating to the result flow: generator, propagator, and connector stages:\nGeneratorscompute their results independently of their neighbor stages and pass them in both directions, backwards and forwards.\nAn example is an IK sampler for geometric poses where approaching and departing motions (neighbor stages) depend on the solution.\nPropagatorsreceive the result of one neighbor stage, solve a subproblem and then propagate their result to the neighbor on the opposite site.\nDepending on the implementation, propagating stages can pass solutions forward, backward or in both directions separately.\nAn example is a stage that computes a Cartesian path based on either a start or a goal state.\nConnectorsdo not propagate any results, but rather attempt to bridge the gap between the resulting states of both neighbors.\nAn example is the computation of a free-motion plan from one given state to another.\nAdditional to the order types, there are different hierarchy types allowing to encapsulate subordinate stages.\nStages without subordinate stages are calledprimitive stages, higher-level stages are calledcontainer stages.\nThere are three container types:\nWrappersencapsulate a single subordinate stage and modify or filter the results.\nFor example, a filter stage that only accepts solutions of its child stage that satisfy a certain constraint can be realized as a wrapper.\nAnother standard use of this type includes the IK wrapper stage, which generates inverse kinematics solutions based on planning scenes annotated with a pose target property.\nSerial Containershold a sequence of subordinate stages and only consider end-to-end solutions as results.\nAn example is a picking motion that consists of a sequence of coherent steps.\nParallel Containerscombine set of subordinate stages and can be used for passing the best of alternative results, running fallback solvers or for merging multiple independent solutions.\nExamples are running alternative planners for a free-motion plan, picking objects with the right hand or with the left hand as a fallback, or moving the arm and opening the gripper at the same time.\nStages not only support solving motion planning problems.\nThey can also be used for all kinds of state transitions, as for instance modifying the planning scene.\nCombined with the possibility of using class inheritance it is possible to construct very complex behavior while only relying on a well-structured set of primitive stages.\nMore detailed information on MTC can be found in theMoveIt Task Constructor concepts page\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nMove into your colcon workspace and pull the MoveIt Task Constructor source, where<branch>can be e.g.humblefor ROS Humble, orros2for the latest version compatible with MoveIt 2main:\nInstall missing packages with rosdep:\nBuild the workspace:\nThe MoveIt Task Constructor package contains several basic examples and a pick-and-place demo.\nFor all demos you should launch the basic environment:\nSubsequently, you can run the individual demos:\nOn the right side you should see theMotion Planning Taskspanel outlining the hierarchical stage structure of the tasks.\nWhen you select a particular stage, the list of successful and failed solutions will be\nshown in the right-most window. Selecting one of those solutions will start its visualization.\nThis section walks through the steps required to build a simple task with MoveIt Task Constructor.\nCreate a new package with the following command:\nThis will create a new package and folder calledmtc_tutorialwith a dependency onmoveit_task_constructor_coreas well as a hello world example insrc/mtc_node.\nOpenmtc_node.cppin your editor of choice, and paste in the following code.\nThe top of the code includes the ROS and MoveIt Libraries that this package uses.\nrclcpp/rclcpp.hppincludes core ROS2 functionality\nmoveit/planning_scene/planning_scene.handmoveit/planning_scene_interface/planning_scene_interface.hinclude functionality to interface with the robot model and collision objects\nmoveit/task_constructor/task.h,moveit/task_constructor/solvers.h, andmoveit/task_constructor/stages.hinclude different components of MoveIt Task Constructor that are used in the example\ntf2_geometry_msgs/tf2_geometry_msgs.hppandtf2_eigen/tf2_eigen.hppwon’t be used in this initial example, but they will be used for pose generation when we add more stages to the MoveIt Task Constructor task.\nThe next line gets a logger for our new node. We also create a namespace alias formoveit::task_constructorfor convenience.\nWe start by defining a class that will contain the main MoveIt Task Constructor functionality.  We also declare the MoveIt Task Constructor task object as a member variable for our class: this isn’t strictly necessary for a given application, but it helps save the task for later visualization purposes. We will explore each function individually below.\nThese lines initialize the node with specified options (it is the constructor of ourMTCTaskNodeclass).\nThese next lines define a getter function to get the node base interface, which will be used for the executor later.\nThis class method is used to set up the planning scene that is used in the example. It creates a cylinder with dimensions specified byobject.primitives[0].dimensionsand position specified bypose.position.xandpose.position.y.\nYou can try changing these numbers to resize and move the cylinder around. If we move the cylinder out of the robot’s reach, planning will fail.\nThis function interfaces with the MoveIt Task Constructor task object. It first creates a task, which includes setting some properties and adding stages. This will be discussed further in thecreateTaskfunction definition. Next,task.init()initializes the task andtask.plan(5)generates a plan, stopping after 5 successful plans are found. The next line publishes the solution to be visualized in RViz - this line can be removed if you don’t care for visualization. Finally,task.execute()executes the plan. Execution occurs via an action server interface with the RViz plugin.\nAs mentioned above, this function creates a MoveIt Task Constructor object and sets some initial properties. In this case, we set the task name to “demo_task”, load the robot model, define the names of some useful frames, and set those frame names as properties of the task withtask.setProperty(property_name,value). The next few code blocks will fill out this function body.\nNow, we add an example stage to the node. The first line setscurrent_state_ptrtonullptr; this creates a pointer to a stage such that we can reuse stage information in specific scenarios. This line is not used at this moment, but will be used later when more stages are added to the task. Next, we make acurrent_statestage (a generator stage) and add it to our task - this starts the robot off in its current state. Now that we’ve created theCurrentStatestage, we save a pointer to it in thecurrent_state_ptrfor later use.\nSolvers are used to define the type of robot motion. MoveIt Task Constructor has three options for solvers:\nPipelinePlanneruses MoveIt’s planning pipeline, which typically defaults toOMPL.\nJointInterpolationis a simple planner that interpolates between the start and goal joint states. It is typically used for simple motions as it computes quickly but doesn’t support complex motions.\nCartesianPathis used to move the end effector in a straight line in Cartesian space.\nFeel free to try out the different solvers and see how the robot motion changes. For the first stage we will use the Cartesian planner, which requires the following properties to be set:\nNow that we added in the planners, we can add a stage that will move the robot.\nThe following lines use aMoveTostage (a propagator stage). Since opening the hand is a relatively simple movement, we can use the joint interpolation planner.\nThis stage plans a move to the “open hand” pose, which is a named pose defined in theSRDFfor the Panda robot.\nWe return the task and finish with thecreateTask()function.\nFinally, we havemain: the following lines create a node using the class defined above, and calls the class methods to set up and execute a basic MTC task. In this example, we do not cancel the executor once the task has finished executing to keep the node alive to inspect the solutions in RViz.\nWe will need a launch file to launch themove_group,ros2_control,static_tf,robot_state_publisher, andrviznodes that provide us the environment to run the demo. The one we will use for this example can be foundhere.\nTo run the MoveIt Task Constructor node, we will use a second launch file to start themtc_tutorialexecutable with the proper parameters. Here we can load URDF, SRDF, and OMPL parameters, or use MoveIt Configs Utils to do so. Your launch file should look something like the one found in this tutorial packagehere(pay close attention to thepackageandexecutablearguments below as they are different from the launch file linked) :\nSave a launch file aspick_place_demo.launch.pyor download one to the package’s launch directory. Make sure to edit theCMakeLists.txtso it includes the launch folder by adding the following lines:\nNow we can build and source the colcon workspace.\nStart by launching the first launch file. If you want to use the one provided by the tutorials:\nRViz will now load. If you’re using your own launch file and haven’t included an rviz configsuch as this, you will need to configure RViz before you see anything displayed. If you’re using the launch file from the tutorials package, RViz will already be configured for you and you can jump to the end of the next section.\nIf you are not using the RViz configuration provided, we’ll have to make some changes to the RViz configuration to see your robot and the MoveIt Task Constructor solutions. First, start RViz. The following steps will cover how to set up RViz for MoveIt Task Constructor solution visualization.\nIf theMotionPlanningdisplay is active, uncheck it to hide it for now.\nUnderGlobal Options, change theFixed Framefrommaptopanda_link0if not already done.\nOn the bottom left of the window, click theAddbutton.\nUndermoveit_task_constructor_visualizationselectMotion Planning Tasksand click OK. TheMotion Planning Tasksdisplay should appear on the bottom left.\nIn theDisplays, underMotion Planning Tasks,  changeTask Solution Topicto/solution\nYou should see the panda arm in the main view with Motion Planning Tasks display open in the bottom left and nothing in it. Your MTC task will show up in this panel once you launch themtc_tutorialnode. If you’re usingmtc_demo.launch.pyfrom the tutorials, jump back in here.\nLaunch themtc_tutorialnode with\nYou should see the arm execute the task with the single stage to open the hand, with the cylinder in green in front of it. It should look something like this:\nIf you haven’t made your own package, but still want to see what this looks like, you can launch this file from the tutorials:\nSo far, we’ve walked through creating and executing a simple task, which runs but does not do much. Now, we will start adding the pick-and-place stages to the task. The image below shows an outline of the stages we will use in our task.\nWe will start adding stages after our existing open hand stage. Openmtc_node.cppand locate the following lines:\nWe need to move the arm to a position where we can pick up our object. This is done with aConnectstage, which as its name implies, is a Connector stage. This means that it tries to bridge between the results of the stage before and after it. This stage is initialized with a name,move_to_pick, and aGroupPlannerVectorthat specifies the planning group and the planner. We then set a timeout for the stage, set the properties for the stage, and add it to our task.\nNext, we create a pointer to a MoveIt Task Constructor stage object, and set it tonullptrfor now. Later, we will use this to save a stage.\nThis next block of code creates aSerialContainer.\nThis is a container that can be added to our task and can hold several substages.\nIn this case, we create a serial container that will contain the stages relevant to the picking action.\nInstead of adding the stages to the task, we will add the relevant stages to the serial container. We useexposeTo()to declare the task properties from the parent task in the new serial container, and useconfigureInitFrom()to initialize them.\nThis allows the contained stages to access these properties.\nWe then create a stage to approach the object. This stage is aMoveRelativestage, which allows us to specify a relative movement from our current position.MoveRelativeis a propagator stage: it receives the solution from its neighbouring stages and propagates it to the next or previous stage. Usingcartesian_plannerfinds a solution that involves moving the end effector in a straight line. We set the properties, and set the minimum and maximum distance to move. Now we create aVector3Stampedmessage to indicate the direction we want to move - in this case, in the Z direction from the hand frame. Finally, we add this stage to our serial container\nNow, create a stage to generate the grasp pose.\nThis is a generator stage, so it computes its results without regard to the stages before and after it.\nThe first stage,CurrentStateis a generator stage as well - to connect the first stage and this stage, a connecting stage must be used, which we already created above.\nThis code sets the stage properties, sets the pose before grasping, the angle delta, and the monitored stage.\nAngle delta is a property of theGenerateGraspPosestage that is used to determine the number of poses to generate; when generating solutions, MoveIt Task Constructor will try to grasp the object from many different orientations, with the difference between the orientations specified by the angle delta. The smaller the delta, the closer together the grasp orientations will be. When defining the current stage, we setcurrent_state_ptr, which is now used to forward information about the object pose and shape to the inverse kinematics solver.\nThis stage won’t be directly added to the serial container like previously, as we still need to do inverse kinematics on the poses it generates.\nBefore we compute inverse kinematics for the poses generated above, we first need to define the frame. This can be done with aPoseStampedmessage fromgeometry_msgsor in this case, we define the transform using Eigen transformation matrix and the name of the relevant link. Here, we define the transformation matrix.\nNow, we create theComputeIKstage, and give it the namegenerateposeIKas well as thegenerategraspposestage defined above. Some robots have multiple inverse kinematics solutions for a given pose - we set the limit on the amount of solutions to solve for up to 8. We also set the minimum solution distance, which is a threshold on how different solutions must be: if the joint positions in a solution are too similar to a previous solution, it will be marked as invalid. Next, we configure some additional properties, and add theComputeIKstage to the serial container.\nTo pick up the object, we must allow collision between the hand and the object. This can be done with aModifyPlanningScenestage. TheallowCollisionsfunction lets us specify which collisions to disable.allowCollisionscan be used with a container of names, so we can usegetLinkModelNamesWithCollisionGeometryto get all the names of links with collision geometry in the hand group.\nWith collisions allowed, we now can close the hand. This is done with aMoveTostage, similarly to theopenhandstage from above, except moving to thecloseposition as defined in the SRDF.\nWe now use aModifyPlanningScenestage again, this time to attach the object to the hand usingattachObject. Similarly to what we did with thecurrent_state_ptr, we get a pointer to this stage for later use when generating the place pose for the object.\nNext, we lift the object with aMoveRelativestage, similarly to theapproach_objectstage.\nWith this, we have all the stages needed to pick the object. Now, we add the serial container (with all its substages) to the task. If you build the package as-is, you can see the robot plan to pick up the object.\nTo test out the newly created stage, build the code and execute:\nNow that the stages that define the pick are complete, we move on to defining the stages for placing the object. Picking up where we left off, we add aConnectstage to connect the two, as we will soon be using a generator stage to generate the pose for placing the object.\nWe also create a serial container for the place stages. This is done similarly to the pick serial container.\nThe next stages will be added to the serial container rather than the task.\nThis next stage generates the poses used to place the object and compute the inverse kinematics for those poses - it is somewhat similar to thegenerategraspposestage from the pick serial container.\nWe start by creating a stage to generate the poses and inheriting the task properties.\nWe specify the pose where we want to place the object with aPoseStampedmessage fromgeometry_msgs- in this case, we choosey=0.5in the\"object\"frame.\nWe then pass the target pose to the stage withsetPose.\nNext, we usesetMonitoredStageand pass it the pointer to theattach_objectstage from earlier.\nThis allows the stage to know how the object is attached.\nWe then create aComputeIKstage and pass it ourGeneratePlacePosestage - the rest follows the same logic as above with the pick stages.\nNow that we’re ready to place the object, we open the hand withMoveTostage and the joint interpolation planner.\nWe also can re-enable collisions with the object now that we no longer need to hold it.\nThis is done usingallowCollisionsalmost exactly the same way as disabling collisions, except setting the last argument tofalserather thantrue.\nNow, we can detach the object usingdetachObject.\nWe retreat from the object using aMoveRelativestage, which is done similarly to theapproachobjectandliftobjectstages.\nWe finish our place serial container and add it to the task.\nThe final step is to return home: we use aMoveTostage and pass it the goal pose ofready, which is a pose defined in the Panda SRDF.\nAll these stages should be added above these lines.\nCongratulations! You’ve now defined a pick and place task using MoveIt Task Constructor! To try it out, build the code and execute:\nThe task with each comprising stage is shown in the Motion Planning Tasks pane. Click on a stage and additional information about the stage will show up to the right. The right pane shows different solutions as well as their associated costs. Depending on the stage type and the robot configuration, there may only be one solution shown.\nClick one of the solution costs to see an animation of the robot following the plan for that stage. Click the “Exec” button in the upper-right portion of the pane to execute the motion.\nTo run the complete MoveIt Task Constructor example included with the MoveIt tutorials:\nAnd in a second terminal:\nWhen running MTC, it prints a diagram like this to terminal:\nThis example^ shows two stages. The first stage (“initial_state”) is aCurrentStatetype of stage, which initializes aPlanningSceneand captures any collision objects that are present at that moment.\nA pointer to this stage can be used to retrieve the state of the robot.\nSinceCurrentStateinherits fromGenerator, it propagates solutions both forward and backward.\nThis is denoted by the arrows in both directions.\nThe first1indicates that one solution was successfully propagated backwards to the previous stage.\nThe second1, between the arrows, indicates that one solution was generated.\nThe0indicates that a solution was not propagated forward successfully to the next stage, because the next stage failed.\nThe second stage (“move_to_home”) is aMoveTotype of stage. It inherits its propagation direction from the previous stage, so both arrows point forward. The0’s indicate that this stage failed completely. From left to right, the0’s mean:\nThe stage did not receive a solution from the previous stage\nThe stage did not generate a solution\nThe stage did not propagate a solution forward to the next stage\nIn this case, we could tell that “move_to_home” was the root cause of the failure. The problem was a home state that was in collision. Defining a new, collision-free home position fixed the issue.\nInformation about individual stages can be retrieved from the task. For example, here we retrieve the unique ID for a stage:\nACurrentStatetype stage does not just retrieve the current state of the robot.\nIt also initializes aPlanningSceneobject, capturing any collision objects that are present at that moment.\nMTC stages can be propagated in forward and backward order.\nYou can easily check which direction a stage propagates by the arrow in the RViz GUI.\nWhen propagating backwards, the logic of many operations is reversed.\nFor example, to allow collisions with an object in aModifyPlanningScenestage, you would callallowCollisions(false)rather thanallowCollisions(true). There is a discussion to be readhere.\n© Copyright 2024, PickNik Robotics.\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit_task_constructor.git\nrosdep install --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncd~/ws_moveitcolconbuild--mixinrelease\nros2launchmoveit_task_constructor_demodemo.launch.py\nros2launchmoveit_task_constructor_demorun.launch.pyexe:=cartesianros2launchmoveit_task_constructor_demorun.launch.pyexe:=modularros2launchmoveit_task_constructor_demorun.launch.pyexe:=pick_place_demo\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_task_constructor_corerclcpp\\--node-namemtc_nodemtc_tutorial\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endifstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;classMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};MTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}rclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}voidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;pose.orientation.w=1.0;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}voidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}mtc::TaskMTCTaskNode::createTask(){mtc::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);// Disable warnings for this line, as it's a variable that's set but not used in this example#pragma GCC diagnostic push#pragma GCC diagnostic ignored \"-Wunused-but-set-variable\"mtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generator#pragma GCC diagnostic popautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));autosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);autointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();autocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);autostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}intmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endif\nstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;\nclassMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};\nMTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}\nrclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}\nvoidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}\nvoidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}\nmtc::TaskMTCTaskNode::createTask(){moveit::task_constructor::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);\nmtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generatorautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));\nautosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);\nautointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}\nintmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\nfromlaunchimportLaunchDescriptionfromlaunch_ros.actionsimportNodefrommoveit_configs_utilsimportMoveItConfigsBuilderdefgenerate_launch_description():moveit_config=MoveItConfigsBuilder(\"moveit_resources_panda\").to_dict()# MTC Demo nodepick_place_demo=Node(package=\"mtc_tutorial\",executable=\"mtc_node\",output=\"screen\",parameters=[moveit_config,],)returnLaunchDescription([pick_place_demo])\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}\n  )\ncd~/ws_moveitcolconbuild--mixinreleasesource~/ws_moveit/install/setup.bash\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo_minimal.launch.py\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));// Add the next lines of codes to define more stages here\nautostage_move_to_pick=std::make_unique<mtc::stages::Connect>(\"move to pick\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner}});stage_move_to_pick->setTimeout(5.0);stage_move_to_pick->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_pick));\nmtc::Stage*attach_object_stage=nullptr;// Forward attach_object_stage to place pose generator\n{autograsp=std::make_unique<mtc::SerialContainer>(\"pick object\");task.properties().exposeTo(grasp->properties(),{\"eef\",\"group\",\"ik_frame\"});grasp->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"approach object\",cartesian_planner);stage->properties().set(\"marker_ns\",\"approach_object\");stage->properties().set(\"link\",hand_frame);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.15);// Set hand forward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=hand_frame;vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\n{// Sample grasp poseautostage=std::make_unique<mtc::stages::GenerateGraspPose>(\"generate grasp pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"grasp_pose\");stage->setPreGraspPose(\"open\");stage->setObject(\"object\");stage->setAngleDelta(M_PI/12);stage->setMonitoredStage(current_state_ptr);// Hook into current state\nEigen::Isometry3dgrasp_frame_transform;Eigen::Quaterniondq=Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitX())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitY())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitZ());grasp_frame_transform.linear()=q.matrix();grasp_frame_transform.translation().z()=0.1;\n// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"grasp pose IK\",std::move(stage));wrapper->setMaxIKSolutions(8);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(grasp_frame_transform,hand_frame);wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});grasp->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"allow collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),true);grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"close hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"close\");grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"attach object\");stage->attachObject(\"object\",hand_frame);attach_object_stage=stage.get();grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"lift object\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"lift_object\");// Set upward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\ntask.add(std::move(grasp));}\nros2launchmtc_tutorialpick_place_demo.launch.py\n{autostage_move_to_place=std::make_unique<mtc::stages::Connect>(\"move to place\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner},{hand_group_name,interpolation_planner}});stage_move_to_place->setTimeout(5.0);stage_move_to_place->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_place));}\n{autoplace=std::make_unique<mtc::SerialContainer>(\"place object\");task.properties().exposeTo(place->properties(),{\"eef\",\"group\",\"ik_frame\"});place->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{// Sample place poseautostage=std::make_unique<mtc::stages::GeneratePlacePose>(\"generate place pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"place_pose\");stage->setObject(\"object\");geometry_msgs::msg::PoseStampedtarget_pose_msg;target_pose_msg.header.frame_id=\"object\";target_pose_msg.pose.position.y=0.5;target_pose_msg.pose.orientation.w=1.0;stage->setPose(target_pose_msg);stage->setMonitoredStage(attach_object_stage);// Hook into attach_object_stage// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"place pose IK\",std::move(stage));wrapper->setMaxIKSolutions(2);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(\"object\");wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});place->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"open\");place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"forbid collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),false);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"detach object\");stage->detachObject(\"object\",hand_frame);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"retreat\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"retreat\");// Set retreat directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.x=-0.5;stage->setDirection(vec);place->insert(std::move(stage));}\ntask.add(std::move(place));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"return home\",interpolation_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setGoal(\"ready\");task.add(std::move(stage));}\n// Stages all added to the task above this linereturntask;}\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmoveit2_tutorialspick_place_demo.launch.py\n[demo_node-1]1-←1→-0/initial_state[demo_node-1]-0→0→-0/move_to_home\nuint32_tconstunique_stage_id=task_.stages()->findChild(stage_name)->introspectionId();\n<branch>\nhumble\nros2\nmain\nmtc_tutorial\nmoveit_task_constructor_core\nsrc/mtc_node\nmtc_node.cpp\nrclcpp/rclcpp.hpp\nmoveit/planning_scene/planning_scene.h\nmoveit/planning_scene_interface/planning_scene_interface.h\nmoveit/task_constructor/task.h\nmoveit/task_constructor/solvers.h\nmoveit/task_constructor/stages.h\ntf2_geometry_msgs/tf2_geometry_msgs.hpp\ntf2_eigen/tf2_eigen.hpp\nmoveit::task_constructor\nMTCTaskNode\nobject.primitives[0].dimensions\npose.position.x\npose.position.y\ncreateTask\ntask.init()\ntask.plan(5)\ntask.execute()\ntask.setProperty(property_name,value)\ncurrent_state_ptr\nnullptr\ncurrent_state\nCurrentState\ncurrent_state_ptr\nMoveTo\ncreateTask()\nmain\nmove_group\nros2_control\nstatic_tf\nrobot_state_publisher\nrviz\nmtc_tutorial\npackage\nexecutable\npick_place_demo.launch.py\nCMakeLists.txt\nmap\npanda_link0\nmoveit_task_constructor_visualization\n/solution\nmtc_tutorial\nmtc_demo.launch.py\nmtc_tutorial\nmtc_node.cpp\nConnect\nmove_to_pick\nGroupPlannerVector\nnullptr\nSerialContainer\nexposeTo()\nconfigureInitFrom()\nMoveRelative\nMoveRelative\ncartesian_planner\nVector3Stamped\nCurrentState\nGenerateGraspPose\ncurrent_state_ptr\nPoseStamped\ngeometry_msgs\nComputeIK\ngenerateposeIK\ngenerategrasppose\nComputeIK\nModifyPlanningScene\nallowCollisions\nallowCollisions\ngetLinkModelNamesWithCollisionGeometry\nMoveTo\nopenhand\nclose\nModifyPlanningScene\nattachObject\ncurrent_state_ptr\nMoveRelative\napproach_object\nConnect\ngenerategrasppose\nPoseStamped\ngeometry_msgs\ny=0.5\n\"object\"\nsetPose\nsetMonitoredStage\nattach_object\nComputeIK\nGeneratePlacePose\nMoveTo\nallowCollisions\nfalse\ntrue\ndetachObject\nMoveRelative\napproachobject\nliftobject\nMoveTo\nready\nCurrentState\nPlanningScene\nCurrentState\nGenerator\n1\n1\n0\nMoveTo\n0\n0\nCurrentState\nPlanningScene\nModifyPlanningScene\nallowCollisions(false)\nallowCollisions(true)",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0e4"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/pick_and_place_with_moveit_task_constructor/pick_and_place_with_moveit_task_constructor.html#debugging-information-printed-to-the-terminal",
  "title": "Pick and Place with MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "Pick and Place with MoveIt Task Constructor\n1 Basic Concepts\n2 Getting Started\n3 Trying It Out\n4 Setting up a Project with MoveIt Task Constructor\n5 Running the Demo\n6 Adding Stages\n7 Further Discussion\nThis tutorial will walk you through creating a package that plans a pick and place operation usingMoveIt Task Constructor. MoveIt Task Constructor provides a way to plan for tasks that consist of multiple different subtasks (known as stages). If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nThe fundamental idea of MTC is that complex motion planning problems can be composed into a set of simpler subproblems.\nThe top-level planning problem is specified as aTaskwhile all subproblems are specified byStages.\nStages can be arranged in any arbitrary order and hierarchy only limited by the individual stages types.\nThe order in which stages can be arranged is restricted by the direction in which results are passed.\nThere are three possible stages relating to the result flow: generator, propagator, and connector stages:\nGeneratorscompute their results independently of their neighbor stages and pass them in both directions, backwards and forwards.\nAn example is an IK sampler for geometric poses where approaching and departing motions (neighbor stages) depend on the solution.\nPropagatorsreceive the result of one neighbor stage, solve a subproblem and then propagate their result to the neighbor on the opposite site.\nDepending on the implementation, propagating stages can pass solutions forward, backward or in both directions separately.\nAn example is a stage that computes a Cartesian path based on either a start or a goal state.\nConnectorsdo not propagate any results, but rather attempt to bridge the gap between the resulting states of both neighbors.\nAn example is the computation of a free-motion plan from one given state to another.\nAdditional to the order types, there are different hierarchy types allowing to encapsulate subordinate stages.\nStages without subordinate stages are calledprimitive stages, higher-level stages are calledcontainer stages.\nThere are three container types:\nWrappersencapsulate a single subordinate stage and modify or filter the results.\nFor example, a filter stage that only accepts solutions of its child stage that satisfy a certain constraint can be realized as a wrapper.\nAnother standard use of this type includes the IK wrapper stage, which generates inverse kinematics solutions based on planning scenes annotated with a pose target property.\nSerial Containershold a sequence of subordinate stages and only consider end-to-end solutions as results.\nAn example is a picking motion that consists of a sequence of coherent steps.\nParallel Containerscombine set of subordinate stages and can be used for passing the best of alternative results, running fallback solvers or for merging multiple independent solutions.\nExamples are running alternative planners for a free-motion plan, picking objects with the right hand or with the left hand as a fallback, or moving the arm and opening the gripper at the same time.\nStages not only support solving motion planning problems.\nThey can also be used for all kinds of state transitions, as for instance modifying the planning scene.\nCombined with the possibility of using class inheritance it is possible to construct very complex behavior while only relying on a well-structured set of primitive stages.\nMore detailed information on MTC can be found in theMoveIt Task Constructor concepts page\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nMove into your colcon workspace and pull the MoveIt Task Constructor source, where<branch>can be e.g.humblefor ROS Humble, orros2for the latest version compatible with MoveIt 2main:\nInstall missing packages with rosdep:\nBuild the workspace:\nThe MoveIt Task Constructor package contains several basic examples and a pick-and-place demo.\nFor all demos you should launch the basic environment:\nSubsequently, you can run the individual demos:\nOn the right side you should see theMotion Planning Taskspanel outlining the hierarchical stage structure of the tasks.\nWhen you select a particular stage, the list of successful and failed solutions will be\nshown in the right-most window. Selecting one of those solutions will start its visualization.\nThis section walks through the steps required to build a simple task with MoveIt Task Constructor.\nCreate a new package with the following command:\nThis will create a new package and folder calledmtc_tutorialwith a dependency onmoveit_task_constructor_coreas well as a hello world example insrc/mtc_node.\nOpenmtc_node.cppin your editor of choice, and paste in the following code.\nThe top of the code includes the ROS and MoveIt Libraries that this package uses.\nrclcpp/rclcpp.hppincludes core ROS2 functionality\nmoveit/planning_scene/planning_scene.handmoveit/planning_scene_interface/planning_scene_interface.hinclude functionality to interface with the robot model and collision objects\nmoveit/task_constructor/task.h,moveit/task_constructor/solvers.h, andmoveit/task_constructor/stages.hinclude different components of MoveIt Task Constructor that are used in the example\ntf2_geometry_msgs/tf2_geometry_msgs.hppandtf2_eigen/tf2_eigen.hppwon’t be used in this initial example, but they will be used for pose generation when we add more stages to the MoveIt Task Constructor task.\nThe next line gets a logger for our new node. We also create a namespace alias formoveit::task_constructorfor convenience.\nWe start by defining a class that will contain the main MoveIt Task Constructor functionality.  We also declare the MoveIt Task Constructor task object as a member variable for our class: this isn’t strictly necessary for a given application, but it helps save the task for later visualization purposes. We will explore each function individually below.\nThese lines initialize the node with specified options (it is the constructor of ourMTCTaskNodeclass).\nThese next lines define a getter function to get the node base interface, which will be used for the executor later.\nThis class method is used to set up the planning scene that is used in the example. It creates a cylinder with dimensions specified byobject.primitives[0].dimensionsand position specified bypose.position.xandpose.position.y.\nYou can try changing these numbers to resize and move the cylinder around. If we move the cylinder out of the robot’s reach, planning will fail.\nThis function interfaces with the MoveIt Task Constructor task object. It first creates a task, which includes setting some properties and adding stages. This will be discussed further in thecreateTaskfunction definition. Next,task.init()initializes the task andtask.plan(5)generates a plan, stopping after 5 successful plans are found. The next line publishes the solution to be visualized in RViz - this line can be removed if you don’t care for visualization. Finally,task.execute()executes the plan. Execution occurs via an action server interface with the RViz plugin.\nAs mentioned above, this function creates a MoveIt Task Constructor object and sets some initial properties. In this case, we set the task name to “demo_task”, load the robot model, define the names of some useful frames, and set those frame names as properties of the task withtask.setProperty(property_name,value). The next few code blocks will fill out this function body.\nNow, we add an example stage to the node. The first line setscurrent_state_ptrtonullptr; this creates a pointer to a stage such that we can reuse stage information in specific scenarios. This line is not used at this moment, but will be used later when more stages are added to the task. Next, we make acurrent_statestage (a generator stage) and add it to our task - this starts the robot off in its current state. Now that we’ve created theCurrentStatestage, we save a pointer to it in thecurrent_state_ptrfor later use.\nSolvers are used to define the type of robot motion. MoveIt Task Constructor has three options for solvers:\nPipelinePlanneruses MoveIt’s planning pipeline, which typically defaults toOMPL.\nJointInterpolationis a simple planner that interpolates between the start and goal joint states. It is typically used for simple motions as it computes quickly but doesn’t support complex motions.\nCartesianPathis used to move the end effector in a straight line in Cartesian space.\nFeel free to try out the different solvers and see how the robot motion changes. For the first stage we will use the Cartesian planner, which requires the following properties to be set:\nNow that we added in the planners, we can add a stage that will move the robot.\nThe following lines use aMoveTostage (a propagator stage). Since opening the hand is a relatively simple movement, we can use the joint interpolation planner.\nThis stage plans a move to the “open hand” pose, which is a named pose defined in theSRDFfor the Panda robot.\nWe return the task and finish with thecreateTask()function.\nFinally, we havemain: the following lines create a node using the class defined above, and calls the class methods to set up and execute a basic MTC task. In this example, we do not cancel the executor once the task has finished executing to keep the node alive to inspect the solutions in RViz.\nWe will need a launch file to launch themove_group,ros2_control,static_tf,robot_state_publisher, andrviznodes that provide us the environment to run the demo. The one we will use for this example can be foundhere.\nTo run the MoveIt Task Constructor node, we will use a second launch file to start themtc_tutorialexecutable with the proper parameters. Here we can load URDF, SRDF, and OMPL parameters, or use MoveIt Configs Utils to do so. Your launch file should look something like the one found in this tutorial packagehere(pay close attention to thepackageandexecutablearguments below as they are different from the launch file linked) :\nSave a launch file aspick_place_demo.launch.pyor download one to the package’s launch directory. Make sure to edit theCMakeLists.txtso it includes the launch folder by adding the following lines:\nNow we can build and source the colcon workspace.\nStart by launching the first launch file. If you want to use the one provided by the tutorials:\nRViz will now load. If you’re using your own launch file and haven’t included an rviz configsuch as this, you will need to configure RViz before you see anything displayed. If you’re using the launch file from the tutorials package, RViz will already be configured for you and you can jump to the end of the next section.\nIf you are not using the RViz configuration provided, we’ll have to make some changes to the RViz configuration to see your robot and the MoveIt Task Constructor solutions. First, start RViz. The following steps will cover how to set up RViz for MoveIt Task Constructor solution visualization.\nIf theMotionPlanningdisplay is active, uncheck it to hide it for now.\nUnderGlobal Options, change theFixed Framefrommaptopanda_link0if not already done.\nOn the bottom left of the window, click theAddbutton.\nUndermoveit_task_constructor_visualizationselectMotion Planning Tasksand click OK. TheMotion Planning Tasksdisplay should appear on the bottom left.\nIn theDisplays, underMotion Planning Tasks,  changeTask Solution Topicto/solution\nYou should see the panda arm in the main view with Motion Planning Tasks display open in the bottom left and nothing in it. Your MTC task will show up in this panel once you launch themtc_tutorialnode. If you’re usingmtc_demo.launch.pyfrom the tutorials, jump back in here.\nLaunch themtc_tutorialnode with\nYou should see the arm execute the task with the single stage to open the hand, with the cylinder in green in front of it. It should look something like this:\nIf you haven’t made your own package, but still want to see what this looks like, you can launch this file from the tutorials:\nSo far, we’ve walked through creating and executing a simple task, which runs but does not do much. Now, we will start adding the pick-and-place stages to the task. The image below shows an outline of the stages we will use in our task.\nWe will start adding stages after our existing open hand stage. Openmtc_node.cppand locate the following lines:\nWe need to move the arm to a position where we can pick up our object. This is done with aConnectstage, which as its name implies, is a Connector stage. This means that it tries to bridge between the results of the stage before and after it. This stage is initialized with a name,move_to_pick, and aGroupPlannerVectorthat specifies the planning group and the planner. We then set a timeout for the stage, set the properties for the stage, and add it to our task.\nNext, we create a pointer to a MoveIt Task Constructor stage object, and set it tonullptrfor now. Later, we will use this to save a stage.\nThis next block of code creates aSerialContainer.\nThis is a container that can be added to our task and can hold several substages.\nIn this case, we create a serial container that will contain the stages relevant to the picking action.\nInstead of adding the stages to the task, we will add the relevant stages to the serial container. We useexposeTo()to declare the task properties from the parent task in the new serial container, and useconfigureInitFrom()to initialize them.\nThis allows the contained stages to access these properties.\nWe then create a stage to approach the object. This stage is aMoveRelativestage, which allows us to specify a relative movement from our current position.MoveRelativeis a propagator stage: it receives the solution from its neighbouring stages and propagates it to the next or previous stage. Usingcartesian_plannerfinds a solution that involves moving the end effector in a straight line. We set the properties, and set the minimum and maximum distance to move. Now we create aVector3Stampedmessage to indicate the direction we want to move - in this case, in the Z direction from the hand frame. Finally, we add this stage to our serial container\nNow, create a stage to generate the grasp pose.\nThis is a generator stage, so it computes its results without regard to the stages before and after it.\nThe first stage,CurrentStateis a generator stage as well - to connect the first stage and this stage, a connecting stage must be used, which we already created above.\nThis code sets the stage properties, sets the pose before grasping, the angle delta, and the monitored stage.\nAngle delta is a property of theGenerateGraspPosestage that is used to determine the number of poses to generate; when generating solutions, MoveIt Task Constructor will try to grasp the object from many different orientations, with the difference between the orientations specified by the angle delta. The smaller the delta, the closer together the grasp orientations will be. When defining the current stage, we setcurrent_state_ptr, which is now used to forward information about the object pose and shape to the inverse kinematics solver.\nThis stage won’t be directly added to the serial container like previously, as we still need to do inverse kinematics on the poses it generates.\nBefore we compute inverse kinematics for the poses generated above, we first need to define the frame. This can be done with aPoseStampedmessage fromgeometry_msgsor in this case, we define the transform using Eigen transformation matrix and the name of the relevant link. Here, we define the transformation matrix.\nNow, we create theComputeIKstage, and give it the namegenerateposeIKas well as thegenerategraspposestage defined above. Some robots have multiple inverse kinematics solutions for a given pose - we set the limit on the amount of solutions to solve for up to 8. We also set the minimum solution distance, which is a threshold on how different solutions must be: if the joint positions in a solution are too similar to a previous solution, it will be marked as invalid. Next, we configure some additional properties, and add theComputeIKstage to the serial container.\nTo pick up the object, we must allow collision between the hand and the object. This can be done with aModifyPlanningScenestage. TheallowCollisionsfunction lets us specify which collisions to disable.allowCollisionscan be used with a container of names, so we can usegetLinkModelNamesWithCollisionGeometryto get all the names of links with collision geometry in the hand group.\nWith collisions allowed, we now can close the hand. This is done with aMoveTostage, similarly to theopenhandstage from above, except moving to thecloseposition as defined in the SRDF.\nWe now use aModifyPlanningScenestage again, this time to attach the object to the hand usingattachObject. Similarly to what we did with thecurrent_state_ptr, we get a pointer to this stage for later use when generating the place pose for the object.\nNext, we lift the object with aMoveRelativestage, similarly to theapproach_objectstage.\nWith this, we have all the stages needed to pick the object. Now, we add the serial container (with all its substages) to the task. If you build the package as-is, you can see the robot plan to pick up the object.\nTo test out the newly created stage, build the code and execute:\nNow that the stages that define the pick are complete, we move on to defining the stages for placing the object. Picking up where we left off, we add aConnectstage to connect the two, as we will soon be using a generator stage to generate the pose for placing the object.\nWe also create a serial container for the place stages. This is done similarly to the pick serial container.\nThe next stages will be added to the serial container rather than the task.\nThis next stage generates the poses used to place the object and compute the inverse kinematics for those poses - it is somewhat similar to thegenerategraspposestage from the pick serial container.\nWe start by creating a stage to generate the poses and inheriting the task properties.\nWe specify the pose where we want to place the object with aPoseStampedmessage fromgeometry_msgs- in this case, we choosey=0.5in the\"object\"frame.\nWe then pass the target pose to the stage withsetPose.\nNext, we usesetMonitoredStageand pass it the pointer to theattach_objectstage from earlier.\nThis allows the stage to know how the object is attached.\nWe then create aComputeIKstage and pass it ourGeneratePlacePosestage - the rest follows the same logic as above with the pick stages.\nNow that we’re ready to place the object, we open the hand withMoveTostage and the joint interpolation planner.\nWe also can re-enable collisions with the object now that we no longer need to hold it.\nThis is done usingallowCollisionsalmost exactly the same way as disabling collisions, except setting the last argument tofalserather thantrue.\nNow, we can detach the object usingdetachObject.\nWe retreat from the object using aMoveRelativestage, which is done similarly to theapproachobjectandliftobjectstages.\nWe finish our place serial container and add it to the task.\nThe final step is to return home: we use aMoveTostage and pass it the goal pose ofready, which is a pose defined in the Panda SRDF.\nAll these stages should be added above these lines.\nCongratulations! You’ve now defined a pick and place task using MoveIt Task Constructor! To try it out, build the code and execute:\nThe task with each comprising stage is shown in the Motion Planning Tasks pane. Click on a stage and additional information about the stage will show up to the right. The right pane shows different solutions as well as their associated costs. Depending on the stage type and the robot configuration, there may only be one solution shown.\nClick one of the solution costs to see an animation of the robot following the plan for that stage. Click the “Exec” button in the upper-right portion of the pane to execute the motion.\nTo run the complete MoveIt Task Constructor example included with the MoveIt tutorials:\nAnd in a second terminal:\nWhen running MTC, it prints a diagram like this to terminal:\nThis example^ shows two stages. The first stage (“initial_state”) is aCurrentStatetype of stage, which initializes aPlanningSceneand captures any collision objects that are present at that moment.\nA pointer to this stage can be used to retrieve the state of the robot.\nSinceCurrentStateinherits fromGenerator, it propagates solutions both forward and backward.\nThis is denoted by the arrows in both directions.\nThe first1indicates that one solution was successfully propagated backwards to the previous stage.\nThe second1, between the arrows, indicates that one solution was generated.\nThe0indicates that a solution was not propagated forward successfully to the next stage, because the next stage failed.\nThe second stage (“move_to_home”) is aMoveTotype of stage. It inherits its propagation direction from the previous stage, so both arrows point forward. The0’s indicate that this stage failed completely. From left to right, the0’s mean:\nThe stage did not receive a solution from the previous stage\nThe stage did not generate a solution\nThe stage did not propagate a solution forward to the next stage\nIn this case, we could tell that “move_to_home” was the root cause of the failure. The problem was a home state that was in collision. Defining a new, collision-free home position fixed the issue.\nInformation about individual stages can be retrieved from the task. For example, here we retrieve the unique ID for a stage:\nACurrentStatetype stage does not just retrieve the current state of the robot.\nIt also initializes aPlanningSceneobject, capturing any collision objects that are present at that moment.\nMTC stages can be propagated in forward and backward order.\nYou can easily check which direction a stage propagates by the arrow in the RViz GUI.\nWhen propagating backwards, the logic of many operations is reversed.\nFor example, to allow collisions with an object in aModifyPlanningScenestage, you would callallowCollisions(false)rather thanallowCollisions(true). There is a discussion to be readhere.\n© Copyright 2024, PickNik Robotics.\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit_task_constructor.git\nrosdep install --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncd~/ws_moveitcolconbuild--mixinrelease\nros2launchmoveit_task_constructor_demodemo.launch.py\nros2launchmoveit_task_constructor_demorun.launch.pyexe:=cartesianros2launchmoveit_task_constructor_demorun.launch.pyexe:=modularros2launchmoveit_task_constructor_demorun.launch.pyexe:=pick_place_demo\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_task_constructor_corerclcpp\\--node-namemtc_nodemtc_tutorial\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endifstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;classMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};MTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}rclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}voidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;pose.orientation.w=1.0;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}voidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}mtc::TaskMTCTaskNode::createTask(){mtc::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);// Disable warnings for this line, as it's a variable that's set but not used in this example#pragma GCC diagnostic push#pragma GCC diagnostic ignored \"-Wunused-but-set-variable\"mtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generator#pragma GCC diagnostic popautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));autosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);autointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();autocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);autostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}intmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endif\nstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;\nclassMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};\nMTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}\nrclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}\nvoidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}\nvoidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}\nmtc::TaskMTCTaskNode::createTask(){moveit::task_constructor::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);\nmtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generatorautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));\nautosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);\nautointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}\nintmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\nfromlaunchimportLaunchDescriptionfromlaunch_ros.actionsimportNodefrommoveit_configs_utilsimportMoveItConfigsBuilderdefgenerate_launch_description():moveit_config=MoveItConfigsBuilder(\"moveit_resources_panda\").to_dict()# MTC Demo nodepick_place_demo=Node(package=\"mtc_tutorial\",executable=\"mtc_node\",output=\"screen\",parameters=[moveit_config,],)returnLaunchDescription([pick_place_demo])\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}\n  )\ncd~/ws_moveitcolconbuild--mixinreleasesource~/ws_moveit/install/setup.bash\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo_minimal.launch.py\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));// Add the next lines of codes to define more stages here\nautostage_move_to_pick=std::make_unique<mtc::stages::Connect>(\"move to pick\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner}});stage_move_to_pick->setTimeout(5.0);stage_move_to_pick->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_pick));\nmtc::Stage*attach_object_stage=nullptr;// Forward attach_object_stage to place pose generator\n{autograsp=std::make_unique<mtc::SerialContainer>(\"pick object\");task.properties().exposeTo(grasp->properties(),{\"eef\",\"group\",\"ik_frame\"});grasp->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"approach object\",cartesian_planner);stage->properties().set(\"marker_ns\",\"approach_object\");stage->properties().set(\"link\",hand_frame);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.15);// Set hand forward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=hand_frame;vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\n{// Sample grasp poseautostage=std::make_unique<mtc::stages::GenerateGraspPose>(\"generate grasp pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"grasp_pose\");stage->setPreGraspPose(\"open\");stage->setObject(\"object\");stage->setAngleDelta(M_PI/12);stage->setMonitoredStage(current_state_ptr);// Hook into current state\nEigen::Isometry3dgrasp_frame_transform;Eigen::Quaterniondq=Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitX())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitY())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitZ());grasp_frame_transform.linear()=q.matrix();grasp_frame_transform.translation().z()=0.1;\n// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"grasp pose IK\",std::move(stage));wrapper->setMaxIKSolutions(8);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(grasp_frame_transform,hand_frame);wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});grasp->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"allow collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),true);grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"close hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"close\");grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"attach object\");stage->attachObject(\"object\",hand_frame);attach_object_stage=stage.get();grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"lift object\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"lift_object\");// Set upward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\ntask.add(std::move(grasp));}\nros2launchmtc_tutorialpick_place_demo.launch.py\n{autostage_move_to_place=std::make_unique<mtc::stages::Connect>(\"move to place\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner},{hand_group_name,interpolation_planner}});stage_move_to_place->setTimeout(5.0);stage_move_to_place->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_place));}\n{autoplace=std::make_unique<mtc::SerialContainer>(\"place object\");task.properties().exposeTo(place->properties(),{\"eef\",\"group\",\"ik_frame\"});place->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{// Sample place poseautostage=std::make_unique<mtc::stages::GeneratePlacePose>(\"generate place pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"place_pose\");stage->setObject(\"object\");geometry_msgs::msg::PoseStampedtarget_pose_msg;target_pose_msg.header.frame_id=\"object\";target_pose_msg.pose.position.y=0.5;target_pose_msg.pose.orientation.w=1.0;stage->setPose(target_pose_msg);stage->setMonitoredStage(attach_object_stage);// Hook into attach_object_stage// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"place pose IK\",std::move(stage));wrapper->setMaxIKSolutions(2);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(\"object\");wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});place->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"open\");place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"forbid collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),false);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"detach object\");stage->detachObject(\"object\",hand_frame);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"retreat\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"retreat\");// Set retreat directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.x=-0.5;stage->setDirection(vec);place->insert(std::move(stage));}\ntask.add(std::move(place));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"return home\",interpolation_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setGoal(\"ready\");task.add(std::move(stage));}\n// Stages all added to the task above this linereturntask;}\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmoveit2_tutorialspick_place_demo.launch.py\n[demo_node-1]1-←1→-0/initial_state[demo_node-1]-0→0→-0/move_to_home\nuint32_tconstunique_stage_id=task_.stages()->findChild(stage_name)->introspectionId();\n<branch>\nhumble\nros2\nmain\nmtc_tutorial\nmoveit_task_constructor_core\nsrc/mtc_node\nmtc_node.cpp\nrclcpp/rclcpp.hpp\nmoveit/planning_scene/planning_scene.h\nmoveit/planning_scene_interface/planning_scene_interface.h\nmoveit/task_constructor/task.h\nmoveit/task_constructor/solvers.h\nmoveit/task_constructor/stages.h\ntf2_geometry_msgs/tf2_geometry_msgs.hpp\ntf2_eigen/tf2_eigen.hpp\nmoveit::task_constructor\nMTCTaskNode\nobject.primitives[0].dimensions\npose.position.x\npose.position.y\ncreateTask\ntask.init()\ntask.plan(5)\ntask.execute()\ntask.setProperty(property_name,value)\ncurrent_state_ptr\nnullptr\ncurrent_state\nCurrentState\ncurrent_state_ptr\nMoveTo\ncreateTask()\nmain\nmove_group\nros2_control\nstatic_tf\nrobot_state_publisher\nrviz\nmtc_tutorial\npackage\nexecutable\npick_place_demo.launch.py\nCMakeLists.txt\nmap\npanda_link0\nmoveit_task_constructor_visualization\n/solution\nmtc_tutorial\nmtc_demo.launch.py\nmtc_tutorial\nmtc_node.cpp\nConnect\nmove_to_pick\nGroupPlannerVector\nnullptr\nSerialContainer\nexposeTo()\nconfigureInitFrom()\nMoveRelative\nMoveRelative\ncartesian_planner\nVector3Stamped\nCurrentState\nGenerateGraspPose\ncurrent_state_ptr\nPoseStamped\ngeometry_msgs\nComputeIK\ngenerateposeIK\ngenerategrasppose\nComputeIK\nModifyPlanningScene\nallowCollisions\nallowCollisions\ngetLinkModelNamesWithCollisionGeometry\nMoveTo\nopenhand\nclose\nModifyPlanningScene\nattachObject\ncurrent_state_ptr\nMoveRelative\napproach_object\nConnect\ngenerategrasppose\nPoseStamped\ngeometry_msgs\ny=0.5\n\"object\"\nsetPose\nsetMonitoredStage\nattach_object\nComputeIK\nGeneratePlacePose\nMoveTo\nallowCollisions\nfalse\ntrue\ndetachObject\nMoveRelative\napproachobject\nliftobject\nMoveTo\nready\nCurrentState\nPlanningScene\nCurrentState\nGenerator\n1\n1\n0\nMoveTo\n0\n0\nCurrentState\nPlanningScene\nModifyPlanningScene\nallowCollisions(false)\nallowCollisions(true)",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0e5"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/pick_and_place_with_moveit_task_constructor/pick_and_place_with_moveit_task_constructor.html#stages",
  "title": "Pick and Place with MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "Pick and Place with MoveIt Task Constructor\n1 Basic Concepts\n2 Getting Started\n3 Trying It Out\n4 Setting up a Project with MoveIt Task Constructor\n5 Running the Demo\n6 Adding Stages\n7 Further Discussion\nThis tutorial will walk you through creating a package that plans a pick and place operation usingMoveIt Task Constructor. MoveIt Task Constructor provides a way to plan for tasks that consist of multiple different subtasks (known as stages). If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nThe fundamental idea of MTC is that complex motion planning problems can be composed into a set of simpler subproblems.\nThe top-level planning problem is specified as aTaskwhile all subproblems are specified byStages.\nStages can be arranged in any arbitrary order and hierarchy only limited by the individual stages types.\nThe order in which stages can be arranged is restricted by the direction in which results are passed.\nThere are three possible stages relating to the result flow: generator, propagator, and connector stages:\nGeneratorscompute their results independently of their neighbor stages and pass them in both directions, backwards and forwards.\nAn example is an IK sampler for geometric poses where approaching and departing motions (neighbor stages) depend on the solution.\nPropagatorsreceive the result of one neighbor stage, solve a subproblem and then propagate their result to the neighbor on the opposite site.\nDepending on the implementation, propagating stages can pass solutions forward, backward or in both directions separately.\nAn example is a stage that computes a Cartesian path based on either a start or a goal state.\nConnectorsdo not propagate any results, but rather attempt to bridge the gap between the resulting states of both neighbors.\nAn example is the computation of a free-motion plan from one given state to another.\nAdditional to the order types, there are different hierarchy types allowing to encapsulate subordinate stages.\nStages without subordinate stages are calledprimitive stages, higher-level stages are calledcontainer stages.\nThere are three container types:\nWrappersencapsulate a single subordinate stage and modify or filter the results.\nFor example, a filter stage that only accepts solutions of its child stage that satisfy a certain constraint can be realized as a wrapper.\nAnother standard use of this type includes the IK wrapper stage, which generates inverse kinematics solutions based on planning scenes annotated with a pose target property.\nSerial Containershold a sequence of subordinate stages and only consider end-to-end solutions as results.\nAn example is a picking motion that consists of a sequence of coherent steps.\nParallel Containerscombine set of subordinate stages and can be used for passing the best of alternative results, running fallback solvers or for merging multiple independent solutions.\nExamples are running alternative planners for a free-motion plan, picking objects with the right hand or with the left hand as a fallback, or moving the arm and opening the gripper at the same time.\nStages not only support solving motion planning problems.\nThey can also be used for all kinds of state transitions, as for instance modifying the planning scene.\nCombined with the possibility of using class inheritance it is possible to construct very complex behavior while only relying on a well-structured set of primitive stages.\nMore detailed information on MTC can be found in theMoveIt Task Constructor concepts page\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nMove into your colcon workspace and pull the MoveIt Task Constructor source, where<branch>can be e.g.humblefor ROS Humble, orros2for the latest version compatible with MoveIt 2main:\nInstall missing packages with rosdep:\nBuild the workspace:\nThe MoveIt Task Constructor package contains several basic examples and a pick-and-place demo.\nFor all demos you should launch the basic environment:\nSubsequently, you can run the individual demos:\nOn the right side you should see theMotion Planning Taskspanel outlining the hierarchical stage structure of the tasks.\nWhen you select a particular stage, the list of successful and failed solutions will be\nshown in the right-most window. Selecting one of those solutions will start its visualization.\nThis section walks through the steps required to build a simple task with MoveIt Task Constructor.\nCreate a new package with the following command:\nThis will create a new package and folder calledmtc_tutorialwith a dependency onmoveit_task_constructor_coreas well as a hello world example insrc/mtc_node.\nOpenmtc_node.cppin your editor of choice, and paste in the following code.\nThe top of the code includes the ROS and MoveIt Libraries that this package uses.\nrclcpp/rclcpp.hppincludes core ROS2 functionality\nmoveit/planning_scene/planning_scene.handmoveit/planning_scene_interface/planning_scene_interface.hinclude functionality to interface with the robot model and collision objects\nmoveit/task_constructor/task.h,moveit/task_constructor/solvers.h, andmoveit/task_constructor/stages.hinclude different components of MoveIt Task Constructor that are used in the example\ntf2_geometry_msgs/tf2_geometry_msgs.hppandtf2_eigen/tf2_eigen.hppwon’t be used in this initial example, but they will be used for pose generation when we add more stages to the MoveIt Task Constructor task.\nThe next line gets a logger for our new node. We also create a namespace alias formoveit::task_constructorfor convenience.\nWe start by defining a class that will contain the main MoveIt Task Constructor functionality.  We also declare the MoveIt Task Constructor task object as a member variable for our class: this isn’t strictly necessary for a given application, but it helps save the task for later visualization purposes. We will explore each function individually below.\nThese lines initialize the node with specified options (it is the constructor of ourMTCTaskNodeclass).\nThese next lines define a getter function to get the node base interface, which will be used for the executor later.\nThis class method is used to set up the planning scene that is used in the example. It creates a cylinder with dimensions specified byobject.primitives[0].dimensionsand position specified bypose.position.xandpose.position.y.\nYou can try changing these numbers to resize and move the cylinder around. If we move the cylinder out of the robot’s reach, planning will fail.\nThis function interfaces with the MoveIt Task Constructor task object. It first creates a task, which includes setting some properties and adding stages. This will be discussed further in thecreateTaskfunction definition. Next,task.init()initializes the task andtask.plan(5)generates a plan, stopping after 5 successful plans are found. The next line publishes the solution to be visualized in RViz - this line can be removed if you don’t care for visualization. Finally,task.execute()executes the plan. Execution occurs via an action server interface with the RViz plugin.\nAs mentioned above, this function creates a MoveIt Task Constructor object and sets some initial properties. In this case, we set the task name to “demo_task”, load the robot model, define the names of some useful frames, and set those frame names as properties of the task withtask.setProperty(property_name,value). The next few code blocks will fill out this function body.\nNow, we add an example stage to the node. The first line setscurrent_state_ptrtonullptr; this creates a pointer to a stage such that we can reuse stage information in specific scenarios. This line is not used at this moment, but will be used later when more stages are added to the task. Next, we make acurrent_statestage (a generator stage) and add it to our task - this starts the robot off in its current state. Now that we’ve created theCurrentStatestage, we save a pointer to it in thecurrent_state_ptrfor later use.\nSolvers are used to define the type of robot motion. MoveIt Task Constructor has three options for solvers:\nPipelinePlanneruses MoveIt’s planning pipeline, which typically defaults toOMPL.\nJointInterpolationis a simple planner that interpolates between the start and goal joint states. It is typically used for simple motions as it computes quickly but doesn’t support complex motions.\nCartesianPathis used to move the end effector in a straight line in Cartesian space.\nFeel free to try out the different solvers and see how the robot motion changes. For the first stage we will use the Cartesian planner, which requires the following properties to be set:\nNow that we added in the planners, we can add a stage that will move the robot.\nThe following lines use aMoveTostage (a propagator stage). Since opening the hand is a relatively simple movement, we can use the joint interpolation planner.\nThis stage plans a move to the “open hand” pose, which is a named pose defined in theSRDFfor the Panda robot.\nWe return the task and finish with thecreateTask()function.\nFinally, we havemain: the following lines create a node using the class defined above, and calls the class methods to set up and execute a basic MTC task. In this example, we do not cancel the executor once the task has finished executing to keep the node alive to inspect the solutions in RViz.\nWe will need a launch file to launch themove_group,ros2_control,static_tf,robot_state_publisher, andrviznodes that provide us the environment to run the demo. The one we will use for this example can be foundhere.\nTo run the MoveIt Task Constructor node, we will use a second launch file to start themtc_tutorialexecutable with the proper parameters. Here we can load URDF, SRDF, and OMPL parameters, or use MoveIt Configs Utils to do so. Your launch file should look something like the one found in this tutorial packagehere(pay close attention to thepackageandexecutablearguments below as they are different from the launch file linked) :\nSave a launch file aspick_place_demo.launch.pyor download one to the package’s launch directory. Make sure to edit theCMakeLists.txtso it includes the launch folder by adding the following lines:\nNow we can build and source the colcon workspace.\nStart by launching the first launch file. If you want to use the one provided by the tutorials:\nRViz will now load. If you’re using your own launch file and haven’t included an rviz configsuch as this, you will need to configure RViz before you see anything displayed. If you’re using the launch file from the tutorials package, RViz will already be configured for you and you can jump to the end of the next section.\nIf you are not using the RViz configuration provided, we’ll have to make some changes to the RViz configuration to see your robot and the MoveIt Task Constructor solutions. First, start RViz. The following steps will cover how to set up RViz for MoveIt Task Constructor solution visualization.\nIf theMotionPlanningdisplay is active, uncheck it to hide it for now.\nUnderGlobal Options, change theFixed Framefrommaptopanda_link0if not already done.\nOn the bottom left of the window, click theAddbutton.\nUndermoveit_task_constructor_visualizationselectMotion Planning Tasksand click OK. TheMotion Planning Tasksdisplay should appear on the bottom left.\nIn theDisplays, underMotion Planning Tasks,  changeTask Solution Topicto/solution\nYou should see the panda arm in the main view with Motion Planning Tasks display open in the bottom left and nothing in it. Your MTC task will show up in this panel once you launch themtc_tutorialnode. If you’re usingmtc_demo.launch.pyfrom the tutorials, jump back in here.\nLaunch themtc_tutorialnode with\nYou should see the arm execute the task with the single stage to open the hand, with the cylinder in green in front of it. It should look something like this:\nIf you haven’t made your own package, but still want to see what this looks like, you can launch this file from the tutorials:\nSo far, we’ve walked through creating and executing a simple task, which runs but does not do much. Now, we will start adding the pick-and-place stages to the task. The image below shows an outline of the stages we will use in our task.\nWe will start adding stages after our existing open hand stage. Openmtc_node.cppand locate the following lines:\nWe need to move the arm to a position where we can pick up our object. This is done with aConnectstage, which as its name implies, is a Connector stage. This means that it tries to bridge between the results of the stage before and after it. This stage is initialized with a name,move_to_pick, and aGroupPlannerVectorthat specifies the planning group and the planner. We then set a timeout for the stage, set the properties for the stage, and add it to our task.\nNext, we create a pointer to a MoveIt Task Constructor stage object, and set it tonullptrfor now. Later, we will use this to save a stage.\nThis next block of code creates aSerialContainer.\nThis is a container that can be added to our task and can hold several substages.\nIn this case, we create a serial container that will contain the stages relevant to the picking action.\nInstead of adding the stages to the task, we will add the relevant stages to the serial container. We useexposeTo()to declare the task properties from the parent task in the new serial container, and useconfigureInitFrom()to initialize them.\nThis allows the contained stages to access these properties.\nWe then create a stage to approach the object. This stage is aMoveRelativestage, which allows us to specify a relative movement from our current position.MoveRelativeis a propagator stage: it receives the solution from its neighbouring stages and propagates it to the next or previous stage. Usingcartesian_plannerfinds a solution that involves moving the end effector in a straight line. We set the properties, and set the minimum and maximum distance to move. Now we create aVector3Stampedmessage to indicate the direction we want to move - in this case, in the Z direction from the hand frame. Finally, we add this stage to our serial container\nNow, create a stage to generate the grasp pose.\nThis is a generator stage, so it computes its results without regard to the stages before and after it.\nThe first stage,CurrentStateis a generator stage as well - to connect the first stage and this stage, a connecting stage must be used, which we already created above.\nThis code sets the stage properties, sets the pose before grasping, the angle delta, and the monitored stage.\nAngle delta is a property of theGenerateGraspPosestage that is used to determine the number of poses to generate; when generating solutions, MoveIt Task Constructor will try to grasp the object from many different orientations, with the difference between the orientations specified by the angle delta. The smaller the delta, the closer together the grasp orientations will be. When defining the current stage, we setcurrent_state_ptr, which is now used to forward information about the object pose and shape to the inverse kinematics solver.\nThis stage won’t be directly added to the serial container like previously, as we still need to do inverse kinematics on the poses it generates.\nBefore we compute inverse kinematics for the poses generated above, we first need to define the frame. This can be done with aPoseStampedmessage fromgeometry_msgsor in this case, we define the transform using Eigen transformation matrix and the name of the relevant link. Here, we define the transformation matrix.\nNow, we create theComputeIKstage, and give it the namegenerateposeIKas well as thegenerategraspposestage defined above. Some robots have multiple inverse kinematics solutions for a given pose - we set the limit on the amount of solutions to solve for up to 8. We also set the minimum solution distance, which is a threshold on how different solutions must be: if the joint positions in a solution are too similar to a previous solution, it will be marked as invalid. Next, we configure some additional properties, and add theComputeIKstage to the serial container.\nTo pick up the object, we must allow collision between the hand and the object. This can be done with aModifyPlanningScenestage. TheallowCollisionsfunction lets us specify which collisions to disable.allowCollisionscan be used with a container of names, so we can usegetLinkModelNamesWithCollisionGeometryto get all the names of links with collision geometry in the hand group.\nWith collisions allowed, we now can close the hand. This is done with aMoveTostage, similarly to theopenhandstage from above, except moving to thecloseposition as defined in the SRDF.\nWe now use aModifyPlanningScenestage again, this time to attach the object to the hand usingattachObject. Similarly to what we did with thecurrent_state_ptr, we get a pointer to this stage for later use when generating the place pose for the object.\nNext, we lift the object with aMoveRelativestage, similarly to theapproach_objectstage.\nWith this, we have all the stages needed to pick the object. Now, we add the serial container (with all its substages) to the task. If you build the package as-is, you can see the robot plan to pick up the object.\nTo test out the newly created stage, build the code and execute:\nNow that the stages that define the pick are complete, we move on to defining the stages for placing the object. Picking up where we left off, we add aConnectstage to connect the two, as we will soon be using a generator stage to generate the pose for placing the object.\nWe also create a serial container for the place stages. This is done similarly to the pick serial container.\nThe next stages will be added to the serial container rather than the task.\nThis next stage generates the poses used to place the object and compute the inverse kinematics for those poses - it is somewhat similar to thegenerategraspposestage from the pick serial container.\nWe start by creating a stage to generate the poses and inheriting the task properties.\nWe specify the pose where we want to place the object with aPoseStampedmessage fromgeometry_msgs- in this case, we choosey=0.5in the\"object\"frame.\nWe then pass the target pose to the stage withsetPose.\nNext, we usesetMonitoredStageand pass it the pointer to theattach_objectstage from earlier.\nThis allows the stage to know how the object is attached.\nWe then create aComputeIKstage and pass it ourGeneratePlacePosestage - the rest follows the same logic as above with the pick stages.\nNow that we’re ready to place the object, we open the hand withMoveTostage and the joint interpolation planner.\nWe also can re-enable collisions with the object now that we no longer need to hold it.\nThis is done usingallowCollisionsalmost exactly the same way as disabling collisions, except setting the last argument tofalserather thantrue.\nNow, we can detach the object usingdetachObject.\nWe retreat from the object using aMoveRelativestage, which is done similarly to theapproachobjectandliftobjectstages.\nWe finish our place serial container and add it to the task.\nThe final step is to return home: we use aMoveTostage and pass it the goal pose ofready, which is a pose defined in the Panda SRDF.\nAll these stages should be added above these lines.\nCongratulations! You’ve now defined a pick and place task using MoveIt Task Constructor! To try it out, build the code and execute:\nThe task with each comprising stage is shown in the Motion Planning Tasks pane. Click on a stage and additional information about the stage will show up to the right. The right pane shows different solutions as well as their associated costs. Depending on the stage type and the robot configuration, there may only be one solution shown.\nClick one of the solution costs to see an animation of the robot following the plan for that stage. Click the “Exec” button in the upper-right portion of the pane to execute the motion.\nTo run the complete MoveIt Task Constructor example included with the MoveIt tutorials:\nAnd in a second terminal:\nWhen running MTC, it prints a diagram like this to terminal:\nThis example^ shows two stages. The first stage (“initial_state”) is aCurrentStatetype of stage, which initializes aPlanningSceneand captures any collision objects that are present at that moment.\nA pointer to this stage can be used to retrieve the state of the robot.\nSinceCurrentStateinherits fromGenerator, it propagates solutions both forward and backward.\nThis is denoted by the arrows in both directions.\nThe first1indicates that one solution was successfully propagated backwards to the previous stage.\nThe second1, between the arrows, indicates that one solution was generated.\nThe0indicates that a solution was not propagated forward successfully to the next stage, because the next stage failed.\nThe second stage (“move_to_home”) is aMoveTotype of stage. It inherits its propagation direction from the previous stage, so both arrows point forward. The0’s indicate that this stage failed completely. From left to right, the0’s mean:\nThe stage did not receive a solution from the previous stage\nThe stage did not generate a solution\nThe stage did not propagate a solution forward to the next stage\nIn this case, we could tell that “move_to_home” was the root cause of the failure. The problem was a home state that was in collision. Defining a new, collision-free home position fixed the issue.\nInformation about individual stages can be retrieved from the task. For example, here we retrieve the unique ID for a stage:\nACurrentStatetype stage does not just retrieve the current state of the robot.\nIt also initializes aPlanningSceneobject, capturing any collision objects that are present at that moment.\nMTC stages can be propagated in forward and backward order.\nYou can easily check which direction a stage propagates by the arrow in the RViz GUI.\nWhen propagating backwards, the logic of many operations is reversed.\nFor example, to allow collisions with an object in aModifyPlanningScenestage, you would callallowCollisions(false)rather thanallowCollisions(true). There is a discussion to be readhere.\n© Copyright 2024, PickNik Robotics.\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit_task_constructor.git\nrosdep install --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncd~/ws_moveitcolconbuild--mixinrelease\nros2launchmoveit_task_constructor_demodemo.launch.py\nros2launchmoveit_task_constructor_demorun.launch.pyexe:=cartesianros2launchmoveit_task_constructor_demorun.launch.pyexe:=modularros2launchmoveit_task_constructor_demorun.launch.pyexe:=pick_place_demo\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_task_constructor_corerclcpp\\--node-namemtc_nodemtc_tutorial\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endifstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;classMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};MTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}rclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}voidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;pose.orientation.w=1.0;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}voidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}mtc::TaskMTCTaskNode::createTask(){mtc::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);// Disable warnings for this line, as it's a variable that's set but not used in this example#pragma GCC diagnostic push#pragma GCC diagnostic ignored \"-Wunused-but-set-variable\"mtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generator#pragma GCC diagnostic popautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));autosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);autointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();autocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);autostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}intmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endif\nstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;\nclassMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};\nMTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}\nrclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}\nvoidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}\nvoidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}\nmtc::TaskMTCTaskNode::createTask(){moveit::task_constructor::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);\nmtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generatorautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));\nautosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);\nautointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}\nintmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\nfromlaunchimportLaunchDescriptionfromlaunch_ros.actionsimportNodefrommoveit_configs_utilsimportMoveItConfigsBuilderdefgenerate_launch_description():moveit_config=MoveItConfigsBuilder(\"moveit_resources_panda\").to_dict()# MTC Demo nodepick_place_demo=Node(package=\"mtc_tutorial\",executable=\"mtc_node\",output=\"screen\",parameters=[moveit_config,],)returnLaunchDescription([pick_place_demo])\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}\n  )\ncd~/ws_moveitcolconbuild--mixinreleasesource~/ws_moveit/install/setup.bash\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo_minimal.launch.py\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));// Add the next lines of codes to define more stages here\nautostage_move_to_pick=std::make_unique<mtc::stages::Connect>(\"move to pick\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner}});stage_move_to_pick->setTimeout(5.0);stage_move_to_pick->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_pick));\nmtc::Stage*attach_object_stage=nullptr;// Forward attach_object_stage to place pose generator\n{autograsp=std::make_unique<mtc::SerialContainer>(\"pick object\");task.properties().exposeTo(grasp->properties(),{\"eef\",\"group\",\"ik_frame\"});grasp->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"approach object\",cartesian_planner);stage->properties().set(\"marker_ns\",\"approach_object\");stage->properties().set(\"link\",hand_frame);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.15);// Set hand forward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=hand_frame;vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\n{// Sample grasp poseautostage=std::make_unique<mtc::stages::GenerateGraspPose>(\"generate grasp pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"grasp_pose\");stage->setPreGraspPose(\"open\");stage->setObject(\"object\");stage->setAngleDelta(M_PI/12);stage->setMonitoredStage(current_state_ptr);// Hook into current state\nEigen::Isometry3dgrasp_frame_transform;Eigen::Quaterniondq=Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitX())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitY())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitZ());grasp_frame_transform.linear()=q.matrix();grasp_frame_transform.translation().z()=0.1;\n// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"grasp pose IK\",std::move(stage));wrapper->setMaxIKSolutions(8);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(grasp_frame_transform,hand_frame);wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});grasp->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"allow collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),true);grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"close hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"close\");grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"attach object\");stage->attachObject(\"object\",hand_frame);attach_object_stage=stage.get();grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"lift object\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"lift_object\");// Set upward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\ntask.add(std::move(grasp));}\nros2launchmtc_tutorialpick_place_demo.launch.py\n{autostage_move_to_place=std::make_unique<mtc::stages::Connect>(\"move to place\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner},{hand_group_name,interpolation_planner}});stage_move_to_place->setTimeout(5.0);stage_move_to_place->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_place));}\n{autoplace=std::make_unique<mtc::SerialContainer>(\"place object\");task.properties().exposeTo(place->properties(),{\"eef\",\"group\",\"ik_frame\"});place->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{// Sample place poseautostage=std::make_unique<mtc::stages::GeneratePlacePose>(\"generate place pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"place_pose\");stage->setObject(\"object\");geometry_msgs::msg::PoseStampedtarget_pose_msg;target_pose_msg.header.frame_id=\"object\";target_pose_msg.pose.position.y=0.5;target_pose_msg.pose.orientation.w=1.0;stage->setPose(target_pose_msg);stage->setMonitoredStage(attach_object_stage);// Hook into attach_object_stage// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"place pose IK\",std::move(stage));wrapper->setMaxIKSolutions(2);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(\"object\");wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});place->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"open\");place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"forbid collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),false);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"detach object\");stage->detachObject(\"object\",hand_frame);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"retreat\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"retreat\");// Set retreat directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.x=-0.5;stage->setDirection(vec);place->insert(std::move(stage));}\ntask.add(std::move(place));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"return home\",interpolation_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setGoal(\"ready\");task.add(std::move(stage));}\n// Stages all added to the task above this linereturntask;}\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmoveit2_tutorialspick_place_demo.launch.py\n[demo_node-1]1-←1→-0/initial_state[demo_node-1]-0→0→-0/move_to_home\nuint32_tconstunique_stage_id=task_.stages()->findChild(stage_name)->introspectionId();\n<branch>\nhumble\nros2\nmain\nmtc_tutorial\nmoveit_task_constructor_core\nsrc/mtc_node\nmtc_node.cpp\nrclcpp/rclcpp.hpp\nmoveit/planning_scene/planning_scene.h\nmoveit/planning_scene_interface/planning_scene_interface.h\nmoveit/task_constructor/task.h\nmoveit/task_constructor/solvers.h\nmoveit/task_constructor/stages.h\ntf2_geometry_msgs/tf2_geometry_msgs.hpp\ntf2_eigen/tf2_eigen.hpp\nmoveit::task_constructor\nMTCTaskNode\nobject.primitives[0].dimensions\npose.position.x\npose.position.y\ncreateTask\ntask.init()\ntask.plan(5)\ntask.execute()\ntask.setProperty(property_name,value)\ncurrent_state_ptr\nnullptr\ncurrent_state\nCurrentState\ncurrent_state_ptr\nMoveTo\ncreateTask()\nmain\nmove_group\nros2_control\nstatic_tf\nrobot_state_publisher\nrviz\nmtc_tutorial\npackage\nexecutable\npick_place_demo.launch.py\nCMakeLists.txt\nmap\npanda_link0\nmoveit_task_constructor_visualization\n/solution\nmtc_tutorial\nmtc_demo.launch.py\nmtc_tutorial\nmtc_node.cpp\nConnect\nmove_to_pick\nGroupPlannerVector\nnullptr\nSerialContainer\nexposeTo()\nconfigureInitFrom()\nMoveRelative\nMoveRelative\ncartesian_planner\nVector3Stamped\nCurrentState\nGenerateGraspPose\ncurrent_state_ptr\nPoseStamped\ngeometry_msgs\nComputeIK\ngenerateposeIK\ngenerategrasppose\nComputeIK\nModifyPlanningScene\nallowCollisions\nallowCollisions\ngetLinkModelNamesWithCollisionGeometry\nMoveTo\nopenhand\nclose\nModifyPlanningScene\nattachObject\ncurrent_state_ptr\nMoveRelative\napproach_object\nConnect\ngenerategrasppose\nPoseStamped\ngeometry_msgs\ny=0.5\n\"object\"\nsetPose\nsetMonitoredStage\nattach_object\nComputeIK\nGeneratePlacePose\nMoveTo\nallowCollisions\nfalse\ntrue\ndetachObject\nMoveRelative\napproachobject\nliftobject\nMoveTo\nready\nCurrentState\nPlanningScene\nCurrentState\nGenerator\n1\n1\n0\nMoveTo\n0\n0\nCurrentState\nPlanningScene\nModifyPlanningScene\nallowCollisions(false)\nallowCollisions(true)",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0e6"
  },
  "url": "https://moveit.picknik.ai/main/doc/tutorials/pick_and_place_with_moveit_task_constructor/pick_and_place_with_moveit_task_constructor.html#pick-and-place-with-moveit-task-constructor",
  "title": "Pick and Place with MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "Pick and Place with MoveIt Task Constructor\n1 Basic Concepts\n2 Getting Started\n3 Trying It Out\n4 Setting up a Project with MoveIt Task Constructor\n5 Running the Demo\n6 Adding Stages\n7 Further Discussion\nThis tutorial will walk you through creating a package that plans a pick and place operation usingMoveIt Task Constructor. MoveIt Task Constructor provides a way to plan for tasks that consist of multiple different subtasks (known as stages). If you just want to run the tutorial, you can follow theDocker Guideto start a container with the completed tutorial.\nThe fundamental idea of MTC is that complex motion planning problems can be composed into a set of simpler subproblems.\nThe top-level planning problem is specified as aTaskwhile all subproblems are specified byStages.\nStages can be arranged in any arbitrary order and hierarchy only limited by the individual stages types.\nThe order in which stages can be arranged is restricted by the direction in which results are passed.\nThere are three possible stages relating to the result flow: generator, propagator, and connector stages:\nGeneratorscompute their results independently of their neighbor stages and pass them in both directions, backwards and forwards.\nAn example is an IK sampler for geometric poses where approaching and departing motions (neighbor stages) depend on the solution.\nPropagatorsreceive the result of one neighbor stage, solve a subproblem and then propagate their result to the neighbor on the opposite site.\nDepending on the implementation, propagating stages can pass solutions forward, backward or in both directions separately.\nAn example is a stage that computes a Cartesian path based on either a start or a goal state.\nConnectorsdo not propagate any results, but rather attempt to bridge the gap between the resulting states of both neighbors.\nAn example is the computation of a free-motion plan from one given state to another.\nAdditional to the order types, there are different hierarchy types allowing to encapsulate subordinate stages.\nStages without subordinate stages are calledprimitive stages, higher-level stages are calledcontainer stages.\nThere are three container types:\nWrappersencapsulate a single subordinate stage and modify or filter the results.\nFor example, a filter stage that only accepts solutions of its child stage that satisfy a certain constraint can be realized as a wrapper.\nAnother standard use of this type includes the IK wrapper stage, which generates inverse kinematics solutions based on planning scenes annotated with a pose target property.\nSerial Containershold a sequence of subordinate stages and only consider end-to-end solutions as results.\nAn example is a picking motion that consists of a sequence of coherent steps.\nParallel Containerscombine set of subordinate stages and can be used for passing the best of alternative results, running fallback solvers or for merging multiple independent solutions.\nExamples are running alternative planners for a free-motion plan, picking objects with the right hand or with the left hand as a fallback, or moving the arm and opening the gripper at the same time.\nStages not only support solving motion planning problems.\nThey can also be used for all kinds of state transitions, as for instance modifying the planning scene.\nCombined with the possibility of using class inheritance it is possible to construct very complex behavior while only relying on a well-structured set of primitive stages.\nMore detailed information on MTC can be found in theMoveIt Task Constructor concepts page\nIf you haven’t already done so, make sure you’ve completed the steps inGetting Started.\nMove into your colcon workspace and pull the MoveIt Task Constructor source, where<branch>can be e.g.humblefor ROS Humble, orros2for the latest version compatible with MoveIt 2main:\nInstall missing packages with rosdep:\nBuild the workspace:\nThe MoveIt Task Constructor package contains several basic examples and a pick-and-place demo.\nFor all demos you should launch the basic environment:\nSubsequently, you can run the individual demos:\nOn the right side you should see theMotion Planning Taskspanel outlining the hierarchical stage structure of the tasks.\nWhen you select a particular stage, the list of successful and failed solutions will be\nshown in the right-most window. Selecting one of those solutions will start its visualization.\nThis section walks through the steps required to build a simple task with MoveIt Task Constructor.\nCreate a new package with the following command:\nThis will create a new package and folder calledmtc_tutorialwith a dependency onmoveit_task_constructor_coreas well as a hello world example insrc/mtc_node.\nOpenmtc_node.cppin your editor of choice, and paste in the following code.\nThe top of the code includes the ROS and MoveIt Libraries that this package uses.\nrclcpp/rclcpp.hppincludes core ROS2 functionality\nmoveit/planning_scene/planning_scene.handmoveit/planning_scene_interface/planning_scene_interface.hinclude functionality to interface with the robot model and collision objects\nmoveit/task_constructor/task.h,moveit/task_constructor/solvers.h, andmoveit/task_constructor/stages.hinclude different components of MoveIt Task Constructor that are used in the example\ntf2_geometry_msgs/tf2_geometry_msgs.hppandtf2_eigen/tf2_eigen.hppwon’t be used in this initial example, but they will be used for pose generation when we add more stages to the MoveIt Task Constructor task.\nThe next line gets a logger for our new node. We also create a namespace alias formoveit::task_constructorfor convenience.\nWe start by defining a class that will contain the main MoveIt Task Constructor functionality.  We also declare the MoveIt Task Constructor task object as a member variable for our class: this isn’t strictly necessary for a given application, but it helps save the task for later visualization purposes. We will explore each function individually below.\nThese lines initialize the node with specified options (it is the constructor of ourMTCTaskNodeclass).\nThese next lines define a getter function to get the node base interface, which will be used for the executor later.\nThis class method is used to set up the planning scene that is used in the example. It creates a cylinder with dimensions specified byobject.primitives[0].dimensionsand position specified bypose.position.xandpose.position.y.\nYou can try changing these numbers to resize and move the cylinder around. If we move the cylinder out of the robot’s reach, planning will fail.\nThis function interfaces with the MoveIt Task Constructor task object. It first creates a task, which includes setting some properties and adding stages. This will be discussed further in thecreateTaskfunction definition. Next,task.init()initializes the task andtask.plan(5)generates a plan, stopping after 5 successful plans are found. The next line publishes the solution to be visualized in RViz - this line can be removed if you don’t care for visualization. Finally,task.execute()executes the plan. Execution occurs via an action server interface with the RViz plugin.\nAs mentioned above, this function creates a MoveIt Task Constructor object and sets some initial properties. In this case, we set the task name to “demo_task”, load the robot model, define the names of some useful frames, and set those frame names as properties of the task withtask.setProperty(property_name,value). The next few code blocks will fill out this function body.\nNow, we add an example stage to the node. The first line setscurrent_state_ptrtonullptr; this creates a pointer to a stage such that we can reuse stage information in specific scenarios. This line is not used at this moment, but will be used later when more stages are added to the task. Next, we make acurrent_statestage (a generator stage) and add it to our task - this starts the robot off in its current state. Now that we’ve created theCurrentStatestage, we save a pointer to it in thecurrent_state_ptrfor later use.\nSolvers are used to define the type of robot motion. MoveIt Task Constructor has three options for solvers:\nPipelinePlanneruses MoveIt’s planning pipeline, which typically defaults toOMPL.\nJointInterpolationis a simple planner that interpolates between the start and goal joint states. It is typically used for simple motions as it computes quickly but doesn’t support complex motions.\nCartesianPathis used to move the end effector in a straight line in Cartesian space.\nFeel free to try out the different solvers and see how the robot motion changes. For the first stage we will use the Cartesian planner, which requires the following properties to be set:\nNow that we added in the planners, we can add a stage that will move the robot.\nThe following lines use aMoveTostage (a propagator stage). Since opening the hand is a relatively simple movement, we can use the joint interpolation planner.\nThis stage plans a move to the “open hand” pose, which is a named pose defined in theSRDFfor the Panda robot.\nWe return the task and finish with thecreateTask()function.\nFinally, we havemain: the following lines create a node using the class defined above, and calls the class methods to set up and execute a basic MTC task. In this example, we do not cancel the executor once the task has finished executing to keep the node alive to inspect the solutions in RViz.\nWe will need a launch file to launch themove_group,ros2_control,static_tf,robot_state_publisher, andrviznodes that provide us the environment to run the demo. The one we will use for this example can be foundhere.\nTo run the MoveIt Task Constructor node, we will use a second launch file to start themtc_tutorialexecutable with the proper parameters. Here we can load URDF, SRDF, and OMPL parameters, or use MoveIt Configs Utils to do so. Your launch file should look something like the one found in this tutorial packagehere(pay close attention to thepackageandexecutablearguments below as they are different from the launch file linked) :\nSave a launch file aspick_place_demo.launch.pyor download one to the package’s launch directory. Make sure to edit theCMakeLists.txtso it includes the launch folder by adding the following lines:\nNow we can build and source the colcon workspace.\nStart by launching the first launch file. If you want to use the one provided by the tutorials:\nRViz will now load. If you’re using your own launch file and haven’t included an rviz configsuch as this, you will need to configure RViz before you see anything displayed. If you’re using the launch file from the tutorials package, RViz will already be configured for you and you can jump to the end of the next section.\nIf you are not using the RViz configuration provided, we’ll have to make some changes to the RViz configuration to see your robot and the MoveIt Task Constructor solutions. First, start RViz. The following steps will cover how to set up RViz for MoveIt Task Constructor solution visualization.\nIf theMotionPlanningdisplay is active, uncheck it to hide it for now.\nUnderGlobal Options, change theFixed Framefrommaptopanda_link0if not already done.\nOn the bottom left of the window, click theAddbutton.\nUndermoveit_task_constructor_visualizationselectMotion Planning Tasksand click OK. TheMotion Planning Tasksdisplay should appear on the bottom left.\nIn theDisplays, underMotion Planning Tasks,  changeTask Solution Topicto/solution\nYou should see the panda arm in the main view with Motion Planning Tasks display open in the bottom left and nothing in it. Your MTC task will show up in this panel once you launch themtc_tutorialnode. If you’re usingmtc_demo.launch.pyfrom the tutorials, jump back in here.\nLaunch themtc_tutorialnode with\nYou should see the arm execute the task with the single stage to open the hand, with the cylinder in green in front of it. It should look something like this:\nIf you haven’t made your own package, but still want to see what this looks like, you can launch this file from the tutorials:\nSo far, we’ve walked through creating and executing a simple task, which runs but does not do much. Now, we will start adding the pick-and-place stages to the task. The image below shows an outline of the stages we will use in our task.\nWe will start adding stages after our existing open hand stage. Openmtc_node.cppand locate the following lines:\nWe need to move the arm to a position where we can pick up our object. This is done with aConnectstage, which as its name implies, is a Connector stage. This means that it tries to bridge between the results of the stage before and after it. This stage is initialized with a name,move_to_pick, and aGroupPlannerVectorthat specifies the planning group and the planner. We then set a timeout for the stage, set the properties for the stage, and add it to our task.\nNext, we create a pointer to a MoveIt Task Constructor stage object, and set it tonullptrfor now. Later, we will use this to save a stage.\nThis next block of code creates aSerialContainer.\nThis is a container that can be added to our task and can hold several substages.\nIn this case, we create a serial container that will contain the stages relevant to the picking action.\nInstead of adding the stages to the task, we will add the relevant stages to the serial container. We useexposeTo()to declare the task properties from the parent task in the new serial container, and useconfigureInitFrom()to initialize them.\nThis allows the contained stages to access these properties.\nWe then create a stage to approach the object. This stage is aMoveRelativestage, which allows us to specify a relative movement from our current position.MoveRelativeis a propagator stage: it receives the solution from its neighbouring stages and propagates it to the next or previous stage. Usingcartesian_plannerfinds a solution that involves moving the end effector in a straight line. We set the properties, and set the minimum and maximum distance to move. Now we create aVector3Stampedmessage to indicate the direction we want to move - in this case, in the Z direction from the hand frame. Finally, we add this stage to our serial container\nNow, create a stage to generate the grasp pose.\nThis is a generator stage, so it computes its results without regard to the stages before and after it.\nThe first stage,CurrentStateis a generator stage as well - to connect the first stage and this stage, a connecting stage must be used, which we already created above.\nThis code sets the stage properties, sets the pose before grasping, the angle delta, and the monitored stage.\nAngle delta is a property of theGenerateGraspPosestage that is used to determine the number of poses to generate; when generating solutions, MoveIt Task Constructor will try to grasp the object from many different orientations, with the difference between the orientations specified by the angle delta. The smaller the delta, the closer together the grasp orientations will be. When defining the current stage, we setcurrent_state_ptr, which is now used to forward information about the object pose and shape to the inverse kinematics solver.\nThis stage won’t be directly added to the serial container like previously, as we still need to do inverse kinematics on the poses it generates.\nBefore we compute inverse kinematics for the poses generated above, we first need to define the frame. This can be done with aPoseStampedmessage fromgeometry_msgsor in this case, we define the transform using Eigen transformation matrix and the name of the relevant link. Here, we define the transformation matrix.\nNow, we create theComputeIKstage, and give it the namegenerateposeIKas well as thegenerategraspposestage defined above. Some robots have multiple inverse kinematics solutions for a given pose - we set the limit on the amount of solutions to solve for up to 8. We also set the minimum solution distance, which is a threshold on how different solutions must be: if the joint positions in a solution are too similar to a previous solution, it will be marked as invalid. Next, we configure some additional properties, and add theComputeIKstage to the serial container.\nTo pick up the object, we must allow collision between the hand and the object. This can be done with aModifyPlanningScenestage. TheallowCollisionsfunction lets us specify which collisions to disable.allowCollisionscan be used with a container of names, so we can usegetLinkModelNamesWithCollisionGeometryto get all the names of links with collision geometry in the hand group.\nWith collisions allowed, we now can close the hand. This is done with aMoveTostage, similarly to theopenhandstage from above, except moving to thecloseposition as defined in the SRDF.\nWe now use aModifyPlanningScenestage again, this time to attach the object to the hand usingattachObject. Similarly to what we did with thecurrent_state_ptr, we get a pointer to this stage for later use when generating the place pose for the object.\nNext, we lift the object with aMoveRelativestage, similarly to theapproach_objectstage.\nWith this, we have all the stages needed to pick the object. Now, we add the serial container (with all its substages) to the task. If you build the package as-is, you can see the robot plan to pick up the object.\nTo test out the newly created stage, build the code and execute:\nNow that the stages that define the pick are complete, we move on to defining the stages for placing the object. Picking up where we left off, we add aConnectstage to connect the two, as we will soon be using a generator stage to generate the pose for placing the object.\nWe also create a serial container for the place stages. This is done similarly to the pick serial container.\nThe next stages will be added to the serial container rather than the task.\nThis next stage generates the poses used to place the object and compute the inverse kinematics for those poses - it is somewhat similar to thegenerategraspposestage from the pick serial container.\nWe start by creating a stage to generate the poses and inheriting the task properties.\nWe specify the pose where we want to place the object with aPoseStampedmessage fromgeometry_msgs- in this case, we choosey=0.5in the\"object\"frame.\nWe then pass the target pose to the stage withsetPose.\nNext, we usesetMonitoredStageand pass it the pointer to theattach_objectstage from earlier.\nThis allows the stage to know how the object is attached.\nWe then create aComputeIKstage and pass it ourGeneratePlacePosestage - the rest follows the same logic as above with the pick stages.\nNow that we’re ready to place the object, we open the hand withMoveTostage and the joint interpolation planner.\nWe also can re-enable collisions with the object now that we no longer need to hold it.\nThis is done usingallowCollisionsalmost exactly the same way as disabling collisions, except setting the last argument tofalserather thantrue.\nNow, we can detach the object usingdetachObject.\nWe retreat from the object using aMoveRelativestage, which is done similarly to theapproachobjectandliftobjectstages.\nWe finish our place serial container and add it to the task.\nThe final step is to return home: we use aMoveTostage and pass it the goal pose ofready, which is a pose defined in the Panda SRDF.\nAll these stages should be added above these lines.\nCongratulations! You’ve now defined a pick and place task using MoveIt Task Constructor! To try it out, build the code and execute:\nThe task with each comprising stage is shown in the Motion Planning Tasks pane. Click on a stage and additional information about the stage will show up to the right. The right pane shows different solutions as well as their associated costs. Depending on the stage type and the robot configuration, there may only be one solution shown.\nClick one of the solution costs to see an animation of the robot following the plan for that stage. Click the “Exec” button in the upper-right portion of the pane to execute the motion.\nTo run the complete MoveIt Task Constructor example included with the MoveIt tutorials:\nAnd in a second terminal:\nWhen running MTC, it prints a diagram like this to terminal:\nThis example^ shows two stages. The first stage (“initial_state”) is aCurrentStatetype of stage, which initializes aPlanningSceneand captures any collision objects that are present at that moment.\nA pointer to this stage can be used to retrieve the state of the robot.\nSinceCurrentStateinherits fromGenerator, it propagates solutions both forward and backward.\nThis is denoted by the arrows in both directions.\nThe first1indicates that one solution was successfully propagated backwards to the previous stage.\nThe second1, between the arrows, indicates that one solution was generated.\nThe0indicates that a solution was not propagated forward successfully to the next stage, because the next stage failed.\nThe second stage (“move_to_home”) is aMoveTotype of stage. It inherits its propagation direction from the previous stage, so both arrows point forward. The0’s indicate that this stage failed completely. From left to right, the0’s mean:\nThe stage did not receive a solution from the previous stage\nThe stage did not generate a solution\nThe stage did not propagate a solution forward to the next stage\nIn this case, we could tell that “move_to_home” was the root cause of the failure. The problem was a home state that was in collision. Defining a new, collision-free home position fixed the issue.\nInformation about individual stages can be retrieved from the task. For example, here we retrieve the unique ID for a stage:\nACurrentStatetype stage does not just retrieve the current state of the robot.\nIt also initializes aPlanningSceneobject, capturing any collision objects that are present at that moment.\nMTC stages can be propagated in forward and backward order.\nYou can easily check which direction a stage propagates by the arrow in the RViz GUI.\nWhen propagating backwards, the logic of many operations is reversed.\nFor example, to allow collisions with an object in aModifyPlanningScenestage, you would callallowCollisions(false)rather thanallowCollisions(true). There is a discussion to be readhere.\n© Copyright 2024, PickNik Robotics.\ncd~/ws_moveit/srcgitclone-b<branch>https://github.com/moveit/moveit_task_constructor.git\nrosdep install --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncd~/ws_moveitcolconbuild--mixinrelease\nros2launchmoveit_task_constructor_demodemo.launch.py\nros2launchmoveit_task_constructor_demorun.launch.pyexe:=cartesianros2launchmoveit_task_constructor_demorun.launch.pyexe:=modularros2launchmoveit_task_constructor_demorun.launch.pyexe:=pick_place_demo\nros2pkgcreate\\--build-typeament_cmake\\--dependenciesmoveit_task_constructor_corerclcpp\\--node-namemtc_nodemtc_tutorial\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endifstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;classMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};MTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}rclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}voidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;pose.orientation.w=1.0;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}voidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}mtc::TaskMTCTaskNode::createTask(){mtc::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);// Disable warnings for this line, as it's a variable that's set but not used in this example#pragma GCC diagnostic push#pragma GCC diagnostic ignored \"-Wunused-but-set-variable\"mtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generator#pragma GCC diagnostic popautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));autosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);autointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();autocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);autostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}intmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\n#include<rclcpp/rclcpp.hpp>#include<moveit/planning_scene/planning_scene.h>#include<moveit/planning_scene_interface/planning_scene_interface.h>#include<moveit/task_constructor/task.h>#include<moveit/task_constructor/solvers.h>#include<moveit/task_constructor/stages.h>#if __has_include(<tf2_geometry_msgs/tf2_geometry_msgs.hpp>)#include<tf2_geometry_msgs/tf2_geometry_msgs.hpp>#else#include<tf2_geometry_msgs/tf2_geometry_msgs.h>#endif#if __has_include(<tf2_eigen/tf2_eigen.hpp>)#include<tf2_eigen/tf2_eigen.hpp>#else#include<tf2_eigen/tf2_eigen.h>#endif\nstaticconstrclcpp::LoggerLOGGER=rclcpp::get_logger(\"mtc_tutorial\");namespacemtc=moveit::task_constructor;\nclassMTCTaskNode{public:MTCTaskNode(constrclcpp::NodeOptions&options);rclcpp::node_interfaces::NodeBaseInterface::SharedPtrgetNodeBaseInterface();voiddoTask();voidsetupPlanningScene();private:// Compose an MTC task from a series of stages.mtc::TaskcreateTask();mtc::Tasktask_;rclcpp::Node::SharedPtrnode_;};\nMTCTaskNode::MTCTaskNode(constrclcpp::NodeOptions&options):node_{std::make_shared<rclcpp::Node>(\"mtc_node\",options)}{}\nrclcpp::node_interfaces::NodeBaseInterface::SharedPtrMTCTaskNode::getNodeBaseInterface(){returnnode_->get_node_base_interface();}\nvoidMTCTaskNode::setupPlanningScene(){moveit_msgs::msg::CollisionObjectobject;object.id=\"object\";object.header.frame_id=\"world\";object.primitives.resize(1);object.primitives[0].type=shape_msgs::msg::SolidPrimitive::CYLINDER;object.primitives[0].dimensions={0.1,0.02};geometry_msgs::msg::Posepose;pose.position.x=0.5;pose.position.y=-0.25;object.pose=pose;moveit::planning_interface::PlanningSceneInterfacepsi;psi.applyCollisionObject(object);}\nvoidMTCTaskNode::doTask(){task_=createTask();try{task_.init();}catch(mtc::InitStageException&e){RCLCPP_ERROR_STREAM(LOGGER,e);return;}if(!task_.plan(5)){RCLCPP_ERROR_STREAM(LOGGER,\"Task planning failed\");return;}task_.introspection().publishSolution(*task_.solutions().front());autoresult=task_.execute(*task_.solutions().front());if(result.val!=moveit_msgs::msg::MoveItErrorCodes::SUCCESS){RCLCPP_ERROR_STREAM(LOGGER,\"Task execution failed\");return;}return;}\nmtc::TaskMTCTaskNode::createTask(){moveit::task_constructor::Tasktask;task.stages()->setName(\"demo task\");task.loadRobotModel(node_);constauto&arm_group_name=\"panda_arm\";constauto&hand_group_name=\"hand\";constauto&hand_frame=\"panda_hand\";// Set task propertiestask.setProperty(\"group\",arm_group_name);task.setProperty(\"eef\",hand_group_name);task.setProperty(\"ik_frame\",hand_frame);\nmtc::Stage*current_state_ptr=nullptr;// Forward current_state on to grasp pose generatorautostage_state_current=std::make_unique<mtc::stages::CurrentState>(\"current\");current_state_ptr=stage_state_current.get();task.add(std::move(stage_state_current));\nautosampling_planner=std::make_shared<mtc::solvers::PipelinePlanner>(node_);\nautointerpolation_planner=std::make_shared<mtc::solvers::JointInterpolationPlanner>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();\nautocartesian_planner=std::make_shared<mtc::solvers::CartesianPath>();cartesian_planner->setMaxVelocityScalingFactor(1.0);cartesian_planner->setMaxAccelerationScalingFactor(1.0);cartesian_planner->setStepSize(.01);\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));returntask;}\nintmain(intargc,char**argv){rclcpp::init(argc,argv);rclcpp::NodeOptionsoptions;options.automatically_declare_parameters_from_overrides(true);automtc_task_node=std::make_shared<MTCTaskNode>(options);rclcpp::executors::MultiThreadedExecutorexecutor;autospin_thread=std::make_unique<std::thread>([&executor,&mtc_task_node](){executor.add_node(mtc_task_node->getNodeBaseInterface());executor.spin();executor.remove_node(mtc_task_node->getNodeBaseInterface());});mtc_task_node->setupPlanningScene();mtc_task_node->doTask();spin_thread->join();rclcpp::shutdown();return0;}\nfromlaunchimportLaunchDescriptionfromlaunch_ros.actionsimportNodefrommoveit_configs_utilsimportMoveItConfigsBuilderdefgenerate_launch_description():moveit_config=MoveItConfigsBuilder(\"moveit_resources_panda\").to_dict()# MTC Demo nodepick_place_demo=Node(package=\"mtc_tutorial\",executable=\"mtc_node\",output=\"screen\",parameters=[moveit_config,],)returnLaunchDescription([pick_place_demo])\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}\n  )\ncd~/ws_moveitcolconbuild--mixinreleasesource~/ws_moveit/install/setup.bash\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo_minimal.launch.py\nautostage_open_hand=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage_open_hand->setGroup(hand_group_name);stage_open_hand->setGoal(\"open\");task.add(std::move(stage_open_hand));// Add the next lines of codes to define more stages here\nautostage_move_to_pick=std::make_unique<mtc::stages::Connect>(\"move to pick\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner}});stage_move_to_pick->setTimeout(5.0);stage_move_to_pick->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_pick));\nmtc::Stage*attach_object_stage=nullptr;// Forward attach_object_stage to place pose generator\n{autograsp=std::make_unique<mtc::SerialContainer>(\"pick object\");task.properties().exposeTo(grasp->properties(),{\"eef\",\"group\",\"ik_frame\"});grasp->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"approach object\",cartesian_planner);stage->properties().set(\"marker_ns\",\"approach_object\");stage->properties().set(\"link\",hand_frame);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.15);// Set hand forward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=hand_frame;vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\n{// Sample grasp poseautostage=std::make_unique<mtc::stages::GenerateGraspPose>(\"generate grasp pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"grasp_pose\");stage->setPreGraspPose(\"open\");stage->setObject(\"object\");stage->setAngleDelta(M_PI/12);stage->setMonitoredStage(current_state_ptr);// Hook into current state\nEigen::Isometry3dgrasp_frame_transform;Eigen::Quaterniondq=Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitX())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitY())*Eigen::AngleAxisd(M_PI/2,Eigen::Vector3d::UnitZ());grasp_frame_transform.linear()=q.matrix();grasp_frame_transform.translation().z()=0.1;\n// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"grasp pose IK\",std::move(stage));wrapper->setMaxIKSolutions(8);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(grasp_frame_transform,hand_frame);wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});grasp->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"allow collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),true);grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"close hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"close\");grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"attach object\");stage->attachObject(\"object\",hand_frame);attach_object_stage=stage.get();grasp->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"lift object\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"lift_object\");// Set upward directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.z=1.0;stage->setDirection(vec);grasp->insert(std::move(stage));}\ntask.add(std::move(grasp));}\nros2launchmtc_tutorialpick_place_demo.launch.py\n{autostage_move_to_place=std::make_unique<mtc::stages::Connect>(\"move to place\",mtc::stages::Connect::GroupPlannerVector{{arm_group_name,sampling_planner},{hand_group_name,interpolation_planner}});stage_move_to_place->setTimeout(5.0);stage_move_to_place->properties().configureInitFrom(mtc::Stage::PARENT);task.add(std::move(stage_move_to_place));}\n{autoplace=std::make_unique<mtc::SerialContainer>(\"place object\");task.properties().exposeTo(place->properties(),{\"eef\",\"group\",\"ik_frame\"});place->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\",\"ik_frame\"});\n{// Sample place poseautostage=std::make_unique<mtc::stages::GeneratePlacePose>(\"generate place pose\");stage->properties().configureInitFrom(mtc::Stage::PARENT);stage->properties().set(\"marker_ns\",\"place_pose\");stage->setObject(\"object\");geometry_msgs::msg::PoseStampedtarget_pose_msg;target_pose_msg.header.frame_id=\"object\";target_pose_msg.pose.position.y=0.5;target_pose_msg.pose.orientation.w=1.0;stage->setPose(target_pose_msg);stage->setMonitoredStage(attach_object_stage);// Hook into attach_object_stage// Compute IKautowrapper=std::make_unique<mtc::stages::ComputeIK>(\"place pose IK\",std::move(stage));wrapper->setMaxIKSolutions(2);wrapper->setMinSolutionDistance(1.0);wrapper->setIKFrame(\"object\");wrapper->properties().configureInitFrom(mtc::Stage::PARENT,{\"eef\",\"group\"});wrapper->properties().configureInitFrom(mtc::Stage::INTERFACE,{\"target_pose\"});place->insert(std::move(wrapper));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"open hand\",interpolation_planner);stage->setGroup(hand_group_name);stage->setGoal(\"open\");place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"forbid collision (hand,object)\");stage->allowCollisions(\"object\",task.getRobotModel()->getJointModelGroup(hand_group_name)->getLinkModelNamesWithCollisionGeometry(),false);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::ModifyPlanningScene>(\"detach object\");stage->detachObject(\"object\",hand_frame);place->insert(std::move(stage));}\n{autostage=std::make_unique<mtc::stages::MoveRelative>(\"retreat\",cartesian_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setMinMaxDistance(0.1,0.3);stage->setIKFrame(hand_frame);stage->properties().set(\"marker_ns\",\"retreat\");// Set retreat directiongeometry_msgs::msg::Vector3Stampedvec;vec.header.frame_id=\"world\";vec.vector.x=-0.5;stage->setDirection(vec);place->insert(std::move(stage));}\ntask.add(std::move(place));}\n{autostage=std::make_unique<mtc::stages::MoveTo>(\"return home\",interpolation_planner);stage->properties().configureInitFrom(mtc::Stage::PARENT,{\"group\"});stage->setGoal(\"ready\");task.add(std::move(stage));}\n// Stages all added to the task above this linereturntask;}\nros2launchmtc_tutorialpick_place_demo.launch.py\nros2launchmoveit2_tutorialsmtc_demo.launch.py\nros2launchmoveit2_tutorialspick_place_demo.launch.py\n[demo_node-1]1-←1→-0/initial_state[demo_node-1]-0→0→-0/move_to_home\nuint32_tconstunique_stage_id=task_.stages()->findChild(stage_name)->introspectionId();\n<branch>\nhumble\nros2\nmain\nmtc_tutorial\nmoveit_task_constructor_core\nsrc/mtc_node\nmtc_node.cpp\nrclcpp/rclcpp.hpp\nmoveit/planning_scene/planning_scene.h\nmoveit/planning_scene_interface/planning_scene_interface.h\nmoveit/task_constructor/task.h\nmoveit/task_constructor/solvers.h\nmoveit/task_constructor/stages.h\ntf2_geometry_msgs/tf2_geometry_msgs.hpp\ntf2_eigen/tf2_eigen.hpp\nmoveit::task_constructor\nMTCTaskNode\nobject.primitives[0].dimensions\npose.position.x\npose.position.y\ncreateTask\ntask.init()\ntask.plan(5)\ntask.execute()\ntask.setProperty(property_name,value)\ncurrent_state_ptr\nnullptr\ncurrent_state\nCurrentState\ncurrent_state_ptr\nMoveTo\ncreateTask()\nmain\nmove_group\nros2_control\nstatic_tf\nrobot_state_publisher\nrviz\nmtc_tutorial\npackage\nexecutable\npick_place_demo.launch.py\nCMakeLists.txt\nmap\npanda_link0\nmoveit_task_constructor_visualization\n/solution\nmtc_tutorial\nmtc_demo.launch.py\nmtc_tutorial\nmtc_node.cpp\nConnect\nmove_to_pick\nGroupPlannerVector\nnullptr\nSerialContainer\nexposeTo()\nconfigureInitFrom()\nMoveRelative\nMoveRelative\ncartesian_planner\nVector3Stamped\nCurrentState\nGenerateGraspPose\ncurrent_state_ptr\nPoseStamped\ngeometry_msgs\nComputeIK\ngenerateposeIK\ngenerategrasppose\nComputeIK\nModifyPlanningScene\nallowCollisions\nallowCollisions\ngetLinkModelNamesWithCollisionGeometry\nMoveTo\nopenhand\nclose\nModifyPlanningScene\nattachObject\ncurrent_state_ptr\nMoveRelative\napproach_object\nConnect\ngenerategrasppose\nPoseStamped\ngeometry_msgs\ny=0.5\n\"object\"\nsetPose\nsetMonitoredStage\nattach_object\nComputeIK\nGeneratePlacePose\nMoveTo\nallowCollisions\nfalse\ntrue\ndetachObject\nMoveRelative\napproachobject\nliftobject\nMoveTo\nready\nCurrentState\nPlanningScene\nCurrentState\nGenerator\n1\n1\n0\nMoveTo\n0\n0\nCurrentState\nPlanningScene\nModifyPlanningScene\nallowCollisions(false)\nallowCollisions(true)",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0e7"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/kinematics.html",
  "title": "Kinematics — MoveIt Documentation: Rolling  documentation",
  "content": "Kinematics\nThe Kinematics Plugin\nCollision Objects\nAllowed Collision Matrix (ACM)\nMoveIt uses a plugin infrastructure, especially targeted towards allowing users to write their own inverse kinematics algorithms.\nForward kinematics and finding Jacobians is integrated within the RobotState class itself.\nThe default inverse kinematics plugin for MoveIt is configured using theKDLnumerical Jacobian-based solver.\nThis plugin is automatically configured by the MoveIt Setup Assistant.\nCollision checking in MoveIt is configured inside a Planning Scene using theCollisionWorldobject.\nFortunately, MoveIt is set up so that users never really have to worry about how collision checking is happening.\nCollision checking in MoveIt is mainly carried out using theFCLpackage - the primary collision checking library of MoveIt.\nMoveIt supports collision checking for different types of objects including:\nMeshes- you can use either.stl(standard triangle language) or.dae(digital asset exchange) formats to describe objects such as robot links.\nPrimitive Shapes- e.g. boxes, cylinders, cones, spheres and planes\nOctomap- theOctomapobject can be directly used for collision checking\nCollision checking is a very expensive operation often accounting for close to 90% of the computational expense during motion planning.\nTheAllowedCollisionMatrixorACMencodes a binary value corresponding to the need to check for collision between pairs of bodies (which could be on the robot or in the world).\nIf the value corresponding to two bodies is set totruein the ACM, no collision check between the two bodies will be performed.\nThe collision checking would not be required if, e.g., the two bodies are always so far away that they can never collide with each other.\nAlternatively, the two bodies could be in contact with each other by default, in which case the collision detection should be disabled for the pair in the ACM.\n© Copyright 2024, PickNik Robotics.\nmove_group\nCollisionWorld\n.stl\n.dae\nOctomap\nAllowedCollisionMatrix\nACM\ntrue",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0e8"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/kinematics.html#the-kinematics-plugin",
  "title": "Kinematics — MoveIt Documentation: Rolling  documentation",
  "content": "Kinematics\nThe Kinematics Plugin\nCollision Objects\nAllowed Collision Matrix (ACM)\nMoveIt uses a plugin infrastructure, especially targeted towards allowing users to write their own inverse kinematics algorithms.\nForward kinematics and finding Jacobians is integrated within the RobotState class itself.\nThe default inverse kinematics plugin for MoveIt is configured using theKDLnumerical Jacobian-based solver.\nThis plugin is automatically configured by the MoveIt Setup Assistant.\nCollision checking in MoveIt is configured inside a Planning Scene using theCollisionWorldobject.\nFortunately, MoveIt is set up so that users never really have to worry about how collision checking is happening.\nCollision checking in MoveIt is mainly carried out using theFCLpackage - the primary collision checking library of MoveIt.\nMoveIt supports collision checking for different types of objects including:\nMeshes- you can use either.stl(standard triangle language) or.dae(digital asset exchange) formats to describe objects such as robot links.\nPrimitive Shapes- e.g. boxes, cylinders, cones, spheres and planes\nOctomap- theOctomapobject can be directly used for collision checking\nCollision checking is a very expensive operation often accounting for close to 90% of the computational expense during motion planning.\nTheAllowedCollisionMatrixorACMencodes a binary value corresponding to the need to check for collision between pairs of bodies (which could be on the robot or in the world).\nIf the value corresponding to two bodies is set totruein the ACM, no collision check between the two bodies will be performed.\nThe collision checking would not be required if, e.g., the two bodies are always so far away that they can never collide with each other.\nAlternatively, the two bodies could be in contact with each other by default, in which case the collision detection should be disabled for the pair in the ACM.\n© Copyright 2024, PickNik Robotics.\nmove_group\nCollisionWorld\n.stl\n.dae\nOctomap\nAllowedCollisionMatrix\nACM\ntrue",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0e9"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/kinematics.html#collision-checking",
  "title": "Kinematics — MoveIt Documentation: Rolling  documentation",
  "content": "Kinematics\nThe Kinematics Plugin\nCollision Objects\nAllowed Collision Matrix (ACM)\nMoveIt uses a plugin infrastructure, especially targeted towards allowing users to write their own inverse kinematics algorithms.\nForward kinematics and finding Jacobians is integrated within the RobotState class itself.\nThe default inverse kinematics plugin for MoveIt is configured using theKDLnumerical Jacobian-based solver.\nThis plugin is automatically configured by the MoveIt Setup Assistant.\nCollision checking in MoveIt is configured inside a Planning Scene using theCollisionWorldobject.\nFortunately, MoveIt is set up so that users never really have to worry about how collision checking is happening.\nCollision checking in MoveIt is mainly carried out using theFCLpackage - the primary collision checking library of MoveIt.\nMoveIt supports collision checking for different types of objects including:\nMeshes- you can use either.stl(standard triangle language) or.dae(digital asset exchange) formats to describe objects such as robot links.\nPrimitive Shapes- e.g. boxes, cylinders, cones, spheres and planes\nOctomap- theOctomapobject can be directly used for collision checking\nCollision checking is a very expensive operation often accounting for close to 90% of the computational expense during motion planning.\nTheAllowedCollisionMatrixorACMencodes a binary value corresponding to the need to check for collision between pairs of bodies (which could be on the robot or in the world).\nIf the value corresponding to two bodies is set totruein the ACM, no collision check between the two bodies will be performed.\nThe collision checking would not be required if, e.g., the two bodies are always so far away that they can never collide with each other.\nAlternatively, the two bodies could be in contact with each other by default, in which case the collision detection should be disabled for the pair in the ACM.\n© Copyright 2024, PickNik Robotics.\nmove_group\nCollisionWorld\n.stl\n.dae\nOctomap\nAllowedCollisionMatrix\nACM\ntrue",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0ea"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/kinematics.html#collision-objects",
  "title": "Kinematics — MoveIt Documentation: Rolling  documentation",
  "content": "Kinematics\nThe Kinematics Plugin\nCollision Objects\nAllowed Collision Matrix (ACM)\nMoveIt uses a plugin infrastructure, especially targeted towards allowing users to write their own inverse kinematics algorithms.\nForward kinematics and finding Jacobians is integrated within the RobotState class itself.\nThe default inverse kinematics plugin for MoveIt is configured using theKDLnumerical Jacobian-based solver.\nThis plugin is automatically configured by the MoveIt Setup Assistant.\nCollision checking in MoveIt is configured inside a Planning Scene using theCollisionWorldobject.\nFortunately, MoveIt is set up so that users never really have to worry about how collision checking is happening.\nCollision checking in MoveIt is mainly carried out using theFCLpackage - the primary collision checking library of MoveIt.\nMoveIt supports collision checking for different types of objects including:\nMeshes- you can use either.stl(standard triangle language) or.dae(digital asset exchange) formats to describe objects such as robot links.\nPrimitive Shapes- e.g. boxes, cylinders, cones, spheres and planes\nOctomap- theOctomapobject can be directly used for collision checking\nCollision checking is a very expensive operation often accounting for close to 90% of the computational expense during motion planning.\nTheAllowedCollisionMatrixorACMencodes a binary value corresponding to the need to check for collision between pairs of bodies (which could be on the robot or in the world).\nIf the value corresponding to two bodies is set totruein the ACM, no collision check between the two bodies will be performed.\nThe collision checking would not be required if, e.g., the two bodies are always so far away that they can never collide with each other.\nAlternatively, the two bodies could be in contact with each other by default, in which case the collision detection should be disabled for the pair in the ACM.\n© Copyright 2024, PickNik Robotics.\nmove_group\nCollisionWorld\n.stl\n.dae\nOctomap\nAllowedCollisionMatrix\nACM\ntrue",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0eb"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/kinematics.html#allowed-collision-matrix-acm",
  "title": "Kinematics — MoveIt Documentation: Rolling  documentation",
  "content": "Kinematics\nThe Kinematics Plugin\nCollision Objects\nAllowed Collision Matrix (ACM)\nMoveIt uses a plugin infrastructure, especially targeted towards allowing users to write their own inverse kinematics algorithms.\nForward kinematics and finding Jacobians is integrated within the RobotState class itself.\nThe default inverse kinematics plugin for MoveIt is configured using theKDLnumerical Jacobian-based solver.\nThis plugin is automatically configured by the MoveIt Setup Assistant.\nCollision checking in MoveIt is configured inside a Planning Scene using theCollisionWorldobject.\nFortunately, MoveIt is set up so that users never really have to worry about how collision checking is happening.\nCollision checking in MoveIt is mainly carried out using theFCLpackage - the primary collision checking library of MoveIt.\nMoveIt supports collision checking for different types of objects including:\nMeshes- you can use either.stl(standard triangle language) or.dae(digital asset exchange) formats to describe objects such as robot links.\nPrimitive Shapes- e.g. boxes, cylinders, cones, spheres and planes\nOctomap- theOctomapobject can be directly used for collision checking\nCollision checking is a very expensive operation often accounting for close to 90% of the computational expense during motion planning.\nTheAllowedCollisionMatrixorACMencodes a binary value corresponding to the need to check for collision between pairs of bodies (which could be on the robot or in the world).\nIf the value corresponding to two bodies is set totruein the ACM, no collision check between the two bodies will be performed.\nThe collision checking would not be required if, e.g., the two bodies are always so far away that they can never collide with each other.\nAlternatively, the two bodies could be in contact with each other by default, in which case the collision detection should be disabled for the pair in the ACM.\n© Copyright 2024, PickNik Robotics.\nmove_group\nCollisionWorld\n.stl\n.dae\nOctomap\nAllowedCollisionMatrix\nACM\ntrue",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0ec"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/kinematics.html#kinematics",
  "title": "Kinematics — MoveIt Documentation: Rolling  documentation",
  "content": "Kinematics\nThe Kinematics Plugin\nCollision Objects\nAllowed Collision Matrix (ACM)\nMoveIt uses a plugin infrastructure, especially targeted towards allowing users to write their own inverse kinematics algorithms.\nForward kinematics and finding Jacobians is integrated within the RobotState class itself.\nThe default inverse kinematics plugin for MoveIt is configured using theKDLnumerical Jacobian-based solver.\nThis plugin is automatically configured by the MoveIt Setup Assistant.\nCollision checking in MoveIt is configured inside a Planning Scene using theCollisionWorldobject.\nFortunately, MoveIt is set up so that users never really have to worry about how collision checking is happening.\nCollision checking in MoveIt is mainly carried out using theFCLpackage - the primary collision checking library of MoveIt.\nMoveIt supports collision checking for different types of objects including:\nMeshes- you can use either.stl(standard triangle language) or.dae(digital asset exchange) formats to describe objects such as robot links.\nPrimitive Shapes- e.g. boxes, cylinders, cones, spheres and planes\nOctomap- theOctomapobject can be directly used for collision checking\nCollision checking is a very expensive operation often accounting for close to 90% of the computational expense during motion planning.\nTheAllowedCollisionMatrixorACMencodes a binary value corresponding to the need to check for collision between pairs of bodies (which could be on the robot or in the world).\nIf the value corresponding to two bodies is set totruein the ACM, no collision check between the two bodies will be performed.\nThe collision checking would not be required if, e.g., the two bodies are always so far away that they can never collide with each other.\nAlternatively, the two bodies could be in contact with each other by default, in which case the collision detection should be disabled for the pair in the ACM.\n© Copyright 2024, PickNik Robotics.\nmove_group\nCollisionWorld\n.stl\n.dae\nOctomap\nAllowedCollisionMatrix\nACM\ntrue",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0ed"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/motion_planning.html",
  "title": "Motion Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Motion Planning\nThe Motion Planning Plugin\nThe Motion Plan Request\nThe Motion Plan Result\nMotion planning adapters\nOMPL\nMoveIt works with motion planners through aplugin interface.\nThis allows MoveIt to communicate with and use different motion planners from multiple libraries, making MoveIt easily extensible. The interface to the motion planners is through a ROS action or service (offered by themove_groupnode).\nThe default motion planners for move_group are configured using OMPL and the MoveIt interface to OMPL by the MoveIt Setup Assistant.\nOther planners that are available by default are the Pilz industrial motion planner and CHOMP.\nThe motion plan request specifies what you would like the motion planner to do.\nTypically, you will be asking the motion planner to move an arm to a different location (in joint space) or the end-effector to a new pose.\nCollisions are checked for by default (including self-collisions and attached objects).\nYou can also specify the planner via theplanning_pipelineandplanner_idparameters, and the constraints for the motion planner to check - the inbuilt constraints provided by MoveIt arekinematicconstraints:\nPosition constraints: restrict the position of a link to lie within a region of space.\nOrientation constraints: restrict the orientation of a link to lie within specified roll, pitch or yaw limits.\nVisibility constraints: restrict a point on a link to lie within the visibility cone for a particular sensor.\nJoint constraints: restrict a joint to lie between two values.\nUser-specified constraints: it is also possible to specify your own constraints with a user-defined callback.\nThemove_groupnode will generate a desired trajectory in response to your motion plan request.\nThis trajectory will move the arm (or any group of joints) to the desired location.\nNote that the result coming out ofmove_groupis a trajectory and not just a path.move_groupwill use the desired maximum velocities and accelerations (if specified) to generate a trajectory that obeys velocity and acceleration constraints at the joint level.\nThe complete motion planning pipeline chains together a motion planner with other components calledplanning request adapters.\nPlanning request adapters allow for pre-processing motion plan requests and post-processing motion plan responses.\nPre-processing is useful in several situations, e.g. when a start state for the robot is slightly outside the specified joint limits for the robot.\nPost-processing is needed for several other operations, e.g. to convert paths generated for a robot into time-parameterized trajectories.\nMoveIt provides a set of default motion planning adapters that each perform a very specific function.\nThe fix start state bounds adapter fixes the start state to be within the joint limits specified in the URDF.\nThe need for this adapter arises in situations where the joint limits for the physical robot are not properly configured.\nThe robot may then end up in a configuration where one or more of its joints is slightly outside its joint limits.\nIn this case, the motion planner is unable to plan since it will think that the starting state is outside joint limits.\nThe “CheckStartStateBounds” planning request adapter will “fix” the start state by moving it to the joint limit.\nHowever, this is obviously not the right solution every time - e.g. where the joint is really outside its joint limits by a large amount.\nA parameter for the adapter specifies how much the joint can be outside its limits for it to be “fixable”.\nThe fix workspace bounds adapter will specify a default workspace for planning: a cube of size 10 m x 10 m x 10 m.\nThis workspace will only be specified if the planning request to the planner does not have these fields filled in.\nThe fix start state collision adapter will attempt to sample a new collision-free configuration near a specified configuration (in collision) by perturbing the joint values by a small amount.\nThe amount that it will perturb the values by is specified by thejiggle_fractionparameter that controls the perturbation as a percentage of the total range of motion for the joint.\nThe other parameter for this adapter specifies how many random perturbations the adapter will sample before giving up.\nThe motion planners will typically generate “kinematic paths”, i.e., paths that do not obey any velocity or acceleration constraints and are not time parameterized.\nThis adapter will “time parameterize” the motion plans by applying velocity and acceleration constraints.\nGoal constraints can be set using subframes (e.g. a pose goal in the framecup/handle, wherehandleis a subframe on the objectcup).\nThis adapter changes the frame of constraints to an object or robot frame (e.g.cup).\nOMPL (Open Motion Planning Library) is an open-source motion planning library that primarily implements randomized motion planners.\nMoveIt integrates directly with OMPL and uses the motion planners from that library as its primary/default set of planners.\nThe planners in OMPL are abstract; i.e. OMPL has no concept of a robot.\nInstead, MoveIt configures OMPL and provides the back-end for OMPL to work with problems in Robotics.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nplanning_pipeline\nplanner_id\nkinematicconstraints\ncup/handle\nhandle\ncup\ncup",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0ee"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/motion_planning.html#the-motion-planning-plugin",
  "title": "Motion Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Motion Planning\nThe Motion Planning Plugin\nThe Motion Plan Request\nThe Motion Plan Result\nMotion planning adapters\nOMPL\nMoveIt works with motion planners through aplugin interface.\nThis allows MoveIt to communicate with and use different motion planners from multiple libraries, making MoveIt easily extensible. The interface to the motion planners is through a ROS action or service (offered by themove_groupnode).\nThe default motion planners for move_group are configured using OMPL and the MoveIt interface to OMPL by the MoveIt Setup Assistant.\nOther planners that are available by default are the Pilz industrial motion planner and CHOMP.\nThe motion plan request specifies what you would like the motion planner to do.\nTypically, you will be asking the motion planner to move an arm to a different location (in joint space) or the end-effector to a new pose.\nCollisions are checked for by default (including self-collisions and attached objects).\nYou can also specify the planner via theplanning_pipelineandplanner_idparameters, and the constraints for the motion planner to check - the inbuilt constraints provided by MoveIt arekinematicconstraints:\nPosition constraints: restrict the position of a link to lie within a region of space.\nOrientation constraints: restrict the orientation of a link to lie within specified roll, pitch or yaw limits.\nVisibility constraints: restrict a point on a link to lie within the visibility cone for a particular sensor.\nJoint constraints: restrict a joint to lie between two values.\nUser-specified constraints: it is also possible to specify your own constraints with a user-defined callback.\nThemove_groupnode will generate a desired trajectory in response to your motion plan request.\nThis trajectory will move the arm (or any group of joints) to the desired location.\nNote that the result coming out ofmove_groupis a trajectory and not just a path.move_groupwill use the desired maximum velocities and accelerations (if specified) to generate a trajectory that obeys velocity and acceleration constraints at the joint level.\nThe complete motion planning pipeline chains together a motion planner with other components calledplanning request adapters.\nPlanning request adapters allow for pre-processing motion plan requests and post-processing motion plan responses.\nPre-processing is useful in several situations, e.g. when a start state for the robot is slightly outside the specified joint limits for the robot.\nPost-processing is needed for several other operations, e.g. to convert paths generated for a robot into time-parameterized trajectories.\nMoveIt provides a set of default motion planning adapters that each perform a very specific function.\nThe fix start state bounds adapter fixes the start state to be within the joint limits specified in the URDF.\nThe need for this adapter arises in situations where the joint limits for the physical robot are not properly configured.\nThe robot may then end up in a configuration where one or more of its joints is slightly outside its joint limits.\nIn this case, the motion planner is unable to plan since it will think that the starting state is outside joint limits.\nThe “CheckStartStateBounds” planning request adapter will “fix” the start state by moving it to the joint limit.\nHowever, this is obviously not the right solution every time - e.g. where the joint is really outside its joint limits by a large amount.\nA parameter for the adapter specifies how much the joint can be outside its limits for it to be “fixable”.\nThe fix workspace bounds adapter will specify a default workspace for planning: a cube of size 10 m x 10 m x 10 m.\nThis workspace will only be specified if the planning request to the planner does not have these fields filled in.\nThe fix start state collision adapter will attempt to sample a new collision-free configuration near a specified configuration (in collision) by perturbing the joint values by a small amount.\nThe amount that it will perturb the values by is specified by thejiggle_fractionparameter that controls the perturbation as a percentage of the total range of motion for the joint.\nThe other parameter for this adapter specifies how many random perturbations the adapter will sample before giving up.\nThe motion planners will typically generate “kinematic paths”, i.e., paths that do not obey any velocity or acceleration constraints and are not time parameterized.\nThis adapter will “time parameterize” the motion plans by applying velocity and acceleration constraints.\nGoal constraints can be set using subframes (e.g. a pose goal in the framecup/handle, wherehandleis a subframe on the objectcup).\nThis adapter changes the frame of constraints to an object or robot frame (e.g.cup).\nOMPL (Open Motion Planning Library) is an open-source motion planning library that primarily implements randomized motion planners.\nMoveIt integrates directly with OMPL and uses the motion planners from that library as its primary/default set of planners.\nThe planners in OMPL are abstract; i.e. OMPL has no concept of a robot.\nInstead, MoveIt configures OMPL and provides the back-end for OMPL to work with problems in Robotics.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nplanning_pipeline\nplanner_id\nkinematicconstraints\ncup/handle\nhandle\ncup\ncup",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0ef"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/motion_planning.html#the-motion-plan-request",
  "title": "Motion Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Motion Planning\nThe Motion Planning Plugin\nThe Motion Plan Request\nThe Motion Plan Result\nMotion planning adapters\nOMPL\nMoveIt works with motion planners through aplugin interface.\nThis allows MoveIt to communicate with and use different motion planners from multiple libraries, making MoveIt easily extensible. The interface to the motion planners is through a ROS action or service (offered by themove_groupnode).\nThe default motion planners for move_group are configured using OMPL and the MoveIt interface to OMPL by the MoveIt Setup Assistant.\nOther planners that are available by default are the Pilz industrial motion planner and CHOMP.\nThe motion plan request specifies what you would like the motion planner to do.\nTypically, you will be asking the motion planner to move an arm to a different location (in joint space) or the end-effector to a new pose.\nCollisions are checked for by default (including self-collisions and attached objects).\nYou can also specify the planner via theplanning_pipelineandplanner_idparameters, and the constraints for the motion planner to check - the inbuilt constraints provided by MoveIt arekinematicconstraints:\nPosition constraints: restrict the position of a link to lie within a region of space.\nOrientation constraints: restrict the orientation of a link to lie within specified roll, pitch or yaw limits.\nVisibility constraints: restrict a point on a link to lie within the visibility cone for a particular sensor.\nJoint constraints: restrict a joint to lie between two values.\nUser-specified constraints: it is also possible to specify your own constraints with a user-defined callback.\nThemove_groupnode will generate a desired trajectory in response to your motion plan request.\nThis trajectory will move the arm (or any group of joints) to the desired location.\nNote that the result coming out ofmove_groupis a trajectory and not just a path.move_groupwill use the desired maximum velocities and accelerations (if specified) to generate a trajectory that obeys velocity and acceleration constraints at the joint level.\nThe complete motion planning pipeline chains together a motion planner with other components calledplanning request adapters.\nPlanning request adapters allow for pre-processing motion plan requests and post-processing motion plan responses.\nPre-processing is useful in several situations, e.g. when a start state for the robot is slightly outside the specified joint limits for the robot.\nPost-processing is needed for several other operations, e.g. to convert paths generated for a robot into time-parameterized trajectories.\nMoveIt provides a set of default motion planning adapters that each perform a very specific function.\nThe fix start state bounds adapter fixes the start state to be within the joint limits specified in the URDF.\nThe need for this adapter arises in situations where the joint limits for the physical robot are not properly configured.\nThe robot may then end up in a configuration where one or more of its joints is slightly outside its joint limits.\nIn this case, the motion planner is unable to plan since it will think that the starting state is outside joint limits.\nThe “CheckStartStateBounds” planning request adapter will “fix” the start state by moving it to the joint limit.\nHowever, this is obviously not the right solution every time - e.g. where the joint is really outside its joint limits by a large amount.\nA parameter for the adapter specifies how much the joint can be outside its limits for it to be “fixable”.\nThe fix workspace bounds adapter will specify a default workspace for planning: a cube of size 10 m x 10 m x 10 m.\nThis workspace will only be specified if the planning request to the planner does not have these fields filled in.\nThe fix start state collision adapter will attempt to sample a new collision-free configuration near a specified configuration (in collision) by perturbing the joint values by a small amount.\nThe amount that it will perturb the values by is specified by thejiggle_fractionparameter that controls the perturbation as a percentage of the total range of motion for the joint.\nThe other parameter for this adapter specifies how many random perturbations the adapter will sample before giving up.\nThe motion planners will typically generate “kinematic paths”, i.e., paths that do not obey any velocity or acceleration constraints and are not time parameterized.\nThis adapter will “time parameterize” the motion plans by applying velocity and acceleration constraints.\nGoal constraints can be set using subframes (e.g. a pose goal in the framecup/handle, wherehandleis a subframe on the objectcup).\nThis adapter changes the frame of constraints to an object or robot frame (e.g.cup).\nOMPL (Open Motion Planning Library) is an open-source motion planning library that primarily implements randomized motion planners.\nMoveIt integrates directly with OMPL and uses the motion planners from that library as its primary/default set of planners.\nThe planners in OMPL are abstract; i.e. OMPL has no concept of a robot.\nInstead, MoveIt configures OMPL and provides the back-end for OMPL to work with problems in Robotics.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nplanning_pipeline\nplanner_id\nkinematicconstraints\ncup/handle\nhandle\ncup\ncup",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0f0"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/motion_planning.html#the-motion-plan-result",
  "title": "Motion Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Motion Planning\nThe Motion Planning Plugin\nThe Motion Plan Request\nThe Motion Plan Result\nMotion planning adapters\nOMPL\nMoveIt works with motion planners through aplugin interface.\nThis allows MoveIt to communicate with and use different motion planners from multiple libraries, making MoveIt easily extensible. The interface to the motion planners is through a ROS action or service (offered by themove_groupnode).\nThe default motion planners for move_group are configured using OMPL and the MoveIt interface to OMPL by the MoveIt Setup Assistant.\nOther planners that are available by default are the Pilz industrial motion planner and CHOMP.\nThe motion plan request specifies what you would like the motion planner to do.\nTypically, you will be asking the motion planner to move an arm to a different location (in joint space) or the end-effector to a new pose.\nCollisions are checked for by default (including self-collisions and attached objects).\nYou can also specify the planner via theplanning_pipelineandplanner_idparameters, and the constraints for the motion planner to check - the inbuilt constraints provided by MoveIt arekinematicconstraints:\nPosition constraints: restrict the position of a link to lie within a region of space.\nOrientation constraints: restrict the orientation of a link to lie within specified roll, pitch or yaw limits.\nVisibility constraints: restrict a point on a link to lie within the visibility cone for a particular sensor.\nJoint constraints: restrict a joint to lie between two values.\nUser-specified constraints: it is also possible to specify your own constraints with a user-defined callback.\nThemove_groupnode will generate a desired trajectory in response to your motion plan request.\nThis trajectory will move the arm (or any group of joints) to the desired location.\nNote that the result coming out ofmove_groupis a trajectory and not just a path.move_groupwill use the desired maximum velocities and accelerations (if specified) to generate a trajectory that obeys velocity and acceleration constraints at the joint level.\nThe complete motion planning pipeline chains together a motion planner with other components calledplanning request adapters.\nPlanning request adapters allow for pre-processing motion plan requests and post-processing motion plan responses.\nPre-processing is useful in several situations, e.g. when a start state for the robot is slightly outside the specified joint limits for the robot.\nPost-processing is needed for several other operations, e.g. to convert paths generated for a robot into time-parameterized trajectories.\nMoveIt provides a set of default motion planning adapters that each perform a very specific function.\nThe fix start state bounds adapter fixes the start state to be within the joint limits specified in the URDF.\nThe need for this adapter arises in situations where the joint limits for the physical robot are not properly configured.\nThe robot may then end up in a configuration where one or more of its joints is slightly outside its joint limits.\nIn this case, the motion planner is unable to plan since it will think that the starting state is outside joint limits.\nThe “CheckStartStateBounds” planning request adapter will “fix” the start state by moving it to the joint limit.\nHowever, this is obviously not the right solution every time - e.g. where the joint is really outside its joint limits by a large amount.\nA parameter for the adapter specifies how much the joint can be outside its limits for it to be “fixable”.\nThe fix workspace bounds adapter will specify a default workspace for planning: a cube of size 10 m x 10 m x 10 m.\nThis workspace will only be specified if the planning request to the planner does not have these fields filled in.\nThe fix start state collision adapter will attempt to sample a new collision-free configuration near a specified configuration (in collision) by perturbing the joint values by a small amount.\nThe amount that it will perturb the values by is specified by thejiggle_fractionparameter that controls the perturbation as a percentage of the total range of motion for the joint.\nThe other parameter for this adapter specifies how many random perturbations the adapter will sample before giving up.\nThe motion planners will typically generate “kinematic paths”, i.e., paths that do not obey any velocity or acceleration constraints and are not time parameterized.\nThis adapter will “time parameterize” the motion plans by applying velocity and acceleration constraints.\nGoal constraints can be set using subframes (e.g. a pose goal in the framecup/handle, wherehandleis a subframe on the objectcup).\nThis adapter changes the frame of constraints to an object or robot frame (e.g.cup).\nOMPL (Open Motion Planning Library) is an open-source motion planning library that primarily implements randomized motion planners.\nMoveIt integrates directly with OMPL and uses the motion planners from that library as its primary/default set of planners.\nThe planners in OMPL are abstract; i.e. OMPL has no concept of a robot.\nInstead, MoveIt configures OMPL and provides the back-end for OMPL to work with problems in Robotics.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nplanning_pipeline\nplanner_id\nkinematicconstraints\ncup/handle\nhandle\ncup\ncup",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0f1"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/motion_planning.html#motion-planning-adapters",
  "title": "Motion Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Motion Planning\nThe Motion Planning Plugin\nThe Motion Plan Request\nThe Motion Plan Result\nMotion planning adapters\nOMPL\nMoveIt works with motion planners through aplugin interface.\nThis allows MoveIt to communicate with and use different motion planners from multiple libraries, making MoveIt easily extensible. The interface to the motion planners is through a ROS action or service (offered by themove_groupnode).\nThe default motion planners for move_group are configured using OMPL and the MoveIt interface to OMPL by the MoveIt Setup Assistant.\nOther planners that are available by default are the Pilz industrial motion planner and CHOMP.\nThe motion plan request specifies what you would like the motion planner to do.\nTypically, you will be asking the motion planner to move an arm to a different location (in joint space) or the end-effector to a new pose.\nCollisions are checked for by default (including self-collisions and attached objects).\nYou can also specify the planner via theplanning_pipelineandplanner_idparameters, and the constraints for the motion planner to check - the inbuilt constraints provided by MoveIt arekinematicconstraints:\nPosition constraints: restrict the position of a link to lie within a region of space.\nOrientation constraints: restrict the orientation of a link to lie within specified roll, pitch or yaw limits.\nVisibility constraints: restrict a point on a link to lie within the visibility cone for a particular sensor.\nJoint constraints: restrict a joint to lie between two values.\nUser-specified constraints: it is also possible to specify your own constraints with a user-defined callback.\nThemove_groupnode will generate a desired trajectory in response to your motion plan request.\nThis trajectory will move the arm (or any group of joints) to the desired location.\nNote that the result coming out ofmove_groupis a trajectory and not just a path.move_groupwill use the desired maximum velocities and accelerations (if specified) to generate a trajectory that obeys velocity and acceleration constraints at the joint level.\nThe complete motion planning pipeline chains together a motion planner with other components calledplanning request adapters.\nPlanning request adapters allow for pre-processing motion plan requests and post-processing motion plan responses.\nPre-processing is useful in several situations, e.g. when a start state for the robot is slightly outside the specified joint limits for the robot.\nPost-processing is needed for several other operations, e.g. to convert paths generated for a robot into time-parameterized trajectories.\nMoveIt provides a set of default motion planning adapters that each perform a very specific function.\nThe fix start state bounds adapter fixes the start state to be within the joint limits specified in the URDF.\nThe need for this adapter arises in situations where the joint limits for the physical robot are not properly configured.\nThe robot may then end up in a configuration where one or more of its joints is slightly outside its joint limits.\nIn this case, the motion planner is unable to plan since it will think that the starting state is outside joint limits.\nThe “CheckStartStateBounds” planning request adapter will “fix” the start state by moving it to the joint limit.\nHowever, this is obviously not the right solution every time - e.g. where the joint is really outside its joint limits by a large amount.\nA parameter for the adapter specifies how much the joint can be outside its limits for it to be “fixable”.\nThe fix workspace bounds adapter will specify a default workspace for planning: a cube of size 10 m x 10 m x 10 m.\nThis workspace will only be specified if the planning request to the planner does not have these fields filled in.\nThe fix start state collision adapter will attempt to sample a new collision-free configuration near a specified configuration (in collision) by perturbing the joint values by a small amount.\nThe amount that it will perturb the values by is specified by thejiggle_fractionparameter that controls the perturbation as a percentage of the total range of motion for the joint.\nThe other parameter for this adapter specifies how many random perturbations the adapter will sample before giving up.\nThe motion planners will typically generate “kinematic paths”, i.e., paths that do not obey any velocity or acceleration constraints and are not time parameterized.\nThis adapter will “time parameterize” the motion plans by applying velocity and acceleration constraints.\nGoal constraints can be set using subframes (e.g. a pose goal in the framecup/handle, wherehandleis a subframe on the objectcup).\nThis adapter changes the frame of constraints to an object or robot frame (e.g.cup).\nOMPL (Open Motion Planning Library) is an open-source motion planning library that primarily implements randomized motion planners.\nMoveIt integrates directly with OMPL and uses the motion planners from that library as its primary/default set of planners.\nThe planners in OMPL are abstract; i.e. OMPL has no concept of a robot.\nInstead, MoveIt configures OMPL and provides the back-end for OMPL to work with problems in Robotics.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nplanning_pipeline\nplanner_id\nkinematicconstraints\ncup/handle\nhandle\ncup\ncup",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0f2"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/motion_planning.html#checkstartstatebounds",
  "title": "Motion Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Motion Planning\nThe Motion Planning Plugin\nThe Motion Plan Request\nThe Motion Plan Result\nMotion planning adapters\nOMPL\nMoveIt works with motion planners through aplugin interface.\nThis allows MoveIt to communicate with and use different motion planners from multiple libraries, making MoveIt easily extensible. The interface to the motion planners is through a ROS action or service (offered by themove_groupnode).\nThe default motion planners for move_group are configured using OMPL and the MoveIt interface to OMPL by the MoveIt Setup Assistant.\nOther planners that are available by default are the Pilz industrial motion planner and CHOMP.\nThe motion plan request specifies what you would like the motion planner to do.\nTypically, you will be asking the motion planner to move an arm to a different location (in joint space) or the end-effector to a new pose.\nCollisions are checked for by default (including self-collisions and attached objects).\nYou can also specify the planner via theplanning_pipelineandplanner_idparameters, and the constraints for the motion planner to check - the inbuilt constraints provided by MoveIt arekinematicconstraints:\nPosition constraints: restrict the position of a link to lie within a region of space.\nOrientation constraints: restrict the orientation of a link to lie within specified roll, pitch or yaw limits.\nVisibility constraints: restrict a point on a link to lie within the visibility cone for a particular sensor.\nJoint constraints: restrict a joint to lie between two values.\nUser-specified constraints: it is also possible to specify your own constraints with a user-defined callback.\nThemove_groupnode will generate a desired trajectory in response to your motion plan request.\nThis trajectory will move the arm (or any group of joints) to the desired location.\nNote that the result coming out ofmove_groupis a trajectory and not just a path.move_groupwill use the desired maximum velocities and accelerations (if specified) to generate a trajectory that obeys velocity and acceleration constraints at the joint level.\nThe complete motion planning pipeline chains together a motion planner with other components calledplanning request adapters.\nPlanning request adapters allow for pre-processing motion plan requests and post-processing motion plan responses.\nPre-processing is useful in several situations, e.g. when a start state for the robot is slightly outside the specified joint limits for the robot.\nPost-processing is needed for several other operations, e.g. to convert paths generated for a robot into time-parameterized trajectories.\nMoveIt provides a set of default motion planning adapters that each perform a very specific function.\nThe fix start state bounds adapter fixes the start state to be within the joint limits specified in the URDF.\nThe need for this adapter arises in situations where the joint limits for the physical robot are not properly configured.\nThe robot may then end up in a configuration where one or more of its joints is slightly outside its joint limits.\nIn this case, the motion planner is unable to plan since it will think that the starting state is outside joint limits.\nThe “CheckStartStateBounds” planning request adapter will “fix” the start state by moving it to the joint limit.\nHowever, this is obviously not the right solution every time - e.g. where the joint is really outside its joint limits by a large amount.\nA parameter for the adapter specifies how much the joint can be outside its limits for it to be “fixable”.\nThe fix workspace bounds adapter will specify a default workspace for planning: a cube of size 10 m x 10 m x 10 m.\nThis workspace will only be specified if the planning request to the planner does not have these fields filled in.\nThe fix start state collision adapter will attempt to sample a new collision-free configuration near a specified configuration (in collision) by perturbing the joint values by a small amount.\nThe amount that it will perturb the values by is specified by thejiggle_fractionparameter that controls the perturbation as a percentage of the total range of motion for the joint.\nThe other parameter for this adapter specifies how many random perturbations the adapter will sample before giving up.\nThe motion planners will typically generate “kinematic paths”, i.e., paths that do not obey any velocity or acceleration constraints and are not time parameterized.\nThis adapter will “time parameterize” the motion plans by applying velocity and acceleration constraints.\nGoal constraints can be set using subframes (e.g. a pose goal in the framecup/handle, wherehandleis a subframe on the objectcup).\nThis adapter changes the frame of constraints to an object or robot frame (e.g.cup).\nOMPL (Open Motion Planning Library) is an open-source motion planning library that primarily implements randomized motion planners.\nMoveIt integrates directly with OMPL and uses the motion planners from that library as its primary/default set of planners.\nThe planners in OMPL are abstract; i.e. OMPL has no concept of a robot.\nInstead, MoveIt configures OMPL and provides the back-end for OMPL to work with problems in Robotics.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nplanning_pipeline\nplanner_id\nkinematicconstraints\ncup/handle\nhandle\ncup\ncup",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0f3"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/motion_planning.html#validateworkspacebounds",
  "title": "Motion Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Motion Planning\nThe Motion Planning Plugin\nThe Motion Plan Request\nThe Motion Plan Result\nMotion planning adapters\nOMPL\nMoveIt works with motion planners through aplugin interface.\nThis allows MoveIt to communicate with and use different motion planners from multiple libraries, making MoveIt easily extensible. The interface to the motion planners is through a ROS action or service (offered by themove_groupnode).\nThe default motion planners for move_group are configured using OMPL and the MoveIt interface to OMPL by the MoveIt Setup Assistant.\nOther planners that are available by default are the Pilz industrial motion planner and CHOMP.\nThe motion plan request specifies what you would like the motion planner to do.\nTypically, you will be asking the motion planner to move an arm to a different location (in joint space) or the end-effector to a new pose.\nCollisions are checked for by default (including self-collisions and attached objects).\nYou can also specify the planner via theplanning_pipelineandplanner_idparameters, and the constraints for the motion planner to check - the inbuilt constraints provided by MoveIt arekinematicconstraints:\nPosition constraints: restrict the position of a link to lie within a region of space.\nOrientation constraints: restrict the orientation of a link to lie within specified roll, pitch or yaw limits.\nVisibility constraints: restrict a point on a link to lie within the visibility cone for a particular sensor.\nJoint constraints: restrict a joint to lie between two values.\nUser-specified constraints: it is also possible to specify your own constraints with a user-defined callback.\nThemove_groupnode will generate a desired trajectory in response to your motion plan request.\nThis trajectory will move the arm (or any group of joints) to the desired location.\nNote that the result coming out ofmove_groupis a trajectory and not just a path.move_groupwill use the desired maximum velocities and accelerations (if specified) to generate a trajectory that obeys velocity and acceleration constraints at the joint level.\nThe complete motion planning pipeline chains together a motion planner with other components calledplanning request adapters.\nPlanning request adapters allow for pre-processing motion plan requests and post-processing motion plan responses.\nPre-processing is useful in several situations, e.g. when a start state for the robot is slightly outside the specified joint limits for the robot.\nPost-processing is needed for several other operations, e.g. to convert paths generated for a robot into time-parameterized trajectories.\nMoveIt provides a set of default motion planning adapters that each perform a very specific function.\nThe fix start state bounds adapter fixes the start state to be within the joint limits specified in the URDF.\nThe need for this adapter arises in situations where the joint limits for the physical robot are not properly configured.\nThe robot may then end up in a configuration where one or more of its joints is slightly outside its joint limits.\nIn this case, the motion planner is unable to plan since it will think that the starting state is outside joint limits.\nThe “CheckStartStateBounds” planning request adapter will “fix” the start state by moving it to the joint limit.\nHowever, this is obviously not the right solution every time - e.g. where the joint is really outside its joint limits by a large amount.\nA parameter for the adapter specifies how much the joint can be outside its limits for it to be “fixable”.\nThe fix workspace bounds adapter will specify a default workspace for planning: a cube of size 10 m x 10 m x 10 m.\nThis workspace will only be specified if the planning request to the planner does not have these fields filled in.\nThe fix start state collision adapter will attempt to sample a new collision-free configuration near a specified configuration (in collision) by perturbing the joint values by a small amount.\nThe amount that it will perturb the values by is specified by thejiggle_fractionparameter that controls the perturbation as a percentage of the total range of motion for the joint.\nThe other parameter for this adapter specifies how many random perturbations the adapter will sample before giving up.\nThe motion planners will typically generate “kinematic paths”, i.e., paths that do not obey any velocity or acceleration constraints and are not time parameterized.\nThis adapter will “time parameterize” the motion plans by applying velocity and acceleration constraints.\nGoal constraints can be set using subframes (e.g. a pose goal in the framecup/handle, wherehandleis a subframe on the objectcup).\nThis adapter changes the frame of constraints to an object or robot frame (e.g.cup).\nOMPL (Open Motion Planning Library) is an open-source motion planning library that primarily implements randomized motion planners.\nMoveIt integrates directly with OMPL and uses the motion planners from that library as its primary/default set of planners.\nThe planners in OMPL are abstract; i.e. OMPL has no concept of a robot.\nInstead, MoveIt configures OMPL and provides the back-end for OMPL to work with problems in Robotics.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nplanning_pipeline\nplanner_id\nkinematicconstraints\ncup/handle\nhandle\ncup\ncup",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0f4"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/motion_planning.html#checkstartstatecollision",
  "title": "Motion Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Motion Planning\nThe Motion Planning Plugin\nThe Motion Plan Request\nThe Motion Plan Result\nMotion planning adapters\nOMPL\nMoveIt works with motion planners through aplugin interface.\nThis allows MoveIt to communicate with and use different motion planners from multiple libraries, making MoveIt easily extensible. The interface to the motion planners is through a ROS action or service (offered by themove_groupnode).\nThe default motion planners for move_group are configured using OMPL and the MoveIt interface to OMPL by the MoveIt Setup Assistant.\nOther planners that are available by default are the Pilz industrial motion planner and CHOMP.\nThe motion plan request specifies what you would like the motion planner to do.\nTypically, you will be asking the motion planner to move an arm to a different location (in joint space) or the end-effector to a new pose.\nCollisions are checked for by default (including self-collisions and attached objects).\nYou can also specify the planner via theplanning_pipelineandplanner_idparameters, and the constraints for the motion planner to check - the inbuilt constraints provided by MoveIt arekinematicconstraints:\nPosition constraints: restrict the position of a link to lie within a region of space.\nOrientation constraints: restrict the orientation of a link to lie within specified roll, pitch or yaw limits.\nVisibility constraints: restrict a point on a link to lie within the visibility cone for a particular sensor.\nJoint constraints: restrict a joint to lie between two values.\nUser-specified constraints: it is also possible to specify your own constraints with a user-defined callback.\nThemove_groupnode will generate a desired trajectory in response to your motion plan request.\nThis trajectory will move the arm (or any group of joints) to the desired location.\nNote that the result coming out ofmove_groupis a trajectory and not just a path.move_groupwill use the desired maximum velocities and accelerations (if specified) to generate a trajectory that obeys velocity and acceleration constraints at the joint level.\nThe complete motion planning pipeline chains together a motion planner with other components calledplanning request adapters.\nPlanning request adapters allow for pre-processing motion plan requests and post-processing motion plan responses.\nPre-processing is useful in several situations, e.g. when a start state for the robot is slightly outside the specified joint limits for the robot.\nPost-processing is needed for several other operations, e.g. to convert paths generated for a robot into time-parameterized trajectories.\nMoveIt provides a set of default motion planning adapters that each perform a very specific function.\nThe fix start state bounds adapter fixes the start state to be within the joint limits specified in the URDF.\nThe need for this adapter arises in situations where the joint limits for the physical robot are not properly configured.\nThe robot may then end up in a configuration where one or more of its joints is slightly outside its joint limits.\nIn this case, the motion planner is unable to plan since it will think that the starting state is outside joint limits.\nThe “CheckStartStateBounds” planning request adapter will “fix” the start state by moving it to the joint limit.\nHowever, this is obviously not the right solution every time - e.g. where the joint is really outside its joint limits by a large amount.\nA parameter for the adapter specifies how much the joint can be outside its limits for it to be “fixable”.\nThe fix workspace bounds adapter will specify a default workspace for planning: a cube of size 10 m x 10 m x 10 m.\nThis workspace will only be specified if the planning request to the planner does not have these fields filled in.\nThe fix start state collision adapter will attempt to sample a new collision-free configuration near a specified configuration (in collision) by perturbing the joint values by a small amount.\nThe amount that it will perturb the values by is specified by thejiggle_fractionparameter that controls the perturbation as a percentage of the total range of motion for the joint.\nThe other parameter for this adapter specifies how many random perturbations the adapter will sample before giving up.\nThe motion planners will typically generate “kinematic paths”, i.e., paths that do not obey any velocity or acceleration constraints and are not time parameterized.\nThis adapter will “time parameterize” the motion plans by applying velocity and acceleration constraints.\nGoal constraints can be set using subframes (e.g. a pose goal in the framecup/handle, wherehandleis a subframe on the objectcup).\nThis adapter changes the frame of constraints to an object or robot frame (e.g.cup).\nOMPL (Open Motion Planning Library) is an open-source motion planning library that primarily implements randomized motion planners.\nMoveIt integrates directly with OMPL and uses the motion planners from that library as its primary/default set of planners.\nThe planners in OMPL are abstract; i.e. OMPL has no concept of a robot.\nInstead, MoveIt configures OMPL and provides the back-end for OMPL to work with problems in Robotics.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nplanning_pipeline\nplanner_id\nkinematicconstraints\ncup/handle\nhandle\ncup\ncup",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0f5"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/motion_planning.html#addtimeparameterization",
  "title": "Motion Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Motion Planning\nThe Motion Planning Plugin\nThe Motion Plan Request\nThe Motion Plan Result\nMotion planning adapters\nOMPL\nMoveIt works with motion planners through aplugin interface.\nThis allows MoveIt to communicate with and use different motion planners from multiple libraries, making MoveIt easily extensible. The interface to the motion planners is through a ROS action or service (offered by themove_groupnode).\nThe default motion planners for move_group are configured using OMPL and the MoveIt interface to OMPL by the MoveIt Setup Assistant.\nOther planners that are available by default are the Pilz industrial motion planner and CHOMP.\nThe motion plan request specifies what you would like the motion planner to do.\nTypically, you will be asking the motion planner to move an arm to a different location (in joint space) or the end-effector to a new pose.\nCollisions are checked for by default (including self-collisions and attached objects).\nYou can also specify the planner via theplanning_pipelineandplanner_idparameters, and the constraints for the motion planner to check - the inbuilt constraints provided by MoveIt arekinematicconstraints:\nPosition constraints: restrict the position of a link to lie within a region of space.\nOrientation constraints: restrict the orientation of a link to lie within specified roll, pitch or yaw limits.\nVisibility constraints: restrict a point on a link to lie within the visibility cone for a particular sensor.\nJoint constraints: restrict a joint to lie between two values.\nUser-specified constraints: it is also possible to specify your own constraints with a user-defined callback.\nThemove_groupnode will generate a desired trajectory in response to your motion plan request.\nThis trajectory will move the arm (or any group of joints) to the desired location.\nNote that the result coming out ofmove_groupis a trajectory and not just a path.move_groupwill use the desired maximum velocities and accelerations (if specified) to generate a trajectory that obeys velocity and acceleration constraints at the joint level.\nThe complete motion planning pipeline chains together a motion planner with other components calledplanning request adapters.\nPlanning request adapters allow for pre-processing motion plan requests and post-processing motion plan responses.\nPre-processing is useful in several situations, e.g. when a start state for the robot is slightly outside the specified joint limits for the robot.\nPost-processing is needed for several other operations, e.g. to convert paths generated for a robot into time-parameterized trajectories.\nMoveIt provides a set of default motion planning adapters that each perform a very specific function.\nThe fix start state bounds adapter fixes the start state to be within the joint limits specified in the URDF.\nThe need for this adapter arises in situations where the joint limits for the physical robot are not properly configured.\nThe robot may then end up in a configuration where one or more of its joints is slightly outside its joint limits.\nIn this case, the motion planner is unable to plan since it will think that the starting state is outside joint limits.\nThe “CheckStartStateBounds” planning request adapter will “fix” the start state by moving it to the joint limit.\nHowever, this is obviously not the right solution every time - e.g. where the joint is really outside its joint limits by a large amount.\nA parameter for the adapter specifies how much the joint can be outside its limits for it to be “fixable”.\nThe fix workspace bounds adapter will specify a default workspace for planning: a cube of size 10 m x 10 m x 10 m.\nThis workspace will only be specified if the planning request to the planner does not have these fields filled in.\nThe fix start state collision adapter will attempt to sample a new collision-free configuration near a specified configuration (in collision) by perturbing the joint values by a small amount.\nThe amount that it will perturb the values by is specified by thejiggle_fractionparameter that controls the perturbation as a percentage of the total range of motion for the joint.\nThe other parameter for this adapter specifies how many random perturbations the adapter will sample before giving up.\nThe motion planners will typically generate “kinematic paths”, i.e., paths that do not obey any velocity or acceleration constraints and are not time parameterized.\nThis adapter will “time parameterize” the motion plans by applying velocity and acceleration constraints.\nGoal constraints can be set using subframes (e.g. a pose goal in the framecup/handle, wherehandleis a subframe on the objectcup).\nThis adapter changes the frame of constraints to an object or robot frame (e.g.cup).\nOMPL (Open Motion Planning Library) is an open-source motion planning library that primarily implements randomized motion planners.\nMoveIt integrates directly with OMPL and uses the motion planners from that library as its primary/default set of planners.\nThe planners in OMPL are abstract; i.e. OMPL has no concept of a robot.\nInstead, MoveIt configures OMPL and provides the back-end for OMPL to work with problems in Robotics.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nplanning_pipeline\nplanner_id\nkinematicconstraints\ncup/handle\nhandle\ncup\ncup",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0f6"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/motion_planning.html#resolveconstraintframes",
  "title": "Motion Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Motion Planning\nThe Motion Planning Plugin\nThe Motion Plan Request\nThe Motion Plan Result\nMotion planning adapters\nOMPL\nMoveIt works with motion planners through aplugin interface.\nThis allows MoveIt to communicate with and use different motion planners from multiple libraries, making MoveIt easily extensible. The interface to the motion planners is through a ROS action or service (offered by themove_groupnode).\nThe default motion planners for move_group are configured using OMPL and the MoveIt interface to OMPL by the MoveIt Setup Assistant.\nOther planners that are available by default are the Pilz industrial motion planner and CHOMP.\nThe motion plan request specifies what you would like the motion planner to do.\nTypically, you will be asking the motion planner to move an arm to a different location (in joint space) or the end-effector to a new pose.\nCollisions are checked for by default (including self-collisions and attached objects).\nYou can also specify the planner via theplanning_pipelineandplanner_idparameters, and the constraints for the motion planner to check - the inbuilt constraints provided by MoveIt arekinematicconstraints:\nPosition constraints: restrict the position of a link to lie within a region of space.\nOrientation constraints: restrict the orientation of a link to lie within specified roll, pitch or yaw limits.\nVisibility constraints: restrict a point on a link to lie within the visibility cone for a particular sensor.\nJoint constraints: restrict a joint to lie between two values.\nUser-specified constraints: it is also possible to specify your own constraints with a user-defined callback.\nThemove_groupnode will generate a desired trajectory in response to your motion plan request.\nThis trajectory will move the arm (or any group of joints) to the desired location.\nNote that the result coming out ofmove_groupis a trajectory and not just a path.move_groupwill use the desired maximum velocities and accelerations (if specified) to generate a trajectory that obeys velocity and acceleration constraints at the joint level.\nThe complete motion planning pipeline chains together a motion planner with other components calledplanning request adapters.\nPlanning request adapters allow for pre-processing motion plan requests and post-processing motion plan responses.\nPre-processing is useful in several situations, e.g. when a start state for the robot is slightly outside the specified joint limits for the robot.\nPost-processing is needed for several other operations, e.g. to convert paths generated for a robot into time-parameterized trajectories.\nMoveIt provides a set of default motion planning adapters that each perform a very specific function.\nThe fix start state bounds adapter fixes the start state to be within the joint limits specified in the URDF.\nThe need for this adapter arises in situations where the joint limits for the physical robot are not properly configured.\nThe robot may then end up in a configuration where one or more of its joints is slightly outside its joint limits.\nIn this case, the motion planner is unable to plan since it will think that the starting state is outside joint limits.\nThe “CheckStartStateBounds” planning request adapter will “fix” the start state by moving it to the joint limit.\nHowever, this is obviously not the right solution every time - e.g. where the joint is really outside its joint limits by a large amount.\nA parameter for the adapter specifies how much the joint can be outside its limits for it to be “fixable”.\nThe fix workspace bounds adapter will specify a default workspace for planning: a cube of size 10 m x 10 m x 10 m.\nThis workspace will only be specified if the planning request to the planner does not have these fields filled in.\nThe fix start state collision adapter will attempt to sample a new collision-free configuration near a specified configuration (in collision) by perturbing the joint values by a small amount.\nThe amount that it will perturb the values by is specified by thejiggle_fractionparameter that controls the perturbation as a percentage of the total range of motion for the joint.\nThe other parameter for this adapter specifies how many random perturbations the adapter will sample before giving up.\nThe motion planners will typically generate “kinematic paths”, i.e., paths that do not obey any velocity or acceleration constraints and are not time parameterized.\nThis adapter will “time parameterize” the motion plans by applying velocity and acceleration constraints.\nGoal constraints can be set using subframes (e.g. a pose goal in the framecup/handle, wherehandleis a subframe on the objectcup).\nThis adapter changes the frame of constraints to an object or robot frame (e.g.cup).\nOMPL (Open Motion Planning Library) is an open-source motion planning library that primarily implements randomized motion planners.\nMoveIt integrates directly with OMPL and uses the motion planners from that library as its primary/default set of planners.\nThe planners in OMPL are abstract; i.e. OMPL has no concept of a robot.\nInstead, MoveIt configures OMPL and provides the back-end for OMPL to work with problems in Robotics.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nplanning_pipeline\nplanner_id\nkinematicconstraints\ncup/handle\nhandle\ncup\ncup",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0f7"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/motion_planning.html#ompl",
  "title": "Motion Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Motion Planning\nThe Motion Planning Plugin\nThe Motion Plan Request\nThe Motion Plan Result\nMotion planning adapters\nOMPL\nMoveIt works with motion planners through aplugin interface.\nThis allows MoveIt to communicate with and use different motion planners from multiple libraries, making MoveIt easily extensible. The interface to the motion planners is through a ROS action or service (offered by themove_groupnode).\nThe default motion planners for move_group are configured using OMPL and the MoveIt interface to OMPL by the MoveIt Setup Assistant.\nOther planners that are available by default are the Pilz industrial motion planner and CHOMP.\nThe motion plan request specifies what you would like the motion planner to do.\nTypically, you will be asking the motion planner to move an arm to a different location (in joint space) or the end-effector to a new pose.\nCollisions are checked for by default (including self-collisions and attached objects).\nYou can also specify the planner via theplanning_pipelineandplanner_idparameters, and the constraints for the motion planner to check - the inbuilt constraints provided by MoveIt arekinematicconstraints:\nPosition constraints: restrict the position of a link to lie within a region of space.\nOrientation constraints: restrict the orientation of a link to lie within specified roll, pitch or yaw limits.\nVisibility constraints: restrict a point on a link to lie within the visibility cone for a particular sensor.\nJoint constraints: restrict a joint to lie between two values.\nUser-specified constraints: it is also possible to specify your own constraints with a user-defined callback.\nThemove_groupnode will generate a desired trajectory in response to your motion plan request.\nThis trajectory will move the arm (or any group of joints) to the desired location.\nNote that the result coming out ofmove_groupis a trajectory and not just a path.move_groupwill use the desired maximum velocities and accelerations (if specified) to generate a trajectory that obeys velocity and acceleration constraints at the joint level.\nThe complete motion planning pipeline chains together a motion planner with other components calledplanning request adapters.\nPlanning request adapters allow for pre-processing motion plan requests and post-processing motion plan responses.\nPre-processing is useful in several situations, e.g. when a start state for the robot is slightly outside the specified joint limits for the robot.\nPost-processing is needed for several other operations, e.g. to convert paths generated for a robot into time-parameterized trajectories.\nMoveIt provides a set of default motion planning adapters that each perform a very specific function.\nThe fix start state bounds adapter fixes the start state to be within the joint limits specified in the URDF.\nThe need for this adapter arises in situations where the joint limits for the physical robot are not properly configured.\nThe robot may then end up in a configuration where one or more of its joints is slightly outside its joint limits.\nIn this case, the motion planner is unable to plan since it will think that the starting state is outside joint limits.\nThe “CheckStartStateBounds” planning request adapter will “fix” the start state by moving it to the joint limit.\nHowever, this is obviously not the right solution every time - e.g. where the joint is really outside its joint limits by a large amount.\nA parameter for the adapter specifies how much the joint can be outside its limits for it to be “fixable”.\nThe fix workspace bounds adapter will specify a default workspace for planning: a cube of size 10 m x 10 m x 10 m.\nThis workspace will only be specified if the planning request to the planner does not have these fields filled in.\nThe fix start state collision adapter will attempt to sample a new collision-free configuration near a specified configuration (in collision) by perturbing the joint values by a small amount.\nThe amount that it will perturb the values by is specified by thejiggle_fractionparameter that controls the perturbation as a percentage of the total range of motion for the joint.\nThe other parameter for this adapter specifies how many random perturbations the adapter will sample before giving up.\nThe motion planners will typically generate “kinematic paths”, i.e., paths that do not obey any velocity or acceleration constraints and are not time parameterized.\nThis adapter will “time parameterize” the motion plans by applying velocity and acceleration constraints.\nGoal constraints can be set using subframes (e.g. a pose goal in the framecup/handle, wherehandleis a subframe on the objectcup).\nThis adapter changes the frame of constraints to an object or robot frame (e.g.cup).\nOMPL (Open Motion Planning Library) is an open-source motion planning library that primarily implements randomized motion planners.\nMoveIt integrates directly with OMPL and uses the motion planners from that library as its primary/default set of planners.\nThe planners in OMPL are abstract; i.e. OMPL has no concept of a robot.\nInstead, MoveIt configures OMPL and provides the back-end for OMPL to work with problems in Robotics.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nplanning_pipeline\nplanner_id\nkinematicconstraints\ncup/handle\nhandle\ncup\ncup",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0f8"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/motion_planning.html#motion-planning",
  "title": "Motion Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Motion Planning\nThe Motion Planning Plugin\nThe Motion Plan Request\nThe Motion Plan Result\nMotion planning adapters\nOMPL\nMoveIt works with motion planners through aplugin interface.\nThis allows MoveIt to communicate with and use different motion planners from multiple libraries, making MoveIt easily extensible. The interface to the motion planners is through a ROS action or service (offered by themove_groupnode).\nThe default motion planners for move_group are configured using OMPL and the MoveIt interface to OMPL by the MoveIt Setup Assistant.\nOther planners that are available by default are the Pilz industrial motion planner and CHOMP.\nThe motion plan request specifies what you would like the motion planner to do.\nTypically, you will be asking the motion planner to move an arm to a different location (in joint space) or the end-effector to a new pose.\nCollisions are checked for by default (including self-collisions and attached objects).\nYou can also specify the planner via theplanning_pipelineandplanner_idparameters, and the constraints for the motion planner to check - the inbuilt constraints provided by MoveIt arekinematicconstraints:\nPosition constraints: restrict the position of a link to lie within a region of space.\nOrientation constraints: restrict the orientation of a link to lie within specified roll, pitch or yaw limits.\nVisibility constraints: restrict a point on a link to lie within the visibility cone for a particular sensor.\nJoint constraints: restrict a joint to lie between two values.\nUser-specified constraints: it is also possible to specify your own constraints with a user-defined callback.\nThemove_groupnode will generate a desired trajectory in response to your motion plan request.\nThis trajectory will move the arm (or any group of joints) to the desired location.\nNote that the result coming out ofmove_groupis a trajectory and not just a path.move_groupwill use the desired maximum velocities and accelerations (if specified) to generate a trajectory that obeys velocity and acceleration constraints at the joint level.\nThe complete motion planning pipeline chains together a motion planner with other components calledplanning request adapters.\nPlanning request adapters allow for pre-processing motion plan requests and post-processing motion plan responses.\nPre-processing is useful in several situations, e.g. when a start state for the robot is slightly outside the specified joint limits for the robot.\nPost-processing is needed for several other operations, e.g. to convert paths generated for a robot into time-parameterized trajectories.\nMoveIt provides a set of default motion planning adapters that each perform a very specific function.\nThe fix start state bounds adapter fixes the start state to be within the joint limits specified in the URDF.\nThe need for this adapter arises in situations where the joint limits for the physical robot are not properly configured.\nThe robot may then end up in a configuration where one or more of its joints is slightly outside its joint limits.\nIn this case, the motion planner is unable to plan since it will think that the starting state is outside joint limits.\nThe “CheckStartStateBounds” planning request adapter will “fix” the start state by moving it to the joint limit.\nHowever, this is obviously not the right solution every time - e.g. where the joint is really outside its joint limits by a large amount.\nA parameter for the adapter specifies how much the joint can be outside its limits for it to be “fixable”.\nThe fix workspace bounds adapter will specify a default workspace for planning: a cube of size 10 m x 10 m x 10 m.\nThis workspace will only be specified if the planning request to the planner does not have these fields filled in.\nThe fix start state collision adapter will attempt to sample a new collision-free configuration near a specified configuration (in collision) by perturbing the joint values by a small amount.\nThe amount that it will perturb the values by is specified by thejiggle_fractionparameter that controls the perturbation as a percentage of the total range of motion for the joint.\nThe other parameter for this adapter specifies how many random perturbations the adapter will sample before giving up.\nThe motion planners will typically generate “kinematic paths”, i.e., paths that do not obey any velocity or acceleration constraints and are not time parameterized.\nThis adapter will “time parameterize” the motion plans by applying velocity and acceleration constraints.\nGoal constraints can be set using subframes (e.g. a pose goal in the framecup/handle, wherehandleis a subframe on the objectcup).\nThis adapter changes the frame of constraints to an object or robot frame (e.g.cup).\nOMPL (Open Motion Planning Library) is an open-source motion planning library that primarily implements randomized motion planners.\nMoveIt integrates directly with OMPL and uses the motion planners from that library as its primary/default set of planners.\nThe planners in OMPL are abstract; i.e. OMPL has no concept of a robot.\nInstead, MoveIt configures OMPL and provides the back-end for OMPL to work with problems in Robotics.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nplanning_pipeline\nplanner_id\nkinematicconstraints\ncup/handle\nhandle\ncup\ncup",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0f9"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/hybrid_planning/hybrid_planning.html",
  "title": "Hybrid Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Hybrid Planning\nWhat is Hybrid Planning?\nThe Hybrid Planning Architecture\nMoveIt’s motion planning architecture follows the “Sense-Plan-Act” approach. In order for motions to get planned and executed, the environment and the robot state are first perceived (“Sense”), then the robot trajectory is computed by the planner (“Plan”) and finally executed in a single run using a trajectory controller (“Act”).\nWhile this solution works well for global motion planning in well-known static environments, the approach is not applicable in many real-world applications, in particular in unstable or dynamic environments. Tasks like serving a glass of water to a person on a table or writing on an uneven chalkboard require more sophisticated methods that allow reacting to unpredictable changes. For instance, the environment of the robot could change dynamically, or there could be certain uncertainties in the task itself, e.g. writing with chalk requires adapting the pressure on the board while the chalk also gets shorter by being used up.\nSolving these challenges requires a method that is able to adapt the executed motion to the immediate conditions or even to react by replanning in case of unforeseen changes in the environment. The Hybrid Planning architecture attempts to solve this problem by combining a pair of recurrent global and local planners.\nHybrid Planning is a term for a motion planning method that combines heterogeneous motion planners to produce more robust or reactive solutions. The general approach is already very established in the Navigation community and successfully implemented in popular projects like navigation2.\nMoveIt’s Hybrid Planning architecture combines a pair of global and local planners that run in parallel and recurrently with different planning speeds and problem scopes.\nThe global planner has the task to solve a global motion planning problem very akin to the planners used in a “Sense-Plan-Act” application. The used planner algorithm should be complete and is therefore assumed to be relatively slow in terms of computation time. Furthermore, the global planner is not required to be real-time safe, meaning that there are no guarantees that the planner finds a solution within a specific deadline. Depending on the planner implementation, the global planner may generate one initial solution or iteratively optimized solutions during the execution.\nThe local planner is running continuously during execution and produces iterative robot commands for following the global trajectory. In a way, the local planner resembles a controller, only that the architecture allows for solving more complicated problems and constraints. The idea is that the planner is able to reason about the world and to have an internal state. That property makes it very versatile and as it can be used for solving combinations of arbitrary local planning problems, as for example:\nUnwinding, blending, or splicing of subsequent global reference trajectories\nDynamically avoiding close-by collisions while following the global path\nAdapting the global trajectory to local constraints (e.g. desired force pressure on uneven surface, readjusting a tool based on visual feedback)\nLocal trajectory optimization and time parameterization (it is computationally cheaper and quicker to optimize a trajectory in a local environment)\nIn order to enable solving these local problems the local planner must be fast, able to react to sensor feedback and in many cases real-time safe. Also, it should be deterministic in order to avoid jerky or unpredictable motions.\nIn general, the local planner relies on the reference trajectory produced by the global planner so that it doesn’t get stuck in a local minimum. Since local minima sometimes still can’t be ruled out, it can be required that the global planner is triggered for a replan in order to still reach the desired goal. This behavior requires a certain way to communicate planner events and to process them accordingly. For that purpose the Hybrid Planning architecture allows implementing an event-based logic that can be customized for the specific use case and planner types.\nGlobal Planner\nLocal Planner\nSolve global solution trajectory\nOptimize trajectory path (continuously)\nFollow global reference trajectory\nSolve local problem constraints\nMay process sensor input\nOptimize solution locally\nCompute controller commands\nComplete\nNo restricted  computation time\nNot real-time safe\nNot necessarily deterministic\nCan get stuck in local minima\nLow computation time\nRealtime-safe (depends on solver)\nDeterministic\nOMPL planner\nSTOMP\nTrajOpt\nCartesian motion planner\nPilz Industrial Motion Planner\nMTC\nIK solver, Jacobian\nPotential field planner\nTrajectory optimization algorithm\nModel Predictive Control (MPC)\nSensor-based Optimal Control\nHybrid Planning can be useful in a broad range of use cases. Most of the applications can be grouped into the following three scenarios.\nOnline motion planning: The global planner creates an initial global solution and continuously optimizes it. Simultaneously, the local planner executes the reference trajectory and blends updated trajectory segments into it.\nReactive Motion: The global planner is used to fix invalidated solutions (replanning) while the local planner slows down or halts before collisions\nAdaptive Motion: The local planner is used to adapt a global solution to dynamic conditions like keeping steady tool contact with an uneven surface\nThe diagram below depicts the fundamental plugin types and ROS interfaces that make up the Hybrid Planning architecture.\nThe architecture is structured in three ROS component nodes:\nHybrid Planning Manager\nProvides a ROS action for Hybrid Planning requests\nRuns the planning logic and coordinates the planners\nGlobal Planner\nSolves the global planning problem and publishes the solution trajectory\nLocal Planner\nProcesses incoming global trajectory updates\nSolves the local planning problem based on robot state, world and reference trajectory\nSends position/velocity commands to the robot driver\nThe architecture components are designed to be generic and highly customizable. Since the components only interact via ROS 2 message interfaces, it’s very easy to replace implementations of each of the architecture’s components or plugins. The plugin interfaces are designed to be minimal and to abstract from the actual algorithm implementation as much as possible. That allows the developer to fully focus on the isolated logic or solvers without having to implement any parts of the infrastructure. This also allows reusing the same components for different setups or planning problems.\nThis component is “The Brain” of the architecture. Its main purpose is to process HybridPlanner action  requests and to coordinate the motion planning and execution process based on the planning logic plugin. The planning logic is implemented in the PlanningLogic plugin and is event-driven by design. Events are defined by string identifiers and may trigger action calls or cancellations targeting the global or local planners. An example event log for a simple planning logic is shown in the diagram below:\nEvents are triggered by the Hybrid Planning action request and by both of the global and local planners action feedback messages. In this example, the Hybrid Planning Manager starts the global planner after the hybrid planning request is received. Upon arrival of the global trajectory the local planner is started and when the local planner is finished the Hybrid Planning Manager returns a Hybrid Planning response.\nA custom implementation of the Planning Logic plugin supports mapping generic events to available actions provided by the architecture like “Start global planning”, “Stop trajectory execution”, or “Switch to local planner constraint x”. With this, the motion planning behavior becomes highly customizable and adaptable.\nThe Global Planner is the simplest component of the architecture. It provides an action server that processes GlobalPlanner requests. These requests include common MotionPlanRequests, which are processed by the Global Planner plugin. By default, this is simply MoveIt’s planning pipeline, but any kind of planner or even MTC could technically be used here. The planning result is reported using the action feedback and the solution trajectory is published to the Local Planner for further processing.\nThe Local Planner also runs an action server that handles requests from the Hybrid Planning Manager. The action is used for starting and stopping execution and may also configure runtime parameters like constraints or solver types.\nThe local planner implementation is based on two plugins:\nTrajectory Operator: This plugin maintains the global reference trajectory, handles trajectory updates from the global planner, and monitors the process of the current robot state.\nLocal Constraint Solver: This plugin implements the iterative solver algorithm that produces the robot commands based on the reference trajectory and the local constraints. It may include additional interfaces for dynamically processing sensor input or event updates.\nThe diagram below shows an example loop cycle of the Local Planner upon action request by the Hybrid Planning Manager:\nEach iteration the local planner requests the current planning scene and matches the current robot state within the reference trajectory. If the goal is reached, the local planning action successfully finishes. Otherwise, the current local planning problem is identified based on the current robot state and solved afterwards. Finally, the resulting control commands are published to the robot controller.\nThe runtime behavior of a Hybrid Planner can best be understood by drawing a workflow diagram that visualizes the communication channels and events of the different components.\nBelow is a diagram that shows the runtime logic of a successful trajectory execution.\nThe planner is invoked by a hybrid planning request which is also the first event the Hybrid Planning Manager reacts to.\nIn this example, the planner logic simply runs both planners in sequence. After the initial hybrid planning request, the Hybrid Planning Manager invokes the global planner.\nThe global planner computes and publishes a trajectory which is received by the Hybrid Planning Manager and the Local Planner Component.\nImportant to notice is, that the Local Planner Component just processes the new trajectory and does not start executing until it is invoked by the Hybrid Planning Manager. Once requested by the Hybrid Planning Manager, the Local Planner Component starts unwinding the reference trajectory and returns the action response successfully when it reaches the final state. After that, the Hybrid Planning Manager returns a successful HybridPlanningResponse.\nNow let’s consider a more difficult scenario where the hybrid planner is designed to avoid an obstacle during execution by replanning.\nThe animation below shows a simple motion that is being fixed at runtime because of changing collision objects.\nHere, the collision object present during the global planning process disappears after the global trajectory is computed. Instead two new collision objects appear that invalidate the initial global trajectory. The local planner detects the imminent collision and pauses the execution until the global planner has provided an updated collision free trajectory.\nBelow you can see the workflow of the described behavior.\nThe startup is the same as in the first example, but during unwinding the reference trajectory the local planner detects a collision. Here, the planner logic reacts by re-invoking the global planner. During the calculation of the new global solution, the local planner must prevent the robot from colliding with the collision object i.e. by keeping its current position. After the global planner finishes its calculation, the new global solution is published to the local planner and the local planner’s Trajectory Operator Plugin blends the update into the reference trajectory. Afterwards, the Local Planner Component continues to follow the reference trajectory as the updated solution enables it to steer around the collision object.\nIf you want to use Hybrid Planning in your application or just want to experiment with it, check out theHybrid Planning Example Tutorial.\n© Copyright 2024, PickNik Robotics.\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0fa"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/hybrid_planning/hybrid_planning.html#what-is-hybrid-planning",
  "title": "Hybrid Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Hybrid Planning\nWhat is Hybrid Planning?\nThe Hybrid Planning Architecture\nMoveIt’s motion planning architecture follows the “Sense-Plan-Act” approach. In order for motions to get planned and executed, the environment and the robot state are first perceived (“Sense”), then the robot trajectory is computed by the planner (“Plan”) and finally executed in a single run using a trajectory controller (“Act”).\nWhile this solution works well for global motion planning in well-known static environments, the approach is not applicable in many real-world applications, in particular in unstable or dynamic environments. Tasks like serving a glass of water to a person on a table or writing on an uneven chalkboard require more sophisticated methods that allow reacting to unpredictable changes. For instance, the environment of the robot could change dynamically, or there could be certain uncertainties in the task itself, e.g. writing with chalk requires adapting the pressure on the board while the chalk also gets shorter by being used up.\nSolving these challenges requires a method that is able to adapt the executed motion to the immediate conditions or even to react by replanning in case of unforeseen changes in the environment. The Hybrid Planning architecture attempts to solve this problem by combining a pair of recurrent global and local planners.\nHybrid Planning is a term for a motion planning method that combines heterogeneous motion planners to produce more robust or reactive solutions. The general approach is already very established in the Navigation community and successfully implemented in popular projects like navigation2.\nMoveIt’s Hybrid Planning architecture combines a pair of global and local planners that run in parallel and recurrently with different planning speeds and problem scopes.\nThe global planner has the task to solve a global motion planning problem very akin to the planners used in a “Sense-Plan-Act” application. The used planner algorithm should be complete and is therefore assumed to be relatively slow in terms of computation time. Furthermore, the global planner is not required to be real-time safe, meaning that there are no guarantees that the planner finds a solution within a specific deadline. Depending on the planner implementation, the global planner may generate one initial solution or iteratively optimized solutions during the execution.\nThe local planner is running continuously during execution and produces iterative robot commands for following the global trajectory. In a way, the local planner resembles a controller, only that the architecture allows for solving more complicated problems and constraints. The idea is that the planner is able to reason about the world and to have an internal state. That property makes it very versatile and as it can be used for solving combinations of arbitrary local planning problems, as for example:\nUnwinding, blending, or splicing of subsequent global reference trajectories\nDynamically avoiding close-by collisions while following the global path\nAdapting the global trajectory to local constraints (e.g. desired force pressure on uneven surface, readjusting a tool based on visual feedback)\nLocal trajectory optimization and time parameterization (it is computationally cheaper and quicker to optimize a trajectory in a local environment)\nIn order to enable solving these local problems the local planner must be fast, able to react to sensor feedback and in many cases real-time safe. Also, it should be deterministic in order to avoid jerky or unpredictable motions.\nIn general, the local planner relies on the reference trajectory produced by the global planner so that it doesn’t get stuck in a local minimum. Since local minima sometimes still can’t be ruled out, it can be required that the global planner is triggered for a replan in order to still reach the desired goal. This behavior requires a certain way to communicate planner events and to process them accordingly. For that purpose the Hybrid Planning architecture allows implementing an event-based logic that can be customized for the specific use case and planner types.\nGlobal Planner\nLocal Planner\nSolve global solution trajectory\nOptimize trajectory path (continuously)\nFollow global reference trajectory\nSolve local problem constraints\nMay process sensor input\nOptimize solution locally\nCompute controller commands\nComplete\nNo restricted  computation time\nNot real-time safe\nNot necessarily deterministic\nCan get stuck in local minima\nLow computation time\nRealtime-safe (depends on solver)\nDeterministic\nOMPL planner\nSTOMP\nTrajOpt\nCartesian motion planner\nPilz Industrial Motion Planner\nMTC\nIK solver, Jacobian\nPotential field planner\nTrajectory optimization algorithm\nModel Predictive Control (MPC)\nSensor-based Optimal Control\nHybrid Planning can be useful in a broad range of use cases. Most of the applications can be grouped into the following three scenarios.\nOnline motion planning: The global planner creates an initial global solution and continuously optimizes it. Simultaneously, the local planner executes the reference trajectory and blends updated trajectory segments into it.\nReactive Motion: The global planner is used to fix invalidated solutions (replanning) while the local planner slows down or halts before collisions\nAdaptive Motion: The local planner is used to adapt a global solution to dynamic conditions like keeping steady tool contact with an uneven surface\nThe diagram below depicts the fundamental plugin types and ROS interfaces that make up the Hybrid Planning architecture.\nThe architecture is structured in three ROS component nodes:\nHybrid Planning Manager\nProvides a ROS action for Hybrid Planning requests\nRuns the planning logic and coordinates the planners\nGlobal Planner\nSolves the global planning problem and publishes the solution trajectory\nLocal Planner\nProcesses incoming global trajectory updates\nSolves the local planning problem based on robot state, world and reference trajectory\nSends position/velocity commands to the robot driver\nThe architecture components are designed to be generic and highly customizable. Since the components only interact via ROS 2 message interfaces, it’s very easy to replace implementations of each of the architecture’s components or plugins. The plugin interfaces are designed to be minimal and to abstract from the actual algorithm implementation as much as possible. That allows the developer to fully focus on the isolated logic or solvers without having to implement any parts of the infrastructure. This also allows reusing the same components for different setups or planning problems.\nThis component is “The Brain” of the architecture. Its main purpose is to process HybridPlanner action  requests and to coordinate the motion planning and execution process based on the planning logic plugin. The planning logic is implemented in the PlanningLogic plugin and is event-driven by design. Events are defined by string identifiers and may trigger action calls or cancellations targeting the global or local planners. An example event log for a simple planning logic is shown in the diagram below:\nEvents are triggered by the Hybrid Planning action request and by both of the global and local planners action feedback messages. In this example, the Hybrid Planning Manager starts the global planner after the hybrid planning request is received. Upon arrival of the global trajectory the local planner is started and when the local planner is finished the Hybrid Planning Manager returns a Hybrid Planning response.\nA custom implementation of the Planning Logic plugin supports mapping generic events to available actions provided by the architecture like “Start global planning”, “Stop trajectory execution”, or “Switch to local planner constraint x”. With this, the motion planning behavior becomes highly customizable and adaptable.\nThe Global Planner is the simplest component of the architecture. It provides an action server that processes GlobalPlanner requests. These requests include common MotionPlanRequests, which are processed by the Global Planner plugin. By default, this is simply MoveIt’s planning pipeline, but any kind of planner or even MTC could technically be used here. The planning result is reported using the action feedback and the solution trajectory is published to the Local Planner for further processing.\nThe Local Planner also runs an action server that handles requests from the Hybrid Planning Manager. The action is used for starting and stopping execution and may also configure runtime parameters like constraints or solver types.\nThe local planner implementation is based on two plugins:\nTrajectory Operator: This plugin maintains the global reference trajectory, handles trajectory updates from the global planner, and monitors the process of the current robot state.\nLocal Constraint Solver: This plugin implements the iterative solver algorithm that produces the robot commands based on the reference trajectory and the local constraints. It may include additional interfaces for dynamically processing sensor input or event updates.\nThe diagram below shows an example loop cycle of the Local Planner upon action request by the Hybrid Planning Manager:\nEach iteration the local planner requests the current planning scene and matches the current robot state within the reference trajectory. If the goal is reached, the local planning action successfully finishes. Otherwise, the current local planning problem is identified based on the current robot state and solved afterwards. Finally, the resulting control commands are published to the robot controller.\nThe runtime behavior of a Hybrid Planner can best be understood by drawing a workflow diagram that visualizes the communication channels and events of the different components.\nBelow is a diagram that shows the runtime logic of a successful trajectory execution.\nThe planner is invoked by a hybrid planning request which is also the first event the Hybrid Planning Manager reacts to.\nIn this example, the planner logic simply runs both planners in sequence. After the initial hybrid planning request, the Hybrid Planning Manager invokes the global planner.\nThe global planner computes and publishes a trajectory which is received by the Hybrid Planning Manager and the Local Planner Component.\nImportant to notice is, that the Local Planner Component just processes the new trajectory and does not start executing until it is invoked by the Hybrid Planning Manager. Once requested by the Hybrid Planning Manager, the Local Planner Component starts unwinding the reference trajectory and returns the action response successfully when it reaches the final state. After that, the Hybrid Planning Manager returns a successful HybridPlanningResponse.\nNow let’s consider a more difficult scenario where the hybrid planner is designed to avoid an obstacle during execution by replanning.\nThe animation below shows a simple motion that is being fixed at runtime because of changing collision objects.\nHere, the collision object present during the global planning process disappears after the global trajectory is computed. Instead two new collision objects appear that invalidate the initial global trajectory. The local planner detects the imminent collision and pauses the execution until the global planner has provided an updated collision free trajectory.\nBelow you can see the workflow of the described behavior.\nThe startup is the same as in the first example, but during unwinding the reference trajectory the local planner detects a collision. Here, the planner logic reacts by re-invoking the global planner. During the calculation of the new global solution, the local planner must prevent the robot from colliding with the collision object i.e. by keeping its current position. After the global planner finishes its calculation, the new global solution is published to the local planner and the local planner’s Trajectory Operator Plugin blends the update into the reference trajectory. Afterwards, the Local Planner Component continues to follow the reference trajectory as the updated solution enables it to steer around the collision object.\nIf you want to use Hybrid Planning in your application or just want to experiment with it, check out theHybrid Planning Example Tutorial.\n© Copyright 2024, PickNik Robotics.\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0fb"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/hybrid_planning/hybrid_planning.html#the-hybrid-planning-architecture",
  "title": "Hybrid Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Hybrid Planning\nWhat is Hybrid Planning?\nThe Hybrid Planning Architecture\nMoveIt’s motion planning architecture follows the “Sense-Plan-Act” approach. In order for motions to get planned and executed, the environment and the robot state are first perceived (“Sense”), then the robot trajectory is computed by the planner (“Plan”) and finally executed in a single run using a trajectory controller (“Act”).\nWhile this solution works well for global motion planning in well-known static environments, the approach is not applicable in many real-world applications, in particular in unstable or dynamic environments. Tasks like serving a glass of water to a person on a table or writing on an uneven chalkboard require more sophisticated methods that allow reacting to unpredictable changes. For instance, the environment of the robot could change dynamically, or there could be certain uncertainties in the task itself, e.g. writing with chalk requires adapting the pressure on the board while the chalk also gets shorter by being used up.\nSolving these challenges requires a method that is able to adapt the executed motion to the immediate conditions or even to react by replanning in case of unforeseen changes in the environment. The Hybrid Planning architecture attempts to solve this problem by combining a pair of recurrent global and local planners.\nHybrid Planning is a term for a motion planning method that combines heterogeneous motion planners to produce more robust or reactive solutions. The general approach is already very established in the Navigation community and successfully implemented in popular projects like navigation2.\nMoveIt’s Hybrid Planning architecture combines a pair of global and local planners that run in parallel and recurrently with different planning speeds and problem scopes.\nThe global planner has the task to solve a global motion planning problem very akin to the planners used in a “Sense-Plan-Act” application. The used planner algorithm should be complete and is therefore assumed to be relatively slow in terms of computation time. Furthermore, the global planner is not required to be real-time safe, meaning that there are no guarantees that the planner finds a solution within a specific deadline. Depending on the planner implementation, the global planner may generate one initial solution or iteratively optimized solutions during the execution.\nThe local planner is running continuously during execution and produces iterative robot commands for following the global trajectory. In a way, the local planner resembles a controller, only that the architecture allows for solving more complicated problems and constraints. The idea is that the planner is able to reason about the world and to have an internal state. That property makes it very versatile and as it can be used for solving combinations of arbitrary local planning problems, as for example:\nUnwinding, blending, or splicing of subsequent global reference trajectories\nDynamically avoiding close-by collisions while following the global path\nAdapting the global trajectory to local constraints (e.g. desired force pressure on uneven surface, readjusting a tool based on visual feedback)\nLocal trajectory optimization and time parameterization (it is computationally cheaper and quicker to optimize a trajectory in a local environment)\nIn order to enable solving these local problems the local planner must be fast, able to react to sensor feedback and in many cases real-time safe. Also, it should be deterministic in order to avoid jerky or unpredictable motions.\nIn general, the local planner relies on the reference trajectory produced by the global planner so that it doesn’t get stuck in a local minimum. Since local minima sometimes still can’t be ruled out, it can be required that the global planner is triggered for a replan in order to still reach the desired goal. This behavior requires a certain way to communicate planner events and to process them accordingly. For that purpose the Hybrid Planning architecture allows implementing an event-based logic that can be customized for the specific use case and planner types.\nGlobal Planner\nLocal Planner\nSolve global solution trajectory\nOptimize trajectory path (continuously)\nFollow global reference trajectory\nSolve local problem constraints\nMay process sensor input\nOptimize solution locally\nCompute controller commands\nComplete\nNo restricted  computation time\nNot real-time safe\nNot necessarily deterministic\nCan get stuck in local minima\nLow computation time\nRealtime-safe (depends on solver)\nDeterministic\nOMPL planner\nSTOMP\nTrajOpt\nCartesian motion planner\nPilz Industrial Motion Planner\nMTC\nIK solver, Jacobian\nPotential field planner\nTrajectory optimization algorithm\nModel Predictive Control (MPC)\nSensor-based Optimal Control\nHybrid Planning can be useful in a broad range of use cases. Most of the applications can be grouped into the following three scenarios.\nOnline motion planning: The global planner creates an initial global solution and continuously optimizes it. Simultaneously, the local planner executes the reference trajectory and blends updated trajectory segments into it.\nReactive Motion: The global planner is used to fix invalidated solutions (replanning) while the local planner slows down or halts before collisions\nAdaptive Motion: The local planner is used to adapt a global solution to dynamic conditions like keeping steady tool contact with an uneven surface\nThe diagram below depicts the fundamental plugin types and ROS interfaces that make up the Hybrid Planning architecture.\nThe architecture is structured in three ROS component nodes:\nHybrid Planning Manager\nProvides a ROS action for Hybrid Planning requests\nRuns the planning logic and coordinates the planners\nGlobal Planner\nSolves the global planning problem and publishes the solution trajectory\nLocal Planner\nProcesses incoming global trajectory updates\nSolves the local planning problem based on robot state, world and reference trajectory\nSends position/velocity commands to the robot driver\nThe architecture components are designed to be generic and highly customizable. Since the components only interact via ROS 2 message interfaces, it’s very easy to replace implementations of each of the architecture’s components or plugins. The plugin interfaces are designed to be minimal and to abstract from the actual algorithm implementation as much as possible. That allows the developer to fully focus on the isolated logic or solvers without having to implement any parts of the infrastructure. This also allows reusing the same components for different setups or planning problems.\nThis component is “The Brain” of the architecture. Its main purpose is to process HybridPlanner action  requests and to coordinate the motion planning and execution process based on the planning logic plugin. The planning logic is implemented in the PlanningLogic plugin and is event-driven by design. Events are defined by string identifiers and may trigger action calls or cancellations targeting the global or local planners. An example event log for a simple planning logic is shown in the diagram below:\nEvents are triggered by the Hybrid Planning action request and by both of the global and local planners action feedback messages. In this example, the Hybrid Planning Manager starts the global planner after the hybrid planning request is received. Upon arrival of the global trajectory the local planner is started and when the local planner is finished the Hybrid Planning Manager returns a Hybrid Planning response.\nA custom implementation of the Planning Logic plugin supports mapping generic events to available actions provided by the architecture like “Start global planning”, “Stop trajectory execution”, or “Switch to local planner constraint x”. With this, the motion planning behavior becomes highly customizable and adaptable.\nThe Global Planner is the simplest component of the architecture. It provides an action server that processes GlobalPlanner requests. These requests include common MotionPlanRequests, which are processed by the Global Planner plugin. By default, this is simply MoveIt’s planning pipeline, but any kind of planner or even MTC could technically be used here. The planning result is reported using the action feedback and the solution trajectory is published to the Local Planner for further processing.\nThe Local Planner also runs an action server that handles requests from the Hybrid Planning Manager. The action is used for starting and stopping execution and may also configure runtime parameters like constraints or solver types.\nThe local planner implementation is based on two plugins:\nTrajectory Operator: This plugin maintains the global reference trajectory, handles trajectory updates from the global planner, and monitors the process of the current robot state.\nLocal Constraint Solver: This plugin implements the iterative solver algorithm that produces the robot commands based on the reference trajectory and the local constraints. It may include additional interfaces for dynamically processing sensor input or event updates.\nThe diagram below shows an example loop cycle of the Local Planner upon action request by the Hybrid Planning Manager:\nEach iteration the local planner requests the current planning scene and matches the current robot state within the reference trajectory. If the goal is reached, the local planning action successfully finishes. Otherwise, the current local planning problem is identified based on the current robot state and solved afterwards. Finally, the resulting control commands are published to the robot controller.\nThe runtime behavior of a Hybrid Planner can best be understood by drawing a workflow diagram that visualizes the communication channels and events of the different components.\nBelow is a diagram that shows the runtime logic of a successful trajectory execution.\nThe planner is invoked by a hybrid planning request which is also the first event the Hybrid Planning Manager reacts to.\nIn this example, the planner logic simply runs both planners in sequence. After the initial hybrid planning request, the Hybrid Planning Manager invokes the global planner.\nThe global planner computes and publishes a trajectory which is received by the Hybrid Planning Manager and the Local Planner Component.\nImportant to notice is, that the Local Planner Component just processes the new trajectory and does not start executing until it is invoked by the Hybrid Planning Manager. Once requested by the Hybrid Planning Manager, the Local Planner Component starts unwinding the reference trajectory and returns the action response successfully when it reaches the final state. After that, the Hybrid Planning Manager returns a successful HybridPlanningResponse.\nNow let’s consider a more difficult scenario where the hybrid planner is designed to avoid an obstacle during execution by replanning.\nThe animation below shows a simple motion that is being fixed at runtime because of changing collision objects.\nHere, the collision object present during the global planning process disappears after the global trajectory is computed. Instead two new collision objects appear that invalidate the initial global trajectory. The local planner detects the imminent collision and pauses the execution until the global planner has provided an updated collision free trajectory.\nBelow you can see the workflow of the described behavior.\nThe startup is the same as in the first example, but during unwinding the reference trajectory the local planner detects a collision. Here, the planner logic reacts by re-invoking the global planner. During the calculation of the new global solution, the local planner must prevent the robot from colliding with the collision object i.e. by keeping its current position. After the global planner finishes its calculation, the new global solution is published to the local planner and the local planner’s Trajectory Operator Plugin blends the update into the reference trajectory. Afterwards, the Local Planner Component continues to follow the reference trajectory as the updated solution enables it to steer around the collision object.\nIf you want to use Hybrid Planning in your application or just want to experiment with it, check out theHybrid Planning Example Tutorial.\n© Copyright 2024, PickNik Robotics.\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0fc"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/hybrid_planning/hybrid_planning.html#hybrid-planning-manager",
  "title": "Hybrid Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Hybrid Planning\nWhat is Hybrid Planning?\nThe Hybrid Planning Architecture\nMoveIt’s motion planning architecture follows the “Sense-Plan-Act” approach. In order for motions to get planned and executed, the environment and the robot state are first perceived (“Sense”), then the robot trajectory is computed by the planner (“Plan”) and finally executed in a single run using a trajectory controller (“Act”).\nWhile this solution works well for global motion planning in well-known static environments, the approach is not applicable in many real-world applications, in particular in unstable or dynamic environments. Tasks like serving a glass of water to a person on a table or writing on an uneven chalkboard require more sophisticated methods that allow reacting to unpredictable changes. For instance, the environment of the robot could change dynamically, or there could be certain uncertainties in the task itself, e.g. writing with chalk requires adapting the pressure on the board while the chalk also gets shorter by being used up.\nSolving these challenges requires a method that is able to adapt the executed motion to the immediate conditions or even to react by replanning in case of unforeseen changes in the environment. The Hybrid Planning architecture attempts to solve this problem by combining a pair of recurrent global and local planners.\nHybrid Planning is a term for a motion planning method that combines heterogeneous motion planners to produce more robust or reactive solutions. The general approach is already very established in the Navigation community and successfully implemented in popular projects like navigation2.\nMoveIt’s Hybrid Planning architecture combines a pair of global and local planners that run in parallel and recurrently with different planning speeds and problem scopes.\nThe global planner has the task to solve a global motion planning problem very akin to the planners used in a “Sense-Plan-Act” application. The used planner algorithm should be complete and is therefore assumed to be relatively slow in terms of computation time. Furthermore, the global planner is not required to be real-time safe, meaning that there are no guarantees that the planner finds a solution within a specific deadline. Depending on the planner implementation, the global planner may generate one initial solution or iteratively optimized solutions during the execution.\nThe local planner is running continuously during execution and produces iterative robot commands for following the global trajectory. In a way, the local planner resembles a controller, only that the architecture allows for solving more complicated problems and constraints. The idea is that the planner is able to reason about the world and to have an internal state. That property makes it very versatile and as it can be used for solving combinations of arbitrary local planning problems, as for example:\nUnwinding, blending, or splicing of subsequent global reference trajectories\nDynamically avoiding close-by collisions while following the global path\nAdapting the global trajectory to local constraints (e.g. desired force pressure on uneven surface, readjusting a tool based on visual feedback)\nLocal trajectory optimization and time parameterization (it is computationally cheaper and quicker to optimize a trajectory in a local environment)\nIn order to enable solving these local problems the local planner must be fast, able to react to sensor feedback and in many cases real-time safe. Also, it should be deterministic in order to avoid jerky or unpredictable motions.\nIn general, the local planner relies on the reference trajectory produced by the global planner so that it doesn’t get stuck in a local minimum. Since local minima sometimes still can’t be ruled out, it can be required that the global planner is triggered for a replan in order to still reach the desired goal. This behavior requires a certain way to communicate planner events and to process them accordingly. For that purpose the Hybrid Planning architecture allows implementing an event-based logic that can be customized for the specific use case and planner types.\nGlobal Planner\nLocal Planner\nSolve global solution trajectory\nOptimize trajectory path (continuously)\nFollow global reference trajectory\nSolve local problem constraints\nMay process sensor input\nOptimize solution locally\nCompute controller commands\nComplete\nNo restricted  computation time\nNot real-time safe\nNot necessarily deterministic\nCan get stuck in local minima\nLow computation time\nRealtime-safe (depends on solver)\nDeterministic\nOMPL planner\nSTOMP\nTrajOpt\nCartesian motion planner\nPilz Industrial Motion Planner\nMTC\nIK solver, Jacobian\nPotential field planner\nTrajectory optimization algorithm\nModel Predictive Control (MPC)\nSensor-based Optimal Control\nHybrid Planning can be useful in a broad range of use cases. Most of the applications can be grouped into the following three scenarios.\nOnline motion planning: The global planner creates an initial global solution and continuously optimizes it. Simultaneously, the local planner executes the reference trajectory and blends updated trajectory segments into it.\nReactive Motion: The global planner is used to fix invalidated solutions (replanning) while the local planner slows down or halts before collisions\nAdaptive Motion: The local planner is used to adapt a global solution to dynamic conditions like keeping steady tool contact with an uneven surface\nThe diagram below depicts the fundamental plugin types and ROS interfaces that make up the Hybrid Planning architecture.\nThe architecture is structured in three ROS component nodes:\nHybrid Planning Manager\nProvides a ROS action for Hybrid Planning requests\nRuns the planning logic and coordinates the planners\nGlobal Planner\nSolves the global planning problem and publishes the solution trajectory\nLocal Planner\nProcesses incoming global trajectory updates\nSolves the local planning problem based on robot state, world and reference trajectory\nSends position/velocity commands to the robot driver\nThe architecture components are designed to be generic and highly customizable. Since the components only interact via ROS 2 message interfaces, it’s very easy to replace implementations of each of the architecture’s components or plugins. The plugin interfaces are designed to be minimal and to abstract from the actual algorithm implementation as much as possible. That allows the developer to fully focus on the isolated logic or solvers without having to implement any parts of the infrastructure. This also allows reusing the same components for different setups or planning problems.\nThis component is “The Brain” of the architecture. Its main purpose is to process HybridPlanner action  requests and to coordinate the motion planning and execution process based on the planning logic plugin. The planning logic is implemented in the PlanningLogic plugin and is event-driven by design. Events are defined by string identifiers and may trigger action calls or cancellations targeting the global or local planners. An example event log for a simple planning logic is shown in the diagram below:\nEvents are triggered by the Hybrid Planning action request and by both of the global and local planners action feedback messages. In this example, the Hybrid Planning Manager starts the global planner after the hybrid planning request is received. Upon arrival of the global trajectory the local planner is started and when the local planner is finished the Hybrid Planning Manager returns a Hybrid Planning response.\nA custom implementation of the Planning Logic plugin supports mapping generic events to available actions provided by the architecture like “Start global planning”, “Stop trajectory execution”, or “Switch to local planner constraint x”. With this, the motion planning behavior becomes highly customizable and adaptable.\nThe Global Planner is the simplest component of the architecture. It provides an action server that processes GlobalPlanner requests. These requests include common MotionPlanRequests, which are processed by the Global Planner plugin. By default, this is simply MoveIt’s planning pipeline, but any kind of planner or even MTC could technically be used here. The planning result is reported using the action feedback and the solution trajectory is published to the Local Planner for further processing.\nThe Local Planner also runs an action server that handles requests from the Hybrid Planning Manager. The action is used for starting and stopping execution and may also configure runtime parameters like constraints or solver types.\nThe local planner implementation is based on two plugins:\nTrajectory Operator: This plugin maintains the global reference trajectory, handles trajectory updates from the global planner, and monitors the process of the current robot state.\nLocal Constraint Solver: This plugin implements the iterative solver algorithm that produces the robot commands based on the reference trajectory and the local constraints. It may include additional interfaces for dynamically processing sensor input or event updates.\nThe diagram below shows an example loop cycle of the Local Planner upon action request by the Hybrid Planning Manager:\nEach iteration the local planner requests the current planning scene and matches the current robot state within the reference trajectory. If the goal is reached, the local planning action successfully finishes. Otherwise, the current local planning problem is identified based on the current robot state and solved afterwards. Finally, the resulting control commands are published to the robot controller.\nThe runtime behavior of a Hybrid Planner can best be understood by drawing a workflow diagram that visualizes the communication channels and events of the different components.\nBelow is a diagram that shows the runtime logic of a successful trajectory execution.\nThe planner is invoked by a hybrid planning request which is also the first event the Hybrid Planning Manager reacts to.\nIn this example, the planner logic simply runs both planners in sequence. After the initial hybrid planning request, the Hybrid Planning Manager invokes the global planner.\nThe global planner computes and publishes a trajectory which is received by the Hybrid Planning Manager and the Local Planner Component.\nImportant to notice is, that the Local Planner Component just processes the new trajectory and does not start executing until it is invoked by the Hybrid Planning Manager. Once requested by the Hybrid Planning Manager, the Local Planner Component starts unwinding the reference trajectory and returns the action response successfully when it reaches the final state. After that, the Hybrid Planning Manager returns a successful HybridPlanningResponse.\nNow let’s consider a more difficult scenario where the hybrid planner is designed to avoid an obstacle during execution by replanning.\nThe animation below shows a simple motion that is being fixed at runtime because of changing collision objects.\nHere, the collision object present during the global planning process disappears after the global trajectory is computed. Instead two new collision objects appear that invalidate the initial global trajectory. The local planner detects the imminent collision and pauses the execution until the global planner has provided an updated collision free trajectory.\nBelow you can see the workflow of the described behavior.\nThe startup is the same as in the first example, but during unwinding the reference trajectory the local planner detects a collision. Here, the planner logic reacts by re-invoking the global planner. During the calculation of the new global solution, the local planner must prevent the robot from colliding with the collision object i.e. by keeping its current position. After the global planner finishes its calculation, the new global solution is published to the local planner and the local planner’s Trajectory Operator Plugin blends the update into the reference trajectory. Afterwards, the Local Planner Component continues to follow the reference trajectory as the updated solution enables it to steer around the collision object.\nIf you want to use Hybrid Planning in your application or just want to experiment with it, check out theHybrid Planning Example Tutorial.\n© Copyright 2024, PickNik Robotics.\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0fd"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/hybrid_planning/hybrid_planning.html#global-planner",
  "title": "Hybrid Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Hybrid Planning\nWhat is Hybrid Planning?\nThe Hybrid Planning Architecture\nMoveIt’s motion planning architecture follows the “Sense-Plan-Act” approach. In order for motions to get planned and executed, the environment and the robot state are first perceived (“Sense”), then the robot trajectory is computed by the planner (“Plan”) and finally executed in a single run using a trajectory controller (“Act”).\nWhile this solution works well for global motion planning in well-known static environments, the approach is not applicable in many real-world applications, in particular in unstable or dynamic environments. Tasks like serving a glass of water to a person on a table or writing on an uneven chalkboard require more sophisticated methods that allow reacting to unpredictable changes. For instance, the environment of the robot could change dynamically, or there could be certain uncertainties in the task itself, e.g. writing with chalk requires adapting the pressure on the board while the chalk also gets shorter by being used up.\nSolving these challenges requires a method that is able to adapt the executed motion to the immediate conditions or even to react by replanning in case of unforeseen changes in the environment. The Hybrid Planning architecture attempts to solve this problem by combining a pair of recurrent global and local planners.\nHybrid Planning is a term for a motion planning method that combines heterogeneous motion planners to produce more robust or reactive solutions. The general approach is already very established in the Navigation community and successfully implemented in popular projects like navigation2.\nMoveIt’s Hybrid Planning architecture combines a pair of global and local planners that run in parallel and recurrently with different planning speeds and problem scopes.\nThe global planner has the task to solve a global motion planning problem very akin to the planners used in a “Sense-Plan-Act” application. The used planner algorithm should be complete and is therefore assumed to be relatively slow in terms of computation time. Furthermore, the global planner is not required to be real-time safe, meaning that there are no guarantees that the planner finds a solution within a specific deadline. Depending on the planner implementation, the global planner may generate one initial solution or iteratively optimized solutions during the execution.\nThe local planner is running continuously during execution and produces iterative robot commands for following the global trajectory. In a way, the local planner resembles a controller, only that the architecture allows for solving more complicated problems and constraints. The idea is that the planner is able to reason about the world and to have an internal state. That property makes it very versatile and as it can be used for solving combinations of arbitrary local planning problems, as for example:\nUnwinding, blending, or splicing of subsequent global reference trajectories\nDynamically avoiding close-by collisions while following the global path\nAdapting the global trajectory to local constraints (e.g. desired force pressure on uneven surface, readjusting a tool based on visual feedback)\nLocal trajectory optimization and time parameterization (it is computationally cheaper and quicker to optimize a trajectory in a local environment)\nIn order to enable solving these local problems the local planner must be fast, able to react to sensor feedback and in many cases real-time safe. Also, it should be deterministic in order to avoid jerky or unpredictable motions.\nIn general, the local planner relies on the reference trajectory produced by the global planner so that it doesn’t get stuck in a local minimum. Since local minima sometimes still can’t be ruled out, it can be required that the global planner is triggered for a replan in order to still reach the desired goal. This behavior requires a certain way to communicate planner events and to process them accordingly. For that purpose the Hybrid Planning architecture allows implementing an event-based logic that can be customized for the specific use case and planner types.\nGlobal Planner\nLocal Planner\nSolve global solution trajectory\nOptimize trajectory path (continuously)\nFollow global reference trajectory\nSolve local problem constraints\nMay process sensor input\nOptimize solution locally\nCompute controller commands\nComplete\nNo restricted  computation time\nNot real-time safe\nNot necessarily deterministic\nCan get stuck in local minima\nLow computation time\nRealtime-safe (depends on solver)\nDeterministic\nOMPL planner\nSTOMP\nTrajOpt\nCartesian motion planner\nPilz Industrial Motion Planner\nMTC\nIK solver, Jacobian\nPotential field planner\nTrajectory optimization algorithm\nModel Predictive Control (MPC)\nSensor-based Optimal Control\nHybrid Planning can be useful in a broad range of use cases. Most of the applications can be grouped into the following three scenarios.\nOnline motion planning: The global planner creates an initial global solution and continuously optimizes it. Simultaneously, the local planner executes the reference trajectory and blends updated trajectory segments into it.\nReactive Motion: The global planner is used to fix invalidated solutions (replanning) while the local planner slows down or halts before collisions\nAdaptive Motion: The local planner is used to adapt a global solution to dynamic conditions like keeping steady tool contact with an uneven surface\nThe diagram below depicts the fundamental plugin types and ROS interfaces that make up the Hybrid Planning architecture.\nThe architecture is structured in three ROS component nodes:\nHybrid Planning Manager\nProvides a ROS action for Hybrid Planning requests\nRuns the planning logic and coordinates the planners\nGlobal Planner\nSolves the global planning problem and publishes the solution trajectory\nLocal Planner\nProcesses incoming global trajectory updates\nSolves the local planning problem based on robot state, world and reference trajectory\nSends position/velocity commands to the robot driver\nThe architecture components are designed to be generic and highly customizable. Since the components only interact via ROS 2 message interfaces, it’s very easy to replace implementations of each of the architecture’s components or plugins. The plugin interfaces are designed to be minimal and to abstract from the actual algorithm implementation as much as possible. That allows the developer to fully focus on the isolated logic or solvers without having to implement any parts of the infrastructure. This also allows reusing the same components for different setups or planning problems.\nThis component is “The Brain” of the architecture. Its main purpose is to process HybridPlanner action  requests and to coordinate the motion planning and execution process based on the planning logic plugin. The planning logic is implemented in the PlanningLogic plugin and is event-driven by design. Events are defined by string identifiers and may trigger action calls or cancellations targeting the global or local planners. An example event log for a simple planning logic is shown in the diagram below:\nEvents are triggered by the Hybrid Planning action request and by both of the global and local planners action feedback messages. In this example, the Hybrid Planning Manager starts the global planner after the hybrid planning request is received. Upon arrival of the global trajectory the local planner is started and when the local planner is finished the Hybrid Planning Manager returns a Hybrid Planning response.\nA custom implementation of the Planning Logic plugin supports mapping generic events to available actions provided by the architecture like “Start global planning”, “Stop trajectory execution”, or “Switch to local planner constraint x”. With this, the motion planning behavior becomes highly customizable and adaptable.\nThe Global Planner is the simplest component of the architecture. It provides an action server that processes GlobalPlanner requests. These requests include common MotionPlanRequests, which are processed by the Global Planner plugin. By default, this is simply MoveIt’s planning pipeline, but any kind of planner or even MTC could technically be used here. The planning result is reported using the action feedback and the solution trajectory is published to the Local Planner for further processing.\nThe Local Planner also runs an action server that handles requests from the Hybrid Planning Manager. The action is used for starting and stopping execution and may also configure runtime parameters like constraints or solver types.\nThe local planner implementation is based on two plugins:\nTrajectory Operator: This plugin maintains the global reference trajectory, handles trajectory updates from the global planner, and monitors the process of the current robot state.\nLocal Constraint Solver: This plugin implements the iterative solver algorithm that produces the robot commands based on the reference trajectory and the local constraints. It may include additional interfaces for dynamically processing sensor input or event updates.\nThe diagram below shows an example loop cycle of the Local Planner upon action request by the Hybrid Planning Manager:\nEach iteration the local planner requests the current planning scene and matches the current robot state within the reference trajectory. If the goal is reached, the local planning action successfully finishes. Otherwise, the current local planning problem is identified based on the current robot state and solved afterwards. Finally, the resulting control commands are published to the robot controller.\nThe runtime behavior of a Hybrid Planner can best be understood by drawing a workflow diagram that visualizes the communication channels and events of the different components.\nBelow is a diagram that shows the runtime logic of a successful trajectory execution.\nThe planner is invoked by a hybrid planning request which is also the first event the Hybrid Planning Manager reacts to.\nIn this example, the planner logic simply runs both planners in sequence. After the initial hybrid planning request, the Hybrid Planning Manager invokes the global planner.\nThe global planner computes and publishes a trajectory which is received by the Hybrid Planning Manager and the Local Planner Component.\nImportant to notice is, that the Local Planner Component just processes the new trajectory and does not start executing until it is invoked by the Hybrid Planning Manager. Once requested by the Hybrid Planning Manager, the Local Planner Component starts unwinding the reference trajectory and returns the action response successfully when it reaches the final state. After that, the Hybrid Planning Manager returns a successful HybridPlanningResponse.\nNow let’s consider a more difficult scenario where the hybrid planner is designed to avoid an obstacle during execution by replanning.\nThe animation below shows a simple motion that is being fixed at runtime because of changing collision objects.\nHere, the collision object present during the global planning process disappears after the global trajectory is computed. Instead two new collision objects appear that invalidate the initial global trajectory. The local planner detects the imminent collision and pauses the execution until the global planner has provided an updated collision free trajectory.\nBelow you can see the workflow of the described behavior.\nThe startup is the same as in the first example, but during unwinding the reference trajectory the local planner detects a collision. Here, the planner logic reacts by re-invoking the global planner. During the calculation of the new global solution, the local planner must prevent the robot from colliding with the collision object i.e. by keeping its current position. After the global planner finishes its calculation, the new global solution is published to the local planner and the local planner’s Trajectory Operator Plugin blends the update into the reference trajectory. Afterwards, the Local Planner Component continues to follow the reference trajectory as the updated solution enables it to steer around the collision object.\nIf you want to use Hybrid Planning in your application or just want to experiment with it, check out theHybrid Planning Example Tutorial.\n© Copyright 2024, PickNik Robotics.\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0fe"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/hybrid_planning/hybrid_planning.html#local-planner",
  "title": "Hybrid Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Hybrid Planning\nWhat is Hybrid Planning?\nThe Hybrid Planning Architecture\nMoveIt’s motion planning architecture follows the “Sense-Plan-Act” approach. In order for motions to get planned and executed, the environment and the robot state are first perceived (“Sense”), then the robot trajectory is computed by the planner (“Plan”) and finally executed in a single run using a trajectory controller (“Act”).\nWhile this solution works well for global motion planning in well-known static environments, the approach is not applicable in many real-world applications, in particular in unstable or dynamic environments. Tasks like serving a glass of water to a person on a table or writing on an uneven chalkboard require more sophisticated methods that allow reacting to unpredictable changes. For instance, the environment of the robot could change dynamically, or there could be certain uncertainties in the task itself, e.g. writing with chalk requires adapting the pressure on the board while the chalk also gets shorter by being used up.\nSolving these challenges requires a method that is able to adapt the executed motion to the immediate conditions or even to react by replanning in case of unforeseen changes in the environment. The Hybrid Planning architecture attempts to solve this problem by combining a pair of recurrent global and local planners.\nHybrid Planning is a term for a motion planning method that combines heterogeneous motion planners to produce more robust or reactive solutions. The general approach is already very established in the Navigation community and successfully implemented in popular projects like navigation2.\nMoveIt’s Hybrid Planning architecture combines a pair of global and local planners that run in parallel and recurrently with different planning speeds and problem scopes.\nThe global planner has the task to solve a global motion planning problem very akin to the planners used in a “Sense-Plan-Act” application. The used planner algorithm should be complete and is therefore assumed to be relatively slow in terms of computation time. Furthermore, the global planner is not required to be real-time safe, meaning that there are no guarantees that the planner finds a solution within a specific deadline. Depending on the planner implementation, the global planner may generate one initial solution or iteratively optimized solutions during the execution.\nThe local planner is running continuously during execution and produces iterative robot commands for following the global trajectory. In a way, the local planner resembles a controller, only that the architecture allows for solving more complicated problems and constraints. The idea is that the planner is able to reason about the world and to have an internal state. That property makes it very versatile and as it can be used for solving combinations of arbitrary local planning problems, as for example:\nUnwinding, blending, or splicing of subsequent global reference trajectories\nDynamically avoiding close-by collisions while following the global path\nAdapting the global trajectory to local constraints (e.g. desired force pressure on uneven surface, readjusting a tool based on visual feedback)\nLocal trajectory optimization and time parameterization (it is computationally cheaper and quicker to optimize a trajectory in a local environment)\nIn order to enable solving these local problems the local planner must be fast, able to react to sensor feedback and in many cases real-time safe. Also, it should be deterministic in order to avoid jerky or unpredictable motions.\nIn general, the local planner relies on the reference trajectory produced by the global planner so that it doesn’t get stuck in a local minimum. Since local minima sometimes still can’t be ruled out, it can be required that the global planner is triggered for a replan in order to still reach the desired goal. This behavior requires a certain way to communicate planner events and to process them accordingly. For that purpose the Hybrid Planning architecture allows implementing an event-based logic that can be customized for the specific use case and planner types.\nGlobal Planner\nLocal Planner\nSolve global solution trajectory\nOptimize trajectory path (continuously)\nFollow global reference trajectory\nSolve local problem constraints\nMay process sensor input\nOptimize solution locally\nCompute controller commands\nComplete\nNo restricted  computation time\nNot real-time safe\nNot necessarily deterministic\nCan get stuck in local minima\nLow computation time\nRealtime-safe (depends on solver)\nDeterministic\nOMPL planner\nSTOMP\nTrajOpt\nCartesian motion planner\nPilz Industrial Motion Planner\nMTC\nIK solver, Jacobian\nPotential field planner\nTrajectory optimization algorithm\nModel Predictive Control (MPC)\nSensor-based Optimal Control\nHybrid Planning can be useful in a broad range of use cases. Most of the applications can be grouped into the following three scenarios.\nOnline motion planning: The global planner creates an initial global solution and continuously optimizes it. Simultaneously, the local planner executes the reference trajectory and blends updated trajectory segments into it.\nReactive Motion: The global planner is used to fix invalidated solutions (replanning) while the local planner slows down or halts before collisions\nAdaptive Motion: The local planner is used to adapt a global solution to dynamic conditions like keeping steady tool contact with an uneven surface\nThe diagram below depicts the fundamental plugin types and ROS interfaces that make up the Hybrid Planning architecture.\nThe architecture is structured in three ROS component nodes:\nHybrid Planning Manager\nProvides a ROS action for Hybrid Planning requests\nRuns the planning logic and coordinates the planners\nGlobal Planner\nSolves the global planning problem and publishes the solution trajectory\nLocal Planner\nProcesses incoming global trajectory updates\nSolves the local planning problem based on robot state, world and reference trajectory\nSends position/velocity commands to the robot driver\nThe architecture components are designed to be generic and highly customizable. Since the components only interact via ROS 2 message interfaces, it’s very easy to replace implementations of each of the architecture’s components or plugins. The plugin interfaces are designed to be minimal and to abstract from the actual algorithm implementation as much as possible. That allows the developer to fully focus on the isolated logic or solvers without having to implement any parts of the infrastructure. This also allows reusing the same components for different setups or planning problems.\nThis component is “The Brain” of the architecture. Its main purpose is to process HybridPlanner action  requests and to coordinate the motion planning and execution process based on the planning logic plugin. The planning logic is implemented in the PlanningLogic plugin and is event-driven by design. Events are defined by string identifiers and may trigger action calls or cancellations targeting the global or local planners. An example event log for a simple planning logic is shown in the diagram below:\nEvents are triggered by the Hybrid Planning action request and by both of the global and local planners action feedback messages. In this example, the Hybrid Planning Manager starts the global planner after the hybrid planning request is received. Upon arrival of the global trajectory the local planner is started and when the local planner is finished the Hybrid Planning Manager returns a Hybrid Planning response.\nA custom implementation of the Planning Logic plugin supports mapping generic events to available actions provided by the architecture like “Start global planning”, “Stop trajectory execution”, or “Switch to local planner constraint x”. With this, the motion planning behavior becomes highly customizable and adaptable.\nThe Global Planner is the simplest component of the architecture. It provides an action server that processes GlobalPlanner requests. These requests include common MotionPlanRequests, which are processed by the Global Planner plugin. By default, this is simply MoveIt’s planning pipeline, but any kind of planner or even MTC could technically be used here. The planning result is reported using the action feedback and the solution trajectory is published to the Local Planner for further processing.\nThe Local Planner also runs an action server that handles requests from the Hybrid Planning Manager. The action is used for starting and stopping execution and may also configure runtime parameters like constraints or solver types.\nThe local planner implementation is based on two plugins:\nTrajectory Operator: This plugin maintains the global reference trajectory, handles trajectory updates from the global planner, and monitors the process of the current robot state.\nLocal Constraint Solver: This plugin implements the iterative solver algorithm that produces the robot commands based on the reference trajectory and the local constraints. It may include additional interfaces for dynamically processing sensor input or event updates.\nThe diagram below shows an example loop cycle of the Local Planner upon action request by the Hybrid Planning Manager:\nEach iteration the local planner requests the current planning scene and matches the current robot state within the reference trajectory. If the goal is reached, the local planning action successfully finishes. Otherwise, the current local planning problem is identified based on the current robot state and solved afterwards. Finally, the resulting control commands are published to the robot controller.\nThe runtime behavior of a Hybrid Planner can best be understood by drawing a workflow diagram that visualizes the communication channels and events of the different components.\nBelow is a diagram that shows the runtime logic of a successful trajectory execution.\nThe planner is invoked by a hybrid planning request which is also the first event the Hybrid Planning Manager reacts to.\nIn this example, the planner logic simply runs both planners in sequence. After the initial hybrid planning request, the Hybrid Planning Manager invokes the global planner.\nThe global planner computes and publishes a trajectory which is received by the Hybrid Planning Manager and the Local Planner Component.\nImportant to notice is, that the Local Planner Component just processes the new trajectory and does not start executing until it is invoked by the Hybrid Planning Manager. Once requested by the Hybrid Planning Manager, the Local Planner Component starts unwinding the reference trajectory and returns the action response successfully when it reaches the final state. After that, the Hybrid Planning Manager returns a successful HybridPlanningResponse.\nNow let’s consider a more difficult scenario where the hybrid planner is designed to avoid an obstacle during execution by replanning.\nThe animation below shows a simple motion that is being fixed at runtime because of changing collision objects.\nHere, the collision object present during the global planning process disappears after the global trajectory is computed. Instead two new collision objects appear that invalidate the initial global trajectory. The local planner detects the imminent collision and pauses the execution until the global planner has provided an updated collision free trajectory.\nBelow you can see the workflow of the described behavior.\nThe startup is the same as in the first example, but during unwinding the reference trajectory the local planner detects a collision. Here, the planner logic reacts by re-invoking the global planner. During the calculation of the new global solution, the local planner must prevent the robot from colliding with the collision object i.e. by keeping its current position. After the global planner finishes its calculation, the new global solution is published to the local planner and the local planner’s Trajectory Operator Plugin blends the update into the reference trajectory. Afterwards, the Local Planner Component continues to follow the reference trajectory as the updated solution enables it to steer around the collision object.\nIf you want to use Hybrid Planning in your application or just want to experiment with it, check out theHybrid Planning Example Tutorial.\n© Copyright 2024, PickNik Robotics.\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee0ff"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/hybrid_planning/hybrid_planning.html#how-does-it-work",
  "title": "Hybrid Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Hybrid Planning\nWhat is Hybrid Planning?\nThe Hybrid Planning Architecture\nMoveIt’s motion planning architecture follows the “Sense-Plan-Act” approach. In order for motions to get planned and executed, the environment and the robot state are first perceived (“Sense”), then the robot trajectory is computed by the planner (“Plan”) and finally executed in a single run using a trajectory controller (“Act”).\nWhile this solution works well for global motion planning in well-known static environments, the approach is not applicable in many real-world applications, in particular in unstable or dynamic environments. Tasks like serving a glass of water to a person on a table or writing on an uneven chalkboard require more sophisticated methods that allow reacting to unpredictable changes. For instance, the environment of the robot could change dynamically, or there could be certain uncertainties in the task itself, e.g. writing with chalk requires adapting the pressure on the board while the chalk also gets shorter by being used up.\nSolving these challenges requires a method that is able to adapt the executed motion to the immediate conditions or even to react by replanning in case of unforeseen changes in the environment. The Hybrid Planning architecture attempts to solve this problem by combining a pair of recurrent global and local planners.\nHybrid Planning is a term for a motion planning method that combines heterogeneous motion planners to produce more robust or reactive solutions. The general approach is already very established in the Navigation community and successfully implemented in popular projects like navigation2.\nMoveIt’s Hybrid Planning architecture combines a pair of global and local planners that run in parallel and recurrently with different planning speeds and problem scopes.\nThe global planner has the task to solve a global motion planning problem very akin to the planners used in a “Sense-Plan-Act” application. The used planner algorithm should be complete and is therefore assumed to be relatively slow in terms of computation time. Furthermore, the global planner is not required to be real-time safe, meaning that there are no guarantees that the planner finds a solution within a specific deadline. Depending on the planner implementation, the global planner may generate one initial solution or iteratively optimized solutions during the execution.\nThe local planner is running continuously during execution and produces iterative robot commands for following the global trajectory. In a way, the local planner resembles a controller, only that the architecture allows for solving more complicated problems and constraints. The idea is that the planner is able to reason about the world and to have an internal state. That property makes it very versatile and as it can be used for solving combinations of arbitrary local planning problems, as for example:\nUnwinding, blending, or splicing of subsequent global reference trajectories\nDynamically avoiding close-by collisions while following the global path\nAdapting the global trajectory to local constraints (e.g. desired force pressure on uneven surface, readjusting a tool based on visual feedback)\nLocal trajectory optimization and time parameterization (it is computationally cheaper and quicker to optimize a trajectory in a local environment)\nIn order to enable solving these local problems the local planner must be fast, able to react to sensor feedback and in many cases real-time safe. Also, it should be deterministic in order to avoid jerky or unpredictable motions.\nIn general, the local planner relies on the reference trajectory produced by the global planner so that it doesn’t get stuck in a local minimum. Since local minima sometimes still can’t be ruled out, it can be required that the global planner is triggered for a replan in order to still reach the desired goal. This behavior requires a certain way to communicate planner events and to process them accordingly. For that purpose the Hybrid Planning architecture allows implementing an event-based logic that can be customized for the specific use case and planner types.\nGlobal Planner\nLocal Planner\nSolve global solution trajectory\nOptimize trajectory path (continuously)\nFollow global reference trajectory\nSolve local problem constraints\nMay process sensor input\nOptimize solution locally\nCompute controller commands\nComplete\nNo restricted  computation time\nNot real-time safe\nNot necessarily deterministic\nCan get stuck in local minima\nLow computation time\nRealtime-safe (depends on solver)\nDeterministic\nOMPL planner\nSTOMP\nTrajOpt\nCartesian motion planner\nPilz Industrial Motion Planner\nMTC\nIK solver, Jacobian\nPotential field planner\nTrajectory optimization algorithm\nModel Predictive Control (MPC)\nSensor-based Optimal Control\nHybrid Planning can be useful in a broad range of use cases. Most of the applications can be grouped into the following three scenarios.\nOnline motion planning: The global planner creates an initial global solution and continuously optimizes it. Simultaneously, the local planner executes the reference trajectory and blends updated trajectory segments into it.\nReactive Motion: The global planner is used to fix invalidated solutions (replanning) while the local planner slows down or halts before collisions\nAdaptive Motion: The local planner is used to adapt a global solution to dynamic conditions like keeping steady tool contact with an uneven surface\nThe diagram below depicts the fundamental plugin types and ROS interfaces that make up the Hybrid Planning architecture.\nThe architecture is structured in three ROS component nodes:\nHybrid Planning Manager\nProvides a ROS action for Hybrid Planning requests\nRuns the planning logic and coordinates the planners\nGlobal Planner\nSolves the global planning problem and publishes the solution trajectory\nLocal Planner\nProcesses incoming global trajectory updates\nSolves the local planning problem based on robot state, world and reference trajectory\nSends position/velocity commands to the robot driver\nThe architecture components are designed to be generic and highly customizable. Since the components only interact via ROS 2 message interfaces, it’s very easy to replace implementations of each of the architecture’s components or plugins. The plugin interfaces are designed to be minimal and to abstract from the actual algorithm implementation as much as possible. That allows the developer to fully focus on the isolated logic or solvers without having to implement any parts of the infrastructure. This also allows reusing the same components for different setups or planning problems.\nThis component is “The Brain” of the architecture. Its main purpose is to process HybridPlanner action  requests and to coordinate the motion planning and execution process based on the planning logic plugin. The planning logic is implemented in the PlanningLogic plugin and is event-driven by design. Events are defined by string identifiers and may trigger action calls or cancellations targeting the global or local planners. An example event log for a simple planning logic is shown in the diagram below:\nEvents are triggered by the Hybrid Planning action request and by both of the global and local planners action feedback messages. In this example, the Hybrid Planning Manager starts the global planner after the hybrid planning request is received. Upon arrival of the global trajectory the local planner is started and when the local planner is finished the Hybrid Planning Manager returns a Hybrid Planning response.\nA custom implementation of the Planning Logic plugin supports mapping generic events to available actions provided by the architecture like “Start global planning”, “Stop trajectory execution”, or “Switch to local planner constraint x”. With this, the motion planning behavior becomes highly customizable and adaptable.\nThe Global Planner is the simplest component of the architecture. It provides an action server that processes GlobalPlanner requests. These requests include common MotionPlanRequests, which are processed by the Global Planner plugin. By default, this is simply MoveIt’s planning pipeline, but any kind of planner or even MTC could technically be used here. The planning result is reported using the action feedback and the solution trajectory is published to the Local Planner for further processing.\nThe Local Planner also runs an action server that handles requests from the Hybrid Planning Manager. The action is used for starting and stopping execution and may also configure runtime parameters like constraints or solver types.\nThe local planner implementation is based on two plugins:\nTrajectory Operator: This plugin maintains the global reference trajectory, handles trajectory updates from the global planner, and monitors the process of the current robot state.\nLocal Constraint Solver: This plugin implements the iterative solver algorithm that produces the robot commands based on the reference trajectory and the local constraints. It may include additional interfaces for dynamically processing sensor input or event updates.\nThe diagram below shows an example loop cycle of the Local Planner upon action request by the Hybrid Planning Manager:\nEach iteration the local planner requests the current planning scene and matches the current robot state within the reference trajectory. If the goal is reached, the local planning action successfully finishes. Otherwise, the current local planning problem is identified based on the current robot state and solved afterwards. Finally, the resulting control commands are published to the robot controller.\nThe runtime behavior of a Hybrid Planner can best be understood by drawing a workflow diagram that visualizes the communication channels and events of the different components.\nBelow is a diagram that shows the runtime logic of a successful trajectory execution.\nThe planner is invoked by a hybrid planning request which is also the first event the Hybrid Planning Manager reacts to.\nIn this example, the planner logic simply runs both planners in sequence. After the initial hybrid planning request, the Hybrid Planning Manager invokes the global planner.\nThe global planner computes and publishes a trajectory which is received by the Hybrid Planning Manager and the Local Planner Component.\nImportant to notice is, that the Local Planner Component just processes the new trajectory and does not start executing until it is invoked by the Hybrid Planning Manager. Once requested by the Hybrid Planning Manager, the Local Planner Component starts unwinding the reference trajectory and returns the action response successfully when it reaches the final state. After that, the Hybrid Planning Manager returns a successful HybridPlanningResponse.\nNow let’s consider a more difficult scenario where the hybrid planner is designed to avoid an obstacle during execution by replanning.\nThe animation below shows a simple motion that is being fixed at runtime because of changing collision objects.\nHere, the collision object present during the global planning process disappears after the global trajectory is computed. Instead two new collision objects appear that invalidate the initial global trajectory. The local planner detects the imminent collision and pauses the execution until the global planner has provided an updated collision free trajectory.\nBelow you can see the workflow of the described behavior.\nThe startup is the same as in the first example, but during unwinding the reference trajectory the local planner detects a collision. Here, the planner logic reacts by re-invoking the global planner. During the calculation of the new global solution, the local planner must prevent the robot from colliding with the collision object i.e. by keeping its current position. After the global planner finishes its calculation, the new global solution is published to the local planner and the local planner’s Trajectory Operator Plugin blends the update into the reference trajectory. Afterwards, the Local Planner Component continues to follow the reference trajectory as the updated solution enables it to steer around the collision object.\nIf you want to use Hybrid Planning in your application or just want to experiment with it, check out theHybrid Planning Example Tutorial.\n© Copyright 2024, PickNik Robotics.\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee100"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/hybrid_planning/hybrid_planning.html#hybrid-planning",
  "title": "Hybrid Planning — MoveIt Documentation: Rolling  documentation",
  "content": "Hybrid Planning\nWhat is Hybrid Planning?\nThe Hybrid Planning Architecture\nMoveIt’s motion planning architecture follows the “Sense-Plan-Act” approach. In order for motions to get planned and executed, the environment and the robot state are first perceived (“Sense”), then the robot trajectory is computed by the planner (“Plan”) and finally executed in a single run using a trajectory controller (“Act”).\nWhile this solution works well for global motion planning in well-known static environments, the approach is not applicable in many real-world applications, in particular in unstable or dynamic environments. Tasks like serving a glass of water to a person on a table or writing on an uneven chalkboard require more sophisticated methods that allow reacting to unpredictable changes. For instance, the environment of the robot could change dynamically, or there could be certain uncertainties in the task itself, e.g. writing with chalk requires adapting the pressure on the board while the chalk also gets shorter by being used up.\nSolving these challenges requires a method that is able to adapt the executed motion to the immediate conditions or even to react by replanning in case of unforeseen changes in the environment. The Hybrid Planning architecture attempts to solve this problem by combining a pair of recurrent global and local planners.\nHybrid Planning is a term for a motion planning method that combines heterogeneous motion planners to produce more robust or reactive solutions. The general approach is already very established in the Navigation community and successfully implemented in popular projects like navigation2.\nMoveIt’s Hybrid Planning architecture combines a pair of global and local planners that run in parallel and recurrently with different planning speeds and problem scopes.\nThe global planner has the task to solve a global motion planning problem very akin to the planners used in a “Sense-Plan-Act” application. The used planner algorithm should be complete and is therefore assumed to be relatively slow in terms of computation time. Furthermore, the global planner is not required to be real-time safe, meaning that there are no guarantees that the planner finds a solution within a specific deadline. Depending on the planner implementation, the global planner may generate one initial solution or iteratively optimized solutions during the execution.\nThe local planner is running continuously during execution and produces iterative robot commands for following the global trajectory. In a way, the local planner resembles a controller, only that the architecture allows for solving more complicated problems and constraints. The idea is that the planner is able to reason about the world and to have an internal state. That property makes it very versatile and as it can be used for solving combinations of arbitrary local planning problems, as for example:\nUnwinding, blending, or splicing of subsequent global reference trajectories\nDynamically avoiding close-by collisions while following the global path\nAdapting the global trajectory to local constraints (e.g. desired force pressure on uneven surface, readjusting a tool based on visual feedback)\nLocal trajectory optimization and time parameterization (it is computationally cheaper and quicker to optimize a trajectory in a local environment)\nIn order to enable solving these local problems the local planner must be fast, able to react to sensor feedback and in many cases real-time safe. Also, it should be deterministic in order to avoid jerky or unpredictable motions.\nIn general, the local planner relies on the reference trajectory produced by the global planner so that it doesn’t get stuck in a local minimum. Since local minima sometimes still can’t be ruled out, it can be required that the global planner is triggered for a replan in order to still reach the desired goal. This behavior requires a certain way to communicate planner events and to process them accordingly. For that purpose the Hybrid Planning architecture allows implementing an event-based logic that can be customized for the specific use case and planner types.\nGlobal Planner\nLocal Planner\nSolve global solution trajectory\nOptimize trajectory path (continuously)\nFollow global reference trajectory\nSolve local problem constraints\nMay process sensor input\nOptimize solution locally\nCompute controller commands\nComplete\nNo restricted  computation time\nNot real-time safe\nNot necessarily deterministic\nCan get stuck in local minima\nLow computation time\nRealtime-safe (depends on solver)\nDeterministic\nOMPL planner\nSTOMP\nTrajOpt\nCartesian motion planner\nPilz Industrial Motion Planner\nMTC\nIK solver, Jacobian\nPotential field planner\nTrajectory optimization algorithm\nModel Predictive Control (MPC)\nSensor-based Optimal Control\nHybrid Planning can be useful in a broad range of use cases. Most of the applications can be grouped into the following three scenarios.\nOnline motion planning: The global planner creates an initial global solution and continuously optimizes it. Simultaneously, the local planner executes the reference trajectory and blends updated trajectory segments into it.\nReactive Motion: The global planner is used to fix invalidated solutions (replanning) while the local planner slows down or halts before collisions\nAdaptive Motion: The local planner is used to adapt a global solution to dynamic conditions like keeping steady tool contact with an uneven surface\nThe diagram below depicts the fundamental plugin types and ROS interfaces that make up the Hybrid Planning architecture.\nThe architecture is structured in three ROS component nodes:\nHybrid Planning Manager\nProvides a ROS action for Hybrid Planning requests\nRuns the planning logic and coordinates the planners\nGlobal Planner\nSolves the global planning problem and publishes the solution trajectory\nLocal Planner\nProcesses incoming global trajectory updates\nSolves the local planning problem based on robot state, world and reference trajectory\nSends position/velocity commands to the robot driver\nThe architecture components are designed to be generic and highly customizable. Since the components only interact via ROS 2 message interfaces, it’s very easy to replace implementations of each of the architecture’s components or plugins. The plugin interfaces are designed to be minimal and to abstract from the actual algorithm implementation as much as possible. That allows the developer to fully focus on the isolated logic or solvers without having to implement any parts of the infrastructure. This also allows reusing the same components for different setups or planning problems.\nThis component is “The Brain” of the architecture. Its main purpose is to process HybridPlanner action  requests and to coordinate the motion planning and execution process based on the planning logic plugin. The planning logic is implemented in the PlanningLogic plugin and is event-driven by design. Events are defined by string identifiers and may trigger action calls or cancellations targeting the global or local planners. An example event log for a simple planning logic is shown in the diagram below:\nEvents are triggered by the Hybrid Planning action request and by both of the global and local planners action feedback messages. In this example, the Hybrid Planning Manager starts the global planner after the hybrid planning request is received. Upon arrival of the global trajectory the local planner is started and when the local planner is finished the Hybrid Planning Manager returns a Hybrid Planning response.\nA custom implementation of the Planning Logic plugin supports mapping generic events to available actions provided by the architecture like “Start global planning”, “Stop trajectory execution”, or “Switch to local planner constraint x”. With this, the motion planning behavior becomes highly customizable and adaptable.\nThe Global Planner is the simplest component of the architecture. It provides an action server that processes GlobalPlanner requests. These requests include common MotionPlanRequests, which are processed by the Global Planner plugin. By default, this is simply MoveIt’s planning pipeline, but any kind of planner or even MTC could technically be used here. The planning result is reported using the action feedback and the solution trajectory is published to the Local Planner for further processing.\nThe Local Planner also runs an action server that handles requests from the Hybrid Planning Manager. The action is used for starting and stopping execution and may also configure runtime parameters like constraints or solver types.\nThe local planner implementation is based on two plugins:\nTrajectory Operator: This plugin maintains the global reference trajectory, handles trajectory updates from the global planner, and monitors the process of the current robot state.\nLocal Constraint Solver: This plugin implements the iterative solver algorithm that produces the robot commands based on the reference trajectory and the local constraints. It may include additional interfaces for dynamically processing sensor input or event updates.\nThe diagram below shows an example loop cycle of the Local Planner upon action request by the Hybrid Planning Manager:\nEach iteration the local planner requests the current planning scene and matches the current robot state within the reference trajectory. If the goal is reached, the local planning action successfully finishes. Otherwise, the current local planning problem is identified based on the current robot state and solved afterwards. Finally, the resulting control commands are published to the robot controller.\nThe runtime behavior of a Hybrid Planner can best be understood by drawing a workflow diagram that visualizes the communication channels and events of the different components.\nBelow is a diagram that shows the runtime logic of a successful trajectory execution.\nThe planner is invoked by a hybrid planning request which is also the first event the Hybrid Planning Manager reacts to.\nIn this example, the planner logic simply runs both planners in sequence. After the initial hybrid planning request, the Hybrid Planning Manager invokes the global planner.\nThe global planner computes and publishes a trajectory which is received by the Hybrid Planning Manager and the Local Planner Component.\nImportant to notice is, that the Local Planner Component just processes the new trajectory and does not start executing until it is invoked by the Hybrid Planning Manager. Once requested by the Hybrid Planning Manager, the Local Planner Component starts unwinding the reference trajectory and returns the action response successfully when it reaches the final state. After that, the Hybrid Planning Manager returns a successful HybridPlanningResponse.\nNow let’s consider a more difficult scenario where the hybrid planner is designed to avoid an obstacle during execution by replanning.\nThe animation below shows a simple motion that is being fixed at runtime because of changing collision objects.\nHere, the collision object present during the global planning process disappears after the global trajectory is computed. Instead two new collision objects appear that invalidate the initial global trajectory. The local planner detects the imminent collision and pauses the execution until the global planner has provided an updated collision free trajectory.\nBelow you can see the workflow of the described behavior.\nThe startup is the same as in the first example, but during unwinding the reference trajectory the local planner detects a collision. Here, the planner logic reacts by re-invoking the global planner. During the calculation of the new global solution, the local planner must prevent the robot from colliding with the collision object i.e. by keeping its current position. After the global planner finishes its calculation, the new global solution is published to the local planner and the local planner’s Trajectory Operator Plugin blends the update into the reference trajectory. Afterwards, the Local Planner Component continues to follow the reference trajectory as the updated solution enables it to steer around the collision object.\nIf you want to use Hybrid Planning in your application or just want to experiment with it, check out theHybrid Planning Example Tutorial.\n© Copyright 2024, PickNik Robotics.\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee101"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/move_group.html",
  "title": "The move_group node — MoveIt Documentation: Rolling  documentation",
  "content": "Themove_groupnode\nUser Interface\nConfiguration\nRobot Interface\nJoint State Information\nTransform Information\nController Interface\nPlanning Scene\nExtensible Capabilities\nThe figure below shows the high-level system architecture for a key node provided by MoveIt calledmove_group.\nThis node serves as an integrator: pulling all the individual components together to provide a set of ROS actions and services for users to use.\nThe users can access the actions and services provided bymove_groupin two ways:\nIn C++- using themove_group_interfacepackage that provides an easy to setup C++ interface to move_group\nThrough a GUI- using theMotion Planning plugin to Rviz(the ROS visualizer)\nmove_groupis a ROS node.\nIt uses the ROS param server to get three kinds of information:\nURDF\nmove_grouplooks for therobot_descriptionparameter to get the URDF for the robot.\nSRDF\nmove_grouplooks for therobot_description_semanticparameter to get the SRDF for the robot. The SRDF is typically created (once) by a user using the MoveIt Setup Assistant.\nMoveIt configuration\nmove_groupwill look for other configuration specific to MoveIt including joint limits, kinematics, motion planning, perception and other information. Config files for these components are automatically generated by the MoveIt Setup Assistant and stored in theconfigdirectory of the corresponding MoveIt config package for the robot.\nmove_grouptalks to the robot through ROS topics and actions.\nIt communicates with the robot to get current state information (positions of the joints, etc.), to get point clouds and other sensor data from the robot sensors and to talk to the controllers on the robot.\nmove_grouplistens on the/joint_statestopic for determining the current state information - i.e. determining where each joint of the robot is.move_groupis capable of listening to multiple publishers on this topic even if they are publishing only partial information about the robot state (e.g. separate publishers may be used for the arm and mobile base of a robot).\nNote thatmove_groupwill not set up its own joint state publisher - this is something that has to be implemented on each robot.\nmove_groupmonitors transform information using the ROS TF library.\nThis allows the node to get global information about the pose of the robot (among other things).\nFor instance, the ROS navigation stack will publish the transform between the map frame and base frame of the robot to TF.move_groupcan use TF to figure out this transform for internal use. Note thatmove_grouponly listens to TF.\nTo publish TF information from your robot, you will need to have arobot_state_publishernode running on your robot.\nmove_grouptalks to the controllers on the robot using the FollowJointTrajectoryAction interface.\nThis is a ROS action interface.\nA server on the robot needs to service this action - this server is not provided bymove_groupitself.move_groupwill only instantiate a client to talk to this controller action server on your robot.\nmove_groupuses the Planning Scene Monitor to maintain aplanning scene, which is a representation of the world and the current state of the robot.\nThe robot state can include any objects attached to (carried by) the robot which are considered to be rigidly attached to the robot.\nMore details on the architecture for maintaining and updating theplanning sceneare outlined in the Planning Scene section below.\nmove_groupis structured to be easily extensible.\nIndividual capabilities like pick and place, kinematics, motion planning are actually implemented as separate plugins with a common base class.\nThe plugins are configurable using ROS through a set of ROS yaml parameters and through the use of the ROS pluginlib library.\nMost users will not have to configure move_group plugins since they come automatically configured in the launch files generated by the MoveIt Setup Assistant.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_description\nmove_group\nrobot_description_semantic\nmove_group\nconfig\nmove_group\nmove_group\n/joint_states\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_state_publisher\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee102"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/move_group.html#user-interface",
  "title": "The move_group node — MoveIt Documentation: Rolling  documentation",
  "content": "Themove_groupnode\nUser Interface\nConfiguration\nRobot Interface\nJoint State Information\nTransform Information\nController Interface\nPlanning Scene\nExtensible Capabilities\nThe figure below shows the high-level system architecture for a key node provided by MoveIt calledmove_group.\nThis node serves as an integrator: pulling all the individual components together to provide a set of ROS actions and services for users to use.\nThe users can access the actions and services provided bymove_groupin two ways:\nIn C++- using themove_group_interfacepackage that provides an easy to setup C++ interface to move_group\nThrough a GUI- using theMotion Planning plugin to Rviz(the ROS visualizer)\nmove_groupis a ROS node.\nIt uses the ROS param server to get three kinds of information:\nURDF\nmove_grouplooks for therobot_descriptionparameter to get the URDF for the robot.\nSRDF\nmove_grouplooks for therobot_description_semanticparameter to get the SRDF for the robot. The SRDF is typically created (once) by a user using the MoveIt Setup Assistant.\nMoveIt configuration\nmove_groupwill look for other configuration specific to MoveIt including joint limits, kinematics, motion planning, perception and other information. Config files for these components are automatically generated by the MoveIt Setup Assistant and stored in theconfigdirectory of the corresponding MoveIt config package for the robot.\nmove_grouptalks to the robot through ROS topics and actions.\nIt communicates with the robot to get current state information (positions of the joints, etc.), to get point clouds and other sensor data from the robot sensors and to talk to the controllers on the robot.\nmove_grouplistens on the/joint_statestopic for determining the current state information - i.e. determining where each joint of the robot is.move_groupis capable of listening to multiple publishers on this topic even if they are publishing only partial information about the robot state (e.g. separate publishers may be used for the arm and mobile base of a robot).\nNote thatmove_groupwill not set up its own joint state publisher - this is something that has to be implemented on each robot.\nmove_groupmonitors transform information using the ROS TF library.\nThis allows the node to get global information about the pose of the robot (among other things).\nFor instance, the ROS navigation stack will publish the transform between the map frame and base frame of the robot to TF.move_groupcan use TF to figure out this transform for internal use. Note thatmove_grouponly listens to TF.\nTo publish TF information from your robot, you will need to have arobot_state_publishernode running on your robot.\nmove_grouptalks to the controllers on the robot using the FollowJointTrajectoryAction interface.\nThis is a ROS action interface.\nA server on the robot needs to service this action - this server is not provided bymove_groupitself.move_groupwill only instantiate a client to talk to this controller action server on your robot.\nmove_groupuses the Planning Scene Monitor to maintain aplanning scene, which is a representation of the world and the current state of the robot.\nThe robot state can include any objects attached to (carried by) the robot which are considered to be rigidly attached to the robot.\nMore details on the architecture for maintaining and updating theplanning sceneare outlined in the Planning Scene section below.\nmove_groupis structured to be easily extensible.\nIndividual capabilities like pick and place, kinematics, motion planning are actually implemented as separate plugins with a common base class.\nThe plugins are configurable using ROS through a set of ROS yaml parameters and through the use of the ROS pluginlib library.\nMost users will not have to configure move_group plugins since they come automatically configured in the launch files generated by the MoveIt Setup Assistant.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_description\nmove_group\nrobot_description_semantic\nmove_group\nconfig\nmove_group\nmove_group\n/joint_states\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_state_publisher\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee103"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/move_group.html#configuration",
  "title": "The move_group node — MoveIt Documentation: Rolling  documentation",
  "content": "Themove_groupnode\nUser Interface\nConfiguration\nRobot Interface\nJoint State Information\nTransform Information\nController Interface\nPlanning Scene\nExtensible Capabilities\nThe figure below shows the high-level system architecture for a key node provided by MoveIt calledmove_group.\nThis node serves as an integrator: pulling all the individual components together to provide a set of ROS actions and services for users to use.\nThe users can access the actions and services provided bymove_groupin two ways:\nIn C++- using themove_group_interfacepackage that provides an easy to setup C++ interface to move_group\nThrough a GUI- using theMotion Planning plugin to Rviz(the ROS visualizer)\nmove_groupis a ROS node.\nIt uses the ROS param server to get three kinds of information:\nURDF\nmove_grouplooks for therobot_descriptionparameter to get the URDF for the robot.\nSRDF\nmove_grouplooks for therobot_description_semanticparameter to get the SRDF for the robot. The SRDF is typically created (once) by a user using the MoveIt Setup Assistant.\nMoveIt configuration\nmove_groupwill look for other configuration specific to MoveIt including joint limits, kinematics, motion planning, perception and other information. Config files for these components are automatically generated by the MoveIt Setup Assistant and stored in theconfigdirectory of the corresponding MoveIt config package for the robot.\nmove_grouptalks to the robot through ROS topics and actions.\nIt communicates with the robot to get current state information (positions of the joints, etc.), to get point clouds and other sensor data from the robot sensors and to talk to the controllers on the robot.\nmove_grouplistens on the/joint_statestopic for determining the current state information - i.e. determining where each joint of the robot is.move_groupis capable of listening to multiple publishers on this topic even if they are publishing only partial information about the robot state (e.g. separate publishers may be used for the arm and mobile base of a robot).\nNote thatmove_groupwill not set up its own joint state publisher - this is something that has to be implemented on each robot.\nmove_groupmonitors transform information using the ROS TF library.\nThis allows the node to get global information about the pose of the robot (among other things).\nFor instance, the ROS navigation stack will publish the transform between the map frame and base frame of the robot to TF.move_groupcan use TF to figure out this transform for internal use. Note thatmove_grouponly listens to TF.\nTo publish TF information from your robot, you will need to have arobot_state_publishernode running on your robot.\nmove_grouptalks to the controllers on the robot using the FollowJointTrajectoryAction interface.\nThis is a ROS action interface.\nA server on the robot needs to service this action - this server is not provided bymove_groupitself.move_groupwill only instantiate a client to talk to this controller action server on your robot.\nmove_groupuses the Planning Scene Monitor to maintain aplanning scene, which is a representation of the world and the current state of the robot.\nThe robot state can include any objects attached to (carried by) the robot which are considered to be rigidly attached to the robot.\nMore details on the architecture for maintaining and updating theplanning sceneare outlined in the Planning Scene section below.\nmove_groupis structured to be easily extensible.\nIndividual capabilities like pick and place, kinematics, motion planning are actually implemented as separate plugins with a common base class.\nThe plugins are configurable using ROS through a set of ROS yaml parameters and through the use of the ROS pluginlib library.\nMost users will not have to configure move_group plugins since they come automatically configured in the launch files generated by the MoveIt Setup Assistant.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_description\nmove_group\nrobot_description_semantic\nmove_group\nconfig\nmove_group\nmove_group\n/joint_states\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_state_publisher\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee104"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/move_group.html#robot-interface",
  "title": "The move_group node — MoveIt Documentation: Rolling  documentation",
  "content": "Themove_groupnode\nUser Interface\nConfiguration\nRobot Interface\nJoint State Information\nTransform Information\nController Interface\nPlanning Scene\nExtensible Capabilities\nThe figure below shows the high-level system architecture for a key node provided by MoveIt calledmove_group.\nThis node serves as an integrator: pulling all the individual components together to provide a set of ROS actions and services for users to use.\nThe users can access the actions and services provided bymove_groupin two ways:\nIn C++- using themove_group_interfacepackage that provides an easy to setup C++ interface to move_group\nThrough a GUI- using theMotion Planning plugin to Rviz(the ROS visualizer)\nmove_groupis a ROS node.\nIt uses the ROS param server to get three kinds of information:\nURDF\nmove_grouplooks for therobot_descriptionparameter to get the URDF for the robot.\nSRDF\nmove_grouplooks for therobot_description_semanticparameter to get the SRDF for the robot. The SRDF is typically created (once) by a user using the MoveIt Setup Assistant.\nMoveIt configuration\nmove_groupwill look for other configuration specific to MoveIt including joint limits, kinematics, motion planning, perception and other information. Config files for these components are automatically generated by the MoveIt Setup Assistant and stored in theconfigdirectory of the corresponding MoveIt config package for the robot.\nmove_grouptalks to the robot through ROS topics and actions.\nIt communicates with the robot to get current state information (positions of the joints, etc.), to get point clouds and other sensor data from the robot sensors and to talk to the controllers on the robot.\nmove_grouplistens on the/joint_statestopic for determining the current state information - i.e. determining where each joint of the robot is.move_groupis capable of listening to multiple publishers on this topic even if they are publishing only partial information about the robot state (e.g. separate publishers may be used for the arm and mobile base of a robot).\nNote thatmove_groupwill not set up its own joint state publisher - this is something that has to be implemented on each robot.\nmove_groupmonitors transform information using the ROS TF library.\nThis allows the node to get global information about the pose of the robot (among other things).\nFor instance, the ROS navigation stack will publish the transform between the map frame and base frame of the robot to TF.move_groupcan use TF to figure out this transform for internal use. Note thatmove_grouponly listens to TF.\nTo publish TF information from your robot, you will need to have arobot_state_publishernode running on your robot.\nmove_grouptalks to the controllers on the robot using the FollowJointTrajectoryAction interface.\nThis is a ROS action interface.\nA server on the robot needs to service this action - this server is not provided bymove_groupitself.move_groupwill only instantiate a client to talk to this controller action server on your robot.\nmove_groupuses the Planning Scene Monitor to maintain aplanning scene, which is a representation of the world and the current state of the robot.\nThe robot state can include any objects attached to (carried by) the robot which are considered to be rigidly attached to the robot.\nMore details on the architecture for maintaining and updating theplanning sceneare outlined in the Planning Scene section below.\nmove_groupis structured to be easily extensible.\nIndividual capabilities like pick and place, kinematics, motion planning are actually implemented as separate plugins with a common base class.\nThe plugins are configurable using ROS through a set of ROS yaml parameters and through the use of the ROS pluginlib library.\nMost users will not have to configure move_group plugins since they come automatically configured in the launch files generated by the MoveIt Setup Assistant.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_description\nmove_group\nrobot_description_semantic\nmove_group\nconfig\nmove_group\nmove_group\n/joint_states\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_state_publisher\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee105"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/move_group.html#joint-state-information",
  "title": "The move_group node — MoveIt Documentation: Rolling  documentation",
  "content": "Themove_groupnode\nUser Interface\nConfiguration\nRobot Interface\nJoint State Information\nTransform Information\nController Interface\nPlanning Scene\nExtensible Capabilities\nThe figure below shows the high-level system architecture for a key node provided by MoveIt calledmove_group.\nThis node serves as an integrator: pulling all the individual components together to provide a set of ROS actions and services for users to use.\nThe users can access the actions and services provided bymove_groupin two ways:\nIn C++- using themove_group_interfacepackage that provides an easy to setup C++ interface to move_group\nThrough a GUI- using theMotion Planning plugin to Rviz(the ROS visualizer)\nmove_groupis a ROS node.\nIt uses the ROS param server to get three kinds of information:\nURDF\nmove_grouplooks for therobot_descriptionparameter to get the URDF for the robot.\nSRDF\nmove_grouplooks for therobot_description_semanticparameter to get the SRDF for the robot. The SRDF is typically created (once) by a user using the MoveIt Setup Assistant.\nMoveIt configuration\nmove_groupwill look for other configuration specific to MoveIt including joint limits, kinematics, motion planning, perception and other information. Config files for these components are automatically generated by the MoveIt Setup Assistant and stored in theconfigdirectory of the corresponding MoveIt config package for the robot.\nmove_grouptalks to the robot through ROS topics and actions.\nIt communicates with the robot to get current state information (positions of the joints, etc.), to get point clouds and other sensor data from the robot sensors and to talk to the controllers on the robot.\nmove_grouplistens on the/joint_statestopic for determining the current state information - i.e. determining where each joint of the robot is.move_groupis capable of listening to multiple publishers on this topic even if they are publishing only partial information about the robot state (e.g. separate publishers may be used for the arm and mobile base of a robot).\nNote thatmove_groupwill not set up its own joint state publisher - this is something that has to be implemented on each robot.\nmove_groupmonitors transform information using the ROS TF library.\nThis allows the node to get global information about the pose of the robot (among other things).\nFor instance, the ROS navigation stack will publish the transform between the map frame and base frame of the robot to TF.move_groupcan use TF to figure out this transform for internal use. Note thatmove_grouponly listens to TF.\nTo publish TF information from your robot, you will need to have arobot_state_publishernode running on your robot.\nmove_grouptalks to the controllers on the robot using the FollowJointTrajectoryAction interface.\nThis is a ROS action interface.\nA server on the robot needs to service this action - this server is not provided bymove_groupitself.move_groupwill only instantiate a client to talk to this controller action server on your robot.\nmove_groupuses the Planning Scene Monitor to maintain aplanning scene, which is a representation of the world and the current state of the robot.\nThe robot state can include any objects attached to (carried by) the robot which are considered to be rigidly attached to the robot.\nMore details on the architecture for maintaining and updating theplanning sceneare outlined in the Planning Scene section below.\nmove_groupis structured to be easily extensible.\nIndividual capabilities like pick and place, kinematics, motion planning are actually implemented as separate plugins with a common base class.\nThe plugins are configurable using ROS through a set of ROS yaml parameters and through the use of the ROS pluginlib library.\nMost users will not have to configure move_group plugins since they come automatically configured in the launch files generated by the MoveIt Setup Assistant.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_description\nmove_group\nrobot_description_semantic\nmove_group\nconfig\nmove_group\nmove_group\n/joint_states\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_state_publisher\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee106"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/move_group.html#transform-information",
  "title": "The move_group node — MoveIt Documentation: Rolling  documentation",
  "content": "Themove_groupnode\nUser Interface\nConfiguration\nRobot Interface\nJoint State Information\nTransform Information\nController Interface\nPlanning Scene\nExtensible Capabilities\nThe figure below shows the high-level system architecture for a key node provided by MoveIt calledmove_group.\nThis node serves as an integrator: pulling all the individual components together to provide a set of ROS actions and services for users to use.\nThe users can access the actions and services provided bymove_groupin two ways:\nIn C++- using themove_group_interfacepackage that provides an easy to setup C++ interface to move_group\nThrough a GUI- using theMotion Planning plugin to Rviz(the ROS visualizer)\nmove_groupis a ROS node.\nIt uses the ROS param server to get three kinds of information:\nURDF\nmove_grouplooks for therobot_descriptionparameter to get the URDF for the robot.\nSRDF\nmove_grouplooks for therobot_description_semanticparameter to get the SRDF for the robot. The SRDF is typically created (once) by a user using the MoveIt Setup Assistant.\nMoveIt configuration\nmove_groupwill look for other configuration specific to MoveIt including joint limits, kinematics, motion planning, perception and other information. Config files for these components are automatically generated by the MoveIt Setup Assistant and stored in theconfigdirectory of the corresponding MoveIt config package for the robot.\nmove_grouptalks to the robot through ROS topics and actions.\nIt communicates with the robot to get current state information (positions of the joints, etc.), to get point clouds and other sensor data from the robot sensors and to talk to the controllers on the robot.\nmove_grouplistens on the/joint_statestopic for determining the current state information - i.e. determining where each joint of the robot is.move_groupis capable of listening to multiple publishers on this topic even if they are publishing only partial information about the robot state (e.g. separate publishers may be used for the arm and mobile base of a robot).\nNote thatmove_groupwill not set up its own joint state publisher - this is something that has to be implemented on each robot.\nmove_groupmonitors transform information using the ROS TF library.\nThis allows the node to get global information about the pose of the robot (among other things).\nFor instance, the ROS navigation stack will publish the transform between the map frame and base frame of the robot to TF.move_groupcan use TF to figure out this transform for internal use. Note thatmove_grouponly listens to TF.\nTo publish TF information from your robot, you will need to have arobot_state_publishernode running on your robot.\nmove_grouptalks to the controllers on the robot using the FollowJointTrajectoryAction interface.\nThis is a ROS action interface.\nA server on the robot needs to service this action - this server is not provided bymove_groupitself.move_groupwill only instantiate a client to talk to this controller action server on your robot.\nmove_groupuses the Planning Scene Monitor to maintain aplanning scene, which is a representation of the world and the current state of the robot.\nThe robot state can include any objects attached to (carried by) the robot which are considered to be rigidly attached to the robot.\nMore details on the architecture for maintaining and updating theplanning sceneare outlined in the Planning Scene section below.\nmove_groupis structured to be easily extensible.\nIndividual capabilities like pick and place, kinematics, motion planning are actually implemented as separate plugins with a common base class.\nThe plugins are configurable using ROS through a set of ROS yaml parameters and through the use of the ROS pluginlib library.\nMost users will not have to configure move_group plugins since they come automatically configured in the launch files generated by the MoveIt Setup Assistant.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_description\nmove_group\nrobot_description_semantic\nmove_group\nconfig\nmove_group\nmove_group\n/joint_states\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_state_publisher\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee107"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/move_group.html#controller-interface",
  "title": "The move_group node — MoveIt Documentation: Rolling  documentation",
  "content": "Themove_groupnode\nUser Interface\nConfiguration\nRobot Interface\nJoint State Information\nTransform Information\nController Interface\nPlanning Scene\nExtensible Capabilities\nThe figure below shows the high-level system architecture for a key node provided by MoveIt calledmove_group.\nThis node serves as an integrator: pulling all the individual components together to provide a set of ROS actions and services for users to use.\nThe users can access the actions and services provided bymove_groupin two ways:\nIn C++- using themove_group_interfacepackage that provides an easy to setup C++ interface to move_group\nThrough a GUI- using theMotion Planning plugin to Rviz(the ROS visualizer)\nmove_groupis a ROS node.\nIt uses the ROS param server to get three kinds of information:\nURDF\nmove_grouplooks for therobot_descriptionparameter to get the URDF for the robot.\nSRDF\nmove_grouplooks for therobot_description_semanticparameter to get the SRDF for the robot. The SRDF is typically created (once) by a user using the MoveIt Setup Assistant.\nMoveIt configuration\nmove_groupwill look for other configuration specific to MoveIt including joint limits, kinematics, motion planning, perception and other information. Config files for these components are automatically generated by the MoveIt Setup Assistant and stored in theconfigdirectory of the corresponding MoveIt config package for the robot.\nmove_grouptalks to the robot through ROS topics and actions.\nIt communicates with the robot to get current state information (positions of the joints, etc.), to get point clouds and other sensor data from the robot sensors and to talk to the controllers on the robot.\nmove_grouplistens on the/joint_statestopic for determining the current state information - i.e. determining where each joint of the robot is.move_groupis capable of listening to multiple publishers on this topic even if they are publishing only partial information about the robot state (e.g. separate publishers may be used for the arm and mobile base of a robot).\nNote thatmove_groupwill not set up its own joint state publisher - this is something that has to be implemented on each robot.\nmove_groupmonitors transform information using the ROS TF library.\nThis allows the node to get global information about the pose of the robot (among other things).\nFor instance, the ROS navigation stack will publish the transform between the map frame and base frame of the robot to TF.move_groupcan use TF to figure out this transform for internal use. Note thatmove_grouponly listens to TF.\nTo publish TF information from your robot, you will need to have arobot_state_publishernode running on your robot.\nmove_grouptalks to the controllers on the robot using the FollowJointTrajectoryAction interface.\nThis is a ROS action interface.\nA server on the robot needs to service this action - this server is not provided bymove_groupitself.move_groupwill only instantiate a client to talk to this controller action server on your robot.\nmove_groupuses the Planning Scene Monitor to maintain aplanning scene, which is a representation of the world and the current state of the robot.\nThe robot state can include any objects attached to (carried by) the robot which are considered to be rigidly attached to the robot.\nMore details on the architecture for maintaining and updating theplanning sceneare outlined in the Planning Scene section below.\nmove_groupis structured to be easily extensible.\nIndividual capabilities like pick and place, kinematics, motion planning are actually implemented as separate plugins with a common base class.\nThe plugins are configurable using ROS through a set of ROS yaml parameters and through the use of the ROS pluginlib library.\nMost users will not have to configure move_group plugins since they come automatically configured in the launch files generated by the MoveIt Setup Assistant.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_description\nmove_group\nrobot_description_semantic\nmove_group\nconfig\nmove_group\nmove_group\n/joint_states\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_state_publisher\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee108"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/move_group.html#planning-scene",
  "title": "The move_group node — MoveIt Documentation: Rolling  documentation",
  "content": "Themove_groupnode\nUser Interface\nConfiguration\nRobot Interface\nJoint State Information\nTransform Information\nController Interface\nPlanning Scene\nExtensible Capabilities\nThe figure below shows the high-level system architecture for a key node provided by MoveIt calledmove_group.\nThis node serves as an integrator: pulling all the individual components together to provide a set of ROS actions and services for users to use.\nThe users can access the actions and services provided bymove_groupin two ways:\nIn C++- using themove_group_interfacepackage that provides an easy to setup C++ interface to move_group\nThrough a GUI- using theMotion Planning plugin to Rviz(the ROS visualizer)\nmove_groupis a ROS node.\nIt uses the ROS param server to get three kinds of information:\nURDF\nmove_grouplooks for therobot_descriptionparameter to get the URDF for the robot.\nSRDF\nmove_grouplooks for therobot_description_semanticparameter to get the SRDF for the robot. The SRDF is typically created (once) by a user using the MoveIt Setup Assistant.\nMoveIt configuration\nmove_groupwill look for other configuration specific to MoveIt including joint limits, kinematics, motion planning, perception and other information. Config files for these components are automatically generated by the MoveIt Setup Assistant and stored in theconfigdirectory of the corresponding MoveIt config package for the robot.\nmove_grouptalks to the robot through ROS topics and actions.\nIt communicates with the robot to get current state information (positions of the joints, etc.), to get point clouds and other sensor data from the robot sensors and to talk to the controllers on the robot.\nmove_grouplistens on the/joint_statestopic for determining the current state information - i.e. determining where each joint of the robot is.move_groupis capable of listening to multiple publishers on this topic even if they are publishing only partial information about the robot state (e.g. separate publishers may be used for the arm and mobile base of a robot).\nNote thatmove_groupwill not set up its own joint state publisher - this is something that has to be implemented on each robot.\nmove_groupmonitors transform information using the ROS TF library.\nThis allows the node to get global information about the pose of the robot (among other things).\nFor instance, the ROS navigation stack will publish the transform between the map frame and base frame of the robot to TF.move_groupcan use TF to figure out this transform for internal use. Note thatmove_grouponly listens to TF.\nTo publish TF information from your robot, you will need to have arobot_state_publishernode running on your robot.\nmove_grouptalks to the controllers on the robot using the FollowJointTrajectoryAction interface.\nThis is a ROS action interface.\nA server on the robot needs to service this action - this server is not provided bymove_groupitself.move_groupwill only instantiate a client to talk to this controller action server on your robot.\nmove_groupuses the Planning Scene Monitor to maintain aplanning scene, which is a representation of the world and the current state of the robot.\nThe robot state can include any objects attached to (carried by) the robot which are considered to be rigidly attached to the robot.\nMore details on the architecture for maintaining and updating theplanning sceneare outlined in the Planning Scene section below.\nmove_groupis structured to be easily extensible.\nIndividual capabilities like pick and place, kinematics, motion planning are actually implemented as separate plugins with a common base class.\nThe plugins are configurable using ROS through a set of ROS yaml parameters and through the use of the ROS pluginlib library.\nMost users will not have to configure move_group plugins since they come automatically configured in the launch files generated by the MoveIt Setup Assistant.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_description\nmove_group\nrobot_description_semantic\nmove_group\nconfig\nmove_group\nmove_group\n/joint_states\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_state_publisher\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee109"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/move_group.html#extensible-capabilities",
  "title": "The move_group node — MoveIt Documentation: Rolling  documentation",
  "content": "Themove_groupnode\nUser Interface\nConfiguration\nRobot Interface\nJoint State Information\nTransform Information\nController Interface\nPlanning Scene\nExtensible Capabilities\nThe figure below shows the high-level system architecture for a key node provided by MoveIt calledmove_group.\nThis node serves as an integrator: pulling all the individual components together to provide a set of ROS actions and services for users to use.\nThe users can access the actions and services provided bymove_groupin two ways:\nIn C++- using themove_group_interfacepackage that provides an easy to setup C++ interface to move_group\nThrough a GUI- using theMotion Planning plugin to Rviz(the ROS visualizer)\nmove_groupis a ROS node.\nIt uses the ROS param server to get three kinds of information:\nURDF\nmove_grouplooks for therobot_descriptionparameter to get the URDF for the robot.\nSRDF\nmove_grouplooks for therobot_description_semanticparameter to get the SRDF for the robot. The SRDF is typically created (once) by a user using the MoveIt Setup Assistant.\nMoveIt configuration\nmove_groupwill look for other configuration specific to MoveIt including joint limits, kinematics, motion planning, perception and other information. Config files for these components are automatically generated by the MoveIt Setup Assistant and stored in theconfigdirectory of the corresponding MoveIt config package for the robot.\nmove_grouptalks to the robot through ROS topics and actions.\nIt communicates with the robot to get current state information (positions of the joints, etc.), to get point clouds and other sensor data from the robot sensors and to talk to the controllers on the robot.\nmove_grouplistens on the/joint_statestopic for determining the current state information - i.e. determining where each joint of the robot is.move_groupis capable of listening to multiple publishers on this topic even if they are publishing only partial information about the robot state (e.g. separate publishers may be used for the arm and mobile base of a robot).\nNote thatmove_groupwill not set up its own joint state publisher - this is something that has to be implemented on each robot.\nmove_groupmonitors transform information using the ROS TF library.\nThis allows the node to get global information about the pose of the robot (among other things).\nFor instance, the ROS navigation stack will publish the transform between the map frame and base frame of the robot to TF.move_groupcan use TF to figure out this transform for internal use. Note thatmove_grouponly listens to TF.\nTo publish TF information from your robot, you will need to have arobot_state_publishernode running on your robot.\nmove_grouptalks to the controllers on the robot using the FollowJointTrajectoryAction interface.\nThis is a ROS action interface.\nA server on the robot needs to service this action - this server is not provided bymove_groupitself.move_groupwill only instantiate a client to talk to this controller action server on your robot.\nmove_groupuses the Planning Scene Monitor to maintain aplanning scene, which is a representation of the world and the current state of the robot.\nThe robot state can include any objects attached to (carried by) the robot which are considered to be rigidly attached to the robot.\nMore details on the architecture for maintaining and updating theplanning sceneare outlined in the Planning Scene section below.\nmove_groupis structured to be easily extensible.\nIndividual capabilities like pick and place, kinematics, motion planning are actually implemented as separate plugins with a common base class.\nThe plugins are configurable using ROS through a set of ROS yaml parameters and through the use of the ROS pluginlib library.\nMost users will not have to configure move_group plugins since they come automatically configured in the launch files generated by the MoveIt Setup Assistant.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_description\nmove_group\nrobot_description_semantic\nmove_group\nconfig\nmove_group\nmove_group\n/joint_states\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_state_publisher\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee10a"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/move_group.html#the-move-group-node",
  "title": "The move_group node — MoveIt Documentation: Rolling  documentation",
  "content": "Themove_groupnode\nUser Interface\nConfiguration\nRobot Interface\nJoint State Information\nTransform Information\nController Interface\nPlanning Scene\nExtensible Capabilities\nThe figure below shows the high-level system architecture for a key node provided by MoveIt calledmove_group.\nThis node serves as an integrator: pulling all the individual components together to provide a set of ROS actions and services for users to use.\nThe users can access the actions and services provided bymove_groupin two ways:\nIn C++- using themove_group_interfacepackage that provides an easy to setup C++ interface to move_group\nThrough a GUI- using theMotion Planning plugin to Rviz(the ROS visualizer)\nmove_groupis a ROS node.\nIt uses the ROS param server to get three kinds of information:\nURDF\nmove_grouplooks for therobot_descriptionparameter to get the URDF for the robot.\nSRDF\nmove_grouplooks for therobot_description_semanticparameter to get the SRDF for the robot. The SRDF is typically created (once) by a user using the MoveIt Setup Assistant.\nMoveIt configuration\nmove_groupwill look for other configuration specific to MoveIt including joint limits, kinematics, motion planning, perception and other information. Config files for these components are automatically generated by the MoveIt Setup Assistant and stored in theconfigdirectory of the corresponding MoveIt config package for the robot.\nmove_grouptalks to the robot through ROS topics and actions.\nIt communicates with the robot to get current state information (positions of the joints, etc.), to get point clouds and other sensor data from the robot sensors and to talk to the controllers on the robot.\nmove_grouplistens on the/joint_statestopic for determining the current state information - i.e. determining where each joint of the robot is.move_groupis capable of listening to multiple publishers on this topic even if they are publishing only partial information about the robot state (e.g. separate publishers may be used for the arm and mobile base of a robot).\nNote thatmove_groupwill not set up its own joint state publisher - this is something that has to be implemented on each robot.\nmove_groupmonitors transform information using the ROS TF library.\nThis allows the node to get global information about the pose of the robot (among other things).\nFor instance, the ROS navigation stack will publish the transform between the map frame and base frame of the robot to TF.move_groupcan use TF to figure out this transform for internal use. Note thatmove_grouponly listens to TF.\nTo publish TF information from your robot, you will need to have arobot_state_publishernode running on your robot.\nmove_grouptalks to the controllers on the robot using the FollowJointTrajectoryAction interface.\nThis is a ROS action interface.\nA server on the robot needs to service this action - this server is not provided bymove_groupitself.move_groupwill only instantiate a client to talk to this controller action server on your robot.\nmove_groupuses the Planning Scene Monitor to maintain aplanning scene, which is a representation of the world and the current state of the robot.\nThe robot state can include any objects attached to (carried by) the robot which are considered to be rigidly attached to the robot.\nMore details on the architecture for maintaining and updating theplanning sceneare outlined in the Planning Scene section below.\nmove_groupis structured to be easily extensible.\nIndividual capabilities like pick and place, kinematics, motion planning are actually implemented as separate plugins with a common base class.\nThe plugins are configurable using ROS through a set of ROS yaml parameters and through the use of the ROS pluginlib library.\nMost users will not have to configure move_group plugins since they come automatically configured in the launch files generated by the MoveIt Setup Assistant.\n© Copyright 2024, PickNik Robotics.\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_description\nmove_group\nrobot_description_semantic\nmove_group\nconfig\nmove_group\nmove_group\n/joint_states\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group\nrobot_state_publisher\nmove_group\nmove_group\nmove_group\nmove_group\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee10b"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/planning_scene_monitor.html",
  "title": "Planning Scene Monitor — MoveIt Documentation: Rolling  documentation",
  "content": "Planning Scene Monitor\nWorld Geometry Monitor\n3D Perception\nOctomap\nTheplanningsceneis an object used for storing the representation of the world around the robot and also the state of the robot itself.\nThe internal state of theplanning_sceneobject is typically maintained by aplanning_scene_monitorcomponent that enables reading and writing the state in a thread-safe manner.\nParticularly, themove_groupnode as well as the rviz planning scene plugin maintain their own Planning Scene Monitor (PSM).\nThemove_groupnode’s PSMlistensto the topic/planning_sceneandpublishesits planning scene state to the topicmonitored_planning_scene.\nThe latter is listened to by the rviz planning scene plugin.\nThe world geometry monitor builds world geometry using information from the sensors on the robot such as LIDARs or depth cameras and from user input.\nIt uses theoccupancymapmonitordescribed below to build a 3D representation of the environment around the robot and augments that with information on theplanning_scenetopic for adding object information.\n3D perception in MoveIt is handled by theoccupancymapmonitor.\nThe occupancy map monitor uses a plugin architecture to handle different kinds of sensor input as shown in the Figure above.\nIn particular, MoveIt has inbuilt support for handling two kinds of inputs:\nPoint clouds: handled by thePointCloudOccupancyMapUpdaterplugin.\nDepth images: handled by theDepthImageOccupancyMapUpdaterplugin.\nNote that you can add your own types of updaters as a plugin to the occupancy map monitor.\nThe Occupancy map monitor uses anOctomapto maintain the occupancy map of the environment.\nThe Octomap can actually encode probabilistic information about individual cells although this information is not currently used in MoveIt.\nThe Octomap can directly be passed into FCL, the collision checking library that MoveIt uses.\nThe depth image occupancy map updater includes its ownself-filter, i.e. it will remove visible parts of the robot from the depth map.\nIt uses current information about the robot (the robot state) to carry out this operation.\n© Copyright 2024, PickNik Robotics.\nmove_group\nplanningscene\nplanning_scene\nplanning_scene_monitor\nmove_group\nmove_group\n/planning_scene\nmonitored_planning_scene\noccupancymapmonitor\nplanning_scene\noccupancymapmonitor\nPointCloudOccupancyMapUpdater\nDepthImageOccupancyMapUpdater",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee10c"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/planning_scene_monitor.html#world-geometry-monitor",
  "title": "Planning Scene Monitor — MoveIt Documentation: Rolling  documentation",
  "content": "Planning Scene Monitor\nWorld Geometry Monitor\n3D Perception\nOctomap\nTheplanningsceneis an object used for storing the representation of the world around the robot and also the state of the robot itself.\nThe internal state of theplanning_sceneobject is typically maintained by aplanning_scene_monitorcomponent that enables reading and writing the state in a thread-safe manner.\nParticularly, themove_groupnode as well as the rviz planning scene plugin maintain their own Planning Scene Monitor (PSM).\nThemove_groupnode’s PSMlistensto the topic/planning_sceneandpublishesits planning scene state to the topicmonitored_planning_scene.\nThe latter is listened to by the rviz planning scene plugin.\nThe world geometry monitor builds world geometry using information from the sensors on the robot such as LIDARs or depth cameras and from user input.\nIt uses theoccupancymapmonitordescribed below to build a 3D representation of the environment around the robot and augments that with information on theplanning_scenetopic for adding object information.\n3D perception in MoveIt is handled by theoccupancymapmonitor.\nThe occupancy map monitor uses a plugin architecture to handle different kinds of sensor input as shown in the Figure above.\nIn particular, MoveIt has inbuilt support for handling two kinds of inputs:\nPoint clouds: handled by thePointCloudOccupancyMapUpdaterplugin.\nDepth images: handled by theDepthImageOccupancyMapUpdaterplugin.\nNote that you can add your own types of updaters as a plugin to the occupancy map monitor.\nThe Occupancy map monitor uses anOctomapto maintain the occupancy map of the environment.\nThe Octomap can actually encode probabilistic information about individual cells although this information is not currently used in MoveIt.\nThe Octomap can directly be passed into FCL, the collision checking library that MoveIt uses.\nThe depth image occupancy map updater includes its ownself-filter, i.e. it will remove visible parts of the robot from the depth map.\nIt uses current information about the robot (the robot state) to carry out this operation.\n© Copyright 2024, PickNik Robotics.\nmove_group\nplanningscene\nplanning_scene\nplanning_scene_monitor\nmove_group\nmove_group\n/planning_scene\nmonitored_planning_scene\noccupancymapmonitor\nplanning_scene\noccupancymapmonitor\nPointCloudOccupancyMapUpdater\nDepthImageOccupancyMapUpdater",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee10d"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/planning_scene_monitor.html#d-perception",
  "title": "Planning Scene Monitor — MoveIt Documentation: Rolling  documentation",
  "content": "Planning Scene Monitor\nWorld Geometry Monitor\n3D Perception\nOctomap\nTheplanningsceneis an object used for storing the representation of the world around the robot and also the state of the robot itself.\nThe internal state of theplanning_sceneobject is typically maintained by aplanning_scene_monitorcomponent that enables reading and writing the state in a thread-safe manner.\nParticularly, themove_groupnode as well as the rviz planning scene plugin maintain their own Planning Scene Monitor (PSM).\nThemove_groupnode’s PSMlistensto the topic/planning_sceneandpublishesits planning scene state to the topicmonitored_planning_scene.\nThe latter is listened to by the rviz planning scene plugin.\nThe world geometry monitor builds world geometry using information from the sensors on the robot such as LIDARs or depth cameras and from user input.\nIt uses theoccupancymapmonitordescribed below to build a 3D representation of the environment around the robot and augments that with information on theplanning_scenetopic for adding object information.\n3D perception in MoveIt is handled by theoccupancymapmonitor.\nThe occupancy map monitor uses a plugin architecture to handle different kinds of sensor input as shown in the Figure above.\nIn particular, MoveIt has inbuilt support for handling two kinds of inputs:\nPoint clouds: handled by thePointCloudOccupancyMapUpdaterplugin.\nDepth images: handled by theDepthImageOccupancyMapUpdaterplugin.\nNote that you can add your own types of updaters as a plugin to the occupancy map monitor.\nThe Occupancy map monitor uses anOctomapto maintain the occupancy map of the environment.\nThe Octomap can actually encode probabilistic information about individual cells although this information is not currently used in MoveIt.\nThe Octomap can directly be passed into FCL, the collision checking library that MoveIt uses.\nThe depth image occupancy map updater includes its ownself-filter, i.e. it will remove visible parts of the robot from the depth map.\nIt uses current information about the robot (the robot state) to carry out this operation.\n© Copyright 2024, PickNik Robotics.\nmove_group\nplanningscene\nplanning_scene\nplanning_scene_monitor\nmove_group\nmove_group\n/planning_scene\nmonitored_planning_scene\noccupancymapmonitor\nplanning_scene\noccupancymapmonitor\nPointCloudOccupancyMapUpdater\nDepthImageOccupancyMapUpdater",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee10e"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/planning_scene_monitor.html#octomap",
  "title": "Planning Scene Monitor — MoveIt Documentation: Rolling  documentation",
  "content": "Planning Scene Monitor\nWorld Geometry Monitor\n3D Perception\nOctomap\nTheplanningsceneis an object used for storing the representation of the world around the robot and also the state of the robot itself.\nThe internal state of theplanning_sceneobject is typically maintained by aplanning_scene_monitorcomponent that enables reading and writing the state in a thread-safe manner.\nParticularly, themove_groupnode as well as the rviz planning scene plugin maintain their own Planning Scene Monitor (PSM).\nThemove_groupnode’s PSMlistensto the topic/planning_sceneandpublishesits planning scene state to the topicmonitored_planning_scene.\nThe latter is listened to by the rviz planning scene plugin.\nThe world geometry monitor builds world geometry using information from the sensors on the robot such as LIDARs or depth cameras and from user input.\nIt uses theoccupancymapmonitordescribed below to build a 3D representation of the environment around the robot and augments that with information on theplanning_scenetopic for adding object information.\n3D perception in MoveIt is handled by theoccupancymapmonitor.\nThe occupancy map monitor uses a plugin architecture to handle different kinds of sensor input as shown in the Figure above.\nIn particular, MoveIt has inbuilt support for handling two kinds of inputs:\nPoint clouds: handled by thePointCloudOccupancyMapUpdaterplugin.\nDepth images: handled by theDepthImageOccupancyMapUpdaterplugin.\nNote that you can add your own types of updaters as a plugin to the occupancy map monitor.\nThe Occupancy map monitor uses anOctomapto maintain the occupancy map of the environment.\nThe Octomap can actually encode probabilistic information about individual cells although this information is not currently used in MoveIt.\nThe Octomap can directly be passed into FCL, the collision checking library that MoveIt uses.\nThe depth image occupancy map updater includes its ownself-filter, i.e. it will remove visible parts of the robot from the depth map.\nIt uses current information about the robot (the robot state) to carry out this operation.\n© Copyright 2024, PickNik Robotics.\nmove_group\nplanningscene\nplanning_scene\nplanning_scene_monitor\nmove_group\nmove_group\n/planning_scene\nmonitored_planning_scene\noccupancymapmonitor\nplanning_scene\noccupancymapmonitor\nPointCloudOccupancyMapUpdater\nDepthImageOccupancyMapUpdater",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee10f"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/planning_scene_monitor.html#depth-image-occupancy-map-updater",
  "title": "Planning Scene Monitor — MoveIt Documentation: Rolling  documentation",
  "content": "Planning Scene Monitor\nWorld Geometry Monitor\n3D Perception\nOctomap\nTheplanningsceneis an object used for storing the representation of the world around the robot and also the state of the robot itself.\nThe internal state of theplanning_sceneobject is typically maintained by aplanning_scene_monitorcomponent that enables reading and writing the state in a thread-safe manner.\nParticularly, themove_groupnode as well as the rviz planning scene plugin maintain their own Planning Scene Monitor (PSM).\nThemove_groupnode’s PSMlistensto the topic/planning_sceneandpublishesits planning scene state to the topicmonitored_planning_scene.\nThe latter is listened to by the rviz planning scene plugin.\nThe world geometry monitor builds world geometry using information from the sensors on the robot such as LIDARs or depth cameras and from user input.\nIt uses theoccupancymapmonitordescribed below to build a 3D representation of the environment around the robot and augments that with information on theplanning_scenetopic for adding object information.\n3D perception in MoveIt is handled by theoccupancymapmonitor.\nThe occupancy map monitor uses a plugin architecture to handle different kinds of sensor input as shown in the Figure above.\nIn particular, MoveIt has inbuilt support for handling two kinds of inputs:\nPoint clouds: handled by thePointCloudOccupancyMapUpdaterplugin.\nDepth images: handled by theDepthImageOccupancyMapUpdaterplugin.\nNote that you can add your own types of updaters as a plugin to the occupancy map monitor.\nThe Occupancy map monitor uses anOctomapto maintain the occupancy map of the environment.\nThe Octomap can actually encode probabilistic information about individual cells although this information is not currently used in MoveIt.\nThe Octomap can directly be passed into FCL, the collision checking library that MoveIt uses.\nThe depth image occupancy map updater includes its ownself-filter, i.e. it will remove visible parts of the robot from the depth map.\nIt uses current information about the robot (the robot state) to carry out this operation.\n© Copyright 2024, PickNik Robotics.\nmove_group\nplanningscene\nplanning_scene\nplanning_scene_monitor\nmove_group\nmove_group\n/planning_scene\nmonitored_planning_scene\noccupancymapmonitor\nplanning_scene\noccupancymapmonitor\nPointCloudOccupancyMapUpdater\nDepthImageOccupancyMapUpdater",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee110"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/planning_scene_monitor.html#planning-scene-monitor",
  "title": "Planning Scene Monitor — MoveIt Documentation: Rolling  documentation",
  "content": "Planning Scene Monitor\nWorld Geometry Monitor\n3D Perception\nOctomap\nTheplanningsceneis an object used for storing the representation of the world around the robot and also the state of the robot itself.\nThe internal state of theplanning_sceneobject is typically maintained by aplanning_scene_monitorcomponent that enables reading and writing the state in a thread-safe manner.\nParticularly, themove_groupnode as well as the rviz planning scene plugin maintain their own Planning Scene Monitor (PSM).\nThemove_groupnode’s PSMlistensto the topic/planning_sceneandpublishesits planning scene state to the topicmonitored_planning_scene.\nThe latter is listened to by the rviz planning scene plugin.\nThe world geometry monitor builds world geometry using information from the sensors on the robot such as LIDARs or depth cameras and from user input.\nIt uses theoccupancymapmonitordescribed below to build a 3D representation of the environment around the robot and augments that with information on theplanning_scenetopic for adding object information.\n3D perception in MoveIt is handled by theoccupancymapmonitor.\nThe occupancy map monitor uses a plugin architecture to handle different kinds of sensor input as shown in the Figure above.\nIn particular, MoveIt has inbuilt support for handling two kinds of inputs:\nPoint clouds: handled by thePointCloudOccupancyMapUpdaterplugin.\nDepth images: handled by theDepthImageOccupancyMapUpdaterplugin.\nNote that you can add your own types of updaters as a plugin to the occupancy map monitor.\nThe Occupancy map monitor uses anOctomapto maintain the occupancy map of the environment.\nThe Octomap can actually encode probabilistic information about individual cells although this information is not currently used in MoveIt.\nThe Octomap can directly be passed into FCL, the collision checking library that MoveIt uses.\nThe depth image occupancy map updater includes its ownself-filter, i.e. it will remove visible parts of the robot from the depth map.\nIt uses current information about the robot (the robot state) to carry out this operation.\n© Copyright 2024, PickNik Robotics.\nmove_group\nplanningscene\nplanning_scene\nplanning_scene_monitor\nmove_group\nmove_group\n/planning_scene\nmonitored_planning_scene\noccupancymapmonitor\nplanning_scene\noccupancymapmonitor\nPointCloudOccupancyMapUpdater\nDepthImageOccupancyMapUpdater",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee111"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/trajectory_processing.html",
  "title": "Trajectory Processing — MoveIt Documentation: Rolling  documentation",
  "content": "Trajectory Processing\nTime parameterization\nMotion planners typically only generate “paths”, i.e. there is no timing information associated with the paths.\nMoveIt includes severaltrajectory processingalgorithms that can work on these paths and generate trajectories that are properly time-parameterized accounting for the maximum velocity and acceleration limits imposed on individual joints.\nThese limits are read from a specialjoint_limits.yamlconfiguration file that is specified for each robot.\nThe configuration file is optional and it overrides any velocity or acceleration limits from the URDF.\nThe recommended algorithm as of January 2023 isTimeOptimalTrajectoryGeneration(TOTG).\nA caveat for this algorithm is that the robot must start and end at rest.\nBy default, the TOTG timestep is 0.1 seconds.\n© Copyright 2024, PickNik Robotics.\nmove_group\njoint_limits.yaml",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee112"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/trajectory_processing.html#time-parameterization",
  "title": "Trajectory Processing — MoveIt Documentation: Rolling  documentation",
  "content": "Trajectory Processing\nTime parameterization\nMotion planners typically only generate “paths”, i.e. there is no timing information associated with the paths.\nMoveIt includes severaltrajectory processingalgorithms that can work on these paths and generate trajectories that are properly time-parameterized accounting for the maximum velocity and acceleration limits imposed on individual joints.\nThese limits are read from a specialjoint_limits.yamlconfiguration file that is specified for each robot.\nThe configuration file is optional and it overrides any velocity or acceleration limits from the URDF.\nThe recommended algorithm as of January 2023 isTimeOptimalTrajectoryGeneration(TOTG).\nA caveat for this algorithm is that the robot must start and end at rest.\nBy default, the TOTG timestep is 0.1 seconds.\n© Copyright 2024, PickNik Robotics.\nmove_group\njoint_limits.yaml",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee113"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/trajectory_processing.html#trajectory-processing",
  "title": "Trajectory Processing — MoveIt Documentation: Rolling  documentation",
  "content": "Trajectory Processing\nTime parameterization\nMotion planners typically only generate “paths”, i.e. there is no timing information associated with the paths.\nMoveIt includes severaltrajectory processingalgorithms that can work on these paths and generate trajectories that are properly time-parameterized accounting for the maximum velocity and acceleration limits imposed on individual joints.\nThese limits are read from a specialjoint_limits.yamlconfiguration file that is specified for each robot.\nThe configuration file is optional and it overrides any velocity or acceleration limits from the URDF.\nThe recommended algorithm as of January 2023 isTimeOptimalTrajectoryGeneration(TOTG).\nA caveat for this algorithm is that the robot must start and end at rest.\nBy default, the TOTG timestep is 0.1 seconds.\n© Copyright 2024, PickNik Robotics.\nmove_group\njoint_limits.yaml",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee114"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/moveit_task_constructor/moveit_task_constructor.html",
  "title": "MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Task Constructor\nWhat is MoveIt Task Constructor?\nMTC Stages\nMTC Containers\nInitializing a MTC Task\nAdding containers and stages to a MTC Task\nSetting planning solvers\nSetting Properties\nCost calculator for Stages\nPlanning and Executing a MTC Task\nLinks to Additional Information\nThere are three possible stages relating to the result flow:\nGenerators\nPropagators\nConnectors\nCurrently available containers:\nSerial\nParallel\nParallel containers combine a set of stages to allow planning alternate solutions.\nThe top-level planning problem is specified as a MTC Task and the subproblems which are specified by Stages are added to the MTC task object.\nAdding a stage to MTC task\nContainers derive from Stage and hence containers can be added to MTC task similarly\nStages that do motion planning need solver information.\nSolvers available in MTC\nPipelinePlanner- Uses MoveIt’s planning pipeline\nJointInterpolation- Interpolates between the start and goal joint states. It does not support complex motions.\nCartesianPath- Moves the end effector in a straight line in Cartesian space.\nCode Example on how to initialize the solver\nThese solvers will be passed into stages likeMoveTo,MoveRelative, andConnect.\nCostTerm is the basic interface to compute costs for solutions for MTC stages.\nCostTerm implementations available in MTC\nConstant- Adds a constant cost to each solution\nPathLength- Cost depends on trajectory length with optional weight for different joints\nTrajectoryDuration- Cost depends on execution duration of the whole trajectory\nTrajectoryCostTerm- Cost terms that only work on SubTrajectory solutions\nLambdaCostTerm- Pass in a lambda expression to calculate cost\nDistanceToReference- Cost depends on weighted joint space distance to a reference point\nLinkMotion- Cost depends on length of Cartesian trajectory of a link\nClearance- Cost is inverse of distance to collision\nExample code on how to set CostTerm usingLambdaCostTerm\nAll stages provided by MTC have default cost terms. Stages which produce trajectories as solutions usually use path length to calculate cost.\nPlanning an MTC task will return aMoveItErrorCode.\nReferhereto identity the different error types.\nIf planning succeeds, you can expect the plan function to returnmoveit_msgs::msg::MoveItErrorCodes::SUCCESS.\nAfter planning, extract the first successful solution and pass it to the execute function. This will create anexecute_task_solutionaction client.\nThe action server resides in theexecute_task_solution_capabilityplugin provided by MTC.\nThe plugin extendsMoveGroupCapability. It constructs aMotionPlanRequestfrom the MTC solution and uses MoveIt’sPlanExecutionto actuate the robot.\nHere is atutorialon how to create a Pick and Place pipeline using MTC.\nThe links listed below contain more information on stages and containers provided by MTC\n© Copyright 2024, PickNik Robotics.\nautonode=std::make_shared<rclcpp::Node>();autotask=std::make_unique<moveit::task_constructor::Task>();task->loadRobotModel(node);// Set controllers used to execute robot motion. If not set, MoveIt has controller discovery logic.task->setProperty(\"trajectory_execution_info\",\"joint_trajectory_controller gripper_controller\");\nautocurrent_state=std::make_unique<moveit::task_constructor::stages::CurrentState>(\"current_state\");task->add(std::move(current_state));\nautocontainer=std::make_unique<moveit::task_constructor::SerialContainer>(\"Pick Object\");// TODO: Add stages to the container before adding the container to MTC tasktask->add(std::move(container));\nconstautomtc_pipeline_planner=std::make_shared<moveit::task_constructor::solvers::PipelinePlanner>(node,\"ompl\",\"RRTConnectkConfigDefault\");constautomtc_joint_interpolation_planner=std::make_shared<moveit::task_constructor::solvers::JointInterpolationPlanner>();constautomtc_cartesian_planner=std::make_shared<moveit::task_constructor::solvers::CartesianPath>();\nvoidsetProperty(conststd::string&name,constboost::any&value);\nstage->setCostTerm(moveit::task_constructor::LambdaCostTerm([](constmoveit::task_constructor::SubTrajectory&traj){return100*traj.cost();}));\nautoerror_code=task.plan()\nautoresult=task.execute(*task.solutions().front());\nmove_group\nCurrentState\nGeneratePose\nCurrentState\nModifyPlanningScene\nCurrentState\nGeneratePose\nMoveRelative\nComputeIK\nGenerateGraspPose\nGenerateGraspPose\nComputeIK\nGeneratePose\nGeneratePose\nPipelinePlanner\nJointInterpolation\nCartesianPath\nMoveTo\nMoveRelative\nConnect\nConstant\nPathLength\nTrajectoryDuration\nTrajectoryCostTerm\nLambdaCostTerm\nDistanceToReference\nLinkMotion\nClearance\nLambdaCostTerm\nMoveItErrorCode\nmoveit_msgs::msg::MoveItErrorCodes::SUCCESS\nexecute_task_solution\nexecute_task_solution_capability\nMoveGroupCapability\nMotionPlanRequest\nPlanExecution",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee115"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/moveit_task_constructor/moveit_task_constructor.html#what-is-moveit-task-constructor",
  "title": "MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Task Constructor\nWhat is MoveIt Task Constructor?\nMTC Stages\nMTC Containers\nInitializing a MTC Task\nAdding containers and stages to a MTC Task\nSetting planning solvers\nSetting Properties\nCost calculator for Stages\nPlanning and Executing a MTC Task\nLinks to Additional Information\nThere are three possible stages relating to the result flow:\nGenerators\nPropagators\nConnectors\nCurrently available containers:\nSerial\nParallel\nParallel containers combine a set of stages to allow planning alternate solutions.\nThe top-level planning problem is specified as a MTC Task and the subproblems which are specified by Stages are added to the MTC task object.\nAdding a stage to MTC task\nContainers derive from Stage and hence containers can be added to MTC task similarly\nStages that do motion planning need solver information.\nSolvers available in MTC\nPipelinePlanner- Uses MoveIt’s planning pipeline\nJointInterpolation- Interpolates between the start and goal joint states. It does not support complex motions.\nCartesianPath- Moves the end effector in a straight line in Cartesian space.\nCode Example on how to initialize the solver\nThese solvers will be passed into stages likeMoveTo,MoveRelative, andConnect.\nCostTerm is the basic interface to compute costs for solutions for MTC stages.\nCostTerm implementations available in MTC\nConstant- Adds a constant cost to each solution\nPathLength- Cost depends on trajectory length with optional weight for different joints\nTrajectoryDuration- Cost depends on execution duration of the whole trajectory\nTrajectoryCostTerm- Cost terms that only work on SubTrajectory solutions\nLambdaCostTerm- Pass in a lambda expression to calculate cost\nDistanceToReference- Cost depends on weighted joint space distance to a reference point\nLinkMotion- Cost depends on length of Cartesian trajectory of a link\nClearance- Cost is inverse of distance to collision\nExample code on how to set CostTerm usingLambdaCostTerm\nAll stages provided by MTC have default cost terms. Stages which produce trajectories as solutions usually use path length to calculate cost.\nPlanning an MTC task will return aMoveItErrorCode.\nReferhereto identity the different error types.\nIf planning succeeds, you can expect the plan function to returnmoveit_msgs::msg::MoveItErrorCodes::SUCCESS.\nAfter planning, extract the first successful solution and pass it to the execute function. This will create anexecute_task_solutionaction client.\nThe action server resides in theexecute_task_solution_capabilityplugin provided by MTC.\nThe plugin extendsMoveGroupCapability. It constructs aMotionPlanRequestfrom the MTC solution and uses MoveIt’sPlanExecutionto actuate the robot.\nHere is atutorialon how to create a Pick and Place pipeline using MTC.\nThe links listed below contain more information on stages and containers provided by MTC\n© Copyright 2024, PickNik Robotics.\nautonode=std::make_shared<rclcpp::Node>();autotask=std::make_unique<moveit::task_constructor::Task>();task->loadRobotModel(node);// Set controllers used to execute robot motion. If not set, MoveIt has controller discovery logic.task->setProperty(\"trajectory_execution_info\",\"joint_trajectory_controller gripper_controller\");\nautocurrent_state=std::make_unique<moveit::task_constructor::stages::CurrentState>(\"current_state\");task->add(std::move(current_state));\nautocontainer=std::make_unique<moveit::task_constructor::SerialContainer>(\"Pick Object\");// TODO: Add stages to the container before adding the container to MTC tasktask->add(std::move(container));\nconstautomtc_pipeline_planner=std::make_shared<moveit::task_constructor::solvers::PipelinePlanner>(node,\"ompl\",\"RRTConnectkConfigDefault\");constautomtc_joint_interpolation_planner=std::make_shared<moveit::task_constructor::solvers::JointInterpolationPlanner>();constautomtc_cartesian_planner=std::make_shared<moveit::task_constructor::solvers::CartesianPath>();\nvoidsetProperty(conststd::string&name,constboost::any&value);\nstage->setCostTerm(moveit::task_constructor::LambdaCostTerm([](constmoveit::task_constructor::SubTrajectory&traj){return100*traj.cost();}));\nautoerror_code=task.plan()\nautoresult=task.execute(*task.solutions().front());\nmove_group\nCurrentState\nGeneratePose\nCurrentState\nModifyPlanningScene\nCurrentState\nGeneratePose\nMoveRelative\nComputeIK\nGenerateGraspPose\nGenerateGraspPose\nComputeIK\nGeneratePose\nGeneratePose\nPipelinePlanner\nJointInterpolation\nCartesianPath\nMoveTo\nMoveRelative\nConnect\nConstant\nPathLength\nTrajectoryDuration\nTrajectoryCostTerm\nLambdaCostTerm\nDistanceToReference\nLinkMotion\nClearance\nLambdaCostTerm\nMoveItErrorCode\nmoveit_msgs::msg::MoveItErrorCodes::SUCCESS\nexecute_task_solution\nexecute_task_solution_capability\nMoveGroupCapability\nMotionPlanRequest\nPlanExecution",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee116"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/moveit_task_constructor/moveit_task_constructor.html#mtc-stages",
  "title": "MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Task Constructor\nWhat is MoveIt Task Constructor?\nMTC Stages\nMTC Containers\nInitializing a MTC Task\nAdding containers and stages to a MTC Task\nSetting planning solvers\nSetting Properties\nCost calculator for Stages\nPlanning and Executing a MTC Task\nLinks to Additional Information\nThere are three possible stages relating to the result flow:\nGenerators\nPropagators\nConnectors\nCurrently available containers:\nSerial\nParallel\nParallel containers combine a set of stages to allow planning alternate solutions.\nThe top-level planning problem is specified as a MTC Task and the subproblems which are specified by Stages are added to the MTC task object.\nAdding a stage to MTC task\nContainers derive from Stage and hence containers can be added to MTC task similarly\nStages that do motion planning need solver information.\nSolvers available in MTC\nPipelinePlanner- Uses MoveIt’s planning pipeline\nJointInterpolation- Interpolates between the start and goal joint states. It does not support complex motions.\nCartesianPath- Moves the end effector in a straight line in Cartesian space.\nCode Example on how to initialize the solver\nThese solvers will be passed into stages likeMoveTo,MoveRelative, andConnect.\nCostTerm is the basic interface to compute costs for solutions for MTC stages.\nCostTerm implementations available in MTC\nConstant- Adds a constant cost to each solution\nPathLength- Cost depends on trajectory length with optional weight for different joints\nTrajectoryDuration- Cost depends on execution duration of the whole trajectory\nTrajectoryCostTerm- Cost terms that only work on SubTrajectory solutions\nLambdaCostTerm- Pass in a lambda expression to calculate cost\nDistanceToReference- Cost depends on weighted joint space distance to a reference point\nLinkMotion- Cost depends on length of Cartesian trajectory of a link\nClearance- Cost is inverse of distance to collision\nExample code on how to set CostTerm usingLambdaCostTerm\nAll stages provided by MTC have default cost terms. Stages which produce trajectories as solutions usually use path length to calculate cost.\nPlanning an MTC task will return aMoveItErrorCode.\nReferhereto identity the different error types.\nIf planning succeeds, you can expect the plan function to returnmoveit_msgs::msg::MoveItErrorCodes::SUCCESS.\nAfter planning, extract the first successful solution and pass it to the execute function. This will create anexecute_task_solutionaction client.\nThe action server resides in theexecute_task_solution_capabilityplugin provided by MTC.\nThe plugin extendsMoveGroupCapability. It constructs aMotionPlanRequestfrom the MTC solution and uses MoveIt’sPlanExecutionto actuate the robot.\nHere is atutorialon how to create a Pick and Place pipeline using MTC.\nThe links listed below contain more information on stages and containers provided by MTC\n© Copyright 2024, PickNik Robotics.\nautonode=std::make_shared<rclcpp::Node>();autotask=std::make_unique<moveit::task_constructor::Task>();task->loadRobotModel(node);// Set controllers used to execute robot motion. If not set, MoveIt has controller discovery logic.task->setProperty(\"trajectory_execution_info\",\"joint_trajectory_controller gripper_controller\");\nautocurrent_state=std::make_unique<moveit::task_constructor::stages::CurrentState>(\"current_state\");task->add(std::move(current_state));\nautocontainer=std::make_unique<moveit::task_constructor::SerialContainer>(\"Pick Object\");// TODO: Add stages to the container before adding the container to MTC tasktask->add(std::move(container));\nconstautomtc_pipeline_planner=std::make_shared<moveit::task_constructor::solvers::PipelinePlanner>(node,\"ompl\",\"RRTConnectkConfigDefault\");constautomtc_joint_interpolation_planner=std::make_shared<moveit::task_constructor::solvers::JointInterpolationPlanner>();constautomtc_cartesian_planner=std::make_shared<moveit::task_constructor::solvers::CartesianPath>();\nvoidsetProperty(conststd::string&name,constboost::any&value);\nstage->setCostTerm(moveit::task_constructor::LambdaCostTerm([](constmoveit::task_constructor::SubTrajectory&traj){return100*traj.cost();}));\nautoerror_code=task.plan()\nautoresult=task.execute(*task.solutions().front());\nmove_group\nCurrentState\nGeneratePose\nCurrentState\nModifyPlanningScene\nCurrentState\nGeneratePose\nMoveRelative\nComputeIK\nGenerateGraspPose\nGenerateGraspPose\nComputeIK\nGeneratePose\nGeneratePose\nPipelinePlanner\nJointInterpolation\nCartesianPath\nMoveTo\nMoveRelative\nConnect\nConstant\nPathLength\nTrajectoryDuration\nTrajectoryCostTerm\nLambdaCostTerm\nDistanceToReference\nLinkMotion\nClearance\nLambdaCostTerm\nMoveItErrorCode\nmoveit_msgs::msg::MoveItErrorCodes::SUCCESS\nexecute_task_solution\nexecute_task_solution_capability\nMoveGroupCapability\nMotionPlanRequest\nPlanExecution",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee117"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/moveit_task_constructor/moveit_task_constructor.html#generator-stage",
  "title": "MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Task Constructor\nWhat is MoveIt Task Constructor?\nMTC Stages\nMTC Containers\nInitializing a MTC Task\nAdding containers and stages to a MTC Task\nSetting planning solvers\nSetting Properties\nCost calculator for Stages\nPlanning and Executing a MTC Task\nLinks to Additional Information\nThere are three possible stages relating to the result flow:\nGenerators\nPropagators\nConnectors\nCurrently available containers:\nSerial\nParallel\nParallel containers combine a set of stages to allow planning alternate solutions.\nThe top-level planning problem is specified as a MTC Task and the subproblems which are specified by Stages are added to the MTC task object.\nAdding a stage to MTC task\nContainers derive from Stage and hence containers can be added to MTC task similarly\nStages that do motion planning need solver information.\nSolvers available in MTC\nPipelinePlanner- Uses MoveIt’s planning pipeline\nJointInterpolation- Interpolates between the start and goal joint states. It does not support complex motions.\nCartesianPath- Moves the end effector in a straight line in Cartesian space.\nCode Example on how to initialize the solver\nThese solvers will be passed into stages likeMoveTo,MoveRelative, andConnect.\nCostTerm is the basic interface to compute costs for solutions for MTC stages.\nCostTerm implementations available in MTC\nConstant- Adds a constant cost to each solution\nPathLength- Cost depends on trajectory length with optional weight for different joints\nTrajectoryDuration- Cost depends on execution duration of the whole trajectory\nTrajectoryCostTerm- Cost terms that only work on SubTrajectory solutions\nLambdaCostTerm- Pass in a lambda expression to calculate cost\nDistanceToReference- Cost depends on weighted joint space distance to a reference point\nLinkMotion- Cost depends on length of Cartesian trajectory of a link\nClearance- Cost is inverse of distance to collision\nExample code on how to set CostTerm usingLambdaCostTerm\nAll stages provided by MTC have default cost terms. Stages which produce trajectories as solutions usually use path length to calculate cost.\nPlanning an MTC task will return aMoveItErrorCode.\nReferhereto identity the different error types.\nIf planning succeeds, you can expect the plan function to returnmoveit_msgs::msg::MoveItErrorCodes::SUCCESS.\nAfter planning, extract the first successful solution and pass it to the execute function. This will create anexecute_task_solutionaction client.\nThe action server resides in theexecute_task_solution_capabilityplugin provided by MTC.\nThe plugin extendsMoveGroupCapability. It constructs aMotionPlanRequestfrom the MTC solution and uses MoveIt’sPlanExecutionto actuate the robot.\nHere is atutorialon how to create a Pick and Place pipeline using MTC.\nThe links listed below contain more information on stages and containers provided by MTC\n© Copyright 2024, PickNik Robotics.\nautonode=std::make_shared<rclcpp::Node>();autotask=std::make_unique<moveit::task_constructor::Task>();task->loadRobotModel(node);// Set controllers used to execute robot motion. If not set, MoveIt has controller discovery logic.task->setProperty(\"trajectory_execution_info\",\"joint_trajectory_controller gripper_controller\");\nautocurrent_state=std::make_unique<moveit::task_constructor::stages::CurrentState>(\"current_state\");task->add(std::move(current_state));\nautocontainer=std::make_unique<moveit::task_constructor::SerialContainer>(\"Pick Object\");// TODO: Add stages to the container before adding the container to MTC tasktask->add(std::move(container));\nconstautomtc_pipeline_planner=std::make_shared<moveit::task_constructor::solvers::PipelinePlanner>(node,\"ompl\",\"RRTConnectkConfigDefault\");constautomtc_joint_interpolation_planner=std::make_shared<moveit::task_constructor::solvers::JointInterpolationPlanner>();constautomtc_cartesian_planner=std::make_shared<moveit::task_constructor::solvers::CartesianPath>();\nvoidsetProperty(conststd::string&name,constboost::any&value);\nstage->setCostTerm(moveit::task_constructor::LambdaCostTerm([](constmoveit::task_constructor::SubTrajectory&traj){return100*traj.cost();}));\nautoerror_code=task.plan()\nautoresult=task.execute(*task.solutions().front());\nmove_group\nCurrentState\nGeneratePose\nCurrentState\nModifyPlanningScene\nCurrentState\nGeneratePose\nMoveRelative\nComputeIK\nGenerateGraspPose\nGenerateGraspPose\nComputeIK\nGeneratePose\nGeneratePose\nPipelinePlanner\nJointInterpolation\nCartesianPath\nMoveTo\nMoveRelative\nConnect\nConstant\nPathLength\nTrajectoryDuration\nTrajectoryCostTerm\nLambdaCostTerm\nDistanceToReference\nLinkMotion\nClearance\nLambdaCostTerm\nMoveItErrorCode\nmoveit_msgs::msg::MoveItErrorCodes::SUCCESS\nexecute_task_solution\nexecute_task_solution_capability\nMoveGroupCapability\nMotionPlanRequest\nPlanExecution",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee118"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/moveit_task_constructor/moveit_task_constructor.html#propagating-stage",
  "title": "MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Task Constructor\nWhat is MoveIt Task Constructor?\nMTC Stages\nMTC Containers\nInitializing a MTC Task\nAdding containers and stages to a MTC Task\nSetting planning solvers\nSetting Properties\nCost calculator for Stages\nPlanning and Executing a MTC Task\nLinks to Additional Information\nThere are three possible stages relating to the result flow:\nGenerators\nPropagators\nConnectors\nCurrently available containers:\nSerial\nParallel\nParallel containers combine a set of stages to allow planning alternate solutions.\nThe top-level planning problem is specified as a MTC Task and the subproblems which are specified by Stages are added to the MTC task object.\nAdding a stage to MTC task\nContainers derive from Stage and hence containers can be added to MTC task similarly\nStages that do motion planning need solver information.\nSolvers available in MTC\nPipelinePlanner- Uses MoveIt’s planning pipeline\nJointInterpolation- Interpolates between the start and goal joint states. It does not support complex motions.\nCartesianPath- Moves the end effector in a straight line in Cartesian space.\nCode Example on how to initialize the solver\nThese solvers will be passed into stages likeMoveTo,MoveRelative, andConnect.\nCostTerm is the basic interface to compute costs for solutions for MTC stages.\nCostTerm implementations available in MTC\nConstant- Adds a constant cost to each solution\nPathLength- Cost depends on trajectory length with optional weight for different joints\nTrajectoryDuration- Cost depends on execution duration of the whole trajectory\nTrajectoryCostTerm- Cost terms that only work on SubTrajectory solutions\nLambdaCostTerm- Pass in a lambda expression to calculate cost\nDistanceToReference- Cost depends on weighted joint space distance to a reference point\nLinkMotion- Cost depends on length of Cartesian trajectory of a link\nClearance- Cost is inverse of distance to collision\nExample code on how to set CostTerm usingLambdaCostTerm\nAll stages provided by MTC have default cost terms. Stages which produce trajectories as solutions usually use path length to calculate cost.\nPlanning an MTC task will return aMoveItErrorCode.\nReferhereto identity the different error types.\nIf planning succeeds, you can expect the plan function to returnmoveit_msgs::msg::MoveItErrorCodes::SUCCESS.\nAfter planning, extract the first successful solution and pass it to the execute function. This will create anexecute_task_solutionaction client.\nThe action server resides in theexecute_task_solution_capabilityplugin provided by MTC.\nThe plugin extendsMoveGroupCapability. It constructs aMotionPlanRequestfrom the MTC solution and uses MoveIt’sPlanExecutionto actuate the robot.\nHere is atutorialon how to create a Pick and Place pipeline using MTC.\nThe links listed below contain more information on stages and containers provided by MTC\n© Copyright 2024, PickNik Robotics.\nautonode=std::make_shared<rclcpp::Node>();autotask=std::make_unique<moveit::task_constructor::Task>();task->loadRobotModel(node);// Set controllers used to execute robot motion. If not set, MoveIt has controller discovery logic.task->setProperty(\"trajectory_execution_info\",\"joint_trajectory_controller gripper_controller\");\nautocurrent_state=std::make_unique<moveit::task_constructor::stages::CurrentState>(\"current_state\");task->add(std::move(current_state));\nautocontainer=std::make_unique<moveit::task_constructor::SerialContainer>(\"Pick Object\");// TODO: Add stages to the container before adding the container to MTC tasktask->add(std::move(container));\nconstautomtc_pipeline_planner=std::make_shared<moveit::task_constructor::solvers::PipelinePlanner>(node,\"ompl\",\"RRTConnectkConfigDefault\");constautomtc_joint_interpolation_planner=std::make_shared<moveit::task_constructor::solvers::JointInterpolationPlanner>();constautomtc_cartesian_planner=std::make_shared<moveit::task_constructor::solvers::CartesianPath>();\nvoidsetProperty(conststd::string&name,constboost::any&value);\nstage->setCostTerm(moveit::task_constructor::LambdaCostTerm([](constmoveit::task_constructor::SubTrajectory&traj){return100*traj.cost();}));\nautoerror_code=task.plan()\nautoresult=task.execute(*task.solutions().front());\nmove_group\nCurrentState\nGeneratePose\nCurrentState\nModifyPlanningScene\nCurrentState\nGeneratePose\nMoveRelative\nComputeIK\nGenerateGraspPose\nGenerateGraspPose\nComputeIK\nGeneratePose\nGeneratePose\nPipelinePlanner\nJointInterpolation\nCartesianPath\nMoveTo\nMoveRelative\nConnect\nConstant\nPathLength\nTrajectoryDuration\nTrajectoryCostTerm\nLambdaCostTerm\nDistanceToReference\nLinkMotion\nClearance\nLambdaCostTerm\nMoveItErrorCode\nmoveit_msgs::msg::MoveItErrorCodes::SUCCESS\nexecute_task_solution\nexecute_task_solution_capability\nMoveGroupCapability\nMotionPlanRequest\nPlanExecution",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee119"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/moveit_task_constructor/moveit_task_constructor.html#connecting-stage",
  "title": "MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Task Constructor\nWhat is MoveIt Task Constructor?\nMTC Stages\nMTC Containers\nInitializing a MTC Task\nAdding containers and stages to a MTC Task\nSetting planning solvers\nSetting Properties\nCost calculator for Stages\nPlanning and Executing a MTC Task\nLinks to Additional Information\nThere are three possible stages relating to the result flow:\nGenerators\nPropagators\nConnectors\nCurrently available containers:\nSerial\nParallel\nParallel containers combine a set of stages to allow planning alternate solutions.\nThe top-level planning problem is specified as a MTC Task and the subproblems which are specified by Stages are added to the MTC task object.\nAdding a stage to MTC task\nContainers derive from Stage and hence containers can be added to MTC task similarly\nStages that do motion planning need solver information.\nSolvers available in MTC\nPipelinePlanner- Uses MoveIt’s planning pipeline\nJointInterpolation- Interpolates between the start and goal joint states. It does not support complex motions.\nCartesianPath- Moves the end effector in a straight line in Cartesian space.\nCode Example on how to initialize the solver\nThese solvers will be passed into stages likeMoveTo,MoveRelative, andConnect.\nCostTerm is the basic interface to compute costs for solutions for MTC stages.\nCostTerm implementations available in MTC\nConstant- Adds a constant cost to each solution\nPathLength- Cost depends on trajectory length with optional weight for different joints\nTrajectoryDuration- Cost depends on execution duration of the whole trajectory\nTrajectoryCostTerm- Cost terms that only work on SubTrajectory solutions\nLambdaCostTerm- Pass in a lambda expression to calculate cost\nDistanceToReference- Cost depends on weighted joint space distance to a reference point\nLinkMotion- Cost depends on length of Cartesian trajectory of a link\nClearance- Cost is inverse of distance to collision\nExample code on how to set CostTerm usingLambdaCostTerm\nAll stages provided by MTC have default cost terms. Stages which produce trajectories as solutions usually use path length to calculate cost.\nPlanning an MTC task will return aMoveItErrorCode.\nReferhereto identity the different error types.\nIf planning succeeds, you can expect the plan function to returnmoveit_msgs::msg::MoveItErrorCodes::SUCCESS.\nAfter planning, extract the first successful solution and pass it to the execute function. This will create anexecute_task_solutionaction client.\nThe action server resides in theexecute_task_solution_capabilityplugin provided by MTC.\nThe plugin extendsMoveGroupCapability. It constructs aMotionPlanRequestfrom the MTC solution and uses MoveIt’sPlanExecutionto actuate the robot.\nHere is atutorialon how to create a Pick and Place pipeline using MTC.\nThe links listed below contain more information on stages and containers provided by MTC\n© Copyright 2024, PickNik Robotics.\nautonode=std::make_shared<rclcpp::Node>();autotask=std::make_unique<moveit::task_constructor::Task>();task->loadRobotModel(node);// Set controllers used to execute robot motion. If not set, MoveIt has controller discovery logic.task->setProperty(\"trajectory_execution_info\",\"joint_trajectory_controller gripper_controller\");\nautocurrent_state=std::make_unique<moveit::task_constructor::stages::CurrentState>(\"current_state\");task->add(std::move(current_state));\nautocontainer=std::make_unique<moveit::task_constructor::SerialContainer>(\"Pick Object\");// TODO: Add stages to the container before adding the container to MTC tasktask->add(std::move(container));\nconstautomtc_pipeline_planner=std::make_shared<moveit::task_constructor::solvers::PipelinePlanner>(node,\"ompl\",\"RRTConnectkConfigDefault\");constautomtc_joint_interpolation_planner=std::make_shared<moveit::task_constructor::solvers::JointInterpolationPlanner>();constautomtc_cartesian_planner=std::make_shared<moveit::task_constructor::solvers::CartesianPath>();\nvoidsetProperty(conststd::string&name,constboost::any&value);\nstage->setCostTerm(moveit::task_constructor::LambdaCostTerm([](constmoveit::task_constructor::SubTrajectory&traj){return100*traj.cost();}));\nautoerror_code=task.plan()\nautoresult=task.execute(*task.solutions().front());\nmove_group\nCurrentState\nGeneratePose\nCurrentState\nModifyPlanningScene\nCurrentState\nGeneratePose\nMoveRelative\nComputeIK\nGenerateGraspPose\nGenerateGraspPose\nComputeIK\nGeneratePose\nGeneratePose\nPipelinePlanner\nJointInterpolation\nCartesianPath\nMoveTo\nMoveRelative\nConnect\nConstant\nPathLength\nTrajectoryDuration\nTrajectoryCostTerm\nLambdaCostTerm\nDistanceToReference\nLinkMotion\nClearance\nLambdaCostTerm\nMoveItErrorCode\nmoveit_msgs::msg::MoveItErrorCodes::SUCCESS\nexecute_task_solution\nexecute_task_solution_capability\nMoveGroupCapability\nMotionPlanRequest\nPlanExecution",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee11a"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/moveit_task_constructor/moveit_task_constructor.html#wrapper",
  "title": "MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Task Constructor\nWhat is MoveIt Task Constructor?\nMTC Stages\nMTC Containers\nInitializing a MTC Task\nAdding containers and stages to a MTC Task\nSetting planning solvers\nSetting Properties\nCost calculator for Stages\nPlanning and Executing a MTC Task\nLinks to Additional Information\nThere are three possible stages relating to the result flow:\nGenerators\nPropagators\nConnectors\nCurrently available containers:\nSerial\nParallel\nParallel containers combine a set of stages to allow planning alternate solutions.\nThe top-level planning problem is specified as a MTC Task and the subproblems which are specified by Stages are added to the MTC task object.\nAdding a stage to MTC task\nContainers derive from Stage and hence containers can be added to MTC task similarly\nStages that do motion planning need solver information.\nSolvers available in MTC\nPipelinePlanner- Uses MoveIt’s planning pipeline\nJointInterpolation- Interpolates between the start and goal joint states. It does not support complex motions.\nCartesianPath- Moves the end effector in a straight line in Cartesian space.\nCode Example on how to initialize the solver\nThese solvers will be passed into stages likeMoveTo,MoveRelative, andConnect.\nCostTerm is the basic interface to compute costs for solutions for MTC stages.\nCostTerm implementations available in MTC\nConstant- Adds a constant cost to each solution\nPathLength- Cost depends on trajectory length with optional weight for different joints\nTrajectoryDuration- Cost depends on execution duration of the whole trajectory\nTrajectoryCostTerm- Cost terms that only work on SubTrajectory solutions\nLambdaCostTerm- Pass in a lambda expression to calculate cost\nDistanceToReference- Cost depends on weighted joint space distance to a reference point\nLinkMotion- Cost depends on length of Cartesian trajectory of a link\nClearance- Cost is inverse of distance to collision\nExample code on how to set CostTerm usingLambdaCostTerm\nAll stages provided by MTC have default cost terms. Stages which produce trajectories as solutions usually use path length to calculate cost.\nPlanning an MTC task will return aMoveItErrorCode.\nReferhereto identity the different error types.\nIf planning succeeds, you can expect the plan function to returnmoveit_msgs::msg::MoveItErrorCodes::SUCCESS.\nAfter planning, extract the first successful solution and pass it to the execute function. This will create anexecute_task_solutionaction client.\nThe action server resides in theexecute_task_solution_capabilityplugin provided by MTC.\nThe plugin extendsMoveGroupCapability. It constructs aMotionPlanRequestfrom the MTC solution and uses MoveIt’sPlanExecutionto actuate the robot.\nHere is atutorialon how to create a Pick and Place pipeline using MTC.\nThe links listed below contain more information on stages and containers provided by MTC\n© Copyright 2024, PickNik Robotics.\nautonode=std::make_shared<rclcpp::Node>();autotask=std::make_unique<moveit::task_constructor::Task>();task->loadRobotModel(node);// Set controllers used to execute robot motion. If not set, MoveIt has controller discovery logic.task->setProperty(\"trajectory_execution_info\",\"joint_trajectory_controller gripper_controller\");\nautocurrent_state=std::make_unique<moveit::task_constructor::stages::CurrentState>(\"current_state\");task->add(std::move(current_state));\nautocontainer=std::make_unique<moveit::task_constructor::SerialContainer>(\"Pick Object\");// TODO: Add stages to the container before adding the container to MTC tasktask->add(std::move(container));\nconstautomtc_pipeline_planner=std::make_shared<moveit::task_constructor::solvers::PipelinePlanner>(node,\"ompl\",\"RRTConnectkConfigDefault\");constautomtc_joint_interpolation_planner=std::make_shared<moveit::task_constructor::solvers::JointInterpolationPlanner>();constautomtc_cartesian_planner=std::make_shared<moveit::task_constructor::solvers::CartesianPath>();\nvoidsetProperty(conststd::string&name,constboost::any&value);\nstage->setCostTerm(moveit::task_constructor::LambdaCostTerm([](constmoveit::task_constructor::SubTrajectory&traj){return100*traj.cost();}));\nautoerror_code=task.plan()\nautoresult=task.execute(*task.solutions().front());\nmove_group\nCurrentState\nGeneratePose\nCurrentState\nModifyPlanningScene\nCurrentState\nGeneratePose\nMoveRelative\nComputeIK\nGenerateGraspPose\nGenerateGraspPose\nComputeIK\nGeneratePose\nGeneratePose\nPipelinePlanner\nJointInterpolation\nCartesianPath\nMoveTo\nMoveRelative\nConnect\nConstant\nPathLength\nTrajectoryDuration\nTrajectoryCostTerm\nLambdaCostTerm\nDistanceToReference\nLinkMotion\nClearance\nLambdaCostTerm\nMoveItErrorCode\nmoveit_msgs::msg::MoveItErrorCodes::SUCCESS\nexecute_task_solution\nexecute_task_solution_capability\nMoveGroupCapability\nMotionPlanRequest\nPlanExecution",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee11b"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/moveit_task_constructor/moveit_task_constructor.html#mtc-containers",
  "title": "MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Task Constructor\nWhat is MoveIt Task Constructor?\nMTC Stages\nMTC Containers\nInitializing a MTC Task\nAdding containers and stages to a MTC Task\nSetting planning solvers\nSetting Properties\nCost calculator for Stages\nPlanning and Executing a MTC Task\nLinks to Additional Information\nThere are three possible stages relating to the result flow:\nGenerators\nPropagators\nConnectors\nCurrently available containers:\nSerial\nParallel\nParallel containers combine a set of stages to allow planning alternate solutions.\nThe top-level planning problem is specified as a MTC Task and the subproblems which are specified by Stages are added to the MTC task object.\nAdding a stage to MTC task\nContainers derive from Stage and hence containers can be added to MTC task similarly\nStages that do motion planning need solver information.\nSolvers available in MTC\nPipelinePlanner- Uses MoveIt’s planning pipeline\nJointInterpolation- Interpolates between the start and goal joint states. It does not support complex motions.\nCartesianPath- Moves the end effector in a straight line in Cartesian space.\nCode Example on how to initialize the solver\nThese solvers will be passed into stages likeMoveTo,MoveRelative, andConnect.\nCostTerm is the basic interface to compute costs for solutions for MTC stages.\nCostTerm implementations available in MTC\nConstant- Adds a constant cost to each solution\nPathLength- Cost depends on trajectory length with optional weight for different joints\nTrajectoryDuration- Cost depends on execution duration of the whole trajectory\nTrajectoryCostTerm- Cost terms that only work on SubTrajectory solutions\nLambdaCostTerm- Pass in a lambda expression to calculate cost\nDistanceToReference- Cost depends on weighted joint space distance to a reference point\nLinkMotion- Cost depends on length of Cartesian trajectory of a link\nClearance- Cost is inverse of distance to collision\nExample code on how to set CostTerm usingLambdaCostTerm\nAll stages provided by MTC have default cost terms. Stages which produce trajectories as solutions usually use path length to calculate cost.\nPlanning an MTC task will return aMoveItErrorCode.\nReferhereto identity the different error types.\nIf planning succeeds, you can expect the plan function to returnmoveit_msgs::msg::MoveItErrorCodes::SUCCESS.\nAfter planning, extract the first successful solution and pass it to the execute function. This will create anexecute_task_solutionaction client.\nThe action server resides in theexecute_task_solution_capabilityplugin provided by MTC.\nThe plugin extendsMoveGroupCapability. It constructs aMotionPlanRequestfrom the MTC solution and uses MoveIt’sPlanExecutionto actuate the robot.\nHere is atutorialon how to create a Pick and Place pipeline using MTC.\nThe links listed below contain more information on stages and containers provided by MTC\n© Copyright 2024, PickNik Robotics.\nautonode=std::make_shared<rclcpp::Node>();autotask=std::make_unique<moveit::task_constructor::Task>();task->loadRobotModel(node);// Set controllers used to execute robot motion. If not set, MoveIt has controller discovery logic.task->setProperty(\"trajectory_execution_info\",\"joint_trajectory_controller gripper_controller\");\nautocurrent_state=std::make_unique<moveit::task_constructor::stages::CurrentState>(\"current_state\");task->add(std::move(current_state));\nautocontainer=std::make_unique<moveit::task_constructor::SerialContainer>(\"Pick Object\");// TODO: Add stages to the container before adding the container to MTC tasktask->add(std::move(container));\nconstautomtc_pipeline_planner=std::make_shared<moveit::task_constructor::solvers::PipelinePlanner>(node,\"ompl\",\"RRTConnectkConfigDefault\");constautomtc_joint_interpolation_planner=std::make_shared<moveit::task_constructor::solvers::JointInterpolationPlanner>();constautomtc_cartesian_planner=std::make_shared<moveit::task_constructor::solvers::CartesianPath>();\nvoidsetProperty(conststd::string&name,constboost::any&value);\nstage->setCostTerm(moveit::task_constructor::LambdaCostTerm([](constmoveit::task_constructor::SubTrajectory&traj){return100*traj.cost();}));\nautoerror_code=task.plan()\nautoresult=task.execute(*task.solutions().front());\nmove_group\nCurrentState\nGeneratePose\nCurrentState\nModifyPlanningScene\nCurrentState\nGeneratePose\nMoveRelative\nComputeIK\nGenerateGraspPose\nGenerateGraspPose\nComputeIK\nGeneratePose\nGeneratePose\nPipelinePlanner\nJointInterpolation\nCartesianPath\nMoveTo\nMoveRelative\nConnect\nConstant\nPathLength\nTrajectoryDuration\nTrajectoryCostTerm\nLambdaCostTerm\nDistanceToReference\nLinkMotion\nClearance\nLambdaCostTerm\nMoveItErrorCode\nmoveit_msgs::msg::MoveItErrorCodes::SUCCESS\nexecute_task_solution\nexecute_task_solution_capability\nMoveGroupCapability\nMotionPlanRequest\nPlanExecution",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee11c"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/moveit_task_constructor/moveit_task_constructor.html#serial-container",
  "title": "MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Task Constructor\nWhat is MoveIt Task Constructor?\nMTC Stages\nMTC Containers\nInitializing a MTC Task\nAdding containers and stages to a MTC Task\nSetting planning solvers\nSetting Properties\nCost calculator for Stages\nPlanning and Executing a MTC Task\nLinks to Additional Information\nThere are three possible stages relating to the result flow:\nGenerators\nPropagators\nConnectors\nCurrently available containers:\nSerial\nParallel\nParallel containers combine a set of stages to allow planning alternate solutions.\nThe top-level planning problem is specified as a MTC Task and the subproblems which are specified by Stages are added to the MTC task object.\nAdding a stage to MTC task\nContainers derive from Stage and hence containers can be added to MTC task similarly\nStages that do motion planning need solver information.\nSolvers available in MTC\nPipelinePlanner- Uses MoveIt’s planning pipeline\nJointInterpolation- Interpolates between the start and goal joint states. It does not support complex motions.\nCartesianPath- Moves the end effector in a straight line in Cartesian space.\nCode Example on how to initialize the solver\nThese solvers will be passed into stages likeMoveTo,MoveRelative, andConnect.\nCostTerm is the basic interface to compute costs for solutions for MTC stages.\nCostTerm implementations available in MTC\nConstant- Adds a constant cost to each solution\nPathLength- Cost depends on trajectory length with optional weight for different joints\nTrajectoryDuration- Cost depends on execution duration of the whole trajectory\nTrajectoryCostTerm- Cost terms that only work on SubTrajectory solutions\nLambdaCostTerm- Pass in a lambda expression to calculate cost\nDistanceToReference- Cost depends on weighted joint space distance to a reference point\nLinkMotion- Cost depends on length of Cartesian trajectory of a link\nClearance- Cost is inverse of distance to collision\nExample code on how to set CostTerm usingLambdaCostTerm\nAll stages provided by MTC have default cost terms. Stages which produce trajectories as solutions usually use path length to calculate cost.\nPlanning an MTC task will return aMoveItErrorCode.\nReferhereto identity the different error types.\nIf planning succeeds, you can expect the plan function to returnmoveit_msgs::msg::MoveItErrorCodes::SUCCESS.\nAfter planning, extract the first successful solution and pass it to the execute function. This will create anexecute_task_solutionaction client.\nThe action server resides in theexecute_task_solution_capabilityplugin provided by MTC.\nThe plugin extendsMoveGroupCapability. It constructs aMotionPlanRequestfrom the MTC solution and uses MoveIt’sPlanExecutionto actuate the robot.\nHere is atutorialon how to create a Pick and Place pipeline using MTC.\nThe links listed below contain more information on stages and containers provided by MTC\n© Copyright 2024, PickNik Robotics.\nautonode=std::make_shared<rclcpp::Node>();autotask=std::make_unique<moveit::task_constructor::Task>();task->loadRobotModel(node);// Set controllers used to execute robot motion. If not set, MoveIt has controller discovery logic.task->setProperty(\"trajectory_execution_info\",\"joint_trajectory_controller gripper_controller\");\nautocurrent_state=std::make_unique<moveit::task_constructor::stages::CurrentState>(\"current_state\");task->add(std::move(current_state));\nautocontainer=std::make_unique<moveit::task_constructor::SerialContainer>(\"Pick Object\");// TODO: Add stages to the container before adding the container to MTC tasktask->add(std::move(container));\nconstautomtc_pipeline_planner=std::make_shared<moveit::task_constructor::solvers::PipelinePlanner>(node,\"ompl\",\"RRTConnectkConfigDefault\");constautomtc_joint_interpolation_planner=std::make_shared<moveit::task_constructor::solvers::JointInterpolationPlanner>();constautomtc_cartesian_planner=std::make_shared<moveit::task_constructor::solvers::CartesianPath>();\nvoidsetProperty(conststd::string&name,constboost::any&value);\nstage->setCostTerm(moveit::task_constructor::LambdaCostTerm([](constmoveit::task_constructor::SubTrajectory&traj){return100*traj.cost();}));\nautoerror_code=task.plan()\nautoresult=task.execute(*task.solutions().front());\nmove_group\nCurrentState\nGeneratePose\nCurrentState\nModifyPlanningScene\nCurrentState\nGeneratePose\nMoveRelative\nComputeIK\nGenerateGraspPose\nGenerateGraspPose\nComputeIK\nGeneratePose\nGeneratePose\nPipelinePlanner\nJointInterpolation\nCartesianPath\nMoveTo\nMoveRelative\nConnect\nConstant\nPathLength\nTrajectoryDuration\nTrajectoryCostTerm\nLambdaCostTerm\nDistanceToReference\nLinkMotion\nClearance\nLambdaCostTerm\nMoveItErrorCode\nmoveit_msgs::msg::MoveItErrorCodes::SUCCESS\nexecute_task_solution\nexecute_task_solution_capability\nMoveGroupCapability\nMotionPlanRequest\nPlanExecution",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee11d"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/moveit_task_constructor/moveit_task_constructor.html#parallel-container",
  "title": "MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Task Constructor\nWhat is MoveIt Task Constructor?\nMTC Stages\nMTC Containers\nInitializing a MTC Task\nAdding containers and stages to a MTC Task\nSetting planning solvers\nSetting Properties\nCost calculator for Stages\nPlanning and Executing a MTC Task\nLinks to Additional Information\nThere are three possible stages relating to the result flow:\nGenerators\nPropagators\nConnectors\nCurrently available containers:\nSerial\nParallel\nParallel containers combine a set of stages to allow planning alternate solutions.\nThe top-level planning problem is specified as a MTC Task and the subproblems which are specified by Stages are added to the MTC task object.\nAdding a stage to MTC task\nContainers derive from Stage and hence containers can be added to MTC task similarly\nStages that do motion planning need solver information.\nSolvers available in MTC\nPipelinePlanner- Uses MoveIt’s planning pipeline\nJointInterpolation- Interpolates between the start and goal joint states. It does not support complex motions.\nCartesianPath- Moves the end effector in a straight line in Cartesian space.\nCode Example on how to initialize the solver\nThese solvers will be passed into stages likeMoveTo,MoveRelative, andConnect.\nCostTerm is the basic interface to compute costs for solutions for MTC stages.\nCostTerm implementations available in MTC\nConstant- Adds a constant cost to each solution\nPathLength- Cost depends on trajectory length with optional weight for different joints\nTrajectoryDuration- Cost depends on execution duration of the whole trajectory\nTrajectoryCostTerm- Cost terms that only work on SubTrajectory solutions\nLambdaCostTerm- Pass in a lambda expression to calculate cost\nDistanceToReference- Cost depends on weighted joint space distance to a reference point\nLinkMotion- Cost depends on length of Cartesian trajectory of a link\nClearance- Cost is inverse of distance to collision\nExample code on how to set CostTerm usingLambdaCostTerm\nAll stages provided by MTC have default cost terms. Stages which produce trajectories as solutions usually use path length to calculate cost.\nPlanning an MTC task will return aMoveItErrorCode.\nReferhereto identity the different error types.\nIf planning succeeds, you can expect the plan function to returnmoveit_msgs::msg::MoveItErrorCodes::SUCCESS.\nAfter planning, extract the first successful solution and pass it to the execute function. This will create anexecute_task_solutionaction client.\nThe action server resides in theexecute_task_solution_capabilityplugin provided by MTC.\nThe plugin extendsMoveGroupCapability. It constructs aMotionPlanRequestfrom the MTC solution and uses MoveIt’sPlanExecutionto actuate the robot.\nHere is atutorialon how to create a Pick and Place pipeline using MTC.\nThe links listed below contain more information on stages and containers provided by MTC\n© Copyright 2024, PickNik Robotics.\nautonode=std::make_shared<rclcpp::Node>();autotask=std::make_unique<moveit::task_constructor::Task>();task->loadRobotModel(node);// Set controllers used to execute robot motion. If not set, MoveIt has controller discovery logic.task->setProperty(\"trajectory_execution_info\",\"joint_trajectory_controller gripper_controller\");\nautocurrent_state=std::make_unique<moveit::task_constructor::stages::CurrentState>(\"current_state\");task->add(std::move(current_state));\nautocontainer=std::make_unique<moveit::task_constructor::SerialContainer>(\"Pick Object\");// TODO: Add stages to the container before adding the container to MTC tasktask->add(std::move(container));\nconstautomtc_pipeline_planner=std::make_shared<moveit::task_constructor::solvers::PipelinePlanner>(node,\"ompl\",\"RRTConnectkConfigDefault\");constautomtc_joint_interpolation_planner=std::make_shared<moveit::task_constructor::solvers::JointInterpolationPlanner>();constautomtc_cartesian_planner=std::make_shared<moveit::task_constructor::solvers::CartesianPath>();\nvoidsetProperty(conststd::string&name,constboost::any&value);\nstage->setCostTerm(moveit::task_constructor::LambdaCostTerm([](constmoveit::task_constructor::SubTrajectory&traj){return100*traj.cost();}));\nautoerror_code=task.plan()\nautoresult=task.execute(*task.solutions().front());\nmove_group\nCurrentState\nGeneratePose\nCurrentState\nModifyPlanningScene\nCurrentState\nGeneratePose\nMoveRelative\nComputeIK\nGenerateGraspPose\nGenerateGraspPose\nComputeIK\nGeneratePose\nGeneratePose\nPipelinePlanner\nJointInterpolation\nCartesianPath\nMoveTo\nMoveRelative\nConnect\nConstant\nPathLength\nTrajectoryDuration\nTrajectoryCostTerm\nLambdaCostTerm\nDistanceToReference\nLinkMotion\nClearance\nLambdaCostTerm\nMoveItErrorCode\nmoveit_msgs::msg::MoveItErrorCodes::SUCCESS\nexecute_task_solution\nexecute_task_solution_capability\nMoveGroupCapability\nMotionPlanRequest\nPlanExecution",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee11e"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/moveit_task_constructor/moveit_task_constructor.html#initializing-a-mtc-task",
  "title": "MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Task Constructor\nWhat is MoveIt Task Constructor?\nMTC Stages\nMTC Containers\nInitializing a MTC Task\nAdding containers and stages to a MTC Task\nSetting planning solvers\nSetting Properties\nCost calculator for Stages\nPlanning and Executing a MTC Task\nLinks to Additional Information\nThere are three possible stages relating to the result flow:\nGenerators\nPropagators\nConnectors\nCurrently available containers:\nSerial\nParallel\nParallel containers combine a set of stages to allow planning alternate solutions.\nThe top-level planning problem is specified as a MTC Task and the subproblems which are specified by Stages are added to the MTC task object.\nAdding a stage to MTC task\nContainers derive from Stage and hence containers can be added to MTC task similarly\nStages that do motion planning need solver information.\nSolvers available in MTC\nPipelinePlanner- Uses MoveIt’s planning pipeline\nJointInterpolation- Interpolates between the start and goal joint states. It does not support complex motions.\nCartesianPath- Moves the end effector in a straight line in Cartesian space.\nCode Example on how to initialize the solver\nThese solvers will be passed into stages likeMoveTo,MoveRelative, andConnect.\nCostTerm is the basic interface to compute costs for solutions for MTC stages.\nCostTerm implementations available in MTC\nConstant- Adds a constant cost to each solution\nPathLength- Cost depends on trajectory length with optional weight for different joints\nTrajectoryDuration- Cost depends on execution duration of the whole trajectory\nTrajectoryCostTerm- Cost terms that only work on SubTrajectory solutions\nLambdaCostTerm- Pass in a lambda expression to calculate cost\nDistanceToReference- Cost depends on weighted joint space distance to a reference point\nLinkMotion- Cost depends on length of Cartesian trajectory of a link\nClearance- Cost is inverse of distance to collision\nExample code on how to set CostTerm usingLambdaCostTerm\nAll stages provided by MTC have default cost terms. Stages which produce trajectories as solutions usually use path length to calculate cost.\nPlanning an MTC task will return aMoveItErrorCode.\nReferhereto identity the different error types.\nIf planning succeeds, you can expect the plan function to returnmoveit_msgs::msg::MoveItErrorCodes::SUCCESS.\nAfter planning, extract the first successful solution and pass it to the execute function. This will create anexecute_task_solutionaction client.\nThe action server resides in theexecute_task_solution_capabilityplugin provided by MTC.\nThe plugin extendsMoveGroupCapability. It constructs aMotionPlanRequestfrom the MTC solution and uses MoveIt’sPlanExecutionto actuate the robot.\nHere is atutorialon how to create a Pick and Place pipeline using MTC.\nThe links listed below contain more information on stages and containers provided by MTC\n© Copyright 2024, PickNik Robotics.\nautonode=std::make_shared<rclcpp::Node>();autotask=std::make_unique<moveit::task_constructor::Task>();task->loadRobotModel(node);// Set controllers used to execute robot motion. If not set, MoveIt has controller discovery logic.task->setProperty(\"trajectory_execution_info\",\"joint_trajectory_controller gripper_controller\");\nautocurrent_state=std::make_unique<moveit::task_constructor::stages::CurrentState>(\"current_state\");task->add(std::move(current_state));\nautocontainer=std::make_unique<moveit::task_constructor::SerialContainer>(\"Pick Object\");// TODO: Add stages to the container before adding the container to MTC tasktask->add(std::move(container));\nconstautomtc_pipeline_planner=std::make_shared<moveit::task_constructor::solvers::PipelinePlanner>(node,\"ompl\",\"RRTConnectkConfigDefault\");constautomtc_joint_interpolation_planner=std::make_shared<moveit::task_constructor::solvers::JointInterpolationPlanner>();constautomtc_cartesian_planner=std::make_shared<moveit::task_constructor::solvers::CartesianPath>();\nvoidsetProperty(conststd::string&name,constboost::any&value);\nstage->setCostTerm(moveit::task_constructor::LambdaCostTerm([](constmoveit::task_constructor::SubTrajectory&traj){return100*traj.cost();}));\nautoerror_code=task.plan()\nautoresult=task.execute(*task.solutions().front());\nmove_group\nCurrentState\nGeneratePose\nCurrentState\nModifyPlanningScene\nCurrentState\nGeneratePose\nMoveRelative\nComputeIK\nGenerateGraspPose\nGenerateGraspPose\nComputeIK\nGeneratePose\nGeneratePose\nPipelinePlanner\nJointInterpolation\nCartesianPath\nMoveTo\nMoveRelative\nConnect\nConstant\nPathLength\nTrajectoryDuration\nTrajectoryCostTerm\nLambdaCostTerm\nDistanceToReference\nLinkMotion\nClearance\nLambdaCostTerm\nMoveItErrorCode\nmoveit_msgs::msg::MoveItErrorCodes::SUCCESS\nexecute_task_solution\nexecute_task_solution_capability\nMoveGroupCapability\nMotionPlanRequest\nPlanExecution",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee11f"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/moveit_task_constructor/moveit_task_constructor.html#adding-containers-and-stages-to-a-mtc-task",
  "title": "MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Task Constructor\nWhat is MoveIt Task Constructor?\nMTC Stages\nMTC Containers\nInitializing a MTC Task\nAdding containers and stages to a MTC Task\nSetting planning solvers\nSetting Properties\nCost calculator for Stages\nPlanning and Executing a MTC Task\nLinks to Additional Information\nThere are three possible stages relating to the result flow:\nGenerators\nPropagators\nConnectors\nCurrently available containers:\nSerial\nParallel\nParallel containers combine a set of stages to allow planning alternate solutions.\nThe top-level planning problem is specified as a MTC Task and the subproblems which are specified by Stages are added to the MTC task object.\nAdding a stage to MTC task\nContainers derive from Stage and hence containers can be added to MTC task similarly\nStages that do motion planning need solver information.\nSolvers available in MTC\nPipelinePlanner- Uses MoveIt’s planning pipeline\nJointInterpolation- Interpolates between the start and goal joint states. It does not support complex motions.\nCartesianPath- Moves the end effector in a straight line in Cartesian space.\nCode Example on how to initialize the solver\nThese solvers will be passed into stages likeMoveTo,MoveRelative, andConnect.\nCostTerm is the basic interface to compute costs for solutions for MTC stages.\nCostTerm implementations available in MTC\nConstant- Adds a constant cost to each solution\nPathLength- Cost depends on trajectory length with optional weight for different joints\nTrajectoryDuration- Cost depends on execution duration of the whole trajectory\nTrajectoryCostTerm- Cost terms that only work on SubTrajectory solutions\nLambdaCostTerm- Pass in a lambda expression to calculate cost\nDistanceToReference- Cost depends on weighted joint space distance to a reference point\nLinkMotion- Cost depends on length of Cartesian trajectory of a link\nClearance- Cost is inverse of distance to collision\nExample code on how to set CostTerm usingLambdaCostTerm\nAll stages provided by MTC have default cost terms. Stages which produce trajectories as solutions usually use path length to calculate cost.\nPlanning an MTC task will return aMoveItErrorCode.\nReferhereto identity the different error types.\nIf planning succeeds, you can expect the plan function to returnmoveit_msgs::msg::MoveItErrorCodes::SUCCESS.\nAfter planning, extract the first successful solution and pass it to the execute function. This will create anexecute_task_solutionaction client.\nThe action server resides in theexecute_task_solution_capabilityplugin provided by MTC.\nThe plugin extendsMoveGroupCapability. It constructs aMotionPlanRequestfrom the MTC solution and uses MoveIt’sPlanExecutionto actuate the robot.\nHere is atutorialon how to create a Pick and Place pipeline using MTC.\nThe links listed below contain more information on stages and containers provided by MTC\n© Copyright 2024, PickNik Robotics.\nautonode=std::make_shared<rclcpp::Node>();autotask=std::make_unique<moveit::task_constructor::Task>();task->loadRobotModel(node);// Set controllers used to execute robot motion. If not set, MoveIt has controller discovery logic.task->setProperty(\"trajectory_execution_info\",\"joint_trajectory_controller gripper_controller\");\nautocurrent_state=std::make_unique<moveit::task_constructor::stages::CurrentState>(\"current_state\");task->add(std::move(current_state));\nautocontainer=std::make_unique<moveit::task_constructor::SerialContainer>(\"Pick Object\");// TODO: Add stages to the container before adding the container to MTC tasktask->add(std::move(container));\nconstautomtc_pipeline_planner=std::make_shared<moveit::task_constructor::solvers::PipelinePlanner>(node,\"ompl\",\"RRTConnectkConfigDefault\");constautomtc_joint_interpolation_planner=std::make_shared<moveit::task_constructor::solvers::JointInterpolationPlanner>();constautomtc_cartesian_planner=std::make_shared<moveit::task_constructor::solvers::CartesianPath>();\nvoidsetProperty(conststd::string&name,constboost::any&value);\nstage->setCostTerm(moveit::task_constructor::LambdaCostTerm([](constmoveit::task_constructor::SubTrajectory&traj){return100*traj.cost();}));\nautoerror_code=task.plan()\nautoresult=task.execute(*task.solutions().front());\nmove_group\nCurrentState\nGeneratePose\nCurrentState\nModifyPlanningScene\nCurrentState\nGeneratePose\nMoveRelative\nComputeIK\nGenerateGraspPose\nGenerateGraspPose\nComputeIK\nGeneratePose\nGeneratePose\nPipelinePlanner\nJointInterpolation\nCartesianPath\nMoveTo\nMoveRelative\nConnect\nConstant\nPathLength\nTrajectoryDuration\nTrajectoryCostTerm\nLambdaCostTerm\nDistanceToReference\nLinkMotion\nClearance\nLambdaCostTerm\nMoveItErrorCode\nmoveit_msgs::msg::MoveItErrorCodes::SUCCESS\nexecute_task_solution\nexecute_task_solution_capability\nMoveGroupCapability\nMotionPlanRequest\nPlanExecution",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee120"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/moveit_task_constructor/moveit_task_constructor.html#setting-planning-solvers",
  "title": "MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Task Constructor\nWhat is MoveIt Task Constructor?\nMTC Stages\nMTC Containers\nInitializing a MTC Task\nAdding containers and stages to a MTC Task\nSetting planning solvers\nSetting Properties\nCost calculator for Stages\nPlanning and Executing a MTC Task\nLinks to Additional Information\nThere are three possible stages relating to the result flow:\nGenerators\nPropagators\nConnectors\nCurrently available containers:\nSerial\nParallel\nParallel containers combine a set of stages to allow planning alternate solutions.\nThe top-level planning problem is specified as a MTC Task and the subproblems which are specified by Stages are added to the MTC task object.\nAdding a stage to MTC task\nContainers derive from Stage and hence containers can be added to MTC task similarly\nStages that do motion planning need solver information.\nSolvers available in MTC\nPipelinePlanner- Uses MoveIt’s planning pipeline\nJointInterpolation- Interpolates between the start and goal joint states. It does not support complex motions.\nCartesianPath- Moves the end effector in a straight line in Cartesian space.\nCode Example on how to initialize the solver\nThese solvers will be passed into stages likeMoveTo,MoveRelative, andConnect.\nCostTerm is the basic interface to compute costs for solutions for MTC stages.\nCostTerm implementations available in MTC\nConstant- Adds a constant cost to each solution\nPathLength- Cost depends on trajectory length with optional weight for different joints\nTrajectoryDuration- Cost depends on execution duration of the whole trajectory\nTrajectoryCostTerm- Cost terms that only work on SubTrajectory solutions\nLambdaCostTerm- Pass in a lambda expression to calculate cost\nDistanceToReference- Cost depends on weighted joint space distance to a reference point\nLinkMotion- Cost depends on length of Cartesian trajectory of a link\nClearance- Cost is inverse of distance to collision\nExample code on how to set CostTerm usingLambdaCostTerm\nAll stages provided by MTC have default cost terms. Stages which produce trajectories as solutions usually use path length to calculate cost.\nPlanning an MTC task will return aMoveItErrorCode.\nReferhereto identity the different error types.\nIf planning succeeds, you can expect the plan function to returnmoveit_msgs::msg::MoveItErrorCodes::SUCCESS.\nAfter planning, extract the first successful solution and pass it to the execute function. This will create anexecute_task_solutionaction client.\nThe action server resides in theexecute_task_solution_capabilityplugin provided by MTC.\nThe plugin extendsMoveGroupCapability. It constructs aMotionPlanRequestfrom the MTC solution and uses MoveIt’sPlanExecutionto actuate the robot.\nHere is atutorialon how to create a Pick and Place pipeline using MTC.\nThe links listed below contain more information on stages and containers provided by MTC\n© Copyright 2024, PickNik Robotics.\nautonode=std::make_shared<rclcpp::Node>();autotask=std::make_unique<moveit::task_constructor::Task>();task->loadRobotModel(node);// Set controllers used to execute robot motion. If not set, MoveIt has controller discovery logic.task->setProperty(\"trajectory_execution_info\",\"joint_trajectory_controller gripper_controller\");\nautocurrent_state=std::make_unique<moveit::task_constructor::stages::CurrentState>(\"current_state\");task->add(std::move(current_state));\nautocontainer=std::make_unique<moveit::task_constructor::SerialContainer>(\"Pick Object\");// TODO: Add stages to the container before adding the container to MTC tasktask->add(std::move(container));\nconstautomtc_pipeline_planner=std::make_shared<moveit::task_constructor::solvers::PipelinePlanner>(node,\"ompl\",\"RRTConnectkConfigDefault\");constautomtc_joint_interpolation_planner=std::make_shared<moveit::task_constructor::solvers::JointInterpolationPlanner>();constautomtc_cartesian_planner=std::make_shared<moveit::task_constructor::solvers::CartesianPath>();\nvoidsetProperty(conststd::string&name,constboost::any&value);\nstage->setCostTerm(moveit::task_constructor::LambdaCostTerm([](constmoveit::task_constructor::SubTrajectory&traj){return100*traj.cost();}));\nautoerror_code=task.plan()\nautoresult=task.execute(*task.solutions().front());\nmove_group\nCurrentState\nGeneratePose\nCurrentState\nModifyPlanningScene\nCurrentState\nGeneratePose\nMoveRelative\nComputeIK\nGenerateGraspPose\nGenerateGraspPose\nComputeIK\nGeneratePose\nGeneratePose\nPipelinePlanner\nJointInterpolation\nCartesianPath\nMoveTo\nMoveRelative\nConnect\nConstant\nPathLength\nTrajectoryDuration\nTrajectoryCostTerm\nLambdaCostTerm\nDistanceToReference\nLinkMotion\nClearance\nLambdaCostTerm\nMoveItErrorCode\nmoveit_msgs::msg::MoveItErrorCodes::SUCCESS\nexecute_task_solution\nexecute_task_solution_capability\nMoveGroupCapability\nMotionPlanRequest\nPlanExecution",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee121"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/moveit_task_constructor/moveit_task_constructor.html#setting-properties",
  "title": "MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Task Constructor\nWhat is MoveIt Task Constructor?\nMTC Stages\nMTC Containers\nInitializing a MTC Task\nAdding containers and stages to a MTC Task\nSetting planning solvers\nSetting Properties\nCost calculator for Stages\nPlanning and Executing a MTC Task\nLinks to Additional Information\nThere are three possible stages relating to the result flow:\nGenerators\nPropagators\nConnectors\nCurrently available containers:\nSerial\nParallel\nParallel containers combine a set of stages to allow planning alternate solutions.\nThe top-level planning problem is specified as a MTC Task and the subproblems which are specified by Stages are added to the MTC task object.\nAdding a stage to MTC task\nContainers derive from Stage and hence containers can be added to MTC task similarly\nStages that do motion planning need solver information.\nSolvers available in MTC\nPipelinePlanner- Uses MoveIt’s planning pipeline\nJointInterpolation- Interpolates between the start and goal joint states. It does not support complex motions.\nCartesianPath- Moves the end effector in a straight line in Cartesian space.\nCode Example on how to initialize the solver\nThese solvers will be passed into stages likeMoveTo,MoveRelative, andConnect.\nCostTerm is the basic interface to compute costs for solutions for MTC stages.\nCostTerm implementations available in MTC\nConstant- Adds a constant cost to each solution\nPathLength- Cost depends on trajectory length with optional weight for different joints\nTrajectoryDuration- Cost depends on execution duration of the whole trajectory\nTrajectoryCostTerm- Cost terms that only work on SubTrajectory solutions\nLambdaCostTerm- Pass in a lambda expression to calculate cost\nDistanceToReference- Cost depends on weighted joint space distance to a reference point\nLinkMotion- Cost depends on length of Cartesian trajectory of a link\nClearance- Cost is inverse of distance to collision\nExample code on how to set CostTerm usingLambdaCostTerm\nAll stages provided by MTC have default cost terms. Stages which produce trajectories as solutions usually use path length to calculate cost.\nPlanning an MTC task will return aMoveItErrorCode.\nReferhereto identity the different error types.\nIf planning succeeds, you can expect the plan function to returnmoveit_msgs::msg::MoveItErrorCodes::SUCCESS.\nAfter planning, extract the first successful solution and pass it to the execute function. This will create anexecute_task_solutionaction client.\nThe action server resides in theexecute_task_solution_capabilityplugin provided by MTC.\nThe plugin extendsMoveGroupCapability. It constructs aMotionPlanRequestfrom the MTC solution and uses MoveIt’sPlanExecutionto actuate the robot.\nHere is atutorialon how to create a Pick and Place pipeline using MTC.\nThe links listed below contain more information on stages and containers provided by MTC\n© Copyright 2024, PickNik Robotics.\nautonode=std::make_shared<rclcpp::Node>();autotask=std::make_unique<moveit::task_constructor::Task>();task->loadRobotModel(node);// Set controllers used to execute robot motion. If not set, MoveIt has controller discovery logic.task->setProperty(\"trajectory_execution_info\",\"joint_trajectory_controller gripper_controller\");\nautocurrent_state=std::make_unique<moveit::task_constructor::stages::CurrentState>(\"current_state\");task->add(std::move(current_state));\nautocontainer=std::make_unique<moveit::task_constructor::SerialContainer>(\"Pick Object\");// TODO: Add stages to the container before adding the container to MTC tasktask->add(std::move(container));\nconstautomtc_pipeline_planner=std::make_shared<moveit::task_constructor::solvers::PipelinePlanner>(node,\"ompl\",\"RRTConnectkConfigDefault\");constautomtc_joint_interpolation_planner=std::make_shared<moveit::task_constructor::solvers::JointInterpolationPlanner>();constautomtc_cartesian_planner=std::make_shared<moveit::task_constructor::solvers::CartesianPath>();\nvoidsetProperty(conststd::string&name,constboost::any&value);\nstage->setCostTerm(moveit::task_constructor::LambdaCostTerm([](constmoveit::task_constructor::SubTrajectory&traj){return100*traj.cost();}));\nautoerror_code=task.plan()\nautoresult=task.execute(*task.solutions().front());\nmove_group\nCurrentState\nGeneratePose\nCurrentState\nModifyPlanningScene\nCurrentState\nGeneratePose\nMoveRelative\nComputeIK\nGenerateGraspPose\nGenerateGraspPose\nComputeIK\nGeneratePose\nGeneratePose\nPipelinePlanner\nJointInterpolation\nCartesianPath\nMoveTo\nMoveRelative\nConnect\nConstant\nPathLength\nTrajectoryDuration\nTrajectoryCostTerm\nLambdaCostTerm\nDistanceToReference\nLinkMotion\nClearance\nLambdaCostTerm\nMoveItErrorCode\nmoveit_msgs::msg::MoveItErrorCodes::SUCCESS\nexecute_task_solution\nexecute_task_solution_capability\nMoveGroupCapability\nMotionPlanRequest\nPlanExecution",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee122"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/moveit_task_constructor/moveit_task_constructor.html#cost-calculator-for-stages",
  "title": "MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Task Constructor\nWhat is MoveIt Task Constructor?\nMTC Stages\nMTC Containers\nInitializing a MTC Task\nAdding containers and stages to a MTC Task\nSetting planning solvers\nSetting Properties\nCost calculator for Stages\nPlanning and Executing a MTC Task\nLinks to Additional Information\nThere are three possible stages relating to the result flow:\nGenerators\nPropagators\nConnectors\nCurrently available containers:\nSerial\nParallel\nParallel containers combine a set of stages to allow planning alternate solutions.\nThe top-level planning problem is specified as a MTC Task and the subproblems which are specified by Stages are added to the MTC task object.\nAdding a stage to MTC task\nContainers derive from Stage and hence containers can be added to MTC task similarly\nStages that do motion planning need solver information.\nSolvers available in MTC\nPipelinePlanner- Uses MoveIt’s planning pipeline\nJointInterpolation- Interpolates between the start and goal joint states. It does not support complex motions.\nCartesianPath- Moves the end effector in a straight line in Cartesian space.\nCode Example on how to initialize the solver\nThese solvers will be passed into stages likeMoveTo,MoveRelative, andConnect.\nCostTerm is the basic interface to compute costs for solutions for MTC stages.\nCostTerm implementations available in MTC\nConstant- Adds a constant cost to each solution\nPathLength- Cost depends on trajectory length with optional weight for different joints\nTrajectoryDuration- Cost depends on execution duration of the whole trajectory\nTrajectoryCostTerm- Cost terms that only work on SubTrajectory solutions\nLambdaCostTerm- Pass in a lambda expression to calculate cost\nDistanceToReference- Cost depends on weighted joint space distance to a reference point\nLinkMotion- Cost depends on length of Cartesian trajectory of a link\nClearance- Cost is inverse of distance to collision\nExample code on how to set CostTerm usingLambdaCostTerm\nAll stages provided by MTC have default cost terms. Stages which produce trajectories as solutions usually use path length to calculate cost.\nPlanning an MTC task will return aMoveItErrorCode.\nReferhereto identity the different error types.\nIf planning succeeds, you can expect the plan function to returnmoveit_msgs::msg::MoveItErrorCodes::SUCCESS.\nAfter planning, extract the first successful solution and pass it to the execute function. This will create anexecute_task_solutionaction client.\nThe action server resides in theexecute_task_solution_capabilityplugin provided by MTC.\nThe plugin extendsMoveGroupCapability. It constructs aMotionPlanRequestfrom the MTC solution and uses MoveIt’sPlanExecutionto actuate the robot.\nHere is atutorialon how to create a Pick and Place pipeline using MTC.\nThe links listed below contain more information on stages and containers provided by MTC\n© Copyright 2024, PickNik Robotics.\nautonode=std::make_shared<rclcpp::Node>();autotask=std::make_unique<moveit::task_constructor::Task>();task->loadRobotModel(node);// Set controllers used to execute robot motion. If not set, MoveIt has controller discovery logic.task->setProperty(\"trajectory_execution_info\",\"joint_trajectory_controller gripper_controller\");\nautocurrent_state=std::make_unique<moveit::task_constructor::stages::CurrentState>(\"current_state\");task->add(std::move(current_state));\nautocontainer=std::make_unique<moveit::task_constructor::SerialContainer>(\"Pick Object\");// TODO: Add stages to the container before adding the container to MTC tasktask->add(std::move(container));\nconstautomtc_pipeline_planner=std::make_shared<moveit::task_constructor::solvers::PipelinePlanner>(node,\"ompl\",\"RRTConnectkConfigDefault\");constautomtc_joint_interpolation_planner=std::make_shared<moveit::task_constructor::solvers::JointInterpolationPlanner>();constautomtc_cartesian_planner=std::make_shared<moveit::task_constructor::solvers::CartesianPath>();\nvoidsetProperty(conststd::string&name,constboost::any&value);\nstage->setCostTerm(moveit::task_constructor::LambdaCostTerm([](constmoveit::task_constructor::SubTrajectory&traj){return100*traj.cost();}));\nautoerror_code=task.plan()\nautoresult=task.execute(*task.solutions().front());\nmove_group\nCurrentState\nGeneratePose\nCurrentState\nModifyPlanningScene\nCurrentState\nGeneratePose\nMoveRelative\nComputeIK\nGenerateGraspPose\nGenerateGraspPose\nComputeIK\nGeneratePose\nGeneratePose\nPipelinePlanner\nJointInterpolation\nCartesianPath\nMoveTo\nMoveRelative\nConnect\nConstant\nPathLength\nTrajectoryDuration\nTrajectoryCostTerm\nLambdaCostTerm\nDistanceToReference\nLinkMotion\nClearance\nLambdaCostTerm\nMoveItErrorCode\nmoveit_msgs::msg::MoveItErrorCodes::SUCCESS\nexecute_task_solution\nexecute_task_solution_capability\nMoveGroupCapability\nMotionPlanRequest\nPlanExecution",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee123"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/moveit_task_constructor/moveit_task_constructor.html#planning-and-executing-a-mtc-task",
  "title": "MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Task Constructor\nWhat is MoveIt Task Constructor?\nMTC Stages\nMTC Containers\nInitializing a MTC Task\nAdding containers and stages to a MTC Task\nSetting planning solvers\nSetting Properties\nCost calculator for Stages\nPlanning and Executing a MTC Task\nLinks to Additional Information\nThere are three possible stages relating to the result flow:\nGenerators\nPropagators\nConnectors\nCurrently available containers:\nSerial\nParallel\nParallel containers combine a set of stages to allow planning alternate solutions.\nThe top-level planning problem is specified as a MTC Task and the subproblems which are specified by Stages are added to the MTC task object.\nAdding a stage to MTC task\nContainers derive from Stage and hence containers can be added to MTC task similarly\nStages that do motion planning need solver information.\nSolvers available in MTC\nPipelinePlanner- Uses MoveIt’s planning pipeline\nJointInterpolation- Interpolates between the start and goal joint states. It does not support complex motions.\nCartesianPath- Moves the end effector in a straight line in Cartesian space.\nCode Example on how to initialize the solver\nThese solvers will be passed into stages likeMoveTo,MoveRelative, andConnect.\nCostTerm is the basic interface to compute costs for solutions for MTC stages.\nCostTerm implementations available in MTC\nConstant- Adds a constant cost to each solution\nPathLength- Cost depends on trajectory length with optional weight for different joints\nTrajectoryDuration- Cost depends on execution duration of the whole trajectory\nTrajectoryCostTerm- Cost terms that only work on SubTrajectory solutions\nLambdaCostTerm- Pass in a lambda expression to calculate cost\nDistanceToReference- Cost depends on weighted joint space distance to a reference point\nLinkMotion- Cost depends on length of Cartesian trajectory of a link\nClearance- Cost is inverse of distance to collision\nExample code on how to set CostTerm usingLambdaCostTerm\nAll stages provided by MTC have default cost terms. Stages which produce trajectories as solutions usually use path length to calculate cost.\nPlanning an MTC task will return aMoveItErrorCode.\nReferhereto identity the different error types.\nIf planning succeeds, you can expect the plan function to returnmoveit_msgs::msg::MoveItErrorCodes::SUCCESS.\nAfter planning, extract the first successful solution and pass it to the execute function. This will create anexecute_task_solutionaction client.\nThe action server resides in theexecute_task_solution_capabilityplugin provided by MTC.\nThe plugin extendsMoveGroupCapability. It constructs aMotionPlanRequestfrom the MTC solution and uses MoveIt’sPlanExecutionto actuate the robot.\nHere is atutorialon how to create a Pick and Place pipeline using MTC.\nThe links listed below contain more information on stages and containers provided by MTC\n© Copyright 2024, PickNik Robotics.\nautonode=std::make_shared<rclcpp::Node>();autotask=std::make_unique<moveit::task_constructor::Task>();task->loadRobotModel(node);// Set controllers used to execute robot motion. If not set, MoveIt has controller discovery logic.task->setProperty(\"trajectory_execution_info\",\"joint_trajectory_controller gripper_controller\");\nautocurrent_state=std::make_unique<moveit::task_constructor::stages::CurrentState>(\"current_state\");task->add(std::move(current_state));\nautocontainer=std::make_unique<moveit::task_constructor::SerialContainer>(\"Pick Object\");// TODO: Add stages to the container before adding the container to MTC tasktask->add(std::move(container));\nconstautomtc_pipeline_planner=std::make_shared<moveit::task_constructor::solvers::PipelinePlanner>(node,\"ompl\",\"RRTConnectkConfigDefault\");constautomtc_joint_interpolation_planner=std::make_shared<moveit::task_constructor::solvers::JointInterpolationPlanner>();constautomtc_cartesian_planner=std::make_shared<moveit::task_constructor::solvers::CartesianPath>();\nvoidsetProperty(conststd::string&name,constboost::any&value);\nstage->setCostTerm(moveit::task_constructor::LambdaCostTerm([](constmoveit::task_constructor::SubTrajectory&traj){return100*traj.cost();}));\nautoerror_code=task.plan()\nautoresult=task.execute(*task.solutions().front());\nmove_group\nCurrentState\nGeneratePose\nCurrentState\nModifyPlanningScene\nCurrentState\nGeneratePose\nMoveRelative\nComputeIK\nGenerateGraspPose\nGenerateGraspPose\nComputeIK\nGeneratePose\nGeneratePose\nPipelinePlanner\nJointInterpolation\nCartesianPath\nMoveTo\nMoveRelative\nConnect\nConstant\nPathLength\nTrajectoryDuration\nTrajectoryCostTerm\nLambdaCostTerm\nDistanceToReference\nLinkMotion\nClearance\nLambdaCostTerm\nMoveItErrorCode\nmoveit_msgs::msg::MoveItErrorCodes::SUCCESS\nexecute_task_solution\nexecute_task_solution_capability\nMoveGroupCapability\nMotionPlanRequest\nPlanExecution",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee124"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/moveit_task_constructor/moveit_task_constructor.html#links-to-additional-information",
  "title": "MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Task Constructor\nWhat is MoveIt Task Constructor?\nMTC Stages\nMTC Containers\nInitializing a MTC Task\nAdding containers and stages to a MTC Task\nSetting planning solvers\nSetting Properties\nCost calculator for Stages\nPlanning and Executing a MTC Task\nLinks to Additional Information\nThere are three possible stages relating to the result flow:\nGenerators\nPropagators\nConnectors\nCurrently available containers:\nSerial\nParallel\nParallel containers combine a set of stages to allow planning alternate solutions.\nThe top-level planning problem is specified as a MTC Task and the subproblems which are specified by Stages are added to the MTC task object.\nAdding a stage to MTC task\nContainers derive from Stage and hence containers can be added to MTC task similarly\nStages that do motion planning need solver information.\nSolvers available in MTC\nPipelinePlanner- Uses MoveIt’s planning pipeline\nJointInterpolation- Interpolates between the start and goal joint states. It does not support complex motions.\nCartesianPath- Moves the end effector in a straight line in Cartesian space.\nCode Example on how to initialize the solver\nThese solvers will be passed into stages likeMoveTo,MoveRelative, andConnect.\nCostTerm is the basic interface to compute costs for solutions for MTC stages.\nCostTerm implementations available in MTC\nConstant- Adds a constant cost to each solution\nPathLength- Cost depends on trajectory length with optional weight for different joints\nTrajectoryDuration- Cost depends on execution duration of the whole trajectory\nTrajectoryCostTerm- Cost terms that only work on SubTrajectory solutions\nLambdaCostTerm- Pass in a lambda expression to calculate cost\nDistanceToReference- Cost depends on weighted joint space distance to a reference point\nLinkMotion- Cost depends on length of Cartesian trajectory of a link\nClearance- Cost is inverse of distance to collision\nExample code on how to set CostTerm usingLambdaCostTerm\nAll stages provided by MTC have default cost terms. Stages which produce trajectories as solutions usually use path length to calculate cost.\nPlanning an MTC task will return aMoveItErrorCode.\nReferhereto identity the different error types.\nIf planning succeeds, you can expect the plan function to returnmoveit_msgs::msg::MoveItErrorCodes::SUCCESS.\nAfter planning, extract the first successful solution and pass it to the execute function. This will create anexecute_task_solutionaction client.\nThe action server resides in theexecute_task_solution_capabilityplugin provided by MTC.\nThe plugin extendsMoveGroupCapability. It constructs aMotionPlanRequestfrom the MTC solution and uses MoveIt’sPlanExecutionto actuate the robot.\nHere is atutorialon how to create a Pick and Place pipeline using MTC.\nThe links listed below contain more information on stages and containers provided by MTC\n© Copyright 2024, PickNik Robotics.\nautonode=std::make_shared<rclcpp::Node>();autotask=std::make_unique<moveit::task_constructor::Task>();task->loadRobotModel(node);// Set controllers used to execute robot motion. If not set, MoveIt has controller discovery logic.task->setProperty(\"trajectory_execution_info\",\"joint_trajectory_controller gripper_controller\");\nautocurrent_state=std::make_unique<moveit::task_constructor::stages::CurrentState>(\"current_state\");task->add(std::move(current_state));\nautocontainer=std::make_unique<moveit::task_constructor::SerialContainer>(\"Pick Object\");// TODO: Add stages to the container before adding the container to MTC tasktask->add(std::move(container));\nconstautomtc_pipeline_planner=std::make_shared<moveit::task_constructor::solvers::PipelinePlanner>(node,\"ompl\",\"RRTConnectkConfigDefault\");constautomtc_joint_interpolation_planner=std::make_shared<moveit::task_constructor::solvers::JointInterpolationPlanner>();constautomtc_cartesian_planner=std::make_shared<moveit::task_constructor::solvers::CartesianPath>();\nvoidsetProperty(conststd::string&name,constboost::any&value);\nstage->setCostTerm(moveit::task_constructor::LambdaCostTerm([](constmoveit::task_constructor::SubTrajectory&traj){return100*traj.cost();}));\nautoerror_code=task.plan()\nautoresult=task.execute(*task.solutions().front());\nmove_group\nCurrentState\nGeneratePose\nCurrentState\nModifyPlanningScene\nCurrentState\nGeneratePose\nMoveRelative\nComputeIK\nGenerateGraspPose\nGenerateGraspPose\nComputeIK\nGeneratePose\nGeneratePose\nPipelinePlanner\nJointInterpolation\nCartesianPath\nMoveTo\nMoveRelative\nConnect\nConstant\nPathLength\nTrajectoryDuration\nTrajectoryCostTerm\nLambdaCostTerm\nDistanceToReference\nLinkMotion\nClearance\nLambdaCostTerm\nMoveItErrorCode\nmoveit_msgs::msg::MoveItErrorCodes::SUCCESS\nexecute_task_solution\nexecute_task_solution_capability\nMoveGroupCapability\nMotionPlanRequest\nPlanExecution",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee125"
  },
  "url": "https://moveit.picknik.ai/main/doc/concepts/moveit_task_constructor/moveit_task_constructor.html#moveit-task-constructor",
  "title": "MoveIt Task Constructor — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt Task Constructor\nWhat is MoveIt Task Constructor?\nMTC Stages\nMTC Containers\nInitializing a MTC Task\nAdding containers and stages to a MTC Task\nSetting planning solvers\nSetting Properties\nCost calculator for Stages\nPlanning and Executing a MTC Task\nLinks to Additional Information\nThere are three possible stages relating to the result flow:\nGenerators\nPropagators\nConnectors\nCurrently available containers:\nSerial\nParallel\nParallel containers combine a set of stages to allow planning alternate solutions.\nThe top-level planning problem is specified as a MTC Task and the subproblems which are specified by Stages are added to the MTC task object.\nAdding a stage to MTC task\nContainers derive from Stage and hence containers can be added to MTC task similarly\nStages that do motion planning need solver information.\nSolvers available in MTC\nPipelinePlanner- Uses MoveIt’s planning pipeline\nJointInterpolation- Interpolates between the start and goal joint states. It does not support complex motions.\nCartesianPath- Moves the end effector in a straight line in Cartesian space.\nCode Example on how to initialize the solver\nThese solvers will be passed into stages likeMoveTo,MoveRelative, andConnect.\nCostTerm is the basic interface to compute costs for solutions for MTC stages.\nCostTerm implementations available in MTC\nConstant- Adds a constant cost to each solution\nPathLength- Cost depends on trajectory length with optional weight for different joints\nTrajectoryDuration- Cost depends on execution duration of the whole trajectory\nTrajectoryCostTerm- Cost terms that only work on SubTrajectory solutions\nLambdaCostTerm- Pass in a lambda expression to calculate cost\nDistanceToReference- Cost depends on weighted joint space distance to a reference point\nLinkMotion- Cost depends on length of Cartesian trajectory of a link\nClearance- Cost is inverse of distance to collision\nExample code on how to set CostTerm usingLambdaCostTerm\nAll stages provided by MTC have default cost terms. Stages which produce trajectories as solutions usually use path length to calculate cost.\nPlanning an MTC task will return aMoveItErrorCode.\nReferhereto identity the different error types.\nIf planning succeeds, you can expect the plan function to returnmoveit_msgs::msg::MoveItErrorCodes::SUCCESS.\nAfter planning, extract the first successful solution and pass it to the execute function. This will create anexecute_task_solutionaction client.\nThe action server resides in theexecute_task_solution_capabilityplugin provided by MTC.\nThe plugin extendsMoveGroupCapability. It constructs aMotionPlanRequestfrom the MTC solution and uses MoveIt’sPlanExecutionto actuate the robot.\nHere is atutorialon how to create a Pick and Place pipeline using MTC.\nThe links listed below contain more information on stages and containers provided by MTC\n© Copyright 2024, PickNik Robotics.\nautonode=std::make_shared<rclcpp::Node>();autotask=std::make_unique<moveit::task_constructor::Task>();task->loadRobotModel(node);// Set controllers used to execute robot motion. If not set, MoveIt has controller discovery logic.task->setProperty(\"trajectory_execution_info\",\"joint_trajectory_controller gripper_controller\");\nautocurrent_state=std::make_unique<moveit::task_constructor::stages::CurrentState>(\"current_state\");task->add(std::move(current_state));\nautocontainer=std::make_unique<moveit::task_constructor::SerialContainer>(\"Pick Object\");// TODO: Add stages to the container before adding the container to MTC tasktask->add(std::move(container));\nconstautomtc_pipeline_planner=std::make_shared<moveit::task_constructor::solvers::PipelinePlanner>(node,\"ompl\",\"RRTConnectkConfigDefault\");constautomtc_joint_interpolation_planner=std::make_shared<moveit::task_constructor::solvers::JointInterpolationPlanner>();constautomtc_cartesian_planner=std::make_shared<moveit::task_constructor::solvers::CartesianPath>();\nvoidsetProperty(conststd::string&name,constboost::any&value);\nstage->setCostTerm(moveit::task_constructor::LambdaCostTerm([](constmoveit::task_constructor::SubTrajectory&traj){return100*traj.cost();}));\nautoerror_code=task.plan()\nautoresult=task.execute(*task.solutions().front());\nmove_group\nCurrentState\nGeneratePose\nCurrentState\nModifyPlanningScene\nCurrentState\nGeneratePose\nMoveRelative\nComputeIK\nGenerateGraspPose\nGenerateGraspPose\nComputeIK\nGeneratePose\nGeneratePose\nPipelinePlanner\nJointInterpolation\nCartesianPath\nMoveTo\nMoveRelative\nConnect\nConstant\nPathLength\nTrajectoryDuration\nTrajectoryCostTerm\nLambdaCostTerm\nDistanceToReference\nLinkMotion\nClearance\nLambdaCostTerm\nMoveItErrorCode\nmoveit_msgs::msg::MoveItErrorCodes::SUCCESS\nexecute_task_solution\nexecute_task_solution_capability\nMoveGroupCapability\nMotionPlanRequest\nPlanExecution",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee126"
  },
  "url": "https://moveit.picknik.ai/main/doc/api/cpp_api/api.html",
  "title": "moveit2: moveit2",
  "content": "Buildfarm\nContinuous Integration Status\nGetting Started\nInstall\nMore Info\nSupporters\nGenerate API Doxygen Documentation\n\nTheMoveIt Motion Planning Framework for ROS 2. For the ROS 1 repository seeMoveIt 1. For the commercially supported version seeMoveIt Pro.\nEasy-to-use open source robotics manipulation platform for developing commercial applications, prototyping designs, and benchmarking algorithms.\n\nSee our extensiveTutorials and Documentation.\nThis open source project is maintained by supporters from around the world — see ourMoveIt Maintainers and Core Contributors.\n\nPickNik Incis leading the development of MoveIt. If you would like to support this project, please contacthello.nosp@m.@pic.nosp@m.knik..nosp@m.ai.\n\nThe port to ROS 2 was supported by ROSIN - ROS-Industrial Quality-Assured Robot Software Components. More information:rosin-project.eu.\n\nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement no. 732287.\nSeeHow To Generate API Doxygen Reference Locally.",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee127"
  },
  "url": "https://moveit.picknik.ai/main/doc/api/python_api/api.html",
  "title": "Python API Documentation — MoveIt documentation  documentation",
  "content": "Python API Documentation\nmoveit.core.collision_detection\n\nmoveit.core.controller_manager\n\nmoveit.core.kinematic_constraints\n\nmoveit.core.planning_interface\n\nmoveit.core.planning_scene\n\nmoveit.core.robot_model\n\nmoveit.core.robot_state\n\nmoveit.core.robot_trajectory\n\nmoveit.planning\n\nmoveit.servo_client\n\n© Copyright 2024, PickNik Robotics.\nmoveit.core.collision_detection\nmoveit.core.controller_manager\nmoveit.core.kinematic_constraints\nmoveit.core.planning_interface\nmoveit.core.planning_scene\nmoveit.core.robot_model\nmoveit.core.robot_state\nmoveit.core.robot_trajectory\nmoveit.planning\nmoveit.servo_client",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee128"
  },
  "url": "https://moveit.picknik.ai/main/doc/api/python_api/api.html#python-api-documentation",
  "title": "Python API Documentation — MoveIt documentation  documentation",
  "content": "Python API Documentation\nmoveit.core.collision_detection\n\nmoveit.core.controller_manager\n\nmoveit.core.kinematic_constraints\n\nmoveit.core.planning_interface\n\nmoveit.core.planning_scene\n\nmoveit.core.robot_model\n\nmoveit.core.robot_state\n\nmoveit.core.robot_trajectory\n\nmoveit.planning\n\nmoveit.servo_client\n\n© Copyright 2024, PickNik Robotics.\nmoveit.core.collision_detection\nmoveit.core.controller_manager\nmoveit.core.kinematic_constraints\nmoveit.core.planning_interface\nmoveit.core.planning_scene\nmoveit.core.robot_model\nmoveit.core.robot_state\nmoveit.core.robot_trajectory\nmoveit.planning\nmoveit.servo_client",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee129"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_contribute_to_site.html",
  "title": "How to Contribute to This Site — MoveIt Documentation: Rolling  documentation",
  "content": "How to Contribute to This Site\nLearning Objectives\nRequirements\nSteps\nFurther Reading\nThis explains what you need to know to successfully submit changes to this site.\nHow to locally build this website.\nHow to check for broken links.\nHow to check spelling and formatting.\nHow to run CI locally for checking code changes.\nUbuntu 20.04\nROS 2 Galactic\nDocker\nA colcon workspace with a copy ofthe moveit2_tutorials repo(if you don’t already have one,Getting Startedwill walk you through the process of creating one).\nBuild and view the website locally\nFirst,cdto the root directory of the moveit2_tutorials repo (if you followed theGetting Startedtutorial, this will be~/ws_moveit/src/moveit2_tutorials).  From that directory, run this command:\nThen you can open the local build of the site in your web browser from:./build/html/index.html. For example, to open the local site in your default browser the command would be:\nTest for broken links\nTo test for broken links, run thehtmlprooferscript. Note that running this script may require you to install some Python dependencies. If usingpipto install these dependencies, you may want to create a virtual environment.\nYou can then install requirements usingpip:\nYou can run thehtmlprooferscript with this command:\nRun the formatters and spellchecker\nWe usepre-committo run the formatting and spelling checkers.\nYou can install it with pip like this:\nTo run pre-commit locally to fix formatting and spelling:\nRun industrial_ci locally to run CI\nClone a copy ofindustrial_ciinto your workspace.\nBuild and source your workspace.\nRun this command from the workspace directory to test code changes just as it is done in CI:\nHow to Write a MoveIt Tutorial\nHow to Write a MoveIt How-To Guide\nHow to Cross-Reference Content\n© Copyright 2024, PickNik Robotics.\nmakehtml# Or run the following lines if you want to automatically rebuild the website on new changeswhileinotifywait-remodify,move,create,delete.;domakehtmldone\nxdg-open./build/html/index.html\npython3-mvenv.venvsource.venv/bin/activate\npip3install--upgrade--requirementrequirements.txt\n./htmlproofer.sh\npython3-mpipinstall--userpre-commit\npre-commitrun--all\nros2runindustrial_cirerun_cisrc/moveit2_tutorials\\DOCKER_IMAGE='moveit/moveit2:rolling-source'\\UPSTREAM_WORKSPACE='moveit2_tutorials.repos'\\TARGET_CMAKE_ARGS='-DCMAKE_BUILD_TYPE=Release'\\CCACHE_DIR=\"$HOME/.ccache\"\\CLANG_TIDY='true'\ncd\n~/ws_moveit/src/moveit2_tutorials\n./build/html/index.html\nhtmlproofer\npip\npip\nhtmlproofer",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee12a"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_contribute_to_site.html#learning-objectives",
  "title": "How to Contribute to This Site — MoveIt Documentation: Rolling  documentation",
  "content": "How to Contribute to This Site\nLearning Objectives\nRequirements\nSteps\nFurther Reading\nThis explains what you need to know to successfully submit changes to this site.\nHow to locally build this website.\nHow to check for broken links.\nHow to check spelling and formatting.\nHow to run CI locally for checking code changes.\nUbuntu 20.04\nROS 2 Galactic\nDocker\nA colcon workspace with a copy ofthe moveit2_tutorials repo(if you don’t already have one,Getting Startedwill walk you through the process of creating one).\nBuild and view the website locally\nFirst,cdto the root directory of the moveit2_tutorials repo (if you followed theGetting Startedtutorial, this will be~/ws_moveit/src/moveit2_tutorials).  From that directory, run this command:\nThen you can open the local build of the site in your web browser from:./build/html/index.html. For example, to open the local site in your default browser the command would be:\nTest for broken links\nTo test for broken links, run thehtmlprooferscript. Note that running this script may require you to install some Python dependencies. If usingpipto install these dependencies, you may want to create a virtual environment.\nYou can then install requirements usingpip:\nYou can run thehtmlprooferscript with this command:\nRun the formatters and spellchecker\nWe usepre-committo run the formatting and spelling checkers.\nYou can install it with pip like this:\nTo run pre-commit locally to fix formatting and spelling:\nRun industrial_ci locally to run CI\nClone a copy ofindustrial_ciinto your workspace.\nBuild and source your workspace.\nRun this command from the workspace directory to test code changes just as it is done in CI:\nHow to Write a MoveIt Tutorial\nHow to Write a MoveIt How-To Guide\nHow to Cross-Reference Content\n© Copyright 2024, PickNik Robotics.\nmakehtml# Or run the following lines if you want to automatically rebuild the website on new changeswhileinotifywait-remodify,move,create,delete.;domakehtmldone\nxdg-open./build/html/index.html\npython3-mvenv.venvsource.venv/bin/activate\npip3install--upgrade--requirementrequirements.txt\n./htmlproofer.sh\npython3-mpipinstall--userpre-commit\npre-commitrun--all\nros2runindustrial_cirerun_cisrc/moveit2_tutorials\\DOCKER_IMAGE='moveit/moveit2:rolling-source'\\UPSTREAM_WORKSPACE='moveit2_tutorials.repos'\\TARGET_CMAKE_ARGS='-DCMAKE_BUILD_TYPE=Release'\\CCACHE_DIR=\"$HOME/.ccache\"\\CLANG_TIDY='true'\ncd\n~/ws_moveit/src/moveit2_tutorials\n./build/html/index.html\nhtmlproofer\npip\npip\nhtmlproofer",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee12b"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_contribute_to_site.html#requirements",
  "title": "How to Contribute to This Site — MoveIt Documentation: Rolling  documentation",
  "content": "How to Contribute to This Site\nLearning Objectives\nRequirements\nSteps\nFurther Reading\nThis explains what you need to know to successfully submit changes to this site.\nHow to locally build this website.\nHow to check for broken links.\nHow to check spelling and formatting.\nHow to run CI locally for checking code changes.\nUbuntu 20.04\nROS 2 Galactic\nDocker\nA colcon workspace with a copy ofthe moveit2_tutorials repo(if you don’t already have one,Getting Startedwill walk you through the process of creating one).\nBuild and view the website locally\nFirst,cdto the root directory of the moveit2_tutorials repo (if you followed theGetting Startedtutorial, this will be~/ws_moveit/src/moveit2_tutorials).  From that directory, run this command:\nThen you can open the local build of the site in your web browser from:./build/html/index.html. For example, to open the local site in your default browser the command would be:\nTest for broken links\nTo test for broken links, run thehtmlprooferscript. Note that running this script may require you to install some Python dependencies. If usingpipto install these dependencies, you may want to create a virtual environment.\nYou can then install requirements usingpip:\nYou can run thehtmlprooferscript with this command:\nRun the formatters and spellchecker\nWe usepre-committo run the formatting and spelling checkers.\nYou can install it with pip like this:\nTo run pre-commit locally to fix formatting and spelling:\nRun industrial_ci locally to run CI\nClone a copy ofindustrial_ciinto your workspace.\nBuild and source your workspace.\nRun this command from the workspace directory to test code changes just as it is done in CI:\nHow to Write a MoveIt Tutorial\nHow to Write a MoveIt How-To Guide\nHow to Cross-Reference Content\n© Copyright 2024, PickNik Robotics.\nmakehtml# Or run the following lines if you want to automatically rebuild the website on new changeswhileinotifywait-remodify,move,create,delete.;domakehtmldone\nxdg-open./build/html/index.html\npython3-mvenv.venvsource.venv/bin/activate\npip3install--upgrade--requirementrequirements.txt\n./htmlproofer.sh\npython3-mpipinstall--userpre-commit\npre-commitrun--all\nros2runindustrial_cirerun_cisrc/moveit2_tutorials\\DOCKER_IMAGE='moveit/moveit2:rolling-source'\\UPSTREAM_WORKSPACE='moveit2_tutorials.repos'\\TARGET_CMAKE_ARGS='-DCMAKE_BUILD_TYPE=Release'\\CCACHE_DIR=\"$HOME/.ccache\"\\CLANG_TIDY='true'\ncd\n~/ws_moveit/src/moveit2_tutorials\n./build/html/index.html\nhtmlproofer\npip\npip\nhtmlproofer",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee12c"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_contribute_to_site.html#steps",
  "title": "How to Contribute to This Site — MoveIt Documentation: Rolling  documentation",
  "content": "How to Contribute to This Site\nLearning Objectives\nRequirements\nSteps\nFurther Reading\nThis explains what you need to know to successfully submit changes to this site.\nHow to locally build this website.\nHow to check for broken links.\nHow to check spelling and formatting.\nHow to run CI locally for checking code changes.\nUbuntu 20.04\nROS 2 Galactic\nDocker\nA colcon workspace with a copy ofthe moveit2_tutorials repo(if you don’t already have one,Getting Startedwill walk you through the process of creating one).\nBuild and view the website locally\nFirst,cdto the root directory of the moveit2_tutorials repo (if you followed theGetting Startedtutorial, this will be~/ws_moveit/src/moveit2_tutorials).  From that directory, run this command:\nThen you can open the local build of the site in your web browser from:./build/html/index.html. For example, to open the local site in your default browser the command would be:\nTest for broken links\nTo test for broken links, run thehtmlprooferscript. Note that running this script may require you to install some Python dependencies. If usingpipto install these dependencies, you may want to create a virtual environment.\nYou can then install requirements usingpip:\nYou can run thehtmlprooferscript with this command:\nRun the formatters and spellchecker\nWe usepre-committo run the formatting and spelling checkers.\nYou can install it with pip like this:\nTo run pre-commit locally to fix formatting and spelling:\nRun industrial_ci locally to run CI\nClone a copy ofindustrial_ciinto your workspace.\nBuild and source your workspace.\nRun this command from the workspace directory to test code changes just as it is done in CI:\nHow to Write a MoveIt Tutorial\nHow to Write a MoveIt How-To Guide\nHow to Cross-Reference Content\n© Copyright 2024, PickNik Robotics.\nmakehtml# Or run the following lines if you want to automatically rebuild the website on new changeswhileinotifywait-remodify,move,create,delete.;domakehtmldone\nxdg-open./build/html/index.html\npython3-mvenv.venvsource.venv/bin/activate\npip3install--upgrade--requirementrequirements.txt\n./htmlproofer.sh\npython3-mpipinstall--userpre-commit\npre-commitrun--all\nros2runindustrial_cirerun_cisrc/moveit2_tutorials\\DOCKER_IMAGE='moveit/moveit2:rolling-source'\\UPSTREAM_WORKSPACE='moveit2_tutorials.repos'\\TARGET_CMAKE_ARGS='-DCMAKE_BUILD_TYPE=Release'\\CCACHE_DIR=\"$HOME/.ccache\"\\CLANG_TIDY='true'\ncd\n~/ws_moveit/src/moveit2_tutorials\n./build/html/index.html\nhtmlproofer\npip\npip\nhtmlproofer",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee12d"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_contribute_to_site.html#further-reading",
  "title": "How to Contribute to This Site — MoveIt Documentation: Rolling  documentation",
  "content": "How to Contribute to This Site\nLearning Objectives\nRequirements\nSteps\nFurther Reading\nThis explains what you need to know to successfully submit changes to this site.\nHow to locally build this website.\nHow to check for broken links.\nHow to check spelling and formatting.\nHow to run CI locally for checking code changes.\nUbuntu 20.04\nROS 2 Galactic\nDocker\nA colcon workspace with a copy ofthe moveit2_tutorials repo(if you don’t already have one,Getting Startedwill walk you through the process of creating one).\nBuild and view the website locally\nFirst,cdto the root directory of the moveit2_tutorials repo (if you followed theGetting Startedtutorial, this will be~/ws_moveit/src/moveit2_tutorials).  From that directory, run this command:\nThen you can open the local build of the site in your web browser from:./build/html/index.html. For example, to open the local site in your default browser the command would be:\nTest for broken links\nTo test for broken links, run thehtmlprooferscript. Note that running this script may require you to install some Python dependencies. If usingpipto install these dependencies, you may want to create a virtual environment.\nYou can then install requirements usingpip:\nYou can run thehtmlprooferscript with this command:\nRun the formatters and spellchecker\nWe usepre-committo run the formatting and spelling checkers.\nYou can install it with pip like this:\nTo run pre-commit locally to fix formatting and spelling:\nRun industrial_ci locally to run CI\nClone a copy ofindustrial_ciinto your workspace.\nBuild and source your workspace.\nRun this command from the workspace directory to test code changes just as it is done in CI:\nHow to Write a MoveIt Tutorial\nHow to Write a MoveIt How-To Guide\nHow to Cross-Reference Content\n© Copyright 2024, PickNik Robotics.\nmakehtml# Or run the following lines if you want to automatically rebuild the website on new changeswhileinotifywait-remodify,move,create,delete.;domakehtmldone\nxdg-open./build/html/index.html\npython3-mvenv.venvsource.venv/bin/activate\npip3install--upgrade--requirementrequirements.txt\n./htmlproofer.sh\npython3-mpipinstall--userpre-commit\npre-commitrun--all\nros2runindustrial_cirerun_cisrc/moveit2_tutorials\\DOCKER_IMAGE='moveit/moveit2:rolling-source'\\UPSTREAM_WORKSPACE='moveit2_tutorials.repos'\\TARGET_CMAKE_ARGS='-DCMAKE_BUILD_TYPE=Release'\\CCACHE_DIR=\"$HOME/.ccache\"\\CLANG_TIDY='true'\ncd\n~/ws_moveit/src/moveit2_tutorials\n./build/html/index.html\nhtmlproofer\npip\npip\nhtmlproofer",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee12e"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_contribute_to_site.html#how-to-contribute-to-this-site",
  "title": "How to Contribute to This Site — MoveIt Documentation: Rolling  documentation",
  "content": "How to Contribute to This Site\nLearning Objectives\nRequirements\nSteps\nFurther Reading\nThis explains what you need to know to successfully submit changes to this site.\nHow to locally build this website.\nHow to check for broken links.\nHow to check spelling and formatting.\nHow to run CI locally for checking code changes.\nUbuntu 20.04\nROS 2 Galactic\nDocker\nA colcon workspace with a copy ofthe moveit2_tutorials repo(if you don’t already have one,Getting Startedwill walk you through the process of creating one).\nBuild and view the website locally\nFirst,cdto the root directory of the moveit2_tutorials repo (if you followed theGetting Startedtutorial, this will be~/ws_moveit/src/moveit2_tutorials).  From that directory, run this command:\nThen you can open the local build of the site in your web browser from:./build/html/index.html. For example, to open the local site in your default browser the command would be:\nTest for broken links\nTo test for broken links, run thehtmlprooferscript. Note that running this script may require you to install some Python dependencies. If usingpipto install these dependencies, you may want to create a virtual environment.\nYou can then install requirements usingpip:\nYou can run thehtmlprooferscript with this command:\nRun the formatters and spellchecker\nWe usepre-committo run the formatting and spelling checkers.\nYou can install it with pip like this:\nTo run pre-commit locally to fix formatting and spelling:\nRun industrial_ci locally to run CI\nClone a copy ofindustrial_ciinto your workspace.\nBuild and source your workspace.\nRun this command from the workspace directory to test code changes just as it is done in CI:\nHow to Write a MoveIt Tutorial\nHow to Write a MoveIt How-To Guide\nHow to Cross-Reference Content\n© Copyright 2024, PickNik Robotics.\nmakehtml# Or run the following lines if you want to automatically rebuild the website on new changeswhileinotifywait-remodify,move,create,delete.;domakehtmldone\nxdg-open./build/html/index.html\npython3-mvenv.venvsource.venv/bin/activate\npip3install--upgrade--requirementrequirements.txt\n./htmlproofer.sh\npython3-mpipinstall--userpre-commit\npre-commitrun--all\nros2runindustrial_cirerun_cisrc/moveit2_tutorials\\DOCKER_IMAGE='moveit/moveit2:rolling-source'\\UPSTREAM_WORKSPACE='moveit2_tutorials.repos'\\TARGET_CMAKE_ARGS='-DCMAKE_BUILD_TYPE=Release'\\CCACHE_DIR=\"$HOME/.ccache\"\\CLANG_TIDY='true'\ncd\n~/ws_moveit/src/moveit2_tutorials\n./build/html/index.html\nhtmlproofer\npip\npip\nhtmlproofer",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee12f"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_write_tutorials.html",
  "title": "How to Write a MoveIt Tutorial — MoveIt Documentation: Rolling  documentation",
  "content": "How to Write a MoveIt Tutorial\nLearning Objectives\nRequirements\nSteps\nTemplate\nFurther Reading\nThis guide explains how to write tutorials for the MoveIt documentation.\nTutorials are one of the most useful contributions you can make because they are the first thing many new users see.\nThis guide is intended for any contributor who wants to submit a new tutorial.\nThere are many additional quality standards and how-tos for contributing to the tutorials located in this repository’sREADME.\nThe type of information that should be included in a tutorial.\nProper formatting for tutorials in the MoveIt documentation section.\nUbuntu 20.04\nROS 2 Galactic\nMoveIt 2\nFork theMoveIt 2 Tutorials repositoryand start a new branch with an intuitive name (e.g.,jack/how-to-tutorials).\nCreate a new file in thedoc/tutorialsdirectory with a.rstextension. The title should be a concise description (e.g., “MoveIt Quickstart in RViz”).\nAdd a link to your tutorial on thetutorials page.\nWrite the introduction using reStructuredText (.rst) using the following guidelines:\nThe introduction should explain the purpose of this tutorial and the intended audience.\nIf you think people may regularly find this particular guide by mistake, add links to the proper resource.\nWrite the specific learning objectives (i.e., what will the reader know when they finish reading this tutorial).\nAdd any system or equipment requirements for this tutorial so users know if this tutorial is appropriate for them.\nLayout the interactive steps to be followed with enough details that the reader can easily follow along.\nWrite a conclusion to summarize the tutorial and provide additional resources.\nAdd a link to the next tutorial the reader should follow.\nSubmit the new page as a PR to theMoveIt 2 Tutorials repository.\nHow to Contribute to This Site\nHow to Cross-Reference Content\n© Copyright 2024, PickNik Robotics.\n<Title>-------<Briefdescriptionofthetutorialwithimageshowingwhatwillbeaccomplished.>Background----------<Explanationofwhattheusershouldhavealreadydonebeforethistutorial.>Steps-----1.<FirstStep>---------------<ThisshoulddescribeanactionthattheusershouldtakesuchascreatingaROSprojectortypingupsomecode.>1.1)<ExplanationFirstStep>^^^^^^^^^^^^^^^^^^^^^^^^^^^^<Usesub-stepslikethistowalktheuserthroughanexplanationofwhattheydid.>1.2)<ActionFirststep>^^^^^^^^^^^^^^^^^^^^^^^<Useasub-steplikethistodescriberunningthenewcodeandwhattheresultsshouldbe.>2.<SecondStep>----------------<...>Conclusion----------<Hereiswhereyouexplainwhattheuserhasreadandprovideadditionalreferences.>NextStep---------<Linktothenexttutorialhere.>\njack/how-to-tutorials\ndoc/tutorials\n.rst",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee130"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_write_tutorials.html#learning-objectives",
  "title": "How to Write a MoveIt Tutorial — MoveIt Documentation: Rolling  documentation",
  "content": "How to Write a MoveIt Tutorial\nLearning Objectives\nRequirements\nSteps\nTemplate\nFurther Reading\nThis guide explains how to write tutorials for the MoveIt documentation.\nTutorials are one of the most useful contributions you can make because they are the first thing many new users see.\nThis guide is intended for any contributor who wants to submit a new tutorial.\nThere are many additional quality standards and how-tos for contributing to the tutorials located in this repository’sREADME.\nThe type of information that should be included in a tutorial.\nProper formatting for tutorials in the MoveIt documentation section.\nUbuntu 20.04\nROS 2 Galactic\nMoveIt 2\nFork theMoveIt 2 Tutorials repositoryand start a new branch with an intuitive name (e.g.,jack/how-to-tutorials).\nCreate a new file in thedoc/tutorialsdirectory with a.rstextension. The title should be a concise description (e.g., “MoveIt Quickstart in RViz”).\nAdd a link to your tutorial on thetutorials page.\nWrite the introduction using reStructuredText (.rst) using the following guidelines:\nThe introduction should explain the purpose of this tutorial and the intended audience.\nIf you think people may regularly find this particular guide by mistake, add links to the proper resource.\nWrite the specific learning objectives (i.e., what will the reader know when they finish reading this tutorial).\nAdd any system or equipment requirements for this tutorial so users know if this tutorial is appropriate for them.\nLayout the interactive steps to be followed with enough details that the reader can easily follow along.\nWrite a conclusion to summarize the tutorial and provide additional resources.\nAdd a link to the next tutorial the reader should follow.\nSubmit the new page as a PR to theMoveIt 2 Tutorials repository.\nHow to Contribute to This Site\nHow to Cross-Reference Content\n© Copyright 2024, PickNik Robotics.\n<Title>-------<Briefdescriptionofthetutorialwithimageshowingwhatwillbeaccomplished.>Background----------<Explanationofwhattheusershouldhavealreadydonebeforethistutorial.>Steps-----1.<FirstStep>---------------<ThisshoulddescribeanactionthattheusershouldtakesuchascreatingaROSprojectortypingupsomecode.>1.1)<ExplanationFirstStep>^^^^^^^^^^^^^^^^^^^^^^^^^^^^<Usesub-stepslikethistowalktheuserthroughanexplanationofwhattheydid.>1.2)<ActionFirststep>^^^^^^^^^^^^^^^^^^^^^^^<Useasub-steplikethistodescriberunningthenewcodeandwhattheresultsshouldbe.>2.<SecondStep>----------------<...>Conclusion----------<Hereiswhereyouexplainwhattheuserhasreadandprovideadditionalreferences.>NextStep---------<Linktothenexttutorialhere.>\njack/how-to-tutorials\ndoc/tutorials\n.rst",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee131"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_write_tutorials.html#requirements",
  "title": "How to Write a MoveIt Tutorial — MoveIt Documentation: Rolling  documentation",
  "content": "How to Write a MoveIt Tutorial\nLearning Objectives\nRequirements\nSteps\nTemplate\nFurther Reading\nThis guide explains how to write tutorials for the MoveIt documentation.\nTutorials are one of the most useful contributions you can make because they are the first thing many new users see.\nThis guide is intended for any contributor who wants to submit a new tutorial.\nThere are many additional quality standards and how-tos for contributing to the tutorials located in this repository’sREADME.\nThe type of information that should be included in a tutorial.\nProper formatting for tutorials in the MoveIt documentation section.\nUbuntu 20.04\nROS 2 Galactic\nMoveIt 2\nFork theMoveIt 2 Tutorials repositoryand start a new branch with an intuitive name (e.g.,jack/how-to-tutorials).\nCreate a new file in thedoc/tutorialsdirectory with a.rstextension. The title should be a concise description (e.g., “MoveIt Quickstart in RViz”).\nAdd a link to your tutorial on thetutorials page.\nWrite the introduction using reStructuredText (.rst) using the following guidelines:\nThe introduction should explain the purpose of this tutorial and the intended audience.\nIf you think people may regularly find this particular guide by mistake, add links to the proper resource.\nWrite the specific learning objectives (i.e., what will the reader know when they finish reading this tutorial).\nAdd any system or equipment requirements for this tutorial so users know if this tutorial is appropriate for them.\nLayout the interactive steps to be followed with enough details that the reader can easily follow along.\nWrite a conclusion to summarize the tutorial and provide additional resources.\nAdd a link to the next tutorial the reader should follow.\nSubmit the new page as a PR to theMoveIt 2 Tutorials repository.\nHow to Contribute to This Site\nHow to Cross-Reference Content\n© Copyright 2024, PickNik Robotics.\n<Title>-------<Briefdescriptionofthetutorialwithimageshowingwhatwillbeaccomplished.>Background----------<Explanationofwhattheusershouldhavealreadydonebeforethistutorial.>Steps-----1.<FirstStep>---------------<ThisshoulddescribeanactionthattheusershouldtakesuchascreatingaROSprojectortypingupsomecode.>1.1)<ExplanationFirstStep>^^^^^^^^^^^^^^^^^^^^^^^^^^^^<Usesub-stepslikethistowalktheuserthroughanexplanationofwhattheydid.>1.2)<ActionFirststep>^^^^^^^^^^^^^^^^^^^^^^^<Useasub-steplikethistodescriberunningthenewcodeandwhattheresultsshouldbe.>2.<SecondStep>----------------<...>Conclusion----------<Hereiswhereyouexplainwhattheuserhasreadandprovideadditionalreferences.>NextStep---------<Linktothenexttutorialhere.>\njack/how-to-tutorials\ndoc/tutorials\n.rst",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee132"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_write_tutorials.html#steps",
  "title": "How to Write a MoveIt Tutorial — MoveIt Documentation: Rolling  documentation",
  "content": "How to Write a MoveIt Tutorial\nLearning Objectives\nRequirements\nSteps\nTemplate\nFurther Reading\nThis guide explains how to write tutorials for the MoveIt documentation.\nTutorials are one of the most useful contributions you can make because they are the first thing many new users see.\nThis guide is intended for any contributor who wants to submit a new tutorial.\nThere are many additional quality standards and how-tos for contributing to the tutorials located in this repository’sREADME.\nThe type of information that should be included in a tutorial.\nProper formatting for tutorials in the MoveIt documentation section.\nUbuntu 20.04\nROS 2 Galactic\nMoveIt 2\nFork theMoveIt 2 Tutorials repositoryand start a new branch with an intuitive name (e.g.,jack/how-to-tutorials).\nCreate a new file in thedoc/tutorialsdirectory with a.rstextension. The title should be a concise description (e.g., “MoveIt Quickstart in RViz”).\nAdd a link to your tutorial on thetutorials page.\nWrite the introduction using reStructuredText (.rst) using the following guidelines:\nThe introduction should explain the purpose of this tutorial and the intended audience.\nIf you think people may regularly find this particular guide by mistake, add links to the proper resource.\nWrite the specific learning objectives (i.e., what will the reader know when they finish reading this tutorial).\nAdd any system or equipment requirements for this tutorial so users know if this tutorial is appropriate for them.\nLayout the interactive steps to be followed with enough details that the reader can easily follow along.\nWrite a conclusion to summarize the tutorial and provide additional resources.\nAdd a link to the next tutorial the reader should follow.\nSubmit the new page as a PR to theMoveIt 2 Tutorials repository.\nHow to Contribute to This Site\nHow to Cross-Reference Content\n© Copyright 2024, PickNik Robotics.\n<Title>-------<Briefdescriptionofthetutorialwithimageshowingwhatwillbeaccomplished.>Background----------<Explanationofwhattheusershouldhavealreadydonebeforethistutorial.>Steps-----1.<FirstStep>---------------<ThisshoulddescribeanactionthattheusershouldtakesuchascreatingaROSprojectortypingupsomecode.>1.1)<ExplanationFirstStep>^^^^^^^^^^^^^^^^^^^^^^^^^^^^<Usesub-stepslikethistowalktheuserthroughanexplanationofwhattheydid.>1.2)<ActionFirststep>^^^^^^^^^^^^^^^^^^^^^^^<Useasub-steplikethistodescriberunningthenewcodeandwhattheresultsshouldbe.>2.<SecondStep>----------------<...>Conclusion----------<Hereiswhereyouexplainwhattheuserhasreadandprovideadditionalreferences.>NextStep---------<Linktothenexttutorialhere.>\njack/how-to-tutorials\ndoc/tutorials\n.rst",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee133"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_write_tutorials.html#template",
  "title": "How to Write a MoveIt Tutorial — MoveIt Documentation: Rolling  documentation",
  "content": "How to Write a MoveIt Tutorial\nLearning Objectives\nRequirements\nSteps\nTemplate\nFurther Reading\nThis guide explains how to write tutorials for the MoveIt documentation.\nTutorials are one of the most useful contributions you can make because they are the first thing many new users see.\nThis guide is intended for any contributor who wants to submit a new tutorial.\nThere are many additional quality standards and how-tos for contributing to the tutorials located in this repository’sREADME.\nThe type of information that should be included in a tutorial.\nProper formatting for tutorials in the MoveIt documentation section.\nUbuntu 20.04\nROS 2 Galactic\nMoveIt 2\nFork theMoveIt 2 Tutorials repositoryand start a new branch with an intuitive name (e.g.,jack/how-to-tutorials).\nCreate a new file in thedoc/tutorialsdirectory with a.rstextension. The title should be a concise description (e.g., “MoveIt Quickstart in RViz”).\nAdd a link to your tutorial on thetutorials page.\nWrite the introduction using reStructuredText (.rst) using the following guidelines:\nThe introduction should explain the purpose of this tutorial and the intended audience.\nIf you think people may regularly find this particular guide by mistake, add links to the proper resource.\nWrite the specific learning objectives (i.e., what will the reader know when they finish reading this tutorial).\nAdd any system or equipment requirements for this tutorial so users know if this tutorial is appropriate for them.\nLayout the interactive steps to be followed with enough details that the reader can easily follow along.\nWrite a conclusion to summarize the tutorial and provide additional resources.\nAdd a link to the next tutorial the reader should follow.\nSubmit the new page as a PR to theMoveIt 2 Tutorials repository.\nHow to Contribute to This Site\nHow to Cross-Reference Content\n© Copyright 2024, PickNik Robotics.\n<Title>-------<Briefdescriptionofthetutorialwithimageshowingwhatwillbeaccomplished.>Background----------<Explanationofwhattheusershouldhavealreadydonebeforethistutorial.>Steps-----1.<FirstStep>---------------<ThisshoulddescribeanactionthattheusershouldtakesuchascreatingaROSprojectortypingupsomecode.>1.1)<ExplanationFirstStep>^^^^^^^^^^^^^^^^^^^^^^^^^^^^<Usesub-stepslikethistowalktheuserthroughanexplanationofwhattheydid.>1.2)<ActionFirststep>^^^^^^^^^^^^^^^^^^^^^^^<Useasub-steplikethistodescriberunningthenewcodeandwhattheresultsshouldbe.>2.<SecondStep>----------------<...>Conclusion----------<Hereiswhereyouexplainwhattheuserhasreadandprovideadditionalreferences.>NextStep---------<Linktothenexttutorialhere.>\njack/how-to-tutorials\ndoc/tutorials\n.rst",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee134"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_write_tutorials.html#further-reading",
  "title": "How to Write a MoveIt Tutorial — MoveIt Documentation: Rolling  documentation",
  "content": "How to Write a MoveIt Tutorial\nLearning Objectives\nRequirements\nSteps\nTemplate\nFurther Reading\nThis guide explains how to write tutorials for the MoveIt documentation.\nTutorials are one of the most useful contributions you can make because they are the first thing many new users see.\nThis guide is intended for any contributor who wants to submit a new tutorial.\nThere are many additional quality standards and how-tos for contributing to the tutorials located in this repository’sREADME.\nThe type of information that should be included in a tutorial.\nProper formatting for tutorials in the MoveIt documentation section.\nUbuntu 20.04\nROS 2 Galactic\nMoveIt 2\nFork theMoveIt 2 Tutorials repositoryand start a new branch with an intuitive name (e.g.,jack/how-to-tutorials).\nCreate a new file in thedoc/tutorialsdirectory with a.rstextension. The title should be a concise description (e.g., “MoveIt Quickstart in RViz”).\nAdd a link to your tutorial on thetutorials page.\nWrite the introduction using reStructuredText (.rst) using the following guidelines:\nThe introduction should explain the purpose of this tutorial and the intended audience.\nIf you think people may regularly find this particular guide by mistake, add links to the proper resource.\nWrite the specific learning objectives (i.e., what will the reader know when they finish reading this tutorial).\nAdd any system or equipment requirements for this tutorial so users know if this tutorial is appropriate for them.\nLayout the interactive steps to be followed with enough details that the reader can easily follow along.\nWrite a conclusion to summarize the tutorial and provide additional resources.\nAdd a link to the next tutorial the reader should follow.\nSubmit the new page as a PR to theMoveIt 2 Tutorials repository.\nHow to Contribute to This Site\nHow to Cross-Reference Content\n© Copyright 2024, PickNik Robotics.\n<Title>-------<Briefdescriptionofthetutorialwithimageshowingwhatwillbeaccomplished.>Background----------<Explanationofwhattheusershouldhavealreadydonebeforethistutorial.>Steps-----1.<FirstStep>---------------<ThisshoulddescribeanactionthattheusershouldtakesuchascreatingaROSprojectortypingupsomecode.>1.1)<ExplanationFirstStep>^^^^^^^^^^^^^^^^^^^^^^^^^^^^<Usesub-stepslikethistowalktheuserthroughanexplanationofwhattheydid.>1.2)<ActionFirststep>^^^^^^^^^^^^^^^^^^^^^^^<Useasub-steplikethistodescriberunningthenewcodeandwhattheresultsshouldbe.>2.<SecondStep>----------------<...>Conclusion----------<Hereiswhereyouexplainwhattheuserhasreadandprovideadditionalreferences.>NextStep---------<Linktothenexttutorialhere.>\njack/how-to-tutorials\ndoc/tutorials\n.rst",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee135"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_write_tutorials.html#how-to-write-a-moveit-tutorial",
  "title": "How to Write a MoveIt Tutorial — MoveIt Documentation: Rolling  documentation",
  "content": "How to Write a MoveIt Tutorial\nLearning Objectives\nRequirements\nSteps\nTemplate\nFurther Reading\nThis guide explains how to write tutorials for the MoveIt documentation.\nTutorials are one of the most useful contributions you can make because they are the first thing many new users see.\nThis guide is intended for any contributor who wants to submit a new tutorial.\nThere are many additional quality standards and how-tos for contributing to the tutorials located in this repository’sREADME.\nThe type of information that should be included in a tutorial.\nProper formatting for tutorials in the MoveIt documentation section.\nUbuntu 20.04\nROS 2 Galactic\nMoveIt 2\nFork theMoveIt 2 Tutorials repositoryand start a new branch with an intuitive name (e.g.,jack/how-to-tutorials).\nCreate a new file in thedoc/tutorialsdirectory with a.rstextension. The title should be a concise description (e.g., “MoveIt Quickstart in RViz”).\nAdd a link to your tutorial on thetutorials page.\nWrite the introduction using reStructuredText (.rst) using the following guidelines:\nThe introduction should explain the purpose of this tutorial and the intended audience.\nIf you think people may regularly find this particular guide by mistake, add links to the proper resource.\nWrite the specific learning objectives (i.e., what will the reader know when they finish reading this tutorial).\nAdd any system or equipment requirements for this tutorial so users know if this tutorial is appropriate for them.\nLayout the interactive steps to be followed with enough details that the reader can easily follow along.\nWrite a conclusion to summarize the tutorial and provide additional resources.\nAdd a link to the next tutorial the reader should follow.\nSubmit the new page as a PR to theMoveIt 2 Tutorials repository.\nHow to Contribute to This Site\nHow to Cross-Reference Content\n© Copyright 2024, PickNik Robotics.\n<Title>-------<Briefdescriptionofthetutorialwithimageshowingwhatwillbeaccomplished.>Background----------<Explanationofwhattheusershouldhavealreadydonebeforethistutorial.>Steps-----1.<FirstStep>---------------<ThisshoulddescribeanactionthattheusershouldtakesuchascreatingaROSprojectortypingupsomecode.>1.1)<ExplanationFirstStep>^^^^^^^^^^^^^^^^^^^^^^^^^^^^<Usesub-stepslikethistowalktheuserthroughanexplanationofwhattheydid.>1.2)<ActionFirststep>^^^^^^^^^^^^^^^^^^^^^^^<Useasub-steplikethistodescriberunningthenewcodeandwhattheresultsshouldbe.>2.<SecondStep>----------------<...>Conclusion----------<Hereiswhereyouexplainwhattheuserhasreadandprovideadditionalreferences.>NextStep---------<Linktothenexttutorialhere.>\njack/how-to-tutorials\ndoc/tutorials\n.rst",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee136"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_write_how_to_guides.html",
  "title": "How to Write a MoveIt How-To Guide — MoveIt Documentation: Rolling  documentation",
  "content": "How to Write a MoveIt How-To Guide\nLearning Objectives\nRequirements\nSteps\nTemplate\nFurther Reading\nThis guide explains how to write How-To’s for MoveIt documentation. This guide is primarily intended for use by employees at PickNik Robotics\nto assist in standardizing how-to documents but can be used by any contributor who wants to submit a new guide. If you are looking for\nhow-to guides for using MoveIt, you can find them here:How-To Guides.\nThe type of information that should be included in a how-to.\nProper formatting for how-to guides in the MoveIt documentation section.\nUbuntu 20.04\nROS 2 Galactic\nMoveIt 2\nFork theMoveIt 2 Tutorials repositoryand start a new branch with an intuitive name (e.g.,jack/how-to-write-how-tos).\nCreate a new file in thedoc/how_to_guidesdirectory with a.rstextension. The title should start with “How to” followed by the specific question being answered (e.g., “How to Write a MoveIt How-To Guide”).\nAdd a link to your guide on the appropriate how-to guide page:\nUser Guides\nContributor Guides\nWrite the introduction using reStructuredText (.rst) using the following guidelines:\nThe title should have the same name as the file.\nThe introduction should explain the purpose of this how-to and the intended audience.\nIf you think people may regularly find this particular guide by mistake, add links to the proper resource.\nWrite the specific learning objectives (i.e., what the reader will know when they finish reading this how-to).\nAdd any system or equipment requirements for this how-to so users know if this how-to is appropriate for them.\nLay out the individual action steps and do not leave out necessary intermediate steps.\nCreate a “Further Reading” section that links to amplifying information.\nSubmit the new page as a PR to theMoveIt 2 Tutorials repository.\nHow to Contribute to This Site\nMoveIt Concepts: How-To Guide\n© Copyright 2024, PickNik Robotics.\n<Title>-------<BriefdescriptionoftheHow-ToGuidewithimageorGIFshowingtheoutcome.>LearningObjectives-------------------<Listofthingstheuserwilllearn.>Requirements----------<Explanationofwhattheusershouldunderstand.Unlikeatutorial,theseguidesstandaloneandcanassumetheuserhasmuchmorebackground.>Steps-----<Alistofstepstotaketosolvetheproblem.>FurtherReading---------------<Alistoflinkstorelatedcontentonandoffthiswebsite.>\njack/how-to-write-how-tos\ndoc/how_to_guides\n.rst",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee137"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_write_how_to_guides.html#learning-objectives",
  "title": "How to Write a MoveIt How-To Guide — MoveIt Documentation: Rolling  documentation",
  "content": "How to Write a MoveIt How-To Guide\nLearning Objectives\nRequirements\nSteps\nTemplate\nFurther Reading\nThis guide explains how to write How-To’s for MoveIt documentation. This guide is primarily intended for use by employees at PickNik Robotics\nto assist in standardizing how-to documents but can be used by any contributor who wants to submit a new guide. If you are looking for\nhow-to guides for using MoveIt, you can find them here:How-To Guides.\nThe type of information that should be included in a how-to.\nProper formatting for how-to guides in the MoveIt documentation section.\nUbuntu 20.04\nROS 2 Galactic\nMoveIt 2\nFork theMoveIt 2 Tutorials repositoryand start a new branch with an intuitive name (e.g.,jack/how-to-write-how-tos).\nCreate a new file in thedoc/how_to_guidesdirectory with a.rstextension. The title should start with “How to” followed by the specific question being answered (e.g., “How to Write a MoveIt How-To Guide”).\nAdd a link to your guide on the appropriate how-to guide page:\nUser Guides\nContributor Guides\nWrite the introduction using reStructuredText (.rst) using the following guidelines:\nThe title should have the same name as the file.\nThe introduction should explain the purpose of this how-to and the intended audience.\nIf you think people may regularly find this particular guide by mistake, add links to the proper resource.\nWrite the specific learning objectives (i.e., what the reader will know when they finish reading this how-to).\nAdd any system or equipment requirements for this how-to so users know if this how-to is appropriate for them.\nLay out the individual action steps and do not leave out necessary intermediate steps.\nCreate a “Further Reading” section that links to amplifying information.\nSubmit the new page as a PR to theMoveIt 2 Tutorials repository.\nHow to Contribute to This Site\nMoveIt Concepts: How-To Guide\n© Copyright 2024, PickNik Robotics.\n<Title>-------<BriefdescriptionoftheHow-ToGuidewithimageorGIFshowingtheoutcome.>LearningObjectives-------------------<Listofthingstheuserwilllearn.>Requirements----------<Explanationofwhattheusershouldunderstand.Unlikeatutorial,theseguidesstandaloneandcanassumetheuserhasmuchmorebackground.>Steps-----<Alistofstepstotaketosolvetheproblem.>FurtherReading---------------<Alistoflinkstorelatedcontentonandoffthiswebsite.>\njack/how-to-write-how-tos\ndoc/how_to_guides\n.rst",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee138"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_write_how_to_guides.html#requirements",
  "title": "How to Write a MoveIt How-To Guide — MoveIt Documentation: Rolling  documentation",
  "content": "How to Write a MoveIt How-To Guide\nLearning Objectives\nRequirements\nSteps\nTemplate\nFurther Reading\nThis guide explains how to write How-To’s for MoveIt documentation. This guide is primarily intended for use by employees at PickNik Robotics\nto assist in standardizing how-to documents but can be used by any contributor who wants to submit a new guide. If you are looking for\nhow-to guides for using MoveIt, you can find them here:How-To Guides.\nThe type of information that should be included in a how-to.\nProper formatting for how-to guides in the MoveIt documentation section.\nUbuntu 20.04\nROS 2 Galactic\nMoveIt 2\nFork theMoveIt 2 Tutorials repositoryand start a new branch with an intuitive name (e.g.,jack/how-to-write-how-tos).\nCreate a new file in thedoc/how_to_guidesdirectory with a.rstextension. The title should start with “How to” followed by the specific question being answered (e.g., “How to Write a MoveIt How-To Guide”).\nAdd a link to your guide on the appropriate how-to guide page:\nUser Guides\nContributor Guides\nWrite the introduction using reStructuredText (.rst) using the following guidelines:\nThe title should have the same name as the file.\nThe introduction should explain the purpose of this how-to and the intended audience.\nIf you think people may regularly find this particular guide by mistake, add links to the proper resource.\nWrite the specific learning objectives (i.e., what the reader will know when they finish reading this how-to).\nAdd any system or equipment requirements for this how-to so users know if this how-to is appropriate for them.\nLay out the individual action steps and do not leave out necessary intermediate steps.\nCreate a “Further Reading” section that links to amplifying information.\nSubmit the new page as a PR to theMoveIt 2 Tutorials repository.\nHow to Contribute to This Site\nMoveIt Concepts: How-To Guide\n© Copyright 2024, PickNik Robotics.\n<Title>-------<BriefdescriptionoftheHow-ToGuidewithimageorGIFshowingtheoutcome.>LearningObjectives-------------------<Listofthingstheuserwilllearn.>Requirements----------<Explanationofwhattheusershouldunderstand.Unlikeatutorial,theseguidesstandaloneandcanassumetheuserhasmuchmorebackground.>Steps-----<Alistofstepstotaketosolvetheproblem.>FurtherReading---------------<Alistoflinkstorelatedcontentonandoffthiswebsite.>\njack/how-to-write-how-tos\ndoc/how_to_guides\n.rst",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee139"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_write_how_to_guides.html#steps",
  "title": "How to Write a MoveIt How-To Guide — MoveIt Documentation: Rolling  documentation",
  "content": "How to Write a MoveIt How-To Guide\nLearning Objectives\nRequirements\nSteps\nTemplate\nFurther Reading\nThis guide explains how to write How-To’s for MoveIt documentation. This guide is primarily intended for use by employees at PickNik Robotics\nto assist in standardizing how-to documents but can be used by any contributor who wants to submit a new guide. If you are looking for\nhow-to guides for using MoveIt, you can find them here:How-To Guides.\nThe type of information that should be included in a how-to.\nProper formatting for how-to guides in the MoveIt documentation section.\nUbuntu 20.04\nROS 2 Galactic\nMoveIt 2\nFork theMoveIt 2 Tutorials repositoryand start a new branch with an intuitive name (e.g.,jack/how-to-write-how-tos).\nCreate a new file in thedoc/how_to_guidesdirectory with a.rstextension. The title should start with “How to” followed by the specific question being answered (e.g., “How to Write a MoveIt How-To Guide”).\nAdd a link to your guide on the appropriate how-to guide page:\nUser Guides\nContributor Guides\nWrite the introduction using reStructuredText (.rst) using the following guidelines:\nThe title should have the same name as the file.\nThe introduction should explain the purpose of this how-to and the intended audience.\nIf you think people may regularly find this particular guide by mistake, add links to the proper resource.\nWrite the specific learning objectives (i.e., what the reader will know when they finish reading this how-to).\nAdd any system or equipment requirements for this how-to so users know if this how-to is appropriate for them.\nLay out the individual action steps and do not leave out necessary intermediate steps.\nCreate a “Further Reading” section that links to amplifying information.\nSubmit the new page as a PR to theMoveIt 2 Tutorials repository.\nHow to Contribute to This Site\nMoveIt Concepts: How-To Guide\n© Copyright 2024, PickNik Robotics.\n<Title>-------<BriefdescriptionoftheHow-ToGuidewithimageorGIFshowingtheoutcome.>LearningObjectives-------------------<Listofthingstheuserwilllearn.>Requirements----------<Explanationofwhattheusershouldunderstand.Unlikeatutorial,theseguidesstandaloneandcanassumetheuserhasmuchmorebackground.>Steps-----<Alistofstepstotaketosolvetheproblem.>FurtherReading---------------<Alistoflinkstorelatedcontentonandoffthiswebsite.>\njack/how-to-write-how-tos\ndoc/how_to_guides\n.rst",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee13a"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_write_how_to_guides.html#template",
  "title": "How to Write a MoveIt How-To Guide — MoveIt Documentation: Rolling  documentation",
  "content": "How to Write a MoveIt How-To Guide\nLearning Objectives\nRequirements\nSteps\nTemplate\nFurther Reading\nThis guide explains how to write How-To’s for MoveIt documentation. This guide is primarily intended for use by employees at PickNik Robotics\nto assist in standardizing how-to documents but can be used by any contributor who wants to submit a new guide. If you are looking for\nhow-to guides for using MoveIt, you can find them here:How-To Guides.\nThe type of information that should be included in a how-to.\nProper formatting for how-to guides in the MoveIt documentation section.\nUbuntu 20.04\nROS 2 Galactic\nMoveIt 2\nFork theMoveIt 2 Tutorials repositoryand start a new branch with an intuitive name (e.g.,jack/how-to-write-how-tos).\nCreate a new file in thedoc/how_to_guidesdirectory with a.rstextension. The title should start with “How to” followed by the specific question being answered (e.g., “How to Write a MoveIt How-To Guide”).\nAdd a link to your guide on the appropriate how-to guide page:\nUser Guides\nContributor Guides\nWrite the introduction using reStructuredText (.rst) using the following guidelines:\nThe title should have the same name as the file.\nThe introduction should explain the purpose of this how-to and the intended audience.\nIf you think people may regularly find this particular guide by mistake, add links to the proper resource.\nWrite the specific learning objectives (i.e., what the reader will know when they finish reading this how-to).\nAdd any system or equipment requirements for this how-to so users know if this how-to is appropriate for them.\nLay out the individual action steps and do not leave out necessary intermediate steps.\nCreate a “Further Reading” section that links to amplifying information.\nSubmit the new page as a PR to theMoveIt 2 Tutorials repository.\nHow to Contribute to This Site\nMoveIt Concepts: How-To Guide\n© Copyright 2024, PickNik Robotics.\n<Title>-------<BriefdescriptionoftheHow-ToGuidewithimageorGIFshowingtheoutcome.>LearningObjectives-------------------<Listofthingstheuserwilllearn.>Requirements----------<Explanationofwhattheusershouldunderstand.Unlikeatutorial,theseguidesstandaloneandcanassumetheuserhasmuchmorebackground.>Steps-----<Alistofstepstotaketosolvetheproblem.>FurtherReading---------------<Alistoflinkstorelatedcontentonandoffthiswebsite.>\njack/how-to-write-how-tos\ndoc/how_to_guides\n.rst",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee13b"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_write_how_to_guides.html#further-reading",
  "title": "How to Write a MoveIt How-To Guide — MoveIt Documentation: Rolling  documentation",
  "content": "How to Write a MoveIt How-To Guide\nLearning Objectives\nRequirements\nSteps\nTemplate\nFurther Reading\nThis guide explains how to write How-To’s for MoveIt documentation. This guide is primarily intended for use by employees at PickNik Robotics\nto assist in standardizing how-to documents but can be used by any contributor who wants to submit a new guide. If you are looking for\nhow-to guides for using MoveIt, you can find them here:How-To Guides.\nThe type of information that should be included in a how-to.\nProper formatting for how-to guides in the MoveIt documentation section.\nUbuntu 20.04\nROS 2 Galactic\nMoveIt 2\nFork theMoveIt 2 Tutorials repositoryand start a new branch with an intuitive name (e.g.,jack/how-to-write-how-tos).\nCreate a new file in thedoc/how_to_guidesdirectory with a.rstextension. The title should start with “How to” followed by the specific question being answered (e.g., “How to Write a MoveIt How-To Guide”).\nAdd a link to your guide on the appropriate how-to guide page:\nUser Guides\nContributor Guides\nWrite the introduction using reStructuredText (.rst) using the following guidelines:\nThe title should have the same name as the file.\nThe introduction should explain the purpose of this how-to and the intended audience.\nIf you think people may regularly find this particular guide by mistake, add links to the proper resource.\nWrite the specific learning objectives (i.e., what the reader will know when they finish reading this how-to).\nAdd any system or equipment requirements for this how-to so users know if this how-to is appropriate for them.\nLay out the individual action steps and do not leave out necessary intermediate steps.\nCreate a “Further Reading” section that links to amplifying information.\nSubmit the new page as a PR to theMoveIt 2 Tutorials repository.\nHow to Contribute to This Site\nMoveIt Concepts: How-To Guide\n© Copyright 2024, PickNik Robotics.\n<Title>-------<BriefdescriptionoftheHow-ToGuidewithimageorGIFshowingtheoutcome.>LearningObjectives-------------------<Listofthingstheuserwilllearn.>Requirements----------<Explanationofwhattheusershouldunderstand.Unlikeatutorial,theseguidesstandaloneandcanassumetheuserhasmuchmorebackground.>Steps-----<Alistofstepstotaketosolvetheproblem.>FurtherReading---------------<Alistoflinkstorelatedcontentonandoffthiswebsite.>\njack/how-to-write-how-tos\ndoc/how_to_guides\n.rst",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee13c"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_write_how_to_guides.html#how-to-write-a-moveit-how-to-guide",
  "title": "How to Write a MoveIt How-To Guide — MoveIt Documentation: Rolling  documentation",
  "content": "How to Write a MoveIt How-To Guide\nLearning Objectives\nRequirements\nSteps\nTemplate\nFurther Reading\nThis guide explains how to write How-To’s for MoveIt documentation. This guide is primarily intended for use by employees at PickNik Robotics\nto assist in standardizing how-to documents but can be used by any contributor who wants to submit a new guide. If you are looking for\nhow-to guides for using MoveIt, you can find them here:How-To Guides.\nThe type of information that should be included in a how-to.\nProper formatting for how-to guides in the MoveIt documentation section.\nUbuntu 20.04\nROS 2 Galactic\nMoveIt 2\nFork theMoveIt 2 Tutorials repositoryand start a new branch with an intuitive name (e.g.,jack/how-to-write-how-tos).\nCreate a new file in thedoc/how_to_guidesdirectory with a.rstextension. The title should start with “How to” followed by the specific question being answered (e.g., “How to Write a MoveIt How-To Guide”).\nAdd a link to your guide on the appropriate how-to guide page:\nUser Guides\nContributor Guides\nWrite the introduction using reStructuredText (.rst) using the following guidelines:\nThe title should have the same name as the file.\nThe introduction should explain the purpose of this how-to and the intended audience.\nIf you think people may regularly find this particular guide by mistake, add links to the proper resource.\nWrite the specific learning objectives (i.e., what the reader will know when they finish reading this how-to).\nAdd any system or equipment requirements for this how-to so users know if this how-to is appropriate for them.\nLay out the individual action steps and do not leave out necessary intermediate steps.\nCreate a “Further Reading” section that links to amplifying information.\nSubmit the new page as a PR to theMoveIt 2 Tutorials repository.\nHow to Contribute to This Site\nMoveIt Concepts: How-To Guide\n© Copyright 2024, PickNik Robotics.\n<Title>-------<BriefdescriptionoftheHow-ToGuidewithimageorGIFshowingtheoutcome.>LearningObjectives-------------------<Listofthingstheuserwilllearn.>Requirements----------<Explanationofwhattheusershouldunderstand.Unlikeatutorial,theseguidesstandaloneandcanassumetheuserhasmuchmorebackground.>Steps-----<Alistofstepstotaketosolvetheproblem.>FurtherReading---------------<Alistoflinkstorelatedcontentonandoffthiswebsite.>\njack/how-to-write-how-tos\ndoc/how_to_guides\n.rst",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee13d"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_cross_reference.html",
  "title": "How to Cross-Reference Content — MoveIt Documentation: Rolling  documentation",
  "content": "How to Cross-Reference Content\nLearning Objectives\nLinking to other Documents and Sections\nReferencing the API Documentation\nDo’s and Don’ts\nFurther Reading\nThis is a primer on how to successfully link to other documents on this website and the API.\nThere are many and maybe too many different ways to reference content and for new contributors it can be hard to understand what method to use.\nSome methods may even work locally but then silently fail to create functional links on the deployed website.\nFor that reason, we are requesting contributors to only use the suggested Sphinx roles for cross-referencing content on this website.\nLinking to documents and sections using Sphinx’s:ref:and:doc:roles\nUsing:ref:ids generated from theautosectionlabelextension\nReferencing the C++ API using the:cpp_api:role from thedoxylinkextension\nSphinx provides the:doc:and:ref:roles forcross-referencing contentand it’s best to stick to them to ensure compatibility with other Sphinx extensions and multi-distro support.\nFor linking to other documents, you can use the:doc:role like this:Getting Started(:doc:`/doc/tutorials/getting_started/getting_started`). The:ref:role accepts ids that link to explicit targets on a page.\nFor convenience, we have enabled the Sphinx extensionautosectionlabelwhich creates unique and human-readable reference targets for all sections.\nSections in all documents can be linked by providing the document path and the section titlelike this(:ref:`likethis<doc/tutorials/getting_started/getting_started:InstallROS2andColcon>`).\nNote that the:doc:role requires absolute paths to start with a/while theautosectionlabelextension builds:refpath labels without it.\nThe API pages are generated using Doxygen and not Sphinx which means that:doc:and:ref:roles are not able to find any API pages.\nWe are usingdoxylinkand the custom:cpp_api:role for generating links to the API pages from symbols.\nHere are some examples, take note that some links use titles and some not:\nnamespaces::cpp_api:`moveit::core`->moveit::core\nclasses::cpp_api:`moveit::core::RobotModel`->moveit::core::RobotModel\nfunctions and members:\n:cpp_api:`RobotModel::getName()<moveit::core::RobotModel::getName>`->RobotModel::getName()\n:cpp_api:`moveit::core::RobotModel::enforcePositionBounds(double*state)const`->moveit::core::RobotModel::enforcePositionBounds(double *state) const\n:cpp_api:`RobotModel::root_link_<moveit::core::RobotModel::root_link_>`->RobotModel::root_link_\nfiles::cpp_api:`robot_model.cpp`->robot_model.cpp\nIf you are unsure about how to link certain symbols, you can find all Doxygen references inside theMoveIt.tagfile.\nThe file is located insidebuild/html/api/orbuild/html/<branch>/api/depending on the build type.\nPleasedo:\nCross-reference as much as possible, especially code\nProvide meaningful titles for links or shorten API symbols to improve readability\nPleasedon’t:\nUse raw URLs for referencing tutorials or the API\nLink to GitHub source files, prefer the Doxygen pages\nHow to Contribute to This Site\nHow to Write a MoveIt Tutorial\nHow to Write a MoveIt How-To Guide\n© Copyright 2024, PickNik Robotics.\n:ref:\n:doc:\n:ref:\n:cpp_api:\n:doc:\n:ref:\n:doc:\n:doc:`/doc/tutorials/getting_started/getting_started`\n:ref:\n:ref:`likethis<doc/tutorials/getting_started/getting_started:InstallROS2andColcon>`\n:doc:\n/\n:ref\n:doc:\n:ref:\n:cpp_api:\n:cpp_api:`moveit::core`\n:cpp_api:`moveit::core::RobotModel`\n:cpp_api:`RobotModel::getName()<moveit::core::RobotModel::getName>`\n:cpp_api:`moveit::core::RobotModel::enforcePositionBounds(double*state)const`\n:cpp_api:`RobotModel::root_link_<moveit::core::RobotModel::root_link_>`\n:cpp_api:`robot_model.cpp`\nMoveIt.tag\nbuild/html/api/\nbuild/html/<branch>/api/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee13e"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_cross_reference.html#learning-objectives",
  "title": "How to Cross-Reference Content — MoveIt Documentation: Rolling  documentation",
  "content": "How to Cross-Reference Content\nLearning Objectives\nLinking to other Documents and Sections\nReferencing the API Documentation\nDo’s and Don’ts\nFurther Reading\nThis is a primer on how to successfully link to other documents on this website and the API.\nThere are many and maybe too many different ways to reference content and for new contributors it can be hard to understand what method to use.\nSome methods may even work locally but then silently fail to create functional links on the deployed website.\nFor that reason, we are requesting contributors to only use the suggested Sphinx roles for cross-referencing content on this website.\nLinking to documents and sections using Sphinx’s:ref:and:doc:roles\nUsing:ref:ids generated from theautosectionlabelextension\nReferencing the C++ API using the:cpp_api:role from thedoxylinkextension\nSphinx provides the:doc:and:ref:roles forcross-referencing contentand it’s best to stick to them to ensure compatibility with other Sphinx extensions and multi-distro support.\nFor linking to other documents, you can use the:doc:role like this:Getting Started(:doc:`/doc/tutorials/getting_started/getting_started`). The:ref:role accepts ids that link to explicit targets on a page.\nFor convenience, we have enabled the Sphinx extensionautosectionlabelwhich creates unique and human-readable reference targets for all sections.\nSections in all documents can be linked by providing the document path and the section titlelike this(:ref:`likethis<doc/tutorials/getting_started/getting_started:InstallROS2andColcon>`).\nNote that the:doc:role requires absolute paths to start with a/while theautosectionlabelextension builds:refpath labels without it.\nThe API pages are generated using Doxygen and not Sphinx which means that:doc:and:ref:roles are not able to find any API pages.\nWe are usingdoxylinkand the custom:cpp_api:role for generating links to the API pages from symbols.\nHere are some examples, take note that some links use titles and some not:\nnamespaces::cpp_api:`moveit::core`->moveit::core\nclasses::cpp_api:`moveit::core::RobotModel`->moveit::core::RobotModel\nfunctions and members:\n:cpp_api:`RobotModel::getName()<moveit::core::RobotModel::getName>`->RobotModel::getName()\n:cpp_api:`moveit::core::RobotModel::enforcePositionBounds(double*state)const`->moveit::core::RobotModel::enforcePositionBounds(double *state) const\n:cpp_api:`RobotModel::root_link_<moveit::core::RobotModel::root_link_>`->RobotModel::root_link_\nfiles::cpp_api:`robot_model.cpp`->robot_model.cpp\nIf you are unsure about how to link certain symbols, you can find all Doxygen references inside theMoveIt.tagfile.\nThe file is located insidebuild/html/api/orbuild/html/<branch>/api/depending on the build type.\nPleasedo:\nCross-reference as much as possible, especially code\nProvide meaningful titles for links or shorten API symbols to improve readability\nPleasedon’t:\nUse raw URLs for referencing tutorials or the API\nLink to GitHub source files, prefer the Doxygen pages\nHow to Contribute to This Site\nHow to Write a MoveIt Tutorial\nHow to Write a MoveIt How-To Guide\n© Copyright 2024, PickNik Robotics.\n:ref:\n:doc:\n:ref:\n:cpp_api:\n:doc:\n:ref:\n:doc:\n:doc:`/doc/tutorials/getting_started/getting_started`\n:ref:\n:ref:`likethis<doc/tutorials/getting_started/getting_started:InstallROS2andColcon>`\n:doc:\n/\n:ref\n:doc:\n:ref:\n:cpp_api:\n:cpp_api:`moveit::core`\n:cpp_api:`moveit::core::RobotModel`\n:cpp_api:`RobotModel::getName()<moveit::core::RobotModel::getName>`\n:cpp_api:`moveit::core::RobotModel::enforcePositionBounds(double*state)const`\n:cpp_api:`RobotModel::root_link_<moveit::core::RobotModel::root_link_>`\n:cpp_api:`robot_model.cpp`\nMoveIt.tag\nbuild/html/api/\nbuild/html/<branch>/api/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee13f"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_cross_reference.html#linking-to-other-documents-and-sections",
  "title": "How to Cross-Reference Content — MoveIt Documentation: Rolling  documentation",
  "content": "How to Cross-Reference Content\nLearning Objectives\nLinking to other Documents and Sections\nReferencing the API Documentation\nDo’s and Don’ts\nFurther Reading\nThis is a primer on how to successfully link to other documents on this website and the API.\nThere are many and maybe too many different ways to reference content and for new contributors it can be hard to understand what method to use.\nSome methods may even work locally but then silently fail to create functional links on the deployed website.\nFor that reason, we are requesting contributors to only use the suggested Sphinx roles for cross-referencing content on this website.\nLinking to documents and sections using Sphinx’s:ref:and:doc:roles\nUsing:ref:ids generated from theautosectionlabelextension\nReferencing the C++ API using the:cpp_api:role from thedoxylinkextension\nSphinx provides the:doc:and:ref:roles forcross-referencing contentand it’s best to stick to them to ensure compatibility with other Sphinx extensions and multi-distro support.\nFor linking to other documents, you can use the:doc:role like this:Getting Started(:doc:`/doc/tutorials/getting_started/getting_started`). The:ref:role accepts ids that link to explicit targets on a page.\nFor convenience, we have enabled the Sphinx extensionautosectionlabelwhich creates unique and human-readable reference targets for all sections.\nSections in all documents can be linked by providing the document path and the section titlelike this(:ref:`likethis<doc/tutorials/getting_started/getting_started:InstallROS2andColcon>`).\nNote that the:doc:role requires absolute paths to start with a/while theautosectionlabelextension builds:refpath labels without it.\nThe API pages are generated using Doxygen and not Sphinx which means that:doc:and:ref:roles are not able to find any API pages.\nWe are usingdoxylinkand the custom:cpp_api:role for generating links to the API pages from symbols.\nHere are some examples, take note that some links use titles and some not:\nnamespaces::cpp_api:`moveit::core`->moveit::core\nclasses::cpp_api:`moveit::core::RobotModel`->moveit::core::RobotModel\nfunctions and members:\n:cpp_api:`RobotModel::getName()<moveit::core::RobotModel::getName>`->RobotModel::getName()\n:cpp_api:`moveit::core::RobotModel::enforcePositionBounds(double*state)const`->moveit::core::RobotModel::enforcePositionBounds(double *state) const\n:cpp_api:`RobotModel::root_link_<moveit::core::RobotModel::root_link_>`->RobotModel::root_link_\nfiles::cpp_api:`robot_model.cpp`->robot_model.cpp\nIf you are unsure about how to link certain symbols, you can find all Doxygen references inside theMoveIt.tagfile.\nThe file is located insidebuild/html/api/orbuild/html/<branch>/api/depending on the build type.\nPleasedo:\nCross-reference as much as possible, especially code\nProvide meaningful titles for links or shorten API symbols to improve readability\nPleasedon’t:\nUse raw URLs for referencing tutorials or the API\nLink to GitHub source files, prefer the Doxygen pages\nHow to Contribute to This Site\nHow to Write a MoveIt Tutorial\nHow to Write a MoveIt How-To Guide\n© Copyright 2024, PickNik Robotics.\n:ref:\n:doc:\n:ref:\n:cpp_api:\n:doc:\n:ref:\n:doc:\n:doc:`/doc/tutorials/getting_started/getting_started`\n:ref:\n:ref:`likethis<doc/tutorials/getting_started/getting_started:InstallROS2andColcon>`\n:doc:\n/\n:ref\n:doc:\n:ref:\n:cpp_api:\n:cpp_api:`moveit::core`\n:cpp_api:`moveit::core::RobotModel`\n:cpp_api:`RobotModel::getName()<moveit::core::RobotModel::getName>`\n:cpp_api:`moveit::core::RobotModel::enforcePositionBounds(double*state)const`\n:cpp_api:`RobotModel::root_link_<moveit::core::RobotModel::root_link_>`\n:cpp_api:`robot_model.cpp`\nMoveIt.tag\nbuild/html/api/\nbuild/html/<branch>/api/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee140"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_cross_reference.html#referencing-the-api-documentation",
  "title": "How to Cross-Reference Content — MoveIt Documentation: Rolling  documentation",
  "content": "How to Cross-Reference Content\nLearning Objectives\nLinking to other Documents and Sections\nReferencing the API Documentation\nDo’s and Don’ts\nFurther Reading\nThis is a primer on how to successfully link to other documents on this website and the API.\nThere are many and maybe too many different ways to reference content and for new contributors it can be hard to understand what method to use.\nSome methods may even work locally but then silently fail to create functional links on the deployed website.\nFor that reason, we are requesting contributors to only use the suggested Sphinx roles for cross-referencing content on this website.\nLinking to documents and sections using Sphinx’s:ref:and:doc:roles\nUsing:ref:ids generated from theautosectionlabelextension\nReferencing the C++ API using the:cpp_api:role from thedoxylinkextension\nSphinx provides the:doc:and:ref:roles forcross-referencing contentand it’s best to stick to them to ensure compatibility with other Sphinx extensions and multi-distro support.\nFor linking to other documents, you can use the:doc:role like this:Getting Started(:doc:`/doc/tutorials/getting_started/getting_started`). The:ref:role accepts ids that link to explicit targets on a page.\nFor convenience, we have enabled the Sphinx extensionautosectionlabelwhich creates unique and human-readable reference targets for all sections.\nSections in all documents can be linked by providing the document path and the section titlelike this(:ref:`likethis<doc/tutorials/getting_started/getting_started:InstallROS2andColcon>`).\nNote that the:doc:role requires absolute paths to start with a/while theautosectionlabelextension builds:refpath labels without it.\nThe API pages are generated using Doxygen and not Sphinx which means that:doc:and:ref:roles are not able to find any API pages.\nWe are usingdoxylinkand the custom:cpp_api:role for generating links to the API pages from symbols.\nHere are some examples, take note that some links use titles and some not:\nnamespaces::cpp_api:`moveit::core`->moveit::core\nclasses::cpp_api:`moveit::core::RobotModel`->moveit::core::RobotModel\nfunctions and members:\n:cpp_api:`RobotModel::getName()<moveit::core::RobotModel::getName>`->RobotModel::getName()\n:cpp_api:`moveit::core::RobotModel::enforcePositionBounds(double*state)const`->moveit::core::RobotModel::enforcePositionBounds(double *state) const\n:cpp_api:`RobotModel::root_link_<moveit::core::RobotModel::root_link_>`->RobotModel::root_link_\nfiles::cpp_api:`robot_model.cpp`->robot_model.cpp\nIf you are unsure about how to link certain symbols, you can find all Doxygen references inside theMoveIt.tagfile.\nThe file is located insidebuild/html/api/orbuild/html/<branch>/api/depending on the build type.\nPleasedo:\nCross-reference as much as possible, especially code\nProvide meaningful titles for links or shorten API symbols to improve readability\nPleasedon’t:\nUse raw URLs for referencing tutorials or the API\nLink to GitHub source files, prefer the Doxygen pages\nHow to Contribute to This Site\nHow to Write a MoveIt Tutorial\nHow to Write a MoveIt How-To Guide\n© Copyright 2024, PickNik Robotics.\n:ref:\n:doc:\n:ref:\n:cpp_api:\n:doc:\n:ref:\n:doc:\n:doc:`/doc/tutorials/getting_started/getting_started`\n:ref:\n:ref:`likethis<doc/tutorials/getting_started/getting_started:InstallROS2andColcon>`\n:doc:\n/\n:ref\n:doc:\n:ref:\n:cpp_api:\n:cpp_api:`moveit::core`\n:cpp_api:`moveit::core::RobotModel`\n:cpp_api:`RobotModel::getName()<moveit::core::RobotModel::getName>`\n:cpp_api:`moveit::core::RobotModel::enforcePositionBounds(double*state)const`\n:cpp_api:`RobotModel::root_link_<moveit::core::RobotModel::root_link_>`\n:cpp_api:`robot_model.cpp`\nMoveIt.tag\nbuild/html/api/\nbuild/html/<branch>/api/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee141"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_cross_reference.html#do-s-and-don-ts",
  "title": "How to Cross-Reference Content — MoveIt Documentation: Rolling  documentation",
  "content": "How to Cross-Reference Content\nLearning Objectives\nLinking to other Documents and Sections\nReferencing the API Documentation\nDo’s and Don’ts\nFurther Reading\nThis is a primer on how to successfully link to other documents on this website and the API.\nThere are many and maybe too many different ways to reference content and for new contributors it can be hard to understand what method to use.\nSome methods may even work locally but then silently fail to create functional links on the deployed website.\nFor that reason, we are requesting contributors to only use the suggested Sphinx roles for cross-referencing content on this website.\nLinking to documents and sections using Sphinx’s:ref:and:doc:roles\nUsing:ref:ids generated from theautosectionlabelextension\nReferencing the C++ API using the:cpp_api:role from thedoxylinkextension\nSphinx provides the:doc:and:ref:roles forcross-referencing contentand it’s best to stick to them to ensure compatibility with other Sphinx extensions and multi-distro support.\nFor linking to other documents, you can use the:doc:role like this:Getting Started(:doc:`/doc/tutorials/getting_started/getting_started`). The:ref:role accepts ids that link to explicit targets on a page.\nFor convenience, we have enabled the Sphinx extensionautosectionlabelwhich creates unique and human-readable reference targets for all sections.\nSections in all documents can be linked by providing the document path and the section titlelike this(:ref:`likethis<doc/tutorials/getting_started/getting_started:InstallROS2andColcon>`).\nNote that the:doc:role requires absolute paths to start with a/while theautosectionlabelextension builds:refpath labels without it.\nThe API pages are generated using Doxygen and not Sphinx which means that:doc:and:ref:roles are not able to find any API pages.\nWe are usingdoxylinkand the custom:cpp_api:role for generating links to the API pages from symbols.\nHere are some examples, take note that some links use titles and some not:\nnamespaces::cpp_api:`moveit::core`->moveit::core\nclasses::cpp_api:`moveit::core::RobotModel`->moveit::core::RobotModel\nfunctions and members:\n:cpp_api:`RobotModel::getName()<moveit::core::RobotModel::getName>`->RobotModel::getName()\n:cpp_api:`moveit::core::RobotModel::enforcePositionBounds(double*state)const`->moveit::core::RobotModel::enforcePositionBounds(double *state) const\n:cpp_api:`RobotModel::root_link_<moveit::core::RobotModel::root_link_>`->RobotModel::root_link_\nfiles::cpp_api:`robot_model.cpp`->robot_model.cpp\nIf you are unsure about how to link certain symbols, you can find all Doxygen references inside theMoveIt.tagfile.\nThe file is located insidebuild/html/api/orbuild/html/<branch>/api/depending on the build type.\nPleasedo:\nCross-reference as much as possible, especially code\nProvide meaningful titles for links or shorten API symbols to improve readability\nPleasedon’t:\nUse raw URLs for referencing tutorials or the API\nLink to GitHub source files, prefer the Doxygen pages\nHow to Contribute to This Site\nHow to Write a MoveIt Tutorial\nHow to Write a MoveIt How-To Guide\n© Copyright 2024, PickNik Robotics.\n:ref:\n:doc:\n:ref:\n:cpp_api:\n:doc:\n:ref:\n:doc:\n:doc:`/doc/tutorials/getting_started/getting_started`\n:ref:\n:ref:`likethis<doc/tutorials/getting_started/getting_started:InstallROS2andColcon>`\n:doc:\n/\n:ref\n:doc:\n:ref:\n:cpp_api:\n:cpp_api:`moveit::core`\n:cpp_api:`moveit::core::RobotModel`\n:cpp_api:`RobotModel::getName()<moveit::core::RobotModel::getName>`\n:cpp_api:`moveit::core::RobotModel::enforcePositionBounds(double*state)const`\n:cpp_api:`RobotModel::root_link_<moveit::core::RobotModel::root_link_>`\n:cpp_api:`robot_model.cpp`\nMoveIt.tag\nbuild/html/api/\nbuild/html/<branch>/api/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee142"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_cross_reference.html#further-reading",
  "title": "How to Cross-Reference Content — MoveIt Documentation: Rolling  documentation",
  "content": "How to Cross-Reference Content\nLearning Objectives\nLinking to other Documents and Sections\nReferencing the API Documentation\nDo’s and Don’ts\nFurther Reading\nThis is a primer on how to successfully link to other documents on this website and the API.\nThere are many and maybe too many different ways to reference content and for new contributors it can be hard to understand what method to use.\nSome methods may even work locally but then silently fail to create functional links on the deployed website.\nFor that reason, we are requesting contributors to only use the suggested Sphinx roles for cross-referencing content on this website.\nLinking to documents and sections using Sphinx’s:ref:and:doc:roles\nUsing:ref:ids generated from theautosectionlabelextension\nReferencing the C++ API using the:cpp_api:role from thedoxylinkextension\nSphinx provides the:doc:and:ref:roles forcross-referencing contentand it’s best to stick to them to ensure compatibility with other Sphinx extensions and multi-distro support.\nFor linking to other documents, you can use the:doc:role like this:Getting Started(:doc:`/doc/tutorials/getting_started/getting_started`). The:ref:role accepts ids that link to explicit targets on a page.\nFor convenience, we have enabled the Sphinx extensionautosectionlabelwhich creates unique and human-readable reference targets for all sections.\nSections in all documents can be linked by providing the document path and the section titlelike this(:ref:`likethis<doc/tutorials/getting_started/getting_started:InstallROS2andColcon>`).\nNote that the:doc:role requires absolute paths to start with a/while theautosectionlabelextension builds:refpath labels without it.\nThe API pages are generated using Doxygen and not Sphinx which means that:doc:and:ref:roles are not able to find any API pages.\nWe are usingdoxylinkand the custom:cpp_api:role for generating links to the API pages from symbols.\nHere are some examples, take note that some links use titles and some not:\nnamespaces::cpp_api:`moveit::core`->moveit::core\nclasses::cpp_api:`moveit::core::RobotModel`->moveit::core::RobotModel\nfunctions and members:\n:cpp_api:`RobotModel::getName()<moveit::core::RobotModel::getName>`->RobotModel::getName()\n:cpp_api:`moveit::core::RobotModel::enforcePositionBounds(double*state)const`->moveit::core::RobotModel::enforcePositionBounds(double *state) const\n:cpp_api:`RobotModel::root_link_<moveit::core::RobotModel::root_link_>`->RobotModel::root_link_\nfiles::cpp_api:`robot_model.cpp`->robot_model.cpp\nIf you are unsure about how to link certain symbols, you can find all Doxygen references inside theMoveIt.tagfile.\nThe file is located insidebuild/html/api/orbuild/html/<branch>/api/depending on the build type.\nPleasedo:\nCross-reference as much as possible, especially code\nProvide meaningful titles for links or shorten API symbols to improve readability\nPleasedon’t:\nUse raw URLs for referencing tutorials or the API\nLink to GitHub source files, prefer the Doxygen pages\nHow to Contribute to This Site\nHow to Write a MoveIt Tutorial\nHow to Write a MoveIt How-To Guide\n© Copyright 2024, PickNik Robotics.\n:ref:\n:doc:\n:ref:\n:cpp_api:\n:doc:\n:ref:\n:doc:\n:doc:`/doc/tutorials/getting_started/getting_started`\n:ref:\n:ref:`likethis<doc/tutorials/getting_started/getting_started:InstallROS2andColcon>`\n:doc:\n/\n:ref\n:doc:\n:ref:\n:cpp_api:\n:cpp_api:`moveit::core`\n:cpp_api:`moveit::core::RobotModel`\n:cpp_api:`RobotModel::getName()<moveit::core::RobotModel::getName>`\n:cpp_api:`moveit::core::RobotModel::enforcePositionBounds(double*state)const`\n:cpp_api:`RobotModel::root_link_<moveit::core::RobotModel::root_link_>`\n:cpp_api:`robot_model.cpp`\nMoveIt.tag\nbuild/html/api/\nbuild/html/<branch>/api/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee143"
  },
  "url": "https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_cross_reference.html#how-to-cross-reference-content",
  "title": "How to Cross-Reference Content — MoveIt Documentation: Rolling  documentation",
  "content": "How to Cross-Reference Content\nLearning Objectives\nLinking to other Documents and Sections\nReferencing the API Documentation\nDo’s and Don’ts\nFurther Reading\nThis is a primer on how to successfully link to other documents on this website and the API.\nThere are many and maybe too many different ways to reference content and for new contributors it can be hard to understand what method to use.\nSome methods may even work locally but then silently fail to create functional links on the deployed website.\nFor that reason, we are requesting contributors to only use the suggested Sphinx roles for cross-referencing content on this website.\nLinking to documents and sections using Sphinx’s:ref:and:doc:roles\nUsing:ref:ids generated from theautosectionlabelextension\nReferencing the C++ API using the:cpp_api:role from thedoxylinkextension\nSphinx provides the:doc:and:ref:roles forcross-referencing contentand it’s best to stick to them to ensure compatibility with other Sphinx extensions and multi-distro support.\nFor linking to other documents, you can use the:doc:role like this:Getting Started(:doc:`/doc/tutorials/getting_started/getting_started`). The:ref:role accepts ids that link to explicit targets on a page.\nFor convenience, we have enabled the Sphinx extensionautosectionlabelwhich creates unique and human-readable reference targets for all sections.\nSections in all documents can be linked by providing the document path and the section titlelike this(:ref:`likethis<doc/tutorials/getting_started/getting_started:InstallROS2andColcon>`).\nNote that the:doc:role requires absolute paths to start with a/while theautosectionlabelextension builds:refpath labels without it.\nThe API pages are generated using Doxygen and not Sphinx which means that:doc:and:ref:roles are not able to find any API pages.\nWe are usingdoxylinkand the custom:cpp_api:role for generating links to the API pages from symbols.\nHere are some examples, take note that some links use titles and some not:\nnamespaces::cpp_api:`moveit::core`->moveit::core\nclasses::cpp_api:`moveit::core::RobotModel`->moveit::core::RobotModel\nfunctions and members:\n:cpp_api:`RobotModel::getName()<moveit::core::RobotModel::getName>`->RobotModel::getName()\n:cpp_api:`moveit::core::RobotModel::enforcePositionBounds(double*state)const`->moveit::core::RobotModel::enforcePositionBounds(double *state) const\n:cpp_api:`RobotModel::root_link_<moveit::core::RobotModel::root_link_>`->RobotModel::root_link_\nfiles::cpp_api:`robot_model.cpp`->robot_model.cpp\nIf you are unsure about how to link certain symbols, you can find all Doxygen references inside theMoveIt.tagfile.\nThe file is located insidebuild/html/api/orbuild/html/<branch>/api/depending on the build type.\nPleasedo:\nCross-reference as much as possible, especially code\nProvide meaningful titles for links or shorten API symbols to improve readability\nPleasedon’t:\nUse raw URLs for referencing tutorials or the API\nLink to GitHub source files, prefer the Doxygen pages\nHow to Contribute to This Site\nHow to Write a MoveIt Tutorial\nHow to Write a MoveIt How-To Guide\n© Copyright 2024, PickNik Robotics.\n:ref:\n:doc:\n:ref:\n:cpp_api:\n:doc:\n:ref:\n:doc:\n:doc:`/doc/tutorials/getting_started/getting_started`\n:ref:\n:ref:`likethis<doc/tutorials/getting_started/getting_started:InstallROS2andColcon>`\n:doc:\n/\n:ref\n:doc:\n:ref:\n:cpp_api:\n:cpp_api:`moveit::core`\n:cpp_api:`moveit::core::RobotModel`\n:cpp_api:`RobotModel::getName()<moveit::core::RobotModel::getName>`\n:cpp_api:`moveit::core::RobotModel::enforcePositionBounds(double*state)const`\n:cpp_api:`RobotModel::root_link_<moveit::core::RobotModel::root_link_>`\n:cpp_api:`robot_model.cpp`\nMoveIt.tag\nbuild/html/api/\nbuild/html/<branch>/api/",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee144"
  },
  "url": "https://moveit.picknik.ai/main/#attribution",
  "title": "MoveIt 2 Documentation — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt 2 Documentation\nHow-To Use This Website\nTable Of Contents\nAttribution\nCorporate Sponsorship\nWelcome to the unified MoveIt documentation, which includes tutorials, how-to guides, core concepts, and more.\nMoveIt 2 is the robotic manipulation platform for ROS 2 and incorporates the latest advances in motion planning, manipulation, 3D perception, kinematics, control, and navigation. MoveIt 2 was first released in 2019; for ROS 1 documentation, seeMoveIt 1 tutorials.\nFor the commercially supported version seeMoveIt Pro tutorials.\nTutorialswill walk you through creating your first project with MoveIt.\nHow-To Guidesanswers the question “How to do X with MoveIt?”\nConceptsdiscusses the design of MoveIt.\nContributingis a place to learn about making changes to MoveIt and this website.\nExamplescontains other useful pages that have not been adapted to the new layout of this site or are not yet ported from ROS 1.\nAPI Documentationwill redirect you to a reference API page.\nSome major past contributors to the MoveIt tutorials are listed in chronological order: Sachin Chitta, Dave Hershberger, Acorn Pooley, Dave Coleman, Michael Goerner, Francisco Suarez, Mike Lautman, Tyler Weaver, David Lu!!, Vatan Tezer, and Andy Zelenak. These are just some of the46+ Contributors over the yearswho have a big impact on this documentation.\nHelp us improve these docs and we’ll be happy to include you here also!\nThe tutorials had a major update in 2018 during a code sprint sponsored by Franka Emika in collaboration withPickNik Robotics(Check out the blog post!)\nThe tutorials had another major update in 2022 during a doc-a-thon sponsored byPickNik Robotics.\n© Copyright 2024, PickNik Robotics.\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563c9e31a67c23afbee145"
  },
  "url": "https://moveit.picknik.ai/main/#corporate-sponsorship",
  "title": "MoveIt 2 Documentation — MoveIt Documentation: Rolling  documentation",
  "content": "MoveIt 2 Documentation\nHow-To Use This Website\nTable Of Contents\nAttribution\nCorporate Sponsorship\nWelcome to the unified MoveIt documentation, which includes tutorials, how-to guides, core concepts, and more.\nMoveIt 2 is the robotic manipulation platform for ROS 2 and incorporates the latest advances in motion planning, manipulation, 3D perception, kinematics, control, and navigation. MoveIt 2 was first released in 2019; for ROS 1 documentation, seeMoveIt 1 tutorials.\nFor the commercially supported version seeMoveIt Pro tutorials.\nTutorialswill walk you through creating your first project with MoveIt.\nHow-To Guidesanswers the question “How to do X with MoveIt?”\nConceptsdiscusses the design of MoveIt.\nContributingis a place to learn about making changes to MoveIt and this website.\nExamplescontains other useful pages that have not been adapted to the new layout of this site or are not yet ported from ROS 1.\nAPI Documentationwill redirect you to a reference API page.\nSome major past contributors to the MoveIt tutorials are listed in chronological order: Sachin Chitta, Dave Hershberger, Acorn Pooley, Dave Coleman, Michael Goerner, Francisco Suarez, Mike Lautman, Tyler Weaver, David Lu!!, Vatan Tezer, and Andy Zelenak. These are just some of the46+ Contributors over the yearswho have a big impact on this documentation.\nHelp us improve these docs and we’ll be happy to include you here also!\nThe tutorials had a major update in 2018 during a code sprint sponsored by Franka Emika in collaboration withPickNik Robotics(Check out the blog post!)\nThe tutorials had another major update in 2022 during a doc-a-thon sponsored byPickNik Robotics.\n© Copyright 2024, PickNik Robotics.\nmove_group",
  "source": "web_scraping"
},
{
  "_id": {
    "$oid": "67563d9831a67c23afbee1ef"
  },
  "file_name": "README.md",
  "url": "https://raw.githubusercontent.com/moveit/moveit2_tutorials/main/README.md",
  "content": "# MoveIt Tutorials\n[Live tutorials here](https://moveit.picknik.ai/)\n\nThis is the primary documentation for the MoveIt project.\n## Build Status\n\nThis repository is built and deployed automatically by GitHub Actions:\n\n- **Rolling** (main): [![CI](https://github.com/moveit/moveit2_tutorials/actions/workflows/ci.yaml/badge.svg?branch=main)](https://github.com/moveit/moveit2_tutorials/actions/workflows/ci.yaml?query=branch%3Amain) [![Format](https://github.com/moveit/moveit2_tutorials/actions/workflows/format.yml/badge.svg?branch=main)](https://github.com/moveit/moveit2_tutorials/actions/workflows/format.yml?query=branch%3Amain) [![Deploy](https://github.com/moveit/moveit2_tutorials/actions/workflows/deploy.yml/badge.svg?branch=main)](https://github.com/moveit/moveit2_tutorials/actions/workflows/deploy.yml?query=branch%3Amain)\n- **Humble**: [![CI](https://github.com/moveit/moveit2_tutorials/actions/workflows/ci.yaml/badge.svg?branch=humble)](https://github.com/moveit/moveit2_tutorials/actions/workflows/ci.yaml?query=branch%3Ahumble) [![Format](https://github.com/moveit/moveit2_tutorials/actions/workflows/format.yml/badge.svg?branch=humble)](https://github.com/moveit/moveit2_tutorials/actions/workflows/format.yml?query=branch%3Ahumble) [![Deploy](https://github.com/moveit/moveit2_tutorials/actions/workflows/deploy.yml/badge.svg?branch=humble)](https://github.com/moveit/moveit2_tutorials/actions/workflows/deploy.yml?query=branch%3Ahumble)\n\n## Contributing\n\nWe strongly encourage you to help improve MoveIt's documentation. Please consider helping improve the tutorials, port old ones from ROS 1, and write new tutorials. We recommend you read the quality standards below as well as the [How to Write a MoveIt Tutorial](https://moveit.picknik.ai/main/doc/how_to_contribute/how_to_write_tutorials.html) page.\n\nIf you find an issue with the tutorials you are not able to fix yourself, please [open an issue on GitHub](https://github.com/moveit/moveit2_tutorials/issues/new) or open a PR with proposed changes.\n\n## Helping with Porting Tutorials to ROS 2\n\nAn issue has been created for each tutorial to be ported to ROS 2. At the top of each tutorial, there is a tag: \":moveit1:\", remove the tag\nafter the tutorial has been successfully updated.\n\nBelow are some links to help with the ports.\n\n* [colcon](https://colcon.readthedocs.io/en/released/user/how-to.html)\n* [ament](https://index.ros.org/doc/ros2/Tutorials/Ament-CMake-Documentation/)\n* [rclcpp](https://docs.ros2.org/latest/api/rclcpp/index.html)\n\n\n## MoveIt Tutorials Source Build\n\nFollow the [MoveIt Source Build](https://moveit.ros.org/install-moveit2/source/) instructions to set up a colcon workspace with MoveIt from the source.\n\nOpen a command line to your colcon workspace:\n\n    cd $COLCON_WS/src\n\nDownload the MoveIt Tutorials source code:\n\n    git clone https://github.com/moveit/moveit2_tutorials.git\n    vcs import < moveit2_tutorials/moveit2_tutorials.repos\n    rosdep install -r --from-paths . --ignore-src --rosdistro rolling -y\n\nConfigure and build the workspace:\n\n    cd $COLCON_WS\n    colcon build --event-handlers desktop_notification- status- --cmake-args -DCMAKE_BUILD_TYPE=Release\n\n## Build HTML Pages Locally\n\nIf you want to test the tutorials by generating the HTML pages locally on your machine, you can use the ``build_locally.sh`` script in this repo.\n\nIf you are using Ubuntu 24.04 (or Python 3.11 or greater), you should first create a virtual environment.\n\n    python3 -m venv .venv\n    source .venv/bin/activate\n\nThen, issue the following commands in the root of the ``moveit2_tutorials`` package:\n\n    cd $COLCON_WS/src/moveit2_tutorials\n\n    export ROS_DISTRO=rolling  # 24.04\n    source /opt/ros/$ROS_DISTRO/setup.bash\n\n    ./build_locally.sh\n\nThe local website ``<LOCAL_PACKAGE_PATH>/build/html/index.html`` should automatically open in your web browser.\n\n### Optional build_locally Settings\n\n - *noinstall* skip the dependencies install step to speed up the script\n - *loop* automatically rebuilds the HTML if a change is detected\n\n### Formatting and Style\n\nThese tutorials use the [reStructuredText](http://www.sphinx-doc.org/en/stable/rest.html) format commonly used in the Sphinx \"Python Documentation Generator\". This unfortunately differs from the common Markdown format, but its advantage is that it supports embedding code directly from source files for inline code tutorials.\n\n**Code Formatting**\n\n* These tutorials use the same [style guidelines](http://moveit.ros.org/documentation/contributing/code/) as the MoveIt project. When modifying or adding to these tutorials, it is required that code is auto-formatted using [clang format](http://moveit.ros.org/documentation/contributing/code/). To check and apply the style guidelines we use [pre-commit](https://pre-commit.com/).\n* Tutorials should exemplify best coding practices. If a contribution wouldn't pass review in the MoveIt project, then it shouldn't pass review in the tutorials.\n* Relevant code should be included and explained using the ``.. tutorial-formatter::`` tag.\n* Irrelevant code should be excluded from the generated HTML using the ``BEGIN_TUTORIAL``, ``END_TUTORIAL``, ``BEGIN_SUB_TUTORIAL``, and ``END_SUB_TUTORIAL`` tags.\n* Whenever possible, links should be created using the ``extlinks`` dictionary defined in ``conf.py``.\n* All demo code should be runnable from within the ``moveit2_tutorials`` package.\n* Python code should be run using ``ros2 run``.\n\n**Style**\n\n* Each tutorial should be focused on teaching the user one feature or interface within MoveIt.\n* Tutorials should flow from show to tell with videos and demos at the beginning followed by explanations.\n* New tutorials should match the formatting, style, and flow of existing tutorials whenever possible.\n\n**pre-commit**\n\npre-commit is a tool that is used in moveit2_tutorials to check and apply style guidelines automatically. To install pre-commit into your system:\n\n    pip3 install pre-commit\n\nThen under the moveit2_tutorials directory install the git hooks like this:\n\n    cd $COLCON_WS/src/moveit2_tutorials && pre-commit install\n\nWith this pre-commit will automatically run and check a list of styling including clang-format, end of files, and trailing whitespaces whenever you run `git commit`. To run pre-commit any time other than `git commit`:\n\n    cd $COLCON_WS/src/moveit2_tutorials && pre-commit run -a\n\n### Including Images and Videos\n#### Images\nThe standard way to include an image in reStructuredText is\n```\n.. image:: filename.png\n   :width: 700px\n```\n\nThis assumes that `filename.png` is in the same folder as the source `.rst` file. Images linked in this way will automatically be copied to the appropriate folder in the build.\n\n[External Documentation](https://sublime-and-sphinx-guide.readthedocs.io/en/latest/images.html)\n\nDo **not** include animated gifs as the file format leads to very large files. Use a video format like `webm` and see the section on the local video below.\n\n#### YouTube and other External Video\nYou can embed video with raw HTML, like in this example from the Pick and Place Tutorial.\n```\n.. raw:: html\n\n    <div style=\"position: relative; padding-bottom: 5%; height: 0; overflow: hidden; max-width: 100%; height: auto;\">\n        <iframe width=\"700px\" height=\"400px\" src=\"https://www.youtube.com/embed/QBJPxx_63Bs?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>\n    </div>\n```\nThis includes [YouTube's suggested embed HTML](https://support.google.com/youtube/answer/171780?hl=en).\n\n#### Local Video\nTo embed a video that is included in this repository, you also will use raw HTML, like this example from the Quickstart in RViz tutorial.\n\n```\n.. raw:: html\n\n    <video width=\"700px\" controls=\"true\" autoplay=\"true\" loop=\"true\">\n        <source src=\"../../../_static/videos/rviz_joints_nullspace.webm\" type=\"video/webm\">\n        The joints move while the end effector stays still\n    </video>\n```\n\nNote that the video file is in the `_static/videos` folder instead of the same folder.\n\n[External Documentation on &lt;video&gt; tag](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/video)\n\n## License\n\nAll content in this repository is open source and released under the [BSD License v3](https://opensource.org/licenses/BSD-3-Clause). Each source code file should contain a copy of the license.\n",
  "path": "README.md",
  "source": "github_fetching"
},
{
  "_id": {
    "$oid": "67563d9831a67c23afbee1f0"
  },
  "file_name": "README.md",
  "url": "https://raw.githubusercontent.com/moveit/moveit2_tutorials/main/.docker/README.md",
  "content": "# MoveIt Docker Containers\n\nFor more information see the pages [Continuous Integration and Docker](http://moveit.ros.org/documentation/contributing/continuous_integration.html) and [Using Docker Containers with MoveIt](https://moveit.picknik.ai/main/doc/how_to_guides/how_to_setup_docker_containers_in_ubuntu.html).\n\nTo build the Docker image locally, run the following from the root folder of this repository.\n\n    docker build -f .docker/Dockerfile -t moveit2_tutorials --build-arg ROS_DISTRO=${ROS_DISTRO}.\n\nwhere `${ROS_DISTRO}` should be available if you have a ROS installation sourced locally, else you can pick a target release, e.g., `humble` or `rolling`.\n",
  "path": ".docker/README.md",
  "source": "github_fetching"
},
{
  "_id": {
    "$oid": "67563d9831a67c23afbee1f1"
  },
  "file_name": "ISSUE_TEMPLATE.md",
  "url": "https://raw.githubusercontent.com/moveit/moveit2_tutorials/main/.github/ISSUE_TEMPLATE.md",
  "content": "### Description\n\nOverview of your issue here.\n\n### Your environment\n* ROS Distro: [Foxy|Galactic|Humble|Rolling]\n* OS Version: e.g. Ubuntu 22.04\n* Source or Binary build?\n* If binary, which release version?\n* If source, which git commit or tag?\n\n### Steps to reproduce\nTell us how to reproduce this issue. Attempt to provide a working demo, perhaps using Docker.\n\n### Expected behaviour\nTell us what should happen\n\n### Backtrace or Console output\n\nUse [gist.github.com](gist.github.com) to copy-paste the console output or segfault backtrace using gdb.\n",
  "path": ".github/ISSUE_TEMPLATE.md",
  "source": "github_fetching"
},
{
  "_id": {
    "$oid": "67563d9831a67c23afbee1f2"
  },
  "file_name": "PULL_REQUEST_TEMPLATE.md",
  "url": "https://raw.githubusercontent.com/moveit/moveit2_tutorials/main/.github/PULL_REQUEST_TEMPLATE.md",
  "content": "### Description\n\nPlease explain the changes you made, including a reference to the related issue if applicable\n\n### Checklist\n- [ ] **Required by CI**: Code is auto formatted using [clang-format](http://moveit.ros.org/documentation/contributing/code)\n- [ ] While waiting for someone to review your request, please consider reviewing [another open pull request](https://github.com/moveit/moveit2/pulls) to support the maintainers\n\n[//]: # \"You can expect a response from a maintainer within 7 days. If you haven't heard anything by then, feel free to ping the thread. Thank you!\"\n",
  "path": ".github/PULL_REQUEST_TEMPLATE.md",
  "source": "github_fetching"
}]